{"url": "https://en.wikipedia.org/wiki?curid=159501", "text": "Alpha–beta pruning\n\nAlpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.\n\nAllen Newell and Herbert A. Simon who used what John McCarthy calls an \"approximation\" in 1958 wrote that alpha–beta \"appears to have been reinvented a number of times\". Arthur Samuel had an early version and Richards, Hart, Levine and/or Edwards invented alpha–beta independently in the United States. McCarthy proposed similar ideas during the Dartmouth Conference in 1956 and suggested it to a group of his students including Alan Kotok at MIT in 1961. Alexander Brudno independently conceived the alpha–beta algorithm, publishing his results in 1963. Donald Knuth and Ronald W. Moore refined the algorithm in 1975. Judea Pearl proved its optimality for trees with randomly assigned leaf values in terms of the expected running time in two papers. The optimality of the randomized version of alpha-beta was shown by Michael Saks and Avi Wigderson in 1986.\n\nThe algorithm maintains two values, alpha and beta, which represent the minimum score that the maximizing player is assured of and the maximum score that the minimizing player is assured of respectively. Initially alpha is negative infinity and beta is positive infinity, i.e. both players start with their worst possible score. Whenever the maximum score that the minimizing player is assured of becomes less than the minimum score that the maximizing player is assured of (i.e. beta <= alpha), the maximising player need not consider the descendants of this node as they will never be reached in actual play.\n\nThe benefit of alpha–beta pruning lies in the fact that branches of the search tree can be eliminated. This way, the search time can be limited to the 'more promising' subtree, and a deeper search can be performed in the same time. Like its predecessor, it belongs to the branch and bound class of algorithms. The optimization reduces the effective depth to slightly more than half that of simple minimax if the nodes are evaluated in an optimal or near optimal order (best choice for side on move ordered first at each node).\n\nWith an (average or constant) branching factor of \"b\", and a search depth of \"d\" plies, the maximum number of leaf node positions evaluated (when the move ordering is pessimal) is \"O\"(\"b\"*\"b\"*...*\"b\") = \"O\"(\"b\") – the same as a simple minimax search. If the move ordering for the search is optimal (meaning the best moves are always searched first), the number of leaf node positions evaluated is about \"O\"(\"b\"*1*\"b\"*1*...*\"b\") for odd depth and \"O\"(\"b\"*1*\"b\"*1*...*1) for even depth, or formula_1. In the latter case, where the ply of a search is even, the effective branching factor is reduced to its square root, or, equivalently, the search can go twice as deep with the same amount of computation. The explanation of \"b\"*1*\"b\"*1*... is that all the first player's moves must be studied to find the best one, but for each, only the best second player's move is needed to refute all but the first (and best) first player move—alpha–beta ensures no other second player moves need be considered. When nodes are considered in a random order (i.e., the algorithm randomizes), asymptotically,\nthe expected number of nodes evaluated in uniform trees with binary leaf-values is formula_2\nFor the same trees, when the values are assigned to the leaf values independently of each other and say zero and one are both equally probable, the expected number of nodes evaluated is formula_3, which is much smaller than the work done by the randomized algorithm, mentioned above, and is again optimal for such random trees. When the leaf values are chosen independently of each other but from the formula_4 interval uniformly at random, the expected number of nodes evaluated increases to formula_5 in the formula_6 limit, which is again optimal for these kind random trees. Note that the actual work for \"small\" values of formula_7 is better approximated using formula_8.\nNormally during alpha–beta, the subtrees are temporarily dominated by either a first player advantage (when many first player moves are good, and at each search depth the first move checked by the first player is adequate, but all second player responses are required to try to find a refutation), or vice versa. This advantage can switch sides many times during the search if the move ordering is incorrect, each time leading to inefficiency. As the number of positions searched decreases exponentially each move nearer the current position, it is worth spending considerable effort on sorting early moves. An improved sort at any depth will exponentially reduce the total number of positions searched, but sorting all positions at depths near the root node is relatively cheap as there are so few of them. In practice, the move ordering is often determined by the results of earlier, smaller searches, such as through iterative deepening.\n\nAdditionally, this algorithm can be trivially modified to return an entire principal variation in addition to the score. Some more aggressive algorithms such as MTD(f) do not easily permit such a modification.\n\nThe pseudo-code for minimax with alpha-beta pruning is as follows:\n\nImplementations of alpha-beta pruning can often be delineated by whether they are \"fail-soft,\" or \"fail-hard.\" The pseudo-code illustrates the fail-soft variation. With fail-soft alpha-beta, the alphabeta function may return values (v) that exceed (v < α or v > β) the α and β bounds set by its function call arguments. In comparison, fail-hard alpha-beta limits its function return value into the inclusive range of α and β.\n\nFurther improvement can be achieved without sacrificing accuracy, by using ordering heuristics to search parts of the tree that are likely to force alpha–beta cutoffs early. For example, in chess, moves that capture pieces may be examined before moves that do not, or moves that have scored highly in earlier passes through the game-tree analysis may be evaluated before others. Another common, and very cheap, heuristic is the killer heuristic, where the last move that caused a beta-cutoff at the same level in the tree search is always examined first. This idea can also be generalized into a set of refutation tables.\n\nAlpha–beta search can be made even faster by considering only a narrow search window (generally determined by guesswork based on experience). This is known as \"aspiration search\". In the extreme case, the search is performed with alpha and beta equal; a technique known as \"zero-window search\", \"null-window search\", or \"scout search\". This is particularly useful for win/loss searches near the end of a game where the extra depth gained from the narrow window and a simple win/loss evaluation function may lead to a conclusive result. If an aspiration search fails, it is straightforward to detect whether it failed \"high\" (high edge of window was too low) or \"low\" (lower edge of window was too high). This gives information about what window values might be useful in a re-search of the position.\n\nOver time, other improvements have been suggested, and indeed the Falphabeta (fail-soft alpha-beta) idea of Fishburn is nearly universal and is already incorporated above in a slightly modified form. Fishburn also suggested a combination of the killer heuristic and zero-window search under the name Lalphabeta (\"last move with minimal window alpha-beta search\").\n\nSince the minimax algorithm and its variants are inherently depth-first, a strategy such as iterative deepening is usually used in conjunction with alpha–beta so that a reasonably good move can be returned even if the algorithm is interrupted before it has finished execution. Another advantage of using iterative deepening is that searches at shallower depths give move-ordering hints, as well as shallow alpha and beta estimates, that both can help produce cutoffs for higher depth searches much earlier than would otherwise be possible.\n\nAlgorithms like SSS*, on the other hand, use the best-first strategy. This can potentially make them more time-efficient, but typically at a heavy cost in space-efficiency.\n\n\n\n", "id": "159501", "title": "Alpha–beta pruning"}
{"url": "https://en.wikipedia.org/wiki?curid=1153192", "text": "Expectiminimax tree\n\nAn expectiminimax tree is a specialized variation of a minimax game tree for use in artificial intelligence systems that play two-player zero-sum games such as backgammon, in which the outcome depends on a combination of the player's skill and chance elements such as dice rolls. In addition to \"min\" and \"max\" nodes of the traditional minimax tree, this variant has \"chance\" (\"move by nature\") nodes, which take the expected value of a random event occurring. In game theory terms, an expectiminimax tree is the game tree of an extensive-form game of perfect, but incomplete information.\n\nIn the traditional minimax method, the levels of the tree alternate from max to min until the depth limit of the tree has been reached. In an expectiminimax tree, the \"chance\" nodes are interleaved with the max and min nodes. Instead of taking the max or min of the utility values of their children, chance nodes take a weighted average, with the weight being the probability that child is reached.\n\nThe interleaving depends on the game. Each \"turn\" of the game is evaluated as a \"max\" node (representing the AI player's turn), a \"min\" node (representing a potentially-optimal opponent's turn), or a \"chance\" node (representing a random effect or player).\n\nFor example, consider a game in which each round consists of a single dice throw, and then decisions made by first the AI player, and then another intelligent opponent. The order of nodes in this game would alternate between \"chance\", \"max\" and then \"min\".\n\nThe expectiminimax algorithm is a variant of the minimax algorithm and was first proposed by Donald Michie in 1966.\nIts pseudocode is given below. \n\nNote that for random nodes, there must be a known probability of reaching each child. (For most games of chance, child nodes will be equally-weighted, which means the return value can simply be the average of all child values.)\n\n", "id": "1153192", "title": "Expectiminimax tree"}
{"url": "https://en.wikipedia.org/wiki?curid=7724220", "text": "Quiescence search\n\nQuiescence search is an algorithm typically used to evaluate minimax game trees in game-playing computer programs. It is a remedy for the horizon problem faced by AI engines for various games like chess and Go.\n\nThe horizon effect is a problem in artificial intelligence where, in many games, the number of possible states or positions is immense and computers can only search a small portion of it, typically a few ply down the game tree. Thus, for a computer searching only five ply, there is a possibility that it could make a move that would prove to be detrimental later on (say, after six moves), but it cannot see the consequences because it cannot search far into the tree. Consider this chess position with black to move:\n\nHere White is down a pawn in material, and a good move for black would be 39... Qxg3+ 40. Kxg3 f5. However, Fritz chooses the suboptimal move 39... Bc2??. This move lets White force many of Black's moves, but Fritz doesn't care because it appears to be able to win more material along the way. White responds with 40. Qxh4 and Black resigns after 40. ... gxh4 41. Rc1 Rxb3? 42. Nxb3 Bxb3 43. a5 Nc4 44. b5 Ba4 45. bxa6 Bc6 46. a7 Kg7 47. a6 Ba8 48. Rb1.\n\nThis problem occurs because computers only search a certain number of moves ahead. Human players usually have enough intuition to decide whether to abandon a bad-looking move, or search a promising move to a great depth. A quiescence search attempts to emulate this behavior by instructing a computer to search \"interesting\" positions to a greater depth than \"quiet\" ones (hence its name) to make sure there are no hidden traps and, usually equivalently, to get a better estimate of its value.\n\nAny sensible criterion may be used to distinguish \"quiet\" moves from \"noisy\" moves; high activity (high movement on a chess board, extensive capturing in Go, for example) is commonly used for board games. As the main motive of quiescence search is usually to get a good value out of a poor evaluation function, it may also make sense to detect wide fluctuations in values returned by a simple heuristic evaluator over several ply. Modern chess engines may search certain moves up to 2 or 3 times deeper than the minimum. In highly \"unstable\" games like Go and reversi, a rather large proportion of computer time may be spent on quiescence searching.\n\nThis pseudocode illustrates the concept in an algorithmic manner:\n", "id": "7724220", "title": "Quiescence search"}
{"url": "https://en.wikipedia.org/wiki?curid=25681069", "text": "Project Milo\n\nProject Milo (also referred to as Milo and Kate) was a project in development by Lionhead Studios for the Xbox 360 video game console. Formerly a secretive project under the early code name \"Dimitri\", \"Project Milo\" was unveiled at the 2009 Electronic Entertainment Expo (E3) in a demonstration for Kinect, as a \"controller-free\" entertainment initiative for the Xbox 360 based on depth-sensing and pattern recognition technologies. The project was a tech demo to showcase the capabilities of Kinect and has not been released, despite conflicting reports that the project was an actual game.\n\nThe project began as work on an \"emotional AI (artificial intelligence)\" after Lionhead had finished work on \"Black & White\" in 2001. The project was code named Dimitri, after the godson of Lionhead creative director Peter Molyneux. Details revealed about the project led some to speculate that \"Dimitri\" had become \"Fable II\", but a 2006 interview with Molyneux confirmed that the projects were separate. For several years the development of Dimitri remained \"experimental\", resulting in scarce news updates during this phase of development. In later interviews, Molyneux began to refer to the project as \"Project X\".\n\nDuring their press briefing at the Electronic Entertainment Expo in June 2009, Lionhead's parent company Microsoft unveiled Kinect, then known as Project Natal, during which it featured a presentation clip from Molyneux demonstrating a woman naturally interacting with a virtual character, referred to as \"Milo.\" In an interview with Eurogamer after the press conference, Molyneux confirmed that the demonstration was of the previously-known \"Dimitri,\" and would be a game developed around Kinect, titled \"Milo and Kate\". In the game, players would interact with a 10-year-old child (Milo or Millie, selected at the start) and a dog named Kate, playing through a story. According to Molyneux, work on the Kinect-specific elements started in December 2008. The game would also feature an in-game store, for purchasing items to enhance gameplay.\n\n\"Milo\" has an AI structure that responds to human interactions, such as spoken word, gestures, or predefined actions in dynamic situations. The game relies on a procedural generation system which is constantly updating a built-in \"dictionary\" that is capable of matching key words in conversations with inherent voice-acting clips to simulate lifelike conversations. Molyneux claims that the technology for the game was developed while working on \"Fable\" and \"Black & White\".\n\nHowever, the game was not present at Microsoft's E3 press briefing the following year. Further confusion arose later in the month with a statement by Microsoft's Aaron Greenberg stating that the game was not a product they were planning to bring to market, but was more of an internal tech demo. This was later refuted by Molyneux who stated that he would reveal a more advanced version of \"Milo\" during his TEDGlobal talk in Oxford in July 2010. Molyneux went on to hint at difficulties in getting Microsoft to see Milo as a full game. Molyneux said \"The biggest challenge for us is convincing people (Microsoft) what we're doing is actually going to work, is going to reach a new audience, is going to be an idea that people love.\" At the TED conference in Oxford in July 2010, more footage was shown. Players can make crucial decisions in Milo's life, or smaller ones such as squashing a snail or not. During the conference it was shown that Milo can be taught how to skip stones. The demonstration also indicated that users are only able to talk to Milo when a red microphone image appeared on the screen.\n\nIn September 2010, Eurogamer ran a story, citing an unnamed source, stating that work on Milo had been halted, and that the Milo tech would be used in a \"Fable themed Kinect game\". This story was seemingly backed up by Microsoft's Alex Kipman in a November 2010 interview with Gamesindustry.biz, declaring that \"Project Milo\" \"was never a product\" and \"was never announced as a game\". However, an interview with the drama director of the game was released in March. It showed part of the creation process that he had to go through and some brief sections of gameplay. Completion of the project was also hinted in the interview.\n\nAt the 2011 Game Developer's Conference, Lionhead lead programmer Ben Sugden showcased a new graphics technology used in \"Project Milo\" for upcoming Xbox 360 titles. At E3 2011, \"\" was announced, which includes elements from \"Milo\", including voice and emotion recognition. In a May 2012 interview with Eurogamer, Lionhead creative director Gary Carr confirmed that a number of Kinect features from \"Project Milo\" had been implemented in \"\".\n\n", "id": "25681069", "title": "Project Milo"}
{"url": "https://en.wikipedia.org/wiki?curid=10809677", "text": "Arthur Samuel\n\nArthur Lee Samuel (December 5, 1901 – July 29, 1990) was an American pioneer in the field of computer gaming and artificial intelligence. He coined the term \"machine learning\" in 1959. The Samuel Checkers-playing Program appears to be the world's first self-learning program, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI). He was also a senior member in TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.\n\nSamuel was born on December 5, 1901 in Emporia, Kansas and graduated from College of Emporia in Kansas in 1923.\nHe received a master's degree in Electrical Engineering from MIT in 1926, and taught for two years as instructor. In 1928, he joined Bell Laboratories, where he worked mostly on vacuum tubes, including improvements of Radar during World War II. He developed a gas-discharge transmit-receive switch (TR tube) that allowed a single antenna to be used for both transmitting and receiving.\nAfter the war he moved to the University of Illinois at Urbana–Champaign, where he initiated the ILLIAC project, but left before its first computer was complete.\nSamuel went to IBM in Poughkeepsie, New York in 1949, where he would conceive and carry out his most successful work. He is credited with one of the first software hash tables, and influencing early research in using transistors for computers at IBM.\nAt IBM he made the first checkers program on IBM's first commercial computer, the IBM 701. The program was a sensational demonstration of the advances in both hardware and skilled programming and caused IBM's stock to increase 15 points overnight. His pioneering non-numerical programming helped shaped the instruction set of processors, as he was one of the first to work with computers on projects other than computation.\nHe was known for writing articles that made complex subjects easy to understand. He was chosen to write an introduction to one of the earliest journals devoted to computing in 1953.\n\nIn 1966, Samuel retired from IBM and became a professor at Stanford, where he worked the remainder of his life. He worked with Donald Knuth on the TeX project, including writing some of the documentation. He continued to write software past his 88th birthday.\nHe was given the Computer Pioneer Award by the IEEE Computer Society in 1987.\nHe died of complications from Parkinson's disease on July 29, 1990.\n\nSamuel is most known within the AI community for his groundbreaking work in computer checkers in 1959, and seminal research on machine learning, beginning in 1949. He graduated from MIT and taught at MIT and UIUC from 1946 to 1949. He believed teaching computers to play games was very fruitful for developing tactics appropriate to general problems, and he chose checkers as it is relatively simple though has a depth of strategy. The main driver of the machine was a search tree of the board positions reachable from the current state. Since he had only a very limited amount of available computer memory, Samuel implemented what is now called alpha-beta pruning.\nInstead of searching each path until it came to the game’s conclusion, Samuel developed a scoring function based on the position of the board at any given time. This function tried to measure the chance of winning for each side at the given position. It took into account such things as the number of pieces on each side, the number of kings, and the proximity of pieces to being “kinged”. The program chose its move based on a minimax strategy, meaning it made the move that optimized the value of this function, assuming that the opponent was trying to optimize the value of the same function from its point of view.\n\nSamuel also designed various mechanisms by which his program could become better. In what he called rote learning, the program remembered every position it had already seen, along with the terminal value of the reward function. This technique effectively extended the search depth at each of these positions. Samuel's later programs reevaluated the reward function based on input from professional games. He also had it play thousands of games against itself as another way of learning. With all of this work, Samuel’s program reached a respectable amateur status, and was the first to play any board game at this high a level. He continued to work on checkers until the mid-1970s, at which point his program achieved sufficient skill to challenge a respectable amateur.\n\n\n\n", "id": "10809677", "title": "Arthur Samuel"}
{"url": "https://en.wikipedia.org/wiki?curid=233463", "text": "Zero-player game\n\nA zero-player game or no-player game is a game that has no sentient players.\n\nIn computer games, the term refers to programs that use artificial intelligence rather than human players.\n\nConway's Game of Life, a cellular automaton devised in 1970 by the British mathematician John Horton Conway, is considered a zero-player game because its evolution is determined by its initial state, requiring no further input from humans. In addition, some fighting and real-time strategy games can be put into zero-player mode where one AI plays against another AI.\n\nThere are various different types of games that can be considered \"zero-player\".\n\nSome games, such as Conway's Game of Life, evolve according to fixed rules from their initial setup. Others such as Snakes and Ladders evolve according to chance, but similarly the players have no decisions to make and have no impact on how the game progresses.\n\nA more complex variation on the above is the case of artificial intelligences playing a game. Humans may have a challenge in designing the AI and giving it sufficient skill to play the game well, but the actual evolution of the game has no human intervention.\n\nFor solved games the optimum strategy for all players is known. Players can maximize their chances of winning by following these strategies, and any deviation would be sub-optimal play. Tic-tac-toe is a trivial example; two players who follow the easy-to-learn optimum strategy (rather than making sub-par decisions of their own) will always find their games end in a draw. More complex games have also been solved, for example checkers, but in this case learning the optimum strategy is beyond human capabilities. Solutions for even more complex games, such as chess or Go must exist (as per Zermelo's theorem) but they have yet to be computed.\n\n", "id": "233463", "title": "Zero-player game"}
{"url": "https://en.wikipedia.org/wiki?curid=207188", "text": "Combinatorial search\n\nIn computer science and artificial intelligence, combinatorial search studies search algorithms for solving instances of problems that are believed to be hard in general, by efficiently exploring the usually large solution space of these instances. Combinatorial search algorithms achieve this efficiency by reducing the effective size of the search space or employing heuristics. Some algorithms are guaranteed to find the optimal solution, while others may only return the best solution found in the part of the state space that was explored.\n\nClassic combinatorial search problems include solving the eight queens puzzle or evaluating moves in games with a large game tree, such as reversi or chess.\n\nA study of computational complexity theory helps to motivate combinatorial search. Combinatorial search algorithms are typically concerned with problems that are NP-hard. Such problems are not believed to be efficiently solvable in general. However, the various approximations of complexity theory suggest that some instances (e.g. \"small\" instances) of these problems could be efficiently solved. This is indeed the case, and such instances often have important practical ramifications.\n\nCommon algorithms for solving combinatorial search problems include:\n\nLookahead is an important component of combinatorial search, which specifies, roughly, how deeply the graph representing the problem is explored. The need for a specific limit on lookahead comes from the large problem graphs in many applications, such as computer chess and computer Go. A naive breadth-first search of these graphs would quickly consume all the memory of any modern computer. By setting a specific lookahead limit, the algorithm's time can be carefully controlled; its time increases exponentially as the lookahead limit increases.\n\nMore sophisticated search techniques such as alpha-beta pruning are able to eliminate entire subtrees of the search tree from consideration. When these techniques are used, lookahead is not a precisely defined quantity, but instead either the maximum depth searched or some type of average.\n\n\n", "id": "207188", "title": "Combinatorial search"}
{"url": "https://en.wikipedia.org/wiki?curid=396118", "text": "Progress Quest\n\nProgress Quest is a video game developed by Eric Fredricksen as a parody of \"EverQuest\" and other massively multiplayer online role-playing games. It is loosely considered a zero-player game, in the sense that once the player has set up their artificial character, there is no user interaction at all; the game \"plays\" itself, with the human player as spectator. The game's source code was released in 2011.\n\nOn starting a new game, the player is presented with a few options, such as the choice of race and character class for their player character. Stats are rolled and unrolled to determine Strength, Constitution, Dexterity, Intelligence, Wisdom, and Charisma. Players start off with subpar equipment, eventually earning better weapons, armor, and spells. Almost none of the above-mentioned character statistics and equipment have any effect on gameplay, however. The only exception is the Strength stat, which affects carrying capacity, indirectly influencing the speed of level gain.\n\nAfter character creation the game runs its course. The game displays the character's stats on the screen, including several progress bars representing how far the player character has advanced in the game. The lengthy, combat free prologue is represented by a set of progress bars, each accompanied by a line of text describing, among other things, the \"foreboding vision\" the character has. Then the first act begins, and the character is \"heading to the killing fields...\" where they will start the endless cycle of \"executing (\"number of monsters\") (\"adjective of monsters\") (\"monster type\")\" or \"executing a passing (\"player character\")\", only disrupted when their strength is no longer sufficient to carry more items. This forces their return to the market, where they will sell all the loot (each group of monsters drops one monster-specific item of loot, player characters will drop random magic loot) and then spend all their accumulated money to buy equipment. With each group of monsters \"executed\" the quest progress bar will advance one step; once it is filled it will be reset and the player awarded either with a magic item, a piece of equipment or a stats upgrade including the learning or leveling of a spell. After a fixed amount of time, the player will enter a new act, which has no effect whatsoever.\n\nAll equipment in the game is randomly given out as a reward for several instances in the game such as completing a quest, killing certain enemies, leveling a character, and especially when at the market, in which the character uses gold from the loot to upgrade his or her equipment. These items usually consist of two adjectives and a noun. Like the games \"Progress Quest\" mocks, these items have statistics and the adjectives are variables describing the item. The character level is used to determine what stats and adjectives each item has.\n\nPlayers may choose to create an account offline for their own enjoyment, or create an online account that allows their character stats to be saved on a ranking website. This allows characters to compare their accomplishments with others while trying to achieve a higher ranking on their respective realm. Players can also join guilds of Good, Neutral, or Evil alignment in certain realms, though there is no benefit for doing so.\n\nThe story tells of the history of Grumdrig and the creation of the realms. Currently, there are five realms: Knoram, Expodrine, Oobag, Spoltog, and Pemptus. The latter two are still open, but Knoram, Expodrine, and Oobag were permanently closed to the creation of new characters upon the arrival of Pemptus, which, being the newest realm, launched on February 8, 2007. The story mimics convoluted fantasy plots, using archaic and made up words such as \"aberdoxy\".\n\n\"Progress Quest\" parodies the stat-gathering aspect of role-playing video games, whereby the player advances their character by accumulating arbitrary statistic points. \"EverQuest\" and many other \"MMORPG\"s of its time are infamous for their \"auto-attack\" combat system, where players press a button to initiate combat mode and, from then on, have little to do other than watch; \"Progress Quest\" mocks this with its totally non-interactive gameplay after the initial character setup. \"Progress Quest\" also pokes fun at traditional RPG races, classes, stories, quests, items, and more.\n\nThe program was made available by Eric Fredricksen in early 2002. Fans quickly joined in on the parody by submitting numerous fake reviews to several popular freeware download sites and game review sites giving \"Progress Quest\" the highest scores possible.\n\nThe game has been upgraded several times throughout its history. One particular upgrade added shaded bars to the screen, which caused some players to refer to the new version as \"Progress Quest 3D\". On the \"game's\" official forums, a popular in-joke leads newbies to believe that there exist Silver, Gold, Platinum or even Diamond-encrusted DVD versions of the game which have enhanced 3D graphics. Fake \"Progress Quest\" screenshots from other games, obscure 3D RPGs, helped spread this rumour.\n\nOn May 20, 2011, Eric Fredricksen released the source code of the game on bitbucket.\n\nCritics have commented that, despite the automatic progress, \"Progress Quest\" was an enjoyable experience. Reviewer Nick Hide compared this with other MMORPGs in which players persevere through dull tasks, just to level up or obtain a new item, owing to an emotional attachment with the character.\n\n\"Progress Quest\" arguably represents the first example of a genre of casual game called the \"idle RPG\" or \"incremental game\", popular on certain web sites. These games generally feature a complex progress system in which progress is made automatically with the passage of time, but unlike \"Progress Quest\", there are often decisions to be made by the player in terms of allocating resources between stats that \"do\" affect the game in order that the idled time should be used as efficiently as possible.\n\nBethesda Game Studios executive producer Todd Howard stated \"Progress Quest\" influenced the \"Fallout\" mobile spin-off \"Fallout Shelter\".\n\n", "id": "396118", "title": "Progress Quest"}
{"url": "https://en.wikipedia.org/wiki?curid=452582", "text": "Arimaa\n\nArimaa () is a two-player strategy board game that was designed to be playable with a standard chess set and difficult for computers while still being easy to learn and fun to play for humans. Every year since 2004, the Arimaa community has held three tournaments: a World Championship (humans only), a Computer Championship (computers only), and the Arimaa Challenge (human vs. computer). In 2015, the challenge was won decisively by the computer (Sharp by David Wu), with top players agreeing that computers had become better at the game than humans. As it was a prerequisite for the prize to be awarded, Wu published a paper describing the algorithm and most of \"ICGA Journal\" Issue 38/1 was dedicated to this topic. The algorithm combined traditional alpha–beta pruning (changing sides every 4 ply) with heuristic functions manually written while analysing human expert games.\n\nArimaa was invented in 2003 by Omar Syed, an Indian-American computer engineer trained in artificial intelligence. Syed was inspired by Garry Kasparov's defeat at the hands of the chess computer Deep Blue to design a new game which could be played with a standard chess set, would be difficult for computers to play well, but would have rules simple enough for his then four-year-old son Aamir to understand. (\"Arimaa\" is \"Aamir\" spelled backwards plus an initial \"a\".)\n\nArimaa has won several awards including \"GAMES Magazine\" 2011 Best Abstract Strategy Game, \"Creative Child Magazine\" 2010 Strategy Game of the Year, and the 2010 Parents' Choice Approved Award. It has also been the subject of several research papers.\n\nArimaa is played on an 8×8 board with four trap squares. There are six kinds of pieces, ranging from elephant (strongest) to rabbit (weakest). Stronger pieces can push or pull weaker pieces, and stronger pieces freeze weaker pieces. Pieces can be captured by dislodging them onto a trap square when they have no orthogonally adjacent friendly pieces. There are three ways to win the game:\n\nThe two players, Gold and Silver, each control sixteen pieces. These are, in order from strongest to weakest: one elephant (), one camel (), two horses (), two dogs (), two cats (), and eight rabbits . These may be represented by the king, queen, rooks, bishops, knights, and pawns respectively when one plays using a chess set.\n\nThe object of the game is to move a rabbit of one's own color onto the home rank of the opponent. Thus Gold wins by moving a gold rabbit to the eighth rank, and Silver wins by moving a silver rabbit to the first rank. However, because it is difficult to usher a rabbit to the goal line while the board is full of pieces, an intermediate objective is to capture opposing pieces by pushing or pulling them into the trap squares.\n\nThe game begins with an empty board. Gold places the sixteen gold pieces in any configuration on the first and second ranks. Silver then places the sixteen silver pieces in any configuration on the seventh and eighth ranks. Diagram 1 shows one possible initial placement.\n\nAfter the pieces are placed on the board, the players alternate turns, starting with Gold. A turn consists of making one to four steps. With each step a piece may move into an unoccupied square one space left, right, forward, or backward, except that rabbits may not step backward. The steps of a turn may be made by a single piece or distributed among several pieces in any order.\n\nA turn must make a net change to the position. Thus one cannot, for example, take one step forward and one step back with the same piece, effectively passing the turn and evading zugzwang. Furthermore, one's turn may not create the same position with the same player to move as has been created twice before. This rule is similar to the situational super ko rule in the game of Go, which prevents endless loops, and is in contrast to chess where endless loops are considered draws. The prohibitions on passing and repetition make Arimaa a drawless game.\n\nThe second diagram, from the same game as the initial position above, helps illustrate the remaining rules of movement.\n\nA player may use two consecutive steps of a turn to dislodge an opposing piece with a stronger friendly piece which is adjacent in one of the four cardinal directions. For example, a player's dog may dislodge an opposing rabbit or cat, but not a dog, horse, camel, or elephant. The stronger piece may \"pull\" or \"push\" the adjacent weaker piece. When pulling, the stronger piece steps into an empty square, and the square it came from is occupied by the weaker piece. The silver elephant on d5 could step to d4 (or c5 or e5) and pull the gold horse from d6 to d5. When pushing, the weaker piece is moved to an adjacent empty square, and the square it came from is occupied by the stronger piece. The gold elephant on d3 could push the silver rabbit on d2 to e2 and then occupy d2. Note that the rabbit on d2 can't be pushed to d1, c2, or d3, because those squares are not empty.\n\nFriendly pieces may not be dislodged. Also, a piece may not push and pull simultaneously. For example, the gold elephant on d3 could not simultaneously push the silver rabbit on d2 to e2 and pull the silver rabbit from c3 to d3. An elephant can never be dislodged, since there is nothing stronger.\n\nA piece which is adjacent in any cardinal direction to a stronger opposing piece is \"frozen\", unless it is also adjacent to a friendly piece. Frozen pieces may not be moved by the owner, but may be dislodged by the opponent. A frozen piece can freeze another still weaker piece. The silver rabbit on a7 is frozen, but the one on d2 is able to move because it is adjacent to a silver piece. Similarly the gold rabbit on b7 is frozen, but the gold cat on c1 is not. The dogs on a6 and b6 do not freeze each other because they are of equal strength. An elephant cannot be frozen, since there is nothing stronger, but an elephant can be .\n\nA piece which enters a trap square is captured and removed from the game unless there is a friendly piece orthogonally adjacent. Silver could move to capture the gold horse on d6 by pushing it to c6 with the elephant on d5. A piece on a trap square is captured when all adjacent friendly pieces move away. Thus if the silver rabbit on c4 and the silver horse on c2 move away, voluntarily or by being dislodged, the silver rabbit on c3 will be captured.\n\nNote that a piece may voluntarily step into a trap square, even if it is thereby captured. Also, the second step of a pulling maneuver is completed even if the piece doing the pulling is captured on the first step. For example, Silver could step the silver rabbit from f4 to g4 (so that it will no longer support pieces at f3), and then step the silver horse from f2 to f3, which captures the horse; the horse's move could still pull the gold rabbit from f1 to f2.\n\nA player who loses every rabbit loses the game (by elimination). A player who cannot make a move at all also loses the game (by immobilization).\n\nFor beginning insights into good play, see the articles on and .\n\nKarl Juhnke, twice Arimaa world champion, has written a book titled \"Beginning Arimaa\" which gives an introduction to Arimaa tactics and strategies. Also Jean Daligault, six time Arimaa world champion, wrote \"Arimaa Strategies and Tactics\" which is geared towards those who have started playing Arimaa and want to improve their game.\n\nEach year since 2004 the Arimaa community has held a World Championship tournament. The tournament is played over the Internet and is open to everyone. The current world champion is Mathew Brown of the United States of America. Past world champion title holders are:\n\nEach year since 2004 the Arimaa community has held a World Computer Championship tournament. The tournament is played over the Internet and is open to everyone. The current champion is bot_sharp developed by David Wu of the USA. Past computer world champion title holders are:\n\nThe Arimaa Challenge was a cash prize of around $10,000 that was to have been available annually until 2020 for the first computer program to win the human-versus-computer Arimaa challenge. As part of the conditions of the prize, the computer program must run on standard, off-the-shelf hardware.\n\nThe Arimaa Challenge has been held twelve times, starting in 2004. Following the third match, Syed changed the format to require the software to win two out of three games against each of three players, to reduce the psychological pressure on individual volunteer defenders. Also Syed called for outside sponsorship of the Arimaa Challenge to build a bigger prize fund.\nIn the first five challenge cycles, David Fotland, programmer of Many Faces of Go, won the Arimaa Computer Championship and the right to play for the prize money, only to see his program beaten decisively each year. In 2009 Fotland's program was surpassed by several new programs in the same year, the strongest of which was Clueless by Jeff Bacher. Humanity's margin of dominance over computers appeared to widen each year from 2004 to 2008 as the best human players improved, but the 2009 Arimaa Challenge was more competitive. Clueless became the first bot to win two games of a Challenge match.\n\nIn 2010, Mattias Hultgren's bot Marwin edged out Clueless in the computer championship. In the Challenge match Marwin became the first bot to win two out of three games against a single human defender, and also the first bot to win three of the nine games overall. In 2011, however, Marwin won only one of the nine games, and that one having received a material handicap. In 2012 a new challenger, Briareus, became the first program to defeat a top-ten player, sweeping all three games from the fifth-ranked human. In 2013, however, the humans struck back against Marwin, with #4 and #6 each sweeping including a handicap win, and #31 winning two of three games. In 2014, the computer bounced back to win two games, albeit no matches.\n\nIn 2015, Sharp made a substantial leap in playing strength. After having scored 6-6 in twelve games against its top two computer rivals the previous year, Sharp went undefeated in the computer tournaments of 2015, including 13-0 against the second- and third-place finishers. Sharp dominated the pre-Challenge screening against human opponents, winning 27 of 29 games. In the Challenge itself, Sharp clinched victory in each of the three mini-matches by winning the first six games, finishing 7-2 overall and winning the Arimaa challenge.\n\n was filed on 3 October 2003, and granted on 3 January 2006. Omar Syed also holds a trademark on the name \"Arimaa\".\n\nSyed has stated that he does not intend to restrict noncommercial use and has released a license called \"The Arimaa Public License\" with the declared intent to \"make Arimaa as much of a public domain game as possible while still protecting its commercial usage\". Items covered by the license are the patent and the trademark.\n\n\n\n", "id": "452582", "title": "Arimaa"}
{"url": "https://en.wikipedia.org/wiki?curid=25191227", "text": "SMA*\n\nSMA* or Simplified Memory Bounded A* is a shortest path algorithm based on the A* algorithm. The main advantage of SMA* is that it uses a bounded memory, while the A* algorithm might need exponential memory. All other characteristics of SMA* are inherited from A*.\n\nLike A*, it expands the most promising branches according to the heuristic. What sets SMA* apart is that it prunes nodes whose expansion has revealed less promising than expected. The approach allows the algorithm to explore branches and backtrack to explore other branches.\n\nExpansion and pruning of nodes is driven by keeping two values of formula_1 for every node. Node formula_2 stores a value formula_3 which estimates the cost of reaching the goal by taking a path through that node. The lower the value, the higher the priority. As in A* this value is initialized to formula_4, but will then be updated to reflect changes to this estimate when its children are expanded. A fully expanded node will have an formula_1 value at least as high as that of its successors. In addition, the node stores the formula_1 value of the best forgotten successor. This value is restored if the forgotten successor is revealed to be the most promising successor.\n\nStarting with the first node, it maintains OPEN, ordered lexicographically by formula_1 and depth. When choosing a node to expand, it chooses the best according to that order. When selecting a node to prune, it chooses the worst.\n\nSMA* has the following properties\n\n\nThe implementation of SMA* is very similar to the one of A*, the only difference is that when there isn't any space left, nodes with the highest f-cost are pruned from the queue. Because those nodes are deleted, the SMA* also has to remember the f-cost of the best forgotten child with the parent node. When it seems that all explored paths are worse than such a forgotten path, the path is re-generated.\n\nPseudo code:\n", "id": "25191227", "title": "SMA*"}
{"url": "https://en.wikipedia.org/wiki?curid=14993828", "text": "B*\n\nIn computer science, B* (pronounced \"B star\") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals). First published by Hans Berliner in 1979, it is related to the A* search algorithm.\n\nThe algorithm stores intervals for nodes of the tree as opposed to single point-valued estimates. Then, leaf nodes of the tree can be searched until one of the top level nodes has an interval which is clearly \"best.\"\n\nLeaf nodes of a B*-tree are given evaluations that are intervals rather than single numbers. The interval is supposed to contain the true value of that node. If all intervals attached to leaf nodes satisfy this property, then B* will identify an optimal path to the goal state.\n\nTo back up the intervals within the tree, a parent's upper bound is set to the maximum of the upper bounds of the children. A parent's lower bound is set to the maximum of the lower bound of the children. Note that different children might supply these bounds.\n\nB* systematically expands nodes in order to create \"separation,\" which occurs when the lower bound of a direct child of the root is at least as large as the upper bound of any other direct child of the root. A tree that creates separation at the root contains a proof that the best child is at least as good as any other child.\n\nIn practice, complex searches might not terminate within practical resource limits. So the algorithm is normally augmented with artificial termination criteria such as time or memory limits. When an artificial limit is hit, then you must make a heuristic judgment about which move to select. Normally, the tree would supply you with extensive evidence, like the intervals of root nodes.\n\nB* is a best-first process, which means that the whole tree is kept in memory, and repeatedly descended to find a leaf to expand. This section describes how to choose the node to expand.\n\nAt the root of the tree, the algorithm applies one of two strategies, called prove-best and disprove-rest. In the prove-best strategy, the algorithm selects the node associated with the highest upper bound. The hope is that expanding that node will raise its lower bound higher than any other node's upper bound.\n\nThe disprove-rest strategy selects the child of the root that has the second-highest upper bound. The hope is that by expanding that node you might be able to reduce the upper bound to less than the lower bound of the best child.\n\nNote that applying the disprove-rest strategy is pointless until the lower bound of the child node that has the highest upper bound is the highest among all lower bounds.\n\nThe original algorithm description did not give any further guidance on which strategy to select. There are several reasonable alternatives, such as expanding the choice that has the smaller tree.\n\nOnce a child of the root has been selected (using prove-best or disprove-best) then the algorithm descends to a leaf node by repeatedly selecting the child that has the highest upper bound.\n\nWhen a leaf node is reached, the algorithm generates all successor nodes and assigns intervals to them using the evaluation function. Then the intervals of all nodes have to be backed up using the backup operation.\n\nWhen transpositions are possible, then the back-up operation might need to alter the values of nodes that did not lie on the selection path. In this case, the algorithm needs pointers from children to all parents so that changes can be propagated. Note that propagation can cease when a backup operation does not change the interval associated with a node.\n\nIf intervals are incorrect (in the sense that the game-theoretic value of the node is not contained within the interval), then B* might not be able to identify the correct path. However, the algorithm is fairly robust to errors in practice.\n\nThe Maven (Scrabble) program has an innovation that improves the robustness of B* when evaluation errors are possible. If a search terminates due to separation then Maven restarts the search after widening all of the evaluation intervals by a small amount. This policy progressively widens the tree, eventually erasing all errors.\n\nThe B* algorithm applies to two-player deterministic zero-sum games. In fact, the only change is to interpret \"best\" with respect to the side moving in that node. So you would take the maximum if your side is moving, and the minimum if the opponent is moving. Equivalently, you can represent all intervals from the perspective of the side to move, and then negate the values during the back-up operation.\n\nAndrew Palay applied B* to chess. Endpoint evaluations were assigned by performing null-move searches. There is no report of how well this system performed compared to alpha-beta pruning search engines running on the same hardware.\n\nThe Maven (Scrabble) program applied B* search to endgames. Endpoint evaluations were assigned using a heuristic planning system. \n\nThe B* search algorithm has been used to compute optimal strategy in a sum game of a set of combinatorial games.\n\n\n", "id": "14993828", "title": "B*"}
{"url": "https://en.wikipedia.org/wiki?curid=2256654", "text": "Iterative deepening A*\n\nIterative deepening A* (IDA*) is a graph traversal and path search algorithm that can find the shortest path between a designated start node and any member of a set of goal nodes in a weighted graph. It is a variant of iterative deepening depth-first search that borrows the idea to use a heuristic function to evaluate the remaining cost to get to the goal from the A* search algorithm. Since it is a depth-first search algorithm, its memory usage is lower than in A*, but unlike ordinary iterative deepening search, it concentrates on exploring the most promising nodes and thus does not go to the same depth everywhere in the search tree. Unlike A*, IDA* does not utilize dynamic programming and therefore often ends up exploring the same nodes many times.\n\nWhile the standard iterative deepening depth-first search uses search depth as the cutoff for each iteration, the IDA* uses the more informative formula_1, where formula_2 is the cost to travel from the root to node formula_3 and formula_4 is a problem-specific heuristic estimate of the cost to travel from formula_3 to the solution.\n\nThe algorithm was first described by Richard Korf in 1985.\n\nIterative-deepening-A* works as follows: at each iteration, perform a depth-first search, cutting off a branch when its total cost formula_1 exceeds a given \"threshold\". This threshold starts at the estimate of the cost at the initial state, and increases for each iteration of the algorithm. At each iteration, the threshold used for the next iteration is the minimum cost of all values that exceeded the current threshold.\n\nAs in A*, the heuristic has to have particular properties to guarantee optimality (shortest paths). See Properties below.\n\n path \"current search path (acts like a stack)\"\n\nLike A*, IDA* is guaranteed to find the shortest path leading from the given start node to any goal node in the problem graph, if the heuristic function is admissible, that is\n\nfor all nodes , where is the true cost of the shortest path from to the nearest goal (the \"perfect heuristic\").\n\nIDA* is beneficial when the problem is memory constrained. A* search keeps a large queue of unexplored nodes that can quickly fill up memory. By contrast, because IDA* does not remember any node except the ones on the current path, it requires an amount of memory that is only linear in the length of the solution that it constructs. Its time complexity is analyzed by Korf \"et al.\" under the assumption that the heuristic cost estimate is \"consistent\", meaning that\n\nfor all nodes and all neighbors of ; they conclude that compared to a brute-force tree search over an exponential-sized problem, IDA* achieves a smaller search depth (by a constant factor), but not a smaller branching factor.\n\nRecursive best-first search is another memory-constrained version of A* search that can be faster in practice than IDA*, since it requires less regenerating of nodes.\n\nApplications of IDA* are found in such problems as planning.\n", "id": "2256654", "title": "Iterative deepening A*"}
{"url": "https://en.wikipedia.org/wiki?curid=27164904", "text": "Expressive Intelligence Studio\n\nThe Expressive Intelligence Studio is a research group at the University of California, Santa Cruz, established to conduct research in the field of game design technology. The studio is currently being run by Michael Mateas, Noah Wardrip-Fruin, Arnav Jhala, and Jim Whitehead, who work closely with the students in their research.\n\nWhen Michael Mateas formed the group back in 2006, his goal was to \"stretch the students into being creative.\" He did not want the game design program at UCSC to be \"just about photo-realistic graphics.\" Instead he wanted to form an ambitious group of students that grabbed the attention of the gaming industry. According to Noah Wardrip-Fruin, a major benefit for establishing the Expressive Intelligence Studio at UCSC is its proximity to Silicon Valley, where many game companies are located. In 2011, UCSC was listed 7th on Princeton Review's list of top graduate game design programs.\n\nMost of the research done by the Expressive Intelligence Studio is done through major research projects, and focuses primarily on video game AI. PhD students in the program work closely with the advisors on these projects, which have a wide range of topics. Some of these topics include automated support for game generation, automatic generation of autonomous character conversations, story management, and authoring tools for interactive storytelling. This type of research in the field of game design is only being done in a small number of other institutions, such as Massachusetts Institute of Technology, University of Southern California, and Georgia Institute of Technology. According to Mateas, however, the program at the University of California, Santa Cruz Expressive Intelligence Studio strikes a balance between technology and design.\n\nThe studio has released several playable games based on research ideas, including Prom Week, the only university-produced finalist in the 2012 Independent Games Festival.\n\n\n", "id": "27164904", "title": "Expressive Intelligence Studio"}
{"url": "https://en.wikipedia.org/wiki?curid=46519807", "text": "General video game playing\n\nGeneral video game playing (GVGP) is the design of artificial intelligence programs to be able to play more than one video game successfully. In recent years, some progress have been made in this area, including programs that can learn to play Atari 2600 games as well as a program that can learn to play NES games.\n\nGVGP could potentially be used to create real video game AI automatically, as well as \"to test game environments, including those created automatically using procedural content generation and to find potential loopholes in the gameplay that a human player could exploit\". GVGP has also been used to generate game rules, and estimate a game's quality based on Relative Algorithm Performance Profiles (RAPP), which compare the skill differentiation that a game allows between good AI and bad AI.\n\nSince 2014, the General Video Game Playing Competition (GVGAI) has offered a way for researchers and practitioners to test and compare their best general video game playing algorithms. The competition has an associated software framework including a large number of games written in the Video Game Description Language (VGDL). VGDL can be used to describe a game specifically for procedural generation of levels, using Answer Set Programming (ASP) and an Evolutionary Algorithm (EA). GVGP can then be used to test the validity of procedural levels, as well as the difficulty or quality of levels based on how an agent performed.\n\nThe games used in GVGP are, for now, often 2 dimensional arcade games, as they are the simplest and easiest to quantify. To simplify the process of creating an AI that can interpret video games, games for this purpose are written in Video Game Description Language (VGDL), which is a coding language using simple semantics and commands that can easily be parsed.\n\n\n", "id": "46519807", "title": "General video game playing"}
{"url": "https://en.wikipedia.org/wiki?curid=68367", "text": "Computer chess\n\nComputer chess is a game of computer architecture encompassing hardware and software capable of playing chess autonomously without human guidance. Computer chess acts as solo entertainment (allowing players to practice and to better themselves when no sufficiently strong human opponents are available), as aids to chess analysis, for computer chess competitions, and as research to provide insights into human cognition.\n\nCurrent chess engines are able to defeat even the strongest human players under normal conditions. Whether computation could ever solve chess remains an open question.\n\nIn December 2017, DeepMind's AlphaZero beats AlphaGo Zero and other top chess and Shōgi programs after only 24 hours of play. After 4 hours of games AlphaZero acquired a superhuman level.\n\nChess-playing computers are now accessible to the average consumer. From the mid-1970s to the present day, dedicated chess computers have been available for purchase. There are many chess engines such as Stockfish, Crafty, Fruit and GNU Chess that can be downloaded from the Internet for free. These engines are able to play a game that, when run on an up-to-date personal computer, can defeat most master players under tournament conditions. Top programs such as the proprietary Shredder or Fritz or the open source program Stockfish have surpassed even world champion caliber players at blitz and short time controls. In October 2008 Rybka was rated top in the CCRL, CEGT, CSS, SSDF, and WBEC rating lists and has won many recent official computer chess tournaments such as CCT 8 and 9, the 2006 Dutch Open Computer Championship, the 16th IPCCC, and the 15th World Computer Chess Championship. As of 3 February 2016, Stockfish is the top rated chess program on the IPON rating list.\n\nCCRL (Computer Chess Rating Lists) is an organisation that tests computer chess engines' strength by playing the programs against each other. CCRL was founded in 2006 by Graham Banks, Ray Banks, Sarah Bird, Kirill Kryukov and Charles Smith, and as of June 2012 its members are Graham Banks, Ray Banks (who only participates in Chess960, or Fischer Random Chess), Shaun Brewer, Adam Hair, Aser Huerga, Kirill Kryukov, Denis Mendoza, Charles Smith and Gabor Szots.\n\nThe organisation runs three different lists: 40/40 (40 minutes for every 40 moves played), 40/4 (4 minutes for every 40 moves played), and 40/4 FRC (same time control but Chess960). Pondering (or permanent brain) is switched off and timing is adjusted to the AMD64 X2 4600+ (2.4 GHz) CPU by using Crafty 19.17 BH as a benchmark. Generic, neutral opening books are used (as opposed to the engine's own book) up to a limit of 12 moves into the game alongside 4 or 5 man tablebases.\n\nUsing \"ends-and-means\" heuristics a human chess player can intuitively determine optimal outcomes and how to achieve them regardless of the number of moves necessary, but a computer must be systematic in its analysis. Most players agree that looking at least five moves ahead (five plies) when necessary is required to play well. Normal tournament rules give each player an average of three minutes per move. On average there are more than 30 legal moves per chess position, so a computer must examine a quadrillion possibilities to look ahead ten plies (five full moves); one that could examine a million positions a second would require more than 30 years.\n\nAfter discovering refutation screening—the application of alpha-beta pruning to optimizing move evaluation—in 1957, a team at Carnegie Mellon University predicted that a computer would defeat the world human champion by 1967. It did not anticipate the difficulty of determining the right order to evaluate branches. Researchers worked to improve programs' ability to identify killer heuristics, unusually high-scoring moves to reexamine when evaluating other branches, but into the 1970s most top chess players believed that computers would not soon be able to play at a Master level. In 1968 International Master David Levy made a famous bet that no chess computer would be able to beat him within ten years, and in 1976 Senior Master and professor of psychology Eliot Hearst of Indiana University wrote that \"the only way a current computer program could ever win a single game against a master player would be for the master, perhaps in a drunken stupor while playing 50 games simultaneously, to commit some once-in-a-year blunder\".\n\nIn the late 1970s chess programs suddenly began defeating top human players. The year of Hearst's statement, Northwestern University's Chess 4.5 at the Paul Masson American Chess Championship's Class B level became the first to win a human tournament. Levy won his bet in 1978 by beating Chess 4.7, but it achieved the first computer victory against a Master-class player at the tournament level by winning one of the six games. In 1980 Belle began often defeating Masters. By 1982 two programs played at Master level and three were slightly weaker.\n\nThe sudden improvement without a theoretical breakthrough surprised humans, who did not expect that Belle's ability to examine 100,000 positions a second—about eight plies—would be sufficient. The Spracklens, creators of the successful microcomputer program \"Sargon\", estimated that 90% of the improvement came from faster evaluation speed and only 10% from improved evaluations. \"New Scientist\" stated in 1982 that computers \"play \"terrible\" chess ... clumsy, inefficient, diffuse, and just plain ugly\", but humans lost to them by making \"horrible blunders, astonishing lapses, incomprehensible oversights, gross miscalculations, and the like\" much more often than they realized; \"in short, computers win primarily through their ability to find and exploit miscalculations in human initiatives\".\n\nBy 1982, microcomputer chess programs could evaluate up to 1,500 moves a second and were as strong as mainframe chess programs of five years earlier, able to defeat almost all players. While only able to look ahead one or two plies more than at their debut in the mid-1970s, doing so improved their play more than experts expected; seemingly minor improvements \"appear to have allowed the crossing of a psychological threshold, after which a rich harvest of human error becomes accessible\", \"New Scientist\" wrote. While reviewing \"SPOC\" in 1984, \"BYTE\" wrote that \"Computers—mainframes, minis, and micros—tend to play ugly, inelegant chess\", but noted Robert Byrne's statement that \"tactically they are freer from error than the average human player\". The magazine described \"SPOC\" as a \"state-of-the-art chess program\" for the IBM PC with a \"surprisingly high\" level of play, and estimated its USCF rating as 1700 (Class B).\n\nAt the 1982 North American Computer Chess Championship, Monroe Newborn predicted that a chess program could become world champion within five years; tournament director and International Master Michael Valvo predicted ten years; the Spracklens predicted 15; Ken Thompson predicted more than 20; and others predicted that it would never happen. The most widely held opinion, however, stated that it would occur around the year 2000. In 1989, Levy was defeated by Deep Thought in an exhibition match. Deep Thought, however, was still considerably below World Championship Level, as the then reigning world champion Garry Kasparov demonstrated in two strong wins in 1989. It was not until a 1996 match with IBM's Deep Blue that Kasparov lost his first game to a computer at tournament time controls in Deep Blue - Kasparov, 1996, Game 1. This game was, in fact, the first time a reigning world champion had lost to a computer using regular time controls. However, Kasparov regrouped to win three and draw two of the remaining five games of the match, for a convincing victory.\n\nIn May 1997, an updated version of Deep Blue defeated Kasparov 3½–2½ in a return match. A documentary mainly about the confrontation was made in 2003, titled \"\". IBM keeps a web site of the event.\n\nWith increasing processing power and improved evaluation functions, chess programs running on commercially available workstations began to rival top flight players. In 1998, Rebel 10 defeated Viswanathan Anand, who at the time was ranked second in the world, by a score of 5–3. However most of those games were not played at normal time controls. Out of the eight games, four were blitz games (five minutes plus five seconds Fischer delay (see time control) for each move); these Rebel won 3–1. Two were semi-blitz games (fifteen minutes for each side) that Rebel won as well (1½–½). Finally, two games were played as regular tournament games (forty moves in two hours, one hour sudden death); here it was Anand who won ½–1½. In fast games, computers played better than humans, but at classical time controls – at which a player's rating is determined – the advantage was not so clear.\n\nIn the early 2000s, commercially available programs such as Junior and Fritz were able to draw matches against former world champion Garry Kasparov and classical world champion Vladimir Kramnik.\n\nIn October 2002, Vladimir Kramnik and Deep Fritz competed in the eight-game Brains in Bahrain match, which ended in a draw. Kramnik won games 2 and 3 by \"conventional\" anti-computer tactics – play conservatively for a long-term advantage the computer is not able to see in its game tree search. Fritz, however, won game 5 after a severe blunder by Kramnik. Game 6 was described by the tournament commentators as \"spectacular.\" Kramnik, in a better position in the early middlegame, tried a piece sacrifice to achieve a strong tactical attack, a strategy known to be highly risky against computers who are at their strongest defending against such attacks. True to form, Fritz found a watertight defense and Kramnik's attack petered out leaving him in a bad position. Kramnik resigned the game, believing the position lost. However, post-game human and computer analysis has shown that the Fritz program was unlikely to have been able to force a win and Kramnik effectively sacrificed a drawn position. The final two games were draws. Given the circumstances, most commentators still rate Kramnik the stronger player in the match.\n\nIn January 2003, Garry Kasparov played Junior, another chess computer program, in New York City. The match ended 3–3.\n\nIn November 2003, Garry Kasparov played X3D Fritz. The match ended 2–2.\n\nIn 2005, Hydra, a dedicated chess computer with custom hardware and sixty-four processors and also winner of the 14th IPCCC in 2005, defeated seventh-ranked Michael Adams 5½–½ in a six-game match (though Adams' preparation was far less thorough than Kramnik's for the 2002 series).\n\nIn November–December 2006, World Champion Vladimir Kramnik played Deep Fritz. This time the computer won; the match ended 2–4. Kramnik was able to view the computer's opening book. In the first five games Kramnik steered the game into a typical \"anti-computer\" positional contest. He lost one game (overlooking a mate in one), and drew the next four. In the final game, in an attempt to draw the match, Kramnik played the more aggressive Sicilian Defence and was crushed.\n\nThere was speculation that interest in human-computer chess competition would plummet as a result of the 2006 Kramnik-Deep Fritz match. According to Newborn, for example, \"the science is done\".\n\nHuman-computer chess matches showed the best computer systems overtaking human chess champions in the late 1990s. For the 40 years prior to that, the trend had been that the best machines gained about 40 points per year in the Elo rating while the best humans only gained roughly 2 points per year. The highest rating obtained by a computer in human competition was Deep Thought's USCF rating of 2551 in 1988 and FIDE no longer accepts human-computer results in their rating lists. Specialized machine-only Elo pools have been created for rating machines, but such numbers, while similar in appearance, should not be directly compared. In 2016, the Swedish Chess Computer Association rated computer program Komodo at 3361.\n\nChess engines continue to improve. In 2009, chess engines running on slower hardware have reached the grandmaster level. A mobile phone won a category 6 tournament with a performance rating 2898: chess engine Hiarcs 13 running inside Pocket Fritz 4 on the mobile phone HTC Touch HD won the Copa Mercosur tournament in Buenos Aires, Argentina with 9 wins and 1 draw on August 4–14, 2009. Pocket Fritz 4 searches fewer than 20,000 positions per second. This is in contrast to supercomputers such as Deep Blue that searched 200 million positions per second.\n\nAdvanced Chess is a form of chess developed in 1998 by Kasparov where a human plays against another human, and both have access to computers to enhance their strength. The resulting \"advanced\" player was argued by Kasparov to be stronger than a human or computer alone, although this has not been proven.\n\nPlayers today are inclined to treat chess engines as analysis tools rather than opponents.\n\nThe developers of a chess-playing computer system must decide on a number of fundamental implementation issues. These include:\n\nComputer chess programs usually support a number of common \"de facto\" standards. Nearly all of today's programs can read and write game moves as Portable Game Notation (PGN), and can read and write individual positions as Forsyth–Edwards Notation (FEN). Older chess programs often only understood long algebraic notation, but today users expect chess programs to understand standard algebraic chess notation.\n\nMost computer chess programs are divided into an \"engine\" (which computes the best move given a current position) and a \"user interface\". Most engines are separate programs from the user interface, and the two parts communicate to each other using a public communication protocol. The most popular protocol is the Chess Engine Communication Protocol (CECP). Another open alternate chess communication protocol is the Universal Chess Interface (UCI). By dividing chess programs into these two pieces, developers can write only the user interface, or only the engine, without needing to write both parts of the program. (See also List of chess engines.)\n\nImplementers also need to decide if they will use endgame databases or other optimizations, and often implement common \"de facto\" chess standards.\n\nThe data structure used to represent each chess position is key to the performance of move generation and position evaluation. Methods include pieces stored in an array (\"mailbox\" and \"0x88\"), piece positions stored in a list (\"piece list\"), collections of bit-sets for piece locations (\"bitboards\"), and huffman coded positions for compact long-term storage.\n\nThe first paper on the subject was by Claude Shannon in 1950. He predicted the two main possible search strategies which would be used, which he labeled \"Type A\" and \"Type B\", before anyone had programmed a computer to play chess.\n\nType A programs would use a \"brute force\" approach, examining every possible position for a fixed number of moves using the minimax algorithm. Shannon believed this would be impractical for two reasons.\n\nFirst, with approximately thirty moves possible in a typical real-life position, he expected that searching the approximately 10 positions involved in looking three moves ahead for both sides (six plies) would take about sixteen minutes, even in the \"very optimistic\" case that the chess computer evaluated a million positions every second. (It took about forty years to achieve this speed.)\n\nSecond, it ignored the problem of quiescence, trying to only evaluate a position that is at the end of an exchange of pieces or other important sequence of moves ('lines'). He expected that adapting type A to cope with this would greatly increase the number of positions needing to be looked at and slow the program down still further.\n\nInstead of wasting processing power examining bad or trivial moves, Shannon suggested that \"type B\" programs would use two improvements:\nThis would enable them to look further ahead ('deeper') at the most significant lines in a reasonable time. The test of time has borne out the first approach; all modern programs employ a terminal quiescence search before evaluating positions. The second approach (now called \"forward pruning\") has been dropped in favor of search extensions.\n\nAdriaan de Groot interviewed a number of chess players of varying strengths, and concluded that both masters and beginners look at around forty to fifty positions before deciding which move to play. What makes the former much better players is that they use pattern recognition skills built from experience. This enables them to examine some lines in much greater depth than others by simply not considering moves they can assume to be poor.\n\nMore evidence for this being the case is the way that good human players find it much easier to recall positions from genuine chess games, breaking them down into a small number of recognizable sub-positions, rather than completely random arrangements of the same pieces. In contrast, poor players have the same level of recall for both.\n\nThe problem with type B is that it relies on the program being able to decide which moves are good enough to be worthy of consideration ('plausible') in any given position and this proved to be a much harder problem to solve than speeding up type A searches with superior hardware and search extension techniques.\n\nOne of the few chess grandmasters to devote himself seriously to computer chess was former World Chess Champion Mikhail Botvinnik, who wrote several works on the subject. He also held a doctorate in electrical engineering. Working with relatively primitive hardware available in the Soviet Union in the early 1960s, Botvinnik had no choice but to investigate software move selection techniques; at the time only the most powerful computers could achieve much beyond a three-ply full-width search, and Botvinnik had no such machines. In 1965 Botvinnik was a consultant to the ITEP team in a US-Soviet computer chess match (see Kotok-McCarthy).\n\nOne developmental milestone occurred when the team from Northwestern University, which was responsible for the Chess series of programs and won the first three ACM Computer Chess Championships (1970–72), abandoned type B searching in 1973. The resulting program, Chess 4.0, won that year's championship and its successors went on to come in second in both the 1974 ACM Championship and that year's inaugural World Computer Chess Championship, before winning the ACM Championship again in 1975, 1976 and 1977.\n\nOne reason they gave for the switch was that they found it less stressful during competition, because it was difficult to anticipate which moves their type B programs would play, and why. They also reported that type A was much easier to debug in the four months they had available and turned out to be just as fast: in the time it used to take to decide which moves were worthy of being searched, it was possible just to search all of them.\n\nIn fact, Chess 4.0 set the paradigm that was and still is followed essentially by all modern Chess programs today. Chess 4.0 type programs won out for the simple reason that their programs played better chess. Such programs did not try to mimic human thought processes, but relied on full width alpha-beta and negascout searches. Most such programs (including all modern programs today) also included a fairly limited \"selective\" part of the search based on quiescence searches, and usually extensions and pruning (particularly null move pruning from the 1990s onwards) which were triggered based on certain conditions in an attempt to weed out or reduce obviously bad moves (history moves) or to investigate interesting nodes (e.g. check extensions, passed pawns on seventh rank, etc.). Extension and pruning triggers have to be used very carefully however. Over extend and the program wastes too much time looking at uninteresting positions. If too much is pruned, there is a risk cutting out interesting nodes. Chess programs differ in terms of how and what types of pruning and extension rules are included as well as in the evaluation function. Some programs are believed to be more selective than others (for example Deep Blue was known to be less selective than most commercial programs because they could afford to do more complete full width searches), but all have a base full width search as a foundation and all have some selective components (Q-search, pruning/extensions).\n\nThough such additions meant that the program did not truly examine every node within its search depth (so it would not be truly brute force in that sense), the rare mistakes due to these selective searches was found to be worth the extra time it saved because it could search deeper. In that way Chess programs can get the best of both worlds.\n\nFurthermore, technological advances by orders of magnitude in processing power have made the brute force approach far more incisive than was the case in the early years. The result is that a very solid, tactical AI player aided by some limited positional knowledge built in by the evaluation function and pruning/extension rules began to match the best players in the world. It turned out to produce excellent results, at least in the field of chess, to let computers do what they do best (calculate) rather than coax them into imitating human thought processes and knowledge. In 1997 Deep Blue defeated World Champion Garry Kasparov, marking the first time a computer has defeated a reigning world chess champion in standard time control.\n\nComputer chess programs consider chess moves as a game tree. In theory, they examine all moves, then all counter-moves to those moves, then all moves countering them, and so on, where each individual move by one player is called a \"ply\". This evaluation continues until a certain maximum search depth or the program determines that a final \"leaf\" position has been reached (e.g. checkmate).\n\nA naive implementation of this approach can only search to a small depth in a practical amount of time, so various methods have been devised to greatly speed the search for good moves.\n\nFor more information, see:\n\n\nFor most chess positions, computers cannot look ahead to all possible final positions. Instead, they must look ahead a few plies and compare the possible positions, known as leaves. The algorithm that evaluates leaves is termed the \"evaluation function\", and these algorithms are often vastly different between different chess programs.\n\nEvaluation functions typically evaluate positions in hundredths of a pawn (called a centipawn), and consider material value along with other factors affecting the strength of each side. When counting up the material for each side, typical values for pieces are 1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen. (See Chess piece relative value.) The king is sometimes given an arbitrary high value such as 200 points (Shannon's paper) or 1,000,000,000 points (1961 USSR program) to ensure that a checkmate outweighs all other factors . By convention, a positive evaluation favors White, and a negative evaluation favors Black.\n\nIn addition to points for pieces, most evaluation functions take many factors into account, such as pawn structure, the fact that a pair of bishops are usually worth more, centralized pieces are worth more, and so on. The protection of kings is usually considered, as well as the phase of the game (opening, middle or endgame).\n\nEndgame play had long been one of the great weaknesses of chess programs, because of the depth of search needed. Some otherwise master-level programs were unable to win in positions where even intermediate human players can force a win.\n\nTo solve this problem, computers have been used to analyze some chess endgame positions completely, starting with king and pawn against king. Such endgame tablebases are generated in advance using a form of retrograde analysis, starting with positions where the final result is known (e.g., where one side has been mated) and seeing which other positions are one move away from them, then which are one move from those, etc. Ken Thompson was a pioneer in this area.\n\nThe results of the computer analysis sometimes surprised people. In 1977 Thompson's Belle chess machine used the endgame tablebase for a king and rook against king and queen and was able to draw that theoretically lost ending against several masters (see Philidor position#Queen versus rook). This was despite not following the usual strategy to delay defeat by keeping the defending king and rook close together for as long as possible. Asked to explain the reasons behind some of the program's moves, Thompson was unable to do so beyond saying the program's database simply returned the best moves.\n\nMost grandmasters declined to play against the computer in the queen versus rook endgame, but Walter Browne accepted the challenge. A queen versus rook position was set up in which the queen can win in thirty moves, with perfect play. Browne was allowed 2½ hours to play fifty moves, otherwise a draw would be claimed under the fifty-move rule. After forty-five moves, Browne agreed to a draw, being unable to force checkmate or win the rook within the next five moves. In the final position, Browne was still seventeen moves away from checkmate, but not quite that far away from winning the rook. Browne studied the endgame, and played the computer again a week later in a different position in which the queen can win in thirty moves. This time, he captured the rook on the fiftieth move, giving him a winning position , .\n\nOther positions, long believed to be won, turned out to take more moves against perfect play to actually win than were allowed by chess's fifty-move rule. As a consequence, for some years the official FIDE rules of chess were changed to extend the number of moves allowed in these endings. After a while, the rule reverted to fifty moves in all positions — more such positions were discovered, complicating the rule still further, and it made no difference in human play, as they could not play the positions perfectly.\n\nOver the years, other endgame database formats have been released including the Edward Tablebase, the De Koning Database and the Nalimov Tablebase which is used by many chess programs such as Rybka, Shredder and Fritz. Tablebases for all positions with six pieces are available. Some seven-piece endgames have been analyzed by Marc Bourzutschky and Yakov Konoval. Programmers using the Lomonosov supercomputers in Moscow have completed a chess tablebase for all endgames with seven pieces or fewer (trivial endgame positions are excluded, such as six white pieces versus a lone black king). In all of these endgame databases it is assumed that castling is no longer possible.\n\nMany tablebases do not consider the fifty-move rule, under which a game where fifty moves pass without a capture or pawn move can be claimed to be a draw by either player. This results in the tablebase returning results such as \"Forced mate in sixty-six moves\" in some positions which would actually be drawn because of the fifty-move rule. One reason for this is that if the rules of chess were to be changed once more, giving more time to win such positions, it will not be necessary to regenerate all the tablebases. It is also very easy for the program using the tablebases to notice and take account of this 'feature' and in any case if using an endgame tablebase will choose the move that leads to the quickest win (even if it would fall foul of the fifty-move rule with perfect play). If playing an opponent not using a tablebase, such a choice will give good chances of winning within fifty moves.\n\nThe Nalimov tablebases, which use state-of-the-art compression techniques, require 7.05 GB of hard disk space for all five-piece endings. To cover all the six-piece endings requires approximately 1.2 TB. It is estimated that a seven-piece tablebase requires between 50 and 200 TB of storage space.\n\nEndgame databases featured prominently in 1999, when Kasparov played an exhibition match on the Internet against the rest of the world. A seven piece Queen and pawn endgame was reached with the World Team fighting to salvage a draw. Eugene Nalimov helped by generating the six piece ending tablebase where both sides had two Queens which was used heavily to aid analysis by both sides.\n\nMany other optimizations can be used to make chess-playing programs stronger. For example, transposition tables are used to record positions that have been previously evaluated, to save recalculation of them. Refutation tables record key moves that \"refute\" what appears to be a good move; these are typically tried first in variant positions (since a move that refutes one position is likely to refute another). Opening books aid computer programs by giving common openings that are considered good play (and good ways to counter poor openings). Many chess engines use pondering to increase their strength.\n\nOf course, faster hardware and additional processors can improve chess-playing program abilities, and some systems (such as Deep Blue) use specialized chess hardware instead of only software. Another way to examine more chess positions is to distribute the analysis of positions to many computers. The ChessBrain project was a chess program that distributed the search tree computation through the Internet. In 2004 the ChessBrain played chess using 2,070 computers.\n\nIt has been estimated that doubling the computer speed gains approximately fifty to seventy Elo points in playing strength .\n\nChess engines have been developed to play some chess variants such as Capablanca chess, but the engines are almost never directly integrated with specific hardware. Even for the software that has been developed, most will not play chess beyond a certain board size, so games played on an unbounded chessboard (infinite chess) remain virtually untouched by both chess computers and software.\n\nThere are several other forms of chess-related computer software, including the following:\n\nWell-known computer chess theorists include:\n\nThe prospects of completely solving chess are generally considered to be rather remote. It is widely conjectured that there is no computationally inexpensive method to solve chess even in the very weak sense of determining with certainty the value of the initial position, and hence the idea of solving chess in the stronger sense of obtaining a practically usable description of a strategy for perfect play for either side seems unrealistic today. However, it has not been proven that no computationally cheap way of determining the best move in a chess position exists, nor even that a traditional alpha-beta-searcher running on present-day computing hardware could not solve the initial position in an acceptable amount of time. The difficulty in proving the latter lies in the fact that, while the number of board positions that could happen in the course of a chess game is huge (on the order of at least 10 to 10), it is hard to rule out with mathematical certainty the possibility that the initial position allows either side to force a mate or a threefold repetition after relatively few moves, in which case the search tree might encompass only a very small subset of the set of possible positions. It has been mathematically proven that \"generalized chess\" (chess played with an arbitrarily large number of pieces on an arbitrarily large chessboard) is EXPTIME-complete, meaning that determining the winning side in an arbitrary position of generalized chess provably takes exponential time in the worst case; however, this theoretical result gives no lower bound on the amount of work required to solve ordinary 8x8 chess.\n\nGardner's Minichess, played on a 5×5 board with approximately 10 possible board positions, has been solved; its game-theoretic value is 1/2 (i.e. a draw can be forced by either side), and the forcing strategy to achieve that result has been described.\n\nProgress has also been made from the other side: as of 2012, all 7 and fewer piece (2 kings and up to 5 other pieces) endgames have been solved.\n\nThe idea of creating a chess-playing machine dates back to the eighteenth century. Around 1769, the chess playing automaton called The Turk became famous before being exposed as a hoax. Before the development of digital computing, serious trials based on automata such as El Ajedrecista of 1912, were too complex and limited to be useful for playing full games of chess. The field of mechanical chess research languished until the advent of the digital computer in the 1950s. Since then, chess enthusiasts and computer engineers have built, with increasing degrees of seriousness and success, chess-playing machines and computer programs.\n\n\n\"Chess engine\" normally refers to the algorithmic part of a chess program or machine. The user interface part is often a separate\nprogram, which the chess engine plugs into as a substitutable or replaceable module. Chess engines may consist of a software chess program running on a conventional digital computer, or a software program running on a conventional computer with dedicated chess-specific microprogramming or hardware to speed esp. tree searching and position evaluation, or a hardware/software/firmware machine dedicated exclusively to playing chess. Distributed computing programs that play chess utilizing multiple processors or multiple networked machines have also been developed. Most generally available commercial chess programs are software that runs on PC-type hardware. Such programs running on even very modest hardware (as small as cellphones) are vastly stronger than most human chess players.\n\n\n\n\nMedia\n", "id": "68367", "title": "Computer chess"}
{"url": "https://en.wikipedia.org/wiki?curid=42146944", "text": "Jump point search\n\nIn computer science, Jump Point Search (JPS) is an optimization to the A* search algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long \"jumps\" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.\n\nJump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude.\n\nHarabor and Grastien's original publication provides algorithms for neighbour pruning and identifying successors. The original algorithm for neighbour pruning allowed corner-cutting to occur, which meant the algorithm could only be used for moving agents with zero width; limiting its application to either real-life agents (e.g. robotics) or simulations (e.g. many games).\n\nThe authors presented modified pruning rules for applications where corner-cutting is not allowed the following year. This paper also presents an algorithm for pre-processing a grid in order to minimise online search times.\n\nA number of further optimisations were published by the authors in 2014. These optimizations include exploring columns or rows of nodes instead of individual nodes, pre-computing \"jumps\" on the grid, and stronger pruning rules.\n\nAlthough jump point search is limited to uniform cost grids and homogeneously sized agents, the authors are placing future research into applying JPS with existing grid-based speed-up techniques such as hierarchical grids.\n", "id": "42146944", "title": "Jump point search"}
{"url": "https://en.wikipedia.org/wiki?curid=1654769", "text": "Artificial intelligence (video games)\n\nIn video games, artificial intelligence is used to generate responsive, adaptive or intelligent behaviors primarily in non-player characters (NPCs), similar to human-like intelligence. The techniques used typically draw upon existing methods from the field of artificial intelligence (AI).\n\nThe term game AI is used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general, and so video game AI may often not constitute \"true AI\" in that such techniques due not facilitate computer learning or other standard criteria, only constituting \"automated computation,\" or a predetermined and limited set of responses to a predetermined and limited set of inputs.\n\nMany industry and corporate voices claim that so-called video game AI has come a long way in the sense that it has revolutionized the way humans interact with all forms of technology, although many expert researchers are skeptical of such claims, and particularly of the notion that such technologies fit the definition of \"intelligence\" standardly used in the cognitive sciences. Industry voices make the argument that AI has become more versatile in the way we use all technological devices for more than their intended purpose because the AI allows the technology to operate in multiple ways, allegedly developing their own personalities and carrying out complex instructions of the user.\n\nHowever, many in the field of AI have argued that video game AI is not true intelligence, but an advertising buzzword used to describe computer programs that use simple sorting and matching algorithms to create the illusion of intelligent behavior while bestowing software with a misleading aura of scientific or technological complexity and advancement. Since game AI for NPCs is centered on appearance of intelligence and good gameplay within environment restrictions, its approach is very different from that of traditional AI.\n\nGame playing was an area of research in AI from its inception. One of the first examples of AI is the computerised game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took the form of a relatively small box and was able to regularly win games even against highly skilled players of the game. In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. These were among the first computer programs ever written. Arthur Samuel's checkers program, developed in the middle 50s and early 60s, eventually achieved sufficient skill to challenge a respectable amateur. Work on checkers and chess would culminate in the defeat of Garry Kasparov by IBM's Deep Blue computer in 1997. The first video games developed in the 1960s and early 1970s, like \"Spacewar!\", \"Pong\", and \"Gotcha\" (1973), were games implemented on discrete logic and strictly based on the competition of two players, without AI.\n\nGames that featured a single player mode with enemies started appearing in the 1970s. The first notable ones for the arcade appeared in 1974: the Taito game \"Speed Race\" (racing video game) and the Atari games \"Qwak\" (duck hunting light gun shooter) and \"Pursuit\" (fighter aircraft dogfighting simulator). Two text-based computer games from 1972, \"Hunt the Wumpus\" and \"Star Trek\", also had enemies. Enemy movement was based on stored patterns. The incorporation of microprocessors would allow more computation and random elements overlaid into movement patterns.\n\nIt was during the golden age of video arcade games that the idea of AI opponents was largely popularized, due to the success of \"Space Invaders\" (1978), which sported an increasing difficulty level, distinct movement patterns, and in-game events dependent on hash functions based on the player's input. \"Galaxian\" (1979) added more complex and varied enemy movements, including maneuvers by individual enemies who break out of formation. \"Pac-Man\" (1980) introduced AI patterns to maze games, with the added quirk of different personalities for each enemy. \"Karate Champ\" (1984) later introduced AI patterns to fighting games, although the poor AI prompted the release of a second version. \"First Queen\" (1988) was a tactical action RPG which featured characters that can be controlled by the computer's AI in following the leader. The role-playing video game \"Dragon Quest IV\" (1990) introduced a \"Tactics\" system, where the user can adjust the AI routines of non-player characters during battle, a concept later introduced to the action role-playing game genre by \"Secret of Mana\" (1993).\n\nGames like \"Madden Football\", \"Earl Weaver Baseball\" and \"Tony La Russa Baseball\" all based their AI on an attempt to duplicate on the computer the coaching or managerial style of the selected celebrity. Madden, Weaver and La Russa all did extensive work with these game development teams to maximize the accuracy of the games. Later sports titles allowed users to \"tune\" variables in the AI to produce a player-defined managerial or coaching strategy.\n\nThe emergence of new game genres in the 1990s prompted the use of formal AI tools like finite state machines. Real-time strategy games taxed the AI with many objects, incomplete information, pathfinding problems, real-time decisions and economic planning, among other things. The first games of the genre had notorious problems. \"Herzog Zwei\" (1989), for example, had almost broken pathfinding and very basic three-state state machines for unit control, and \"Dune II\" (1992) attacked the players' base in a beeline and used numerous cheats. Later games in the genre exhibited more sophisticated AI.\n\nLater games have used bottom-up AI methods, such as the emergent behaviour and evaluation of player actions in games like \"Creatures\" or \"Black & White\". Façade (interactive story) was released in 2005 and used interactive multiple way dialogs and AI as the main aspect of game.\n\nGames have provided an environment for developing artificial intelligence with potential applications beyond gameplay. Examples include Watson, a Jeopardy!-playing computer; and the RoboCup tournament, where robots are trained to compete in soccer.\n\nMany experts complain that the \"AI\" in the term \"game AI\" overstates its worth, as game AI is not about intelligence, and shares few of the objectives of the academic field of AI. Whereas \"real\" AI addresses fields of machine learning, decision making based on arbitrary data input, and even the ultimate goal of strong AI that can reason, \"game AI\" often consists of a half-dozen rules of thumb, or heuristics, that are just enough to give a good gameplay experience. Historically, academic game-AI projects have been relatively separate from commercial products because the academic approaches tended to be simple and non-scalable. Commercial game AI has developed its own set of tools, which have been sufficient to give good performance in many cases.\n\nGame developers' increasing awareness of academic AI and a growing interest in computer games by the academic community is causing the definition of what counts as AI in a game to become less idiosyncratic. Nevertheless, significant differences between different application domains of AI mean that game AI can still be viewed as a distinct subfield of AI. In particular, the ability to legitimately solve some AI problems in games by cheating creates an important distinction. For example, inferring the position of an unseen object from past observations can be a difficult problem when AI is applied to robotics, but in a computer game a NPC can simply look up the position in the game's scene graph. Such cheating can lead to unrealistic behavior and so is not always desirable. But its possibility serves to distinguish game AI and leads to new problems to solve, such as when and how to use cheating.\n\nThe major limitation to strong AI is the inherent depth of thinking and the extreme complexity of the decision making process. This means that although it would be then theoretically possible to make \"smart\" AI the problem would take considerable processing power.\n\n\nGame AI/heuristic algorithms are used in a wide variety of quite disparate fields inside a game. The most obvious is in the control of any NPCs in the game, although scripting is currently the most common means of control. Pathfinding is another common use for AI, widely seen in real-time strategy games. Pathfinding is the method for determining how to get a NPC from one point on a map to another, taking into consideration the terrain, obstacles and possibly \"fog of war\". Beyond pathfinding, navigation is a sub-field of Game AI focusing on giving NPCs the capability to navigate in their environment, finding a path to a target while avoiding collisions with other entities (other NPC, players...) or collaborating with them (group navigation).\n\nThe concept of emergent AI has recently been explored in games such as \"Creatures\", \"Black & White\" and \"Nintendogs\" and toys such as Tamagotchi. The \"pets\" in these games are able to \"learn\" from actions taken by the player and their behavior is modified accordingly. While these choices are taken from a limited pool, it does often give the desired illusion of an intelligence on the other side of the screen.\n\nMany contemporary video games fall under the category of action, first person shooter, or adventure. In most of these types of games there is some level of combat that takes place. The AI's ability to be efficient in combat is important in these genres. A common goal today is to make the AI more human, or at least appear so.\n\nOne of the more positive and efficient features found in modern-day video game AI is the ability to hunt. AI originally reacted in a very black and white manner. If the player were in a specific area then the AI would react in either a complete offensive manner or be entirely defensive. In recent years, the idea of \"hunting\" has been introduced; in this 'hunting' state the AI will look for realistic markers, such as sounds made by the character or footprints they may have left behind. These developments ultimately allow for a more complex form of play. With this feature, the player can actually consider how to approach or avoid an enemy. This is a feature that is particularly prevalent in the stealth genre.\n\nAnother development in recent game AI has been the development of \"survival instinct\". In-game computers can recognize different objects in an environment and determine whether it is beneficial or detrimental to its survival. Like a user, the AI can \"look\" for cover in a firefight before taking actions that would leave it otherwise vulnerable, such as reloading a weapon or throwing a grenade. There can be set markers that tell it when to react in a certain way. For example, if the AI is given a command to check its health throughout a game then further commands can be set so that it reacts a specific way at a certain percentage of health. If the health is below a certain threshold then the AI can be set to run away from the player and avoid it until another function is triggered. Another example could be if the AI notices it is out of bullets, it will find a cover object and hide behind it until it has reloaded. Actions like these make the AI seem more human. However, there is still a need for improvement in this area.\n\nAnother side-effect of combat AI occurs when two AI-controlled characters encounter each other; first popularized in the id Software game \"Doom\", so-called 'monster infighting' can break out in certain situations. Specifically, AI agents that are programmed to respond to hostile attacks will sometimes attack \"each other\" if their cohort's attacks land too close to them. In the case of \"Doom\", published gameplay manuals even suggest taking advantage of monster infighting in order to survive certain levels and difficulty settings.\n\nGeorgios N. Yannakakis suggests that academic AI developments may play roles in game AI beyond the traditional paradigm of AI controlling NPC behavior. He highlights four other potential application areas:\n\nIn the context of artificial intelligence in video games, cheating refers to the programmer giving agents actions and access to information that would be unavailable to the player in the same situation. Believing that the Atari 8-bit could not compete against a human player, Chris Crawford did not fix a bug in \"Eastern Front (1941)\" that benefited the computer-controlled Russian side. \"Computer Gaming World\" in 1994 reported that \"It is a well-known fact that many AIs 'cheat' (or, at least, 'fudge') in order to be able to keep up with human players\".\n\nFor example, if the agents want to know if the player is nearby they can either be given complex, human-like sensors (seeing, hearing, etc.), or they can cheat by simply asking the game engine for the player's position. Common variations include giving AIs higher speeds in racing games to catch up to the player or spawning them in advantageous positions in first person shooters. The use of cheating in AI shows the limitations of the \"intelligence\" achievable artificially; generally speaking, in games where strategic creativity is important, humans could easily beat the AI after a minimum of trial and error if it were not for this advantage. Cheating is often implemented for performance reasons where in many cases it may be considered acceptable as long as the effect is not obvious to the player. While cheating refers only to privileges given specifically to the AI—it does not include the inhuman swiftness and precision natural to a computer—a player might call the computer's inherent advantages \"cheating\" if they result in the agent acting unlike a human player. Sid Meier stated that he omitted multiplayer alliances in \"Civilization\" because he found that the computer was almost as good as humans in using them, which caused players to think that the computer was cheating. Developers say that most are honest but they dislike players erroneously complaining about \"cheating\" AI. in addition, humans use tactics against computers that they would not against other people.\n\nCreatures is an artificial life program where the user \"hatches\" small furry animals and teaches them how to behave. These \"Norns\" can talk, feed themselves, and protect themselves against vicious creatures. It's the first popular application of machine learning into an interactive simulation. Neural networks are used by the creatures to learn what to do. The game is regarded as a breakthrough in artificial life research, which aims to model the behavior of creatures interacting with their environment.\nA first-person shooter where the player assumes the role of the Master Chief, battling various aliens on foot or in vehicles. Enemies use cover very wisely, and employ suppression fire and grenades. The squad situation affects the individuals, so certain enemies flee when their leader dies. A lot of attention is paid to the little details, with enemies notably throwing back grenades or team-members responding to you bothering them. The underlying \"behavior tree\" technology has become very popular in the games industry (especially since Halo 2).\nA first-person shooter where the player helps contain supernatural phenomenon and armies of cloned soldiers. The AI uses a planner to generate context-sensitive behaviors, the first time in a mainstream game. This technology used as a reference for many studios still today. The enemies are capable of using the environment very cleverly, finding cover behind tables, tipping bookshelves, opening doors, crashing through windows, and so on. Squad tactics are used to great effect. The enemies perform flanking maneuvers, use suppression fire, etc.\nA first-person shooter survival horror game where the player must face man-made experiments, military soldiers, and mercenaries known as Stalkers. The various encountered enemies (if the difficulty level is set to its highest) use combat tactics and behaviours such as healing wounded allies, giving orders, out-flanking the player or using weapons with pinpoint accuracy.\nA first-person shooter where the player fights off numerous mercenaries and assassinates faction leaders. The AI is behavior based and uses action selection, essential if an AI is to multitask or react to a situation. The AI can react in an unpredictable fashion in many situations. The enemies respond to sounds and visual distractions such as fire or nearby explosions and can be subject to investigate the hazard, the player can utilize these distractions to his own advantage. There are also social interfaces with an AI but however not in the form of direct conversation but more reactionary, if the player gets too close or even nudges an AI, the player is subject to getting shoved off or sworn at and by extent getting aimed at. Other social interfaces between AI exist when in combat, or neutral situations, if an enemy AI is injured on the ground, he will shout out for help, release emotional distress, etc.\nA real-time strategy game where a player takes control of one of three factions in a 1v1, 2v2, or 3v3 battle arena. The player must defeat his opponents by extinguishing all their opponents units and bases. This is accomplished by creating units that are effective at countering your opponents units. Players can play against multiple different levels of AI difficulty ranging from very easy to cheater 3 (insane). The AI is able to cheat at the difficulty, cheater 1 (vision), where it begins to have vision assistance on your location and units. Cheater 2 gives the AI extra resources, while Cheater 3 give an extensive advantage over its opponent.\n\n\n\n\n", "id": "1654769", "title": "Artificial intelligence (video games)"}
{"url": "https://en.wikipedia.org/wiki?curid=227021", "text": "Computer Go\n\nComputer Go is the field of artificial intelligence (AI) dedicated to creating a computer program that plays the traditional board game Go. The game of Go has been a fertile subject of artificial intelligence research for decades, culminating in 2017 with AlphaGo winning three of three games against Ke Jie, who at the time continuously held the world No. 1 ranking for two years.\n\nGo is a complex board game that requires intuition, creative and strategic thinking. It has long been considered a difficult challenge in the field of artificial intelligence (AI) and is considerably more difficult to solve than chess. Many in the field of artificial intelligence consider Go to require more elements that mimic human thought than chess. Mathematician I. J. Good wrote in 1965:\n\nPrior to 2015, the best Go programs only managed to reach amateur dan level. On the small 9×9 board, the computer fared better, and some programs managed to win a fraction of their 9×9 games against professional players. Prior to AlphaGo, some researchers had claimed that computers would never defeat top humans at Go.\n\nThe first Go program was written by Albert Lindsey Zobrist in 1968 as part of his thesis on pattern recognition. It introduced an influence function to estimate territory and Zobrist hashing to detect ko.\n\nIn April 1981, Jonathan K Millen published an article in \"Byte\" discussing Wally, a Go program with a 15x15 board that fit within the KIM-1 microcomputer's 1K RAM. Bruce F. Webster published an article in the magazine in November 1984 discussing a Go program he had written for the Apple Macintosh, including the MacFORTH source.\n\nIn 1998, very strong players were able to beat computer programs while giving handicaps of 25–30 stones, an enormous handicap that few human players would ever take. There was a case in the 1994 World Computer Go Championship where the winning program, Go Intellect, lost all three games against the youth players while receiving a 15-stone handicap. In general, players who understood and exploited a program's weaknesses could win with much larger handicaps than typical players.\n\nDevelopments in Monte Carlo tree search and machine learning brought the best programs to high dan level on the small 9x9 board. In 2009, the first such programs appeared which could reach and hold low dan-level ranks on the KGS Go Server on the 19x19 board as well.\n\nIn 2010, at the 2010 European Go Congress in Finland, MogoTW played 19x19 Go against Catalin Taranu (5p). MogoTW received a seven-stone handicap and won.\n\nIn 2011, Zen reached 5 dan on the server KGS, playing games of 15 seconds per move. The account which reached that rank uses a cluster version of Zen running on a 26-core machine.\n\nIn 2012, Zen beat Takemiya Masaki (9p) by 11 points at five stones handicap, followed by a 20-point win at four stones handicap.\n\nIn 2013, Crazy Stone beat Yoshio Ishida (9p) in a 19×19 game at four stones handicap.\n\nThe 2014 Codecentric Go Challenge, a best-of-five match in an even 19x19 game, was played between Crazy Stone and Franz-Jozef Dickhut (6d). No stronger player had ever before agreed to play a serious competition against a go program on even terms. Franz-Jozef Dickhut won, though Crazy Stone won the first match by 1.5 points.\n\nIn October 2015, Google DeepMind program AlphaGo beat Fan Hui, the European Go champion, five times out of five in tournament conditions.\n\nIn March 2016, AlphaGo beat Lee Sedol in the first three of five matches. This was the first time that a 9-dan master had played a professional game against a computer without handicap. Lee won the fourth match, describing his win as \"invaluable\". AlphaGo won the final match two days later.\n\nIn May 2017, AlphaGo beat Ke Jie, who at the time was ranked top in the world, in a three-game match during the Future of Go Summit.\n\nIn October 2017, DeepMind revealed a new version of AlphaGo, trained only through self play, that had surpassed all previous versions, beating the Ke Jie version in 89 out of 100 games.\n\nSince the basic principles of AlphaGo had been published in the journal \"Nature\", other teams were able to produce high-level programs. By 2017, both Zen and Tencent's project Fine Art were capable of defeating very high level professionals some of the time.\n\nFor a long time, it was a widely held opinion that computer Go posed a problem fundamentally different from computer chess. It was believed that methods relying on fast global search with relatively little domain knowledge would not be effective against human experts. Therefore, a large part of the computer Go development effort was during these times focused on ways of representing human-like expert knowledge and combining this with local search to answer questions of a tactical nature. The result of this were programs that handled many situations well but which had very pronounced weaknesses compared to their overall handling of the game. Also, these classical programs gained almost nothing from increases in available computing power \"per se\" and progress in the field was generally slow.\n\nA few researchers grasped the potential of probabilistic methods and predicted that they would come to dominate computer game-playing, but many others considered a strong Go-playing program something that could be achieved only in the far future, as a result of fundamental advances in general artificial intelligence technology. Even writing a program capable of automatically determining the winner of a finished game was seen as no trivial matter.\n\nThe advent of programs based on Monte Carlo search starting in 2006 changed this situation in many ways with the first 9-dan professional Go players being defeated in 2013 by multicore computers, albeit with four-stone handicap.\n\nThe large board (19×19, 361 intersections) is often noted as one of the primary reasons why a strong program is hard to create. The large board size prevents an alpha-beta searcher from achieving deep look-ahead without significant search extensions or pruning heuristics.\n\nIn 2002, a computer program called MIGOS (MIni GO Solver) completely solved the game of Go for the 5×5 board. Black wins, taking the whole board.\n\nContinuing the comparison to chess, Go moves are not as limited by the rules of the game. For the first move in chess, the player has twenty choices. Go players begin with a choice of 55 distinct legal moves, accounting for symmetry. This number rises quickly as symmetry is broken, and soon almost all of the 361 points of the board must be evaluated. Some moves are much more popular than others and some are almost never played, but all are possible.\n\nWhile a simple material counting evaluation is not sufficient for decent play in chess, it is often the backbone of a chess evaluation function, when combined with more subtle considerations like isolated/doubled pawns, rooks on open files (columns), pawns in the center of the board and so on. These rules can be formalized easily, providing a reasonably good evaluation function that can run quickly.\n\nThese types of positional evaluation rules cannot efficiently be applied to Go. The value of a Go position depends on a complex analysis to determine whether or not the group is alive, which stones can be connected to one another, and heuristics around the extent to which a strong position has influence, or the extent to which a weak position can be attacked.\n\nMore than one move can be regarded as the best depending on which strategy is used. In order to choose a move, the computer must evaluate different possible outcomes and decide which is best. This is difficult due to the delicate trade-offs present in Go. For example, it may be possible to capture some enemy stones at the cost of strengthening the opponent's stones elsewhere. Whether this is a good trade or not can be a difficult decision, even for human players. The computational complexity also shows here as a move might not be immediately important, but after many moves could become highly important as other areas of the board take shape.\n\nSometimes it is mentioned in this context that various difficult combinatorial problems (in fact, any NP-hard problem) can be converted to Go-like problems on a sufficiently large board; however, the same is true for other abstract board games, including chess and minesweeper, when suitably generalized to a board of arbitrary size. NP-complete problems do not tend in their general case to be easier for unaided humans than for suitably programmed computers: it is doubtful that unaided humans would be able to compete successfully against computers in solving, for example, instances of the subset sum problem. \n\nGiven that the endgame contains fewer possible moves than the opening (\"fuseki\") or middle game, one might suppose that it is easier to play, and thus that a computer should be able to easily tackle it. In chess, computer programs perform worse in chess endgames because the ideas are long-term, unless the number of pieces is reduced to an extent that allows taking advantage of solved endgame tablebases.\n\nThe application of surreal numbers to the endgame in Go, a general game analysis pioneered by John H. Conway, has been further developed by Elwyn R. Berlekamp and David Wolfe and outlined in their book, \"Mathematical Go\" (). While not of general utility in most playing circumstances, it greatly aids the analysis of certain classes of positions.\n\nNonetheless, although elaborate study has been conducted, Go endgames have been proven to be PSPACE-hard. There are many reasons why they are so hard:\n\nThus, it is very unlikely that it will be possible to program a reasonably fast algorithm for playing the Go endgame flawlessly, let alone the whole Go game.\n\nCurrent Monte-Carlo-based Go engines can have difficulty solving problems when the order of moves is important.\n\nOne of the main concerns for a Go player is which groups of stones can be kept alive and which can be captured. This general class of problems is known as life and death. The most direct strategy for calculating life and death is to perform a tree search on the moves which potentially affect the stones in question, and then to record the status of the stones at the end of the main line of play.\n\nHowever, within time and memory constraints, it is not generally possible to determine with complete accuracy which moves could affect the 'life' of a group of stones. This implies that some heuristic must be applied to select which moves to consider. The net effect is that for any given program, there is a trade-off between playing speed and life and death reading abilities.\n\nWith Benson's algorithm, it is possible to determine the chains which are unconditionally alive and therefore would not need to be checked in the future for safety.\n\nAn issue that all Go programs must tackle is how to represent the current state of the game. For programs that use extensive searching techniques, this representation needs to be copied and/or modified for each new hypothetical move considered. This need places the additional constraint that the representation should either be small enough to be copied quickly or flexible enough that a move can be made and undone easily.\n\nThe most direct way of representing a board is as a one- or two-dimensional array, where elements in the array represent points on the board, and can take on a value corresponding to a white stone, a black stone, or an empty intersection. Additional data is needed to store how many stones have been captured, whose turn it is, and which intersections are illegal due to the Ko rule.\n\nMost programs, however, use more than just the raw board information to evaluate positions. Data such as which stones are connected in strings, which strings are associated with each other, which groups of stones are in risk of capture and which groups of stones are effectively dead are necessary to make an accurate evaluation of the position. While this information can be extracted from just the stone positions, much of it can be computed more quickly if it is updated in an incremental, per-move basis. This incremental updating requires more information to be stored as the state of the board, which in turn can make copying the board take longer. This kind of trade-off is indicative of the problems involved in making fast computer Go programs.\n\nAn alternative method is to have a single board and make and take back moves so as to minimize the demands on computer memory and have the results of the evaluation of the board stored. This avoids having to copy the information over and over again.\n\nHistorically, GOFAI (Good Old Fashioned AI) techniques have been used to approach the problem of Go AI. More recently, neural networks have been used as an alternative approach. One example of a program which uses neural networks is WinHonte.\n\nThese approaches attempt to mitigate the problems of the game of Go having a high branching factor and numerous other difficulties.\n\nComputer Go research results are being applied to other similar fields such as cognitive science, pattern recognition and machine learning. Combinatorial Game Theory, a branch of applied mathematics, is a topic relevant to computer Go.\n\nThe only choice a program needs to make is where to place its next stone. However, this decision is made difficult by the wide range of impacts a single stone can have across the entire board, and the complex interactions various stones' groups can have with each other. Various architectures have arisen for handling this problem. The most popular use:\nFew programs use only one of these techniques exclusively; most combine portions of each into one synthetic system.\n\nOne traditional AI technique for creating game playing software is to use a minimax tree search. This involves playing out all hypothetical moves on the board up to a certain point, then using an evaluation function to estimate the value of that position for the current player. The move which leads to the best hypothetical board is selected, and the process is repeated each turn. While tree searches have been very effective in computer chess, they have seen less success in Computer Go programs. This is partly because it has traditionally been difficult to create an effective evaluation function for a Go board, and partly because the large number of possible moves each side can make each leads to a high branching factor. This makes this technique very computationally expensive. Because of this, many programs which use search trees extensively can only play on the smaller 9×9 board, rather than full 19×19 ones.\n\nThere are several techniques, which can greatly improve the performance of search trees in terms of both speed and memory. Pruning techniques such as alpha–beta pruning, Principal Variation Search, and MTD-f can reduce the effective branching factor without loss of strength. In tactical areas such as life and death, Go is particularly amenable to caching techniques such as transposition tables. These can reduce the amount of repeated effort, especially when combined with an iterative deepening approach. In order to quickly store a full-sized Go board in a transposition table, a hashing technique for mathematically summarizing is generally necessary. Zobrist hashing is very popular in Go programs because it has low collision rates, and can be iteratively updated at each move with just two XORs, rather than being calculated from scratch. Even using these performance-enhancing techniques, full tree searches on a full-sized board are still prohibitively slow. Searches can be sped up by using large amounts of domain specific pruning techniques, such as not considering moves where your opponent is already strong, and selective extensions like always considering moves next to groups of stones which are about to be captured. However, both of these options introduce a significant risk of not considering a vital move which would have changed the course of the game.\n\nResults of computer competitions show that pattern matching techniques for choosing a handful of appropriate moves combined with fast localized tactical searches (explained above) were once sufficient to produce a competitive program. For example, GNU Go was competitive until 2008.\n\nNovices often learn a lot from the game records of old games played by master players. There is a strong hypothesis that suggests that acquiring Go knowledge is a key to making a strong computer Go. For example, Tim Kinger and David Mechner argue that \"it is our belief that with better tools for representing and maintaining Go knowledge, it will be possible to develop stronger Go programs.\" They propose two ways: recognizing common configurations of stones and their positions and concentrating on local battles. \"... Go programs are still lacking in both quality and quantity of knowledge.\"\n\nAfter implementation, the use of expert knowledge has been proved very effective in programming Go software. Hundreds of guidelines and rules of thumb for strong play have been formulated by both high level amateurs and professionals. The programmer's task is to take these heuristics, formalize them into computer code, and utilize pattern matching and pattern recognition algorithms to recognize when these rules apply. It is also important to have a system for determining what to do in the event that two conflicting guidelines are applicable.\n\nMost of the relatively successful results come from programmers' individual skills at Go and their personal conjectures about Go, but not from formal mathematical assertions; they are trying to make the computer mimic the way they play Go. \"Most competitive programs have required 5–15 person-years of effort, and contain 50–100 modules dealing with different aspects of the game.\"\n\nThis method has until recently been the most successful technique in generating competitive Go programs on a full-sized board. Some example of programs which have relied heavily on expert knowledge are Handtalk (later known as Goemate), The Many Faces of Go, Go Intellect, and Go++, each of which has at some point been considered the world's best Go program.\n\nNevertheless, adding knowledge of Go sometimes weakens the program because some superficial knowledge might bring mistakes: \"the best programs usually play good, master level moves. However, as every games player knows, just one bad move can ruin a good game. Program performance over a full game can be much lower than master level.\"\n\nOne major alternative to using hand-coded knowledge and searches is the use of Monte Carlo methods. This is done by generating a list of potential moves, and for each move playing out thousands of games at random on the resulting board. The move which leads to the best set of random games for the current player is chosen as the best move. The advantage of this technique is that it requires very little domain knowledge or expert input, the trade-off being increased memory and processor requirements. However, because the moves used for evaluation are generated at random it is possible that a move which would be excellent except for one specific opponent response would be mistakenly evaluated as a good move. The result of this are programs which are strong in an overall strategic sense, but are imperfect tactically. This problem can be mitigated by adding some domain knowledge in the move generation and a greater level of search depth on top of the random evolution. Some programs which use Monte-Carlo techniques are Fuego, The Many Faces of Go v12, Leela, MoGo, Crazy Stone, MyGoFriend, and Zen.\n\nIn 2006, a new search technique, \"upper confidence bounds applied to trees\" (UCT), was developed and applied to many 9x9 Monte-Carlo Go programs with excellent results. UCT uses the results of the \"play outs\" collected so far to guide the search along the more successful lines of play, while still allowing alternative lines to be explored. The UCT technique along with many other optimizations for playing on the larger 19x19 board has led MoGo to become one of the strongest research programs. Successful early applications of UCT methods to 19x19 Go include MoGo, Crazy Stone, and Mango. MoGo won the 2007 Computer Olympiad and won one (out of three) blitz game against Guo Juan, 5th Dan Pro, in the much less complex 9x9 Go. The Many Faces of Go won the 2008 Computer Olympiad after adding UCT search to its traditional knowledge-based engine.\n\nWhile knowledge-based systems have been very effective at Go, their skill level is closely linked to the knowledge of their programmers and associated domain experts. One way to break this limitation is to use machine learning techniques in order to allow the software to automatically generate rules, patterns, and/or rule conflict resolution strategies.\n\nThis is generally done by allowing a neural network or genetic algorithm to either review a large database of professional games, or play many games against itself or other people or programs. These algorithms are then able to utilize this data as a means of improving their performance. AlphaGo used this to great effect. Other programs using neural nets previously have been NeuroGo and WinHonte.\n\nMachine learning techniques can also be used in a less ambitious context to tune specific parameters of programs which rely mainly on other techniques. For example, Crazy Stone learns move generation patterns from several hundred sample games, using a generalization of the Elo rating system.\n\nAlphaGo, developed by Google DeepMind, made a significant advance by beating a professional human player in October 2015, using techniques that combined deep learning and Monte-Carlo tree search. AlphaGo is significantly more powerful than other previous Go programs, and the first to beat a 9 dan human professional in a game without handicaps on a full-sized board.\n\ncomputer-go-dataset (1,645,958 SGFs)\n\n\nSeveral annual competitions take place between Go computer programs, the most prominent being the Go events at the Computer Olympiad. Regular, less formal, competitions between programs occur on the KGS Go Server (monthly) and the Computer Go Server (continuous).\n\nProminent go-playing programs include Crazy Stone, Zen, Aya, Mogo, The Many Faces of Go, pachi and Fuego, all listed above; and Taiwanese-authored coldmilk, Dutch-authored Steenvreter, and Korean-authored DolBaram.\n\nThe first computer Go competition was sponsored by Acornsoft, and the first regular ones by USENIX. They ran from 1984 to 1988. These competitions introduced Nemesis, the first competitive Go program from Bruce Wilcox, and G2.5 by David Fotland, which would later evolve into Cosmos and The Many Faces of Go.\n\nOne of the early drivers of computer Go research was the Ing Prize, a relatively large money award sponsored by Taiwanese banker Ing Chang-ki, offered annually between 1985 and 2000 at the World Computer Go Congress (or Ing Cup). The winner of this tournament was allowed to challenge young players at a handicap in a short match. If the computer won the match, the prize was awarded and a new prize announced: a larger prize for beating the players at a lesser handicap. The series of Ing prizes was set to expire either 1) in the year 2000 or 2) when a program could beat a 1-dan professional at no handicap for 40,000,000 NT dollars. The last winner was Handtalk in 1997, claiming 250,000 NT dollars for winning an 11-stone handicap match against three 11–13 year old amateur 2–6 dans. At the time the prize expired in 2000, the unclaimed prize was 400,000 NT dollars for winning a nine-stone handicap match.\n\nMany other large regional Go tournaments (\"congresses\") had an attached computer Go event. The European Go Congress has sponsored a computer tournament since 1987, and the USENIX event evolved into the US/North American Computer Go Championship, held annually from 1988–2000 at the US Go Congress.\n\nJapan started sponsoring computer Go competitions in 1995. The FOST Cup was held annually from 1995 to 1999 in Tokyo. That tournament was supplanted by the Gifu Challenge, which was held annually from 2003 to 2006 in Ogaki, Gifu. The Computer Go UEC Cup has been held annually since 2007.\n\nWhen two computers play a game of Go against each other, the ideal is to treat the game in a manner identical to two humans playing while avoiding any intervention from actual humans. However, this can be difficult during end game scoring. The main problem is that Go playing software, which usually communicates using the standardized Go Text Protocol (GTP), will not always agree with respect to the alive or dead status of stones.\n\nWhile there is no general way for two different programs to \"talk it out\" and resolve the conflict, this problem is avoided for the most part by using Chinese, Tromp-Taylor, or American Go Association (AGA) rules in which continued play (without penalty) is required until there is no more disagreement on the status of any stones on the board. In practice, such as on the KGS Go Server, the server can mediate a dispute by sending a special GTP command to the two client programs indicating they should continue placing stones until there is no question about the status of any particular group (all dead stones have been captured). The CGOS Go Server usually sees programs resign before a game has even reached the scoring phase, but nevertheless supports a modified version of Tromp-Taylor rules requiring a full play out.\n\nIt should be noted that these rule sets mean that a program which was in a winning position at the end of the game under Japanese rules (when both players have passed) could lose because of poor play in the resolution phase, but this is not a common occurrence and is considered a normal part of the game under all of the area rule sets.\n\nThe main drawback to the above system is that some rule sets (such as the traditional Japanese rules) penalize the players for making these extra moves, precluding the use of additional playout for two computers. Nevertheless, most modern Go Programs support Japanese rules against humans and are competent in both play and scoring (Fuego, Many Faces of Go, SmartGo, etc.).\n\nHistorically, another method for resolving this problem was to have an expert human judge the final board. However, this introduces subjectivity into the results and the risk that the expert would miss something the program saw.\n\nMany programs are available that allow computer Go engines to play against each other and they almost always communicate via the Go Text Protocol (GTP).\n\nGoGUI and its addon gogui-twogtp can be used to play two engines against each other on a single computer system. SmartGo and Many Faces of Go also provide this feature.\n\nTo play as wide a variety of opponents as possible, the KGS Go Server allows Go engine vs. Go engine play as well as Go engine vs. human in both ranked and unranked matches. CGOS is a dedicated computer vs. computer Go server.\n\n\n\n", "id": "227021", "title": "Computer Go"}
{"url": "https://en.wikipedia.org/wiki?curid=64187", "text": "Computer Olympiad\n\nThe Computer Olympiad is a multi-games event in which computer programs compete against each other. For many games, the Computer Olympiads are an opportunity to claim the \"world's best computer player\" title. First contested in 1989, the majority of the games are board games but other games such as Bridge take place as well. In 2010, several puzzles were included in the competition.\n\n Developed in the 1980s by David Levy, the first Computer Olympiad took place in 1989 at the Park Lane Hotel in London. The games ran on a yearly basis until after the 1992 games, when the Olympiad's ruling committee was unable to find a new organiser. This resulted in the games being suspended until 2000 when the Mind Sports Olympiad resurrected them. Recently, the International Computer Games Association (ICGA) has adopted the Computer Olympiad and tries to organise the event on an annual basis.\n\nOriginally held most often in London or Maastricht, cities from around the world have hosted the Olympiad.\n\nThe games which have been contested at each olympiad are:\n\n\nAwari\nBackgammon\nBridge\nCheckers\nChess\nChinese Chess\nConnect-Four\nDominoes\nDraughts\nGo 19×19\nGo 9×9\nGo-Moku\nOthello\nRenju\n\nScrabble\n\n\nAwari\n \nBackgammon\n \nBridge\n \nCheckers\n \nChess\n \nChinese Chess\n \nGo 19×19\n \nGo 9×9\n \nGo-Moku\n \nOthello\n \nQubic\n \nRenju\n \nScrabble\n\n\nAwari\n\nBridge\n\nChess\n\nChinese Chess\n\nBoth programs were awarded the gold medal\n\nDraughts\n\nGo 19×19\n\nGo 9×9\n\nExplorer 90 and Go Intellect were awarded the gold medal.\n\nGo-Moku\n\nNine Men's Morris\n\nOthello\n\nQubic\n\nRenju\n\nScrabble\n\nThe AST 4th Computer Olympiad took place in London, UK from 5 August 1992 to 11 August 1992.\n\n\nAwari\n\nBackgammon\n\nBridge\n\nChess\n\nThe three programs were awarded the gold medal\n\nChinese Chess\n\nDraughts\n\nGin rummy\n\nGo 19×19\n\nBoth Archmage and Neuron were awarded the bronze medal\n\nGo 9×9\n\nGo-Moku\n\nBoth Polygon and Neuron were awarded the silver medal\n\nOthello\n\nRenju\n\nScrabble\n\n2000, London, United Kingdom\n\nThe 5th Computer Olympiad took place at Alexandra Palace, the West Hall in London, UK from 21 August 2000 to 25 August 2000. After an eight-year hiatus, it was revived by bringing it into the Mind Sports Olympiad. The computer programs competed against each other at a variety of games, including Amazons, Awari, Chess, Go, Hex, LOA, and Shogi.\n\nThe chess competition of the Computer Olympiad was a special event, since it was adopted by the ICCA as the 17th World Microcomputer Chess Championship (WMCC 2000).\n\n\nAmazons (6 participants)\n\nAwari (2 participants)\n\nChess (14 participants)\n\nGo 19×19 (6 participants)\n\nHex (3 participants)\n\nLines of Action (3 participants)\n\nShogi (3 participants)\n\n2001, Maastricht, Netherlands\n\nThe CMG 6th Computer Olympiad took place at Ad Fundum of the Universiteit Maastricht in Maastricht, Netherlands from 18 August 2001 to 23 August 2001. As with each year's Computer Olympiad, computer programs competed against each other at a variety of games, including Amazons, chess, Chinese Chess, Gipf, LOA, and Shogi.\n\nThe chess competition of the Computer Olympiad was a special event, since it was adopted by the ICCA as the 18th World Microcomputer Chess Championship (WMCC 2001).\n\nAmazons (4 participants)\n\nChess (18 participants)\n\nChinese Chess (3 participants)\n\nGipf (2 participants)\n\nLines of Action (3 participants)\n\nShogi (3 participants)\n\nThe 7th Computer Olympiad was held in Maastricht, Netherlands in 2002, from July 5 till July 11. There were 68 participants from over 13 countries.\n\nThe chess event played here was adopted by the ICCA as the 10th World Computer Chess Championship.\n\nAmazons (6 participants)\n\nBackgammon (2 participants)\n\nBridge (2 participants)\n\nChess (18 participants)\n\nChinese Chess (4 participants)\n\nDots and Boxes (2 participants)\n\nDraughts (9 participants)\n\nGo 19×19 (5 participants)\n\nGo 9×9 (4 participants)\n\nLines of Action (4 participants)\n\nShogi (5 participants)\n\n2003, Graz, Austria\n\nThe 8th Computer Olympiad was held November 23–27 November 2003, in Graz, Austria.\n\nThe Computer Olympiad was held in conjunction with the International Computer Games Association 11th World Computer Chess Championship 2003 and the 10th Advances in Computer Games Conference. Because of this, no medals were awarded for the two chess events.\n\nAbalone (2 participants)\n\nAmazons (5 participants)\n\nBackgammon (2 participants)\n\nChinese Chess (5 participants)\n\nDots and Boxes (3 participants)\n\nDraughts (4 participants)\n\nGo 19×19 (11 participants)\n\nGo 9×9 (10 participants)\n\nHex (2 participants)\n\nLines of Action (3 participants)\n\nPoker (2 participants)\n\nShogi (3 participants)\n\n2004, Ramat Gan, Israel\n\nThe 9th Computer Olympiad took place in Ramat-Gan, Israel from 3 July 2004 to 12 July 2004. As with each year's Computer Olympiad, computer programs competed against each other at a variety of games, including Amazons, Chinese Chess, Go, Lines of Action, Hex and Octi.\n\nThe event was held in conjunction with the 12th World Computer Chess Championship and Computers and Games 2004 Conference. Because of this, no medals were awarded for the two chess events.\n\nDr. Jonathan Schaeffer and Dr. J.W.H.M. Uiterwijk were the Tournament Directors.\n\n\nAmazons (2 participants)\n\nChinese Chess (2 participants)\n\nGo 19×19 (5 participants)\n\nGo 9×9 (9 participants)\n\nHex (2 participants)\n\nLines of Action (4 participants)\n\nOcti 6×7 (2 participants)\n\n2005, Taipei, Taiwan\n\nThe 10th Computer Olympiad took place in Taipei, Taiwan from 3 September 2005 to 6 September 2005. As with each year's Computer Olympiad, computer programs competed against each other at a variety of games, including Amazons, Chinese Chess, Clobber, Dots and Boxes, Computational Pool (billiards), Go, and Shogi.\n\nThe 11th Advances in Computer Games was also held at the same location and time as the Olympiad.\n\nThe organizing committee for the 10th edition was: H.H.L.M. Donkers, M. Greenspan, J.W. Hellemons (chair), T-s Hsu, H.J. van den Herik, and M. Tiessen.\n\nAmazons (3 participants)\n\nChinese Chess (14 participants)\n\nClobber (2 participants)\n\nDots and Boxes (3 participants)\n\nGo 19×19 (7 participants)\n\nGo 9×9 (9 participants)\n\nPool (4 participants)\n\nShogi (4 participants)\n\nHand Talk, which won the Gold medal in Computer Go, was original written in assembly language by a retired chemistry professor of Sun Yat-sen University, China.\n\n2006, Turin, Italy\n\nThe 11th Computer Olympiad was held in Turin, Italy between May 25 and 4 June 2006 in conjunction with the 14th World Computer Chess Championship and the 5th Computer and Games conference (CG 2006). These events were co-hosted with the human FIDE 37th Chess Olympiad.\n\n\nBackgammon (2 participants)\n\nChinese Chess (5 participants)\n\nClobber (3 participants)\n\nConnect6 (3 participants)\n\nInternational draughts (4 participants)\n\nGo 19×19 (6 participants)\n\nGo 9×9 (11 participants)\n\nHex (3 participants)\n\nKriegspiel (2 participants)\n\nLines of Action (2 participants)\n\nPool (5 participants)\n\nShogi (3 participants)\n\n2007, Amsterdam, Netherlands\n\nThe 12th Computer Olympiad was held in Amsterdam, Netherlands in conjunction with the 15th World Computer Chess Championship and Computer Games Workshop 2007 (CGW2007).\n\nIBM, SARA Computing and Networking Services and NCF (Foundation of National Computing Facilities) are enabling the organization of the Computer Games Workshop 2007 (CGW2007) (15–17 June 2007), the 15th World Computer-Chess Championship (WCCC) (11–18 June) and the 12th Computer Olympiad (CO) (11–18 June) was held in Amsterdam, The Netherlands.\n\nLocation: CGW2007 (The Turing hall - Z011), WCCC (Eulerzaal – Z009) and Computer Olympiad (Newtonzaal – Z010), Science Park Amsterdam, Kruislaan 413, 1098 SJ Amsterdam.\n\n\nAmazons (2 participants)\n\nBackgammon (3 participants)\n\nChess ( participants)\n\nRybka was retroactively disqualified from ICCC events due to findings of plagiarism. Therefore, Zappa and Loop were moved up, and GridChess and Shredder jointly awarded third place.\n\nChinese Chess (5 participants)\n\nConnect6 (4 participants)\n\nInternational draughts (7 participants)\n\nGo 19×19 (8 participants)\n\nGo 9×9 (10 participants)\n\nShogi (3 participants)\n\nPhantom Go (2 participants)\n\nSurakarta (2 participants)\n\n2008, Beijing, China.\n\nThe 13th International Computer Games Championship, 16th World Computer Chess Championship and a scientific conference on computer games was held in Beijing, China from 28 September–5 October 2008. The location was Beijing Golden Century Golf Club, Qinglonghu Township, Fangshan District, Beijing.\n\nAmazons (4 participants)\n\nChess (10 participants)\n\nRybka was retroactively disqualified from all WCCC events in 2011 due to findings of plagiarism. Thus, Cluster Toga was awarded a bronze medal, and the other two winners were upgraded.\n\nSpeed Chess (10 participants)\n\nRybka was retroactively disqualified from all WCCC events in 2011 due to findings of plagiarism.\n\nChinese Chess (18 participants)\n\nConnect6 (10 participants)\n\nDots and Boxes (3 participants)\n\nInternational draughts (2 participants)\n\nGo (13 participants)\n\nGo (9x9) (18 participants)\n\nHex (4 participants)\n\nComputational Pool (4 participants)\n\nShogi (3 participants)\n\nPhantom Go (3 participants)\n\nSurakarta (2 participants)\n\n2009, Pamplona, Spain\n\nThe 14th Computer Olympiad, 17th World Computer Chess Championship and a scientific conference on computer games was held in Pamplona, Spain, 10–18 May 2009.\n\nChess (10 participants)\n\nRybka was retroactively disqualified from ICCC events due to findings of plagiarism. Therefore, the Championship title was awarded jointly to Deep Sjeng, Shredder, and Junior.\n\nChess (Blitz) (9 participants)\n\nLikewise, the Blitz Championship title was awarded to Shredder.\n\nAmazons (4 participants)\n\nChess (no hardware limits) (6 participants)\n\nChinese Chess (5 participants)\n\nConnect6 (6 participants)\n\nDraughts (3 participants)\n\nGo (6 participants)\n\nGo (9x9) (9 participants)\n\nHex (4 participants)\n\nLines of Action (2 participants)\n\nShogi (2 participants)\n\nHavannah (2 participants)\n\nKriegSpiel (3 participants)\n\nPhantom Go (2 participants)\n\n2010, Kanazawa, Japan\n\nThe 15th Computer Olympiad, 18th World Computer Chess Championship and a scientific conference on computer games was held in Kanazawa, Japan, September 24 to 2 October 2010.\n\nChess (10 participants)\n\nRybka was retroactively disqualified from ICCC events due to findings of plagiarism. Therefore, the Championship title was awarded jointly to Rondo and Thinker. Shredder was given third place.\n\nChess (Software) (9 participants)\n\nChess (Blitz)\n\nLikewise, Jonny and Shredder were awarded the blitz championship after Rybka's disqualification.\n\nAmazons (7 participants)\n\nChinese Chess (5 participants)\n\nChinese Dark Chess (6 participants)\n\nClobber (1 participant)\n\nPan.exe won by default, as there were no other entrants.\n\nConnect6 (8 participants)\n\nDots and Boxes (2 participants)\n\nDraughts (2 participants)\n\nGo (8 participants)\n\nGo (13x13) (10 participants)\n\nGo (9x9) (14 participants)\n\nHex (5 participants)\n\nShogi (9 participants)\n\nMinishogi (9 participants)\n\nHavannah (5 participants)\n\nLight Up (2 participants)\n\nCpuzzler was awarded the silver medal.\nNonograms (2 participants)\n\nCpuzzler was awarded the bronze medal\n\nNurikabe (3 participants)\nCpuzzler (Shi-Yuan Chiu) (TW), Enigma (Jr-Chang Chen, Chou Cheng-Wei) (TW), (Derjhong Sun, I-Chen Wu) (TW)\n\nPhantom Go (3 participants)\nGoLois (Tristan Cazenave, Nicolas Jouandeau) (FRA), Moccos (Takuma Toyoda) (JP), IcySoftwoodWine (Yuji Abe) (JP)\n\nQuoridor (4 participants)\n\nSurakarta (3 participants)\n\n2011, Tilburg, Netherlands\n\nThe 16th International Computer Games Championship, 19th World Computer Chess Championship and a scientific conference on computer games was held in Tilburg. The events took place from 18 November to 26 November 2011. The venue was the Tilburg University.\n\nAmazons (4)\n\nBackgammon (3)\n\nChess (World Computer Chess Championship)\n\nClobber (2)\n\nChinese Chess (3)\n\nConnect6 (6)\n\nDots and Boxes (2)\n\nDraughts (6)\n\nEinStein würfelt nicht! (6)\n\nGo (8)\n\nGo (9x9) (7)\n\nGo (13x13) (6)\n\nNoGo (4)\n\nPhantom Go (2)\n\nHavannah (3)\n\nHex (3)\n\nShogi (3)\n\nSurakarta (2)\n\n2013, Yokohama, Japan\n\nThe 17th International Computer Games Championship, 20th World Computer Chess Championship and a scientific conference on computer games was held in Yokohama, Japan from 12 August to 18 August 2013. The venue was the Collaboration Complex at Keio University Hiyoshi Campus.\n\n\n2015, Leiden, Netherlands\n\nThe 18th International Computer Games Championship, a scientific conference on computer games was held in Leiden, Amsterdam from June 29 to 6 July 2015. Organised by the International Computer Games Association (ICGA), the Leiden Institute of Advanced Computer Science (LIACS) and the Leiden Centre of Data Science (LCDS). The venue was Leiden University.\n\nThe following are the competitions in the 18th Computer Olympiad.\n\n1. SIA\n2. Deep Nikita\n\n2016, Leiden, Netherlands\n\nThe 19th International Computer Games Championship, a scientific conference on computer games was held in Leiden, Amsterdam from June 27 to 3 July 2016. Organised by the International Computer Games Association (ICGA), the Leiden Institute of Advanced Computer Science (LIACS) and the Leiden Centre of Data Science (LCDS). The venue was Leiden University\n\n2017, Leiden, Netherlands\n\nThe 20th International Computer Games Championship, a scientific conference on computer games was held in Leiden, Amsterdam from July 1 to 7 July 2017. Organised by the International Computer Games Association (ICGA), the Leiden Institute of Advanced Computer Science (LIACS) and the Leiden Centre of Data Science (LCDS). The venue was Leiden University\n\n Abalone is strategy game using a hexagonal patterned board with 14 marbles for each of two players. The objective is to push six of the opponent's marbles off the edge of the board. \n\n Amazons is played on a 10x10 chessboard by two players each with four \"amazons\" (queen chess pieces). Moves are made to block squares and the winner is the last player able to move his pieces to an unblocked square.\n\n Awari is an abstract strategy game among the Mancala family of board games (pit and pebble games).\n\nBackgammon is a board game for two players where the checker-like playing pieces are moved according to the roll of dice; a player wins by removing all of his pieces from the board before his opponent. \n\n Bridge is a trick-taking card game for four players.\n\nBridge participation in the Computer Olympiad has largely discontinued when in 1996 the American Contract Bridge League established a new official World Computer Bridge Championship, to be run annually at a major bridge tournament. Starting in 1999, that event is now co-sponsored by the World Bridge Federation.\n\n Chess is a two-player board game played on a checkered game-board with 64 squares arranged in an eight-by-eight grid. Each player begins with 16 pieces of varying characteristics, the objective being to capture one's opponent's king piece.\n\nMany computer-versus-computer events are held beyond those of the Computer Olympiad.\n\n Chinese chess, is a strategy board game for two players from the same family as western or international chess. Known primarily as Xiangqi internationally, the game is referred to as Chinese Chess in the Computer Olympiad competitions.\n\nChinese Dark Chess is known as Banqi in eastern cultures.\n\nOcti is an abstract strategy game with similarities to checkers and chess but allowing for multiple jumping, capturing, and special movement of pieces. The object of the game is move one's pieces into the opponent's starting points.\n\nAlso known as computational pool.\n\n\n", "id": "64187", "title": "Computer Olympiad"}
{"url": "https://en.wikipedia.org/wiki?curid=54802047", "text": "Yixin (software)\n\nYixin is a computer software playing Gomoku and Renju. It is the strongest Gomoku and Renju AI, champion of Gomocup 2012-2017, and it tops the Elo rating list of Gomocup.\n\nThere were several matches between Yixin and top Gomoku and Renju players. Two notable ones are: (1) In July 2017, Yixin had a match with the Taiwan’s Meijin title holder Lin Shu-Hsuan, and won the match with a score of 3-1. (2) In the Gomoku World Championship 2017, Yixin had a match with the world champion human player Rudolf Dupszki, and won the match with a score of 2-0, making it the first program to beat the Gomoku world champion.\n", "id": "54802047", "title": "Yixin (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=54451950", "text": "Multi-Prob Cut\n\nMulti-Prob Cut is a heuristic used in alpha–beta pruning search. It is of particular interest in games such as Othello and draughts in which the null-move heuristic would be problematic as it is quite often an advantage to pass.\n", "id": "54451950", "title": "Multi-Prob Cut"}
{"url": "https://en.wikipedia.org/wiki?curid=100558", "text": "A* search algorithm\n\nIn computer science, A* (pronounced as \"A star\") is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently directed path between multiple points, called nodes. It enjoys widespread use due to its performance and accuracy. However, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance,\nalthough other work has found A* to be superior to other approaches.\n\nPeter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first described the algorithm in 1968. It is an extension of Edsger Dijkstra's 1959 algorithm. A* achieves better performance by using heuristics to guide its search.\n\nIn 1968, AI researcher Nils Nilsson was trying to improve the path planning done by Shakey the Robot, a prototype robot that could navigate through a room containing obstacles. This path-finding algorithm, which Nilsson called A1, was a faster version of the then best known method, Dijkstra's algorithm, for finding shortest paths in graphs. Bertram Raphael suggested some significant improvements upon this algorithm, calling the revised version A2. Then Peter E. Hart introduced an argument that established A2, with only minor changes, to be the best possible algorithm for finding shortest paths. Hart, Nilsson and Raphael then jointly developed a proof that the revised A2 algorithm was \"optimal\" for finding shortest paths under certain well-defined conditions.\n\nA* is an informed search algorithm, or a best-first search, meaning that it solves problems by searching among all possible paths to the solution (goal) for the one that incurs the smallest cost (least distance travelled, shortest time, etc.), and among these paths it first considers the ones that \"appear\" to lead most quickly to the solution. It is formulated in terms of weighted graphs: starting from a specific node of a graph, it constructs a tree of paths starting from that node, expanding paths one step at a time, until one of its paths ends at the predetermined goal node.\n\nAt each iteration of its main loop, A* needs to determine which of its partial paths to expand into one or more longer paths. It does so based on an estimate of the cost (total weight) still to go to the goal node. Specifically, A* selects the path that minimizes\n\nwhere is the last node on the path, is the cost of the path from the start node to , and is a heuristic that estimates the cost of the cheapest path from to the goal. The heuristic is problem-specific. For the algorithm to find the actual shortest path, the heuristic function must be admissible, meaning that it never overestimates the actual cost to get to the nearest goal node.\n\nTypical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the \"open set\" or \"fringe\". At each step of the algorithm, the node with the lowest value is removed from the queue, the and values of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a goal node has a lower value than any node in the queue (or until the queue is empty). The value of the goal is then the length of the shortest path, since at the goal is zero in an admissible heuristic.\n\nThe algorithm described so far gives us only the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node's predecessor is the start node.\n\nAs an example, when searching for the shortest route on a map, might represent the straight-line distance to the goal, since that is physically the smallest possible distance between any two points.\n\nIf the heuristic satisfies the additional condition for every edge of the graph (where denotes the length of that edge), then is called monotone, or consistent. In such a case, A* can be implemented more efficiently—roughly speaking, no node needs to be processed more than once (see \"closed set\" below)—and A* is equivalent to running Dijkstra's algorithm with the reduced cost .\n\nThe following pseudocode describes the algorithm:\n\nRemark: the above pseudocode assumes that the heuristic function is \"monotonic\" (or consistent, see below), which is a frequent case in many practical problems, such as the Shortest Distance Path in road networks. However, if the assumption is not true, nodes in the closed set may be rediscovered and their cost improved. \nIn other words, the closed set can be omitted (yielding a tree search algorithm) if a solution is guaranteed to exist, or if the algorithm is adapted so that new nodes are added to the open set only if they have a lower \"f\" value than at any previous iteration.\n\nAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the straight-line distance to target point:\n\nKey: green: start; blue: goal; orange: visited\n\nThe A* algorithm also has real-world applications. In this example, edges are railroads and h(x) is the great-circle distance (the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C. and Los Angeles.\n\nLike breadth-first search, A* is \"complete\" and will always find a solution if one exists provided formula_2 for fixed formula_3.\n\nIf the heuristic function \"h\" is admissible, meaning that it never overestimates the actual minimal cost of reaching the goal, then A* is itself admissible (or \"optimal\") if we do not use a closed set. If a closed set is used, then \"h\" must also be \"monotonic\" (or consistent) for A* to be optimal. This means that for any pair of adjacent nodes \"x\" and \"y\", where denotes the length of the edge between them, we must have:\n\nThis ensures that for any path \"X\" from the initial node to \"x\":\n\nwhere \"L\" is a function that denotes the length of a path, and \"Y\" is the path \"X\" extended to include \"y\". In other words, it is impossible to decrease (total distance so far + estimated remaining distance) by extending a path to include a neighboring node. (This is analogous to the restriction to nonnegative edge weights in Dijkstra's algorithm.) Monotonicity implies admissibility when the heuristic estimate at any goal node itself is zero, since (letting \"P\" = (\"f\",\"v\",\"v\"...,\"v\",\"g\") be a shortest path from any node \"f\" to the nearest goal \"g\"):\n\nA* is also optimally efficient for any heuristic \"h\", meaning that no optimal algorithm employing the same heuristic will expand fewer nodes than A*, except when there are multiple partial solutions where \"h\" exactly predicts the cost of the optimal path. Even in this case, for each graph there exists some order of breaking ties in the priority queue such that A* examines the fewest possible nodes.\n\nDijkstra's algorithm, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* where for all \"x\". General depth-first search can be implemented using A* by considering that there is a global counter \"C\" initialized with a very large value. Every time we process a node we assign \"C\" to all of its newly discovered neighbors. After each single assignment, we decrease the counter \"C\" by one. Thus the earlier a node is discovered, the higher its value. It should be noted, however, that both Dijkstra's algorithm and depth-first search can be implemented more efficiently without including a value at each node.\n\nThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation. The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations. If ties are broken so the queue behaves in a LIFO manner, A* will behave like depth-first search among equal cost paths (avoiding exploring more than one equally optimal solution).\n\nWhen a path is required at the end of the search, it is common to keep with each node a reference to that node's parent. At the end of the search these references can be used to recover the optimal path. If these references are being kept then it can be important that the same node doesn't appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost). A standard approach here is to check if a node about to be added already appears in the priority queue. If it does, then the priority and parent pointers are changed to correspond to the lower cost path. A standard binary heap based priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a hash table that maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a Fibonacci heap can perform the same decrease-priority operations in constant amortized time.\n\nA* is admissible and considers fewer nodes than any other admissible search algorithm with the same heuristic. This is because A* uses an \"optimistic\" estimate of the cost of a path through every node that it considers—optimistic in that the true cost of a path through that node to the goal will be at least as great as the estimate. But, critically, as far as A* \"knows\", that optimistic estimate might be achievable.\n\nTo prove the admissibility of A*, the solution path returned by the algorithm is used as follows:\n\nWhen A* terminates its search, it has found a path whose actual cost is lower than the estimated cost of any path through any open node. But since those estimates are optimistic, A* can safely ignore those nodes. In other words, A* will never overlook the possibility of a lower-cost path and so is admissible.\n\nSuppose now that some other search algorithm B terminates its search with a path whose actual cost is \"not\" less than the estimated cost of a path through some open node. Based on the heuristic information it has, Algorithm B cannot rule out the possibility that a path through that node has a lower cost. So while B might consider fewer nodes than A*, it cannot be admissible. Accordingly, A* considers the fewest nodes of any admissible search algorithm.\n\nThis is only true if both:\n\n\n\nWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 + \"ε\") times the optimal solution path. This new guarantee is referred to as \"ε\"-admissible.\n\nThere are a number of \"ε\"-admissible algorithms:\n\n\n\n\n\n\n\nThe time complexity of A* depends on the heuristic. In the worst case of an unbounded search space, the number of nodes expanded is exponential in the depth of the solution (the shortest path) : , where is the branching factor (the average number of successors per state). This assumes that a goal state exists at all, and is reachable from the start state; if it is not, and the state space is infinite, the algorithm will not terminate.\n\nThe heuristic function has a major effect on the practical performance of A* search, since a good heuristic allows A* to prune away many of the nodes that an uninformed search would expand. Its quality can be expressed in terms of the \"effective\" branching factor , which can be determined empirically for a problem instance by measuring the number of nodes expanded, , and the depth of the solution, then solving\n\nGood heuristics are those with low effective branching factor (the optimal being ).\n\nThe time complexity is polynomial when the search space is a tree, there is a single goal state, and the heuristic function \"h\" meets the following condition:\n\nwhere is the optimal heuristic, the exact cost to get from to the goal. In other words, the error of will not grow faster than the logarithm of the \"perfect heuristic\" that returns the true distance from to the goal.\n\nA* is commonly used for the common pathfinding problem in applications such as games, but was originally designed as a general graph traversal algorithm.\nIt finds applications to diverse problems, including the problem of parsing using stochastic grammars in NLP.\nOther cases include an Informational search with online learning.\n\nWhat sets A* apart from a greedy best-first search algorithm is that it takes the cost/distance already traveled, , into account.\n\nSome common variants of Dijkstra's algorithm can be viewed as a special case of A* where the heuristic formula_14 for all nodes; in turn, both Dijkstra and A* are special cases of dynamic programming.\nA* itself is a special case of a generalization of branch and bound\nand can be derived from the primal-dual algorithm for linear programming.\n\n\nA* can also be adapted to a bidirectional search algorithm. Special care needs to be taken for the stopping criterion.\n\n\n\n", "id": "100558", "title": "A* search algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=1367992", "text": "Anthrobotics\n\nAnthrobotics is the science of developing and studying robots that are either entirely or in some way human-like.\n\nThe term \"anthrobotics\" was originally coined by Mark Rosheim in a paper entitled \"Design of An Omnidirectional Arm\" presented at the IEEE International Conference on Robotics and Automation, May 13–18, 1990, pp. 2162–2167. Rosheim says he derived the term from \"...Anthropomorphic and Robotics to distinguish the new generation of dexterous robots from its simple industrial robot forebears.\" The word gained wider recognition as a result of its use in the title of Rosheim's subsequent book \"Robot Evolution: The Development of Anthrobotics\", which focussed on facsimiles of human physical and psychological skills and attributes.\n\nHowever, a wider definition of the term \"anthrobotics\" has been proposed, in which the meaning is derived from \"anthropology\" rather than \"anthropomorphic\". This usage includes robots that respond to input in a human-like fashion, rather than simply mimicking human actions, thus theoretically being able to respond more flexibly or to adapt to unforeseen circumstances. This expanded definition also encompasses robots that are situated in social environments with the ability to respond to those environments appropriately, such as insect robots, robotic pets, and the like.\n\nAnthrobotics is now taught at some universities, encouraging students not only to design and build robots for environments beyond current industrial applications, but also to speculate on the future of robotics that are embedded in the world at large, as mobile phones and computers are today. In 2016 philosopher Luis de Miranda created the Anthrobotics Cluster at the University of Edinburgh \"a platform of cross-disciplinary research that seeks to investigate some of the biggest questions that will need to be answered\" on the relationship between humans, robots and intelligent systems and \"a think tank on the social spread of robotics, and also how automation is part of the definition of what humans have always been\". to explore the symbiotic relationship between humans and automated protocols.\n\n", "id": "1367992", "title": "Anthrobotics"}
{"url": "https://en.wikipedia.org/wiki?curid=3034653", "text": "Roboteer\n\nThe word \"roboteer\" refers to those with interests or careers in robotics. It dates back to the 1930s and is also used in 'Future Shock' (1970).\n\nThe term \"roboteer\" was used by Barbara Krasnov for a story on Deb Huglin, owner of the Robotorium, Inc., in New York City in the early 1980s. Huglin was a lightweight-robotics applications consultant, sculptor, and repatriation archeologist. Huglin worked with Jim Henson on the design and uses of the robotic mit controller for his experimental television series \"Fraggle Rock\". Huglin died in a fall in the wilderness near Hemet, California in 2008.\n\n\n", "id": "3034653", "title": "Roboteer"}
{"url": "https://en.wikipedia.org/wiki?curid=7278280", "text": "Robotic voice effects\n\nRobotic voice effects became a recurring element in popular music starting in the second half of the twentieth century. Several methods of producing variations on this effect have arisen.\n\nThe vocoder was originally designed to aid in the transmission of voices over telephony systems. In musical applications the original sounds, either from vocals or from other sources such as instruments, are used and fed into a system of filters and noise generators. The input is fed through band-pass filters to separate the tonal characteristics which then trigger noise generators. The sounds generated are mixed back with some of the original sound and this gives the effect.\n\nVocoders have been used in an analog form from as early as 1959 at Siemens Studio for Electronic Music but were made more famous after Robert Moog developed one of the first solid-state musical vocoders.\n\nIn 1970 Wendy Carlos and Robert Moog built another musical vocoder, a 10-band device inspired by the vocoder designs of Homer Dudley which was later referred to simply as a vocoder.\n\nCarlos and Moog's vocoder was featured in several recordings, including the soundtrack to Stanley Kubrick's \"A Clockwork Orange\" for the vocal part of Beethoven's \"Ninth Symphony\" and a piece called \"Timesteps\". In 1974 Isao Tomita used a Moog vocoder on a classical music album, \"Snowflakes are Dancing\", which became a worldwide success. Since then they have been widely used by artists such as: Kraftwerk's album \"Autobahn\" (1974); The Alan Parsons Project's track \"The Raven\" (\"Tales of Mystery and Imagination\" album 1976); Electric Light Orchestra on \"Mr. Blue Sky\" and \"Sweet Talkin' Woman\" (\"Out of the Blue\" album 1977) using EMS Vocoder 2000's.\nOther examples include Pink Floyd's album \"Animals\", where the band put the sound of a barking dog through the device, and the Styx song \"Mr. Roboto\". Vocoders have appeared on pop recordings from time to time ever since, most often simply as a special effect rather than a featured aspect of the work. Many experimental electronic artists of the new-age music genre often utilize the vocoder in a more comprehensive manner in specific works, such as Jean Michel Jarre on \"Zoolook\" (1984), Mike Oldfield on \"QE2\" (1980) and \"Five Miles Out\" (1982). There are also some artists who have made vocoders an essential part of their music, overall or during an extended phase, such as the German synthpop group Kraftwerk, or the jazz-infused metal band Cynic.\n\nThough the vocoder is by far the best-known, the following other pieces of music technology are often confused with it:\n\n\n\n\n\n\n\n", "id": "7278280", "title": "Robotic voice effects"}
{"url": "https://en.wikipedia.org/wiki?curid=2859356", "text": "Pipeline video inspection\n\nPipeline video inspection is a form of telepresence used to visually inspect the interiors of pipelines. A common application is to determine the condition of small diameter sewer lines and household connection pipes.\n\nOlder sewer lines of small diameter, typically , are made by the union of a number of short sections. The pipe segments may be made of cast iron, with to sections, but are more often made of vitrified clay pipe (VCP), a ceramic material, in , & sections. Each iron or clay segment will have an enlargement (a \"bell\") on one end to receive the end of the adjacent segment. Roots from trees and vegetation may work into the joins between segments and can be forceful enough to break open a larger opening in terra cotta or corroded cast iron. Eventually a root ball will form that will impede the flow and this may cleaned out by a cutter mechanism and subsequently inhibited by use of a chemical foam - a \"rooticide\".\n\nWith modern video equipment the interior of the pipe may be inspected - this is a form of non-destructive testing. A small diameter collector pipe will typically have a cleanout access at the far end and will be several hundred feet long, terminating at a manhole. Additional collector pipes may discharge at this manhole and a pipe (perhaps of larger diameter) will carry the effluent to the next manhole, and so forth to a pump station or treatment plant.\n\nThe service truck contains a power supply in the form of a small generator, a small air-conditioned compartment containing video monitoring and recording equipment, and related computer and display for feature recording.\n\nAt the back end of the truck is a powered reel with video cable reinforced with kevlar or steel wire braid. Some trucks also contain a powered winch that booms out from the truck allowing for lowering and retrieval of the inspection equipment from the pipeline.\n\nSometimes referred to as a PIG (pipeline inspection gauge), the camera and lights are mounted in a swiveling head attached to a cylindrical body. The camera head can pan and tilt remotely. Integrated into the camera head are lighting devices, typically LEDs, for illuminating the pipeline. The camera is connected to display equipment via a long cable wound upon a winch. Some companies, such as Rausch Electronics USA, incorporate a series of lasers in the camera to accurately measure the pipe diameter and other data.\n\nA run to be inspected will either start from an access pipe leading at an angle down to the sewer and then run downstream to a manhole, or will run between manholes.\nThe service truck is parked above the access point of the pipe. The camera tractor, with a flexible cable attached to the rear, is then lowered into the pipeline. The tractor is moved forward so that it is barely inside of the pipeline. A \"down-hole roller\" is set up between the camera tractor and the cable reel in the service truck, preventing cable damage from rubbing the top of the pipeline. The operator then retires to the inside of the truck and begins the inspection, remotely operating the camera tractor from the truck. When the inspection is complete or the camera cable is fully extended, the camera tractor is put in reverse gear and the cable is wound up simultaneously. When the camera tractor is near the original access point, the downhole roller is pulled up and the camera tractor is moved into the access point and pulled up to the service truck. A tractor may be used to inspect a complete blockage or collapse that would prevent using a fish and rope as described below.\n\nFor small diameter pipes there may not be enough room for the tractor mechanism. Instead, a somewhat rigid \"fish\" is pushed through the pipe and attached to a rope at the access point near the truck. The fish is then pulled to place the rope along the pipe. The rope is then used to pull the inspection pig and cable through the pipe. Detaching the rope, the cable is then used to pull the pig backwards as the pipe is inspected on the monitor (this is the method shown in the illustrations below).\n\nMuch of the analysis of what was viewed in the pipeline is conducted at the time of the inspection by the camera operator, but the entire inspection is always recorded and saved for review. Commercial software and hardware for video pipe inspection are available from a variety of vendors, including Cues, ITpipes, and WinCan.\n\nDepending mostly upon the change in conditions from a previous inspection various improvements may be made to the pipe. It may be cleaned with a rotating root cutting blade on the end of a segmented rotating chain, or a chemical foam may be applied to discourage root growth. If damage is found limited to only a few locations these may be excavated and repaired. Extensive moderate defects may be repaired by lining with a fabric liner that is pulled through the pipe, inflated, and then made rigid through chemical means. Severe damage may require excavation and replacement of the conduit .\n", "id": "2859356", "title": "Pipeline video inspection"}
{"url": "https://en.wikipedia.org/wiki?curid=14598828", "text": "Perceptual robotics\n\nPerceptual robotics is an interdisciplinary science linking Robotics and Neuroscience. It investigates biologically motivated robot control strategies, concentrating on perceptual rather than cognitive processes and thereby sides with J. J. Gibson's view against the Poverty of the stimulus theory.\n\nAs a working definition, the following quote from Chapter 64 by H. Bülthoff, C. Wallraven and M. Giese from \"The Springer Handbook of Robotics\", edited by Bruno Siciliano and Oussama Khatib, published by Springer in 2007, could be used:\n\nIn the following we will apply the term Perceptual Robotics to signify the design of robots based on principles that are derived from human perception on all three levels in the sense of Marr. This includes a realization in terms of specific neural circuits as well as the transfer of more abstract biologically-inspired strategies for the solution of relevant computational problems.\n\n", "id": "14598828", "title": "Perceptual robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=8888500", "text": "Electroadhesion\n\nElectroadhesion is the electrostatic effect of astriction between two surfaces subjected to an electrical field. Applications include the retention of paper on plotter surfaces, astrictive robotic prehension (electrostatic grippers) etc. Clamping pressures in the range of 0.5 to 1.5 N/cm (0.8 to 2.3 psi) have been claimed.\n\nAn electroadhesive pad consists of conductive electrodes placed upon a polymer substrate. When alternate positive and negative charges are induced on adjacent electrodes, the resulting electric field sets up opposite charges on the surface that the pad touches, and thus causes electrostatic adhesion between the electrodes and the induced charges in the touched surface material.\n\nElectroadhesion can be loosely divided into two basic forms: that which concerns the prehension of electrically conducting materials where the general laws of capacitance hold (D = E ε) and that used with electrically insulating subjects where the more advanced theory of electrostatics (D = E ε + P) applies.\n\n\n", "id": "8888500", "title": "Electroadhesion"}
{"url": "https://en.wikipedia.org/wiki?curid=9403552", "text": "Situated robotics\n\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. In this used, the term is used to refer to robots, but some researchers argue that software agents can also be situated if:\n\n\nBeing situated is generally considered to be part of being embodied, but it is useful to take both perspectives. The situated perspective emphasizes the environment and the agent's interactions with it. These interactions define an agent's embodiment.\n\n\n", "id": "9403552", "title": "Situated robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=5670694", "text": "Laboratory automation\n\nLaboratory automation is a multi-disciplinary strategy to research, develop, optimize and capitalize on technologies in the laboratory that enable new and improved processes. Laboratory automation professionals are academic, commercial and government researchers, scientists and engineers who conduct research and develop new technologies to increase productivity, elevate experimental data quality, reduce lab process cycle times, or enable experimentation that otherwise would be impossible.\n\nThe most widely known application of laboratory automation technology is laboratory robotics. More generally, the field of laboratory automation comprises many different automated laboratory instruments, devices (the most common being autosamplers), software algorithms, and methodologies used to enable, expedite and increase the efficiency and effectiveness of scientific research in laboratories.\n\nThe application of technology in today's laboratories is required to achieve timely progress and remain competitive. Laboratories devoted to activities such as high-throughput screening, combinatorial chemistry, automated clinical and analytical testing, diagnostics, large scale biorepositories, and many others, would not exist without advancements in laboratory automation.\n\nSome universities offer entire programs that focus on lab technologies. For example, Indiana University-Purdue University at Indianapolis offers a graduate program devoted to Laboratory Informatics. Also, the Keck Graduate Institute in California offers a graduate degree with an emphasis on development of assays, instrumentation and data analysis tools required for clinical diagnostics, high-throughput screening, genotyping, microarray technologies, proteomics, imaging and other applications.\n\nAt least since 1875 there have been reports of automated devices for scientific investigation. These first devices were mostly built by scientists themselves in order to solve problems in the laboratory. After the second world war, companies started to provide automated equipment which had become more and more complex.\n\nAutomation was spreading in laboratories steadily through the 20th century, but then a revolution took place: in the early 1980s, the first fully automated laboratory was opened by Dr. Masahide Sasaki. In 1993, Dr. Rod Markin at the University of Nebraska Medical Center created one of the world's first clinical automated laboratory management systems. In the mid-1990s, he chaired a standards group called the Clinical Testing Automation Standards Steering Committee (CTASSC) of the American Association for Clinical Chemistry, which later evolved into an area committee of the Clinical and Laboratory Standards Institute. In 2004, the National Institutes of Health (NIH) and more than 300 nationally recognized leaders in academia, industry, government, and the public completed the NIH Roadmap to accelerate medical discovery to improve health. The NIH Roadmap clearly identifies technology development as a mission critical factor in the Molecular Libraries and Imaging Implementation Group (see the first theme - New Pathways to Discovery - at https://web.archive.org/web/20100611171315/http://nihroadmap.nih.gov/).\n\nDespite the undeniable success of Dr. Sasaki laboratory and others of the kind, the multi-million dollar cost of such laboratories has prevented most laboratories to adopt it. This is all more difficult because usually devices made by different manufactures cannot communicate with each other. However, recent advances based on the use of scripting languages like Autoit have made it possible the integration of equipment from different manufacturers. Using this approach, many low-cost electronic devices, including open-source devices, become compatible to common laboratory instruments.\n\nA large obstacle to the implementation of automation in laboratories has been its high cost. Many laboratory instruments are very expensive. This is justifiable in many cases, as such equipment can perform very specific tasks employing cutting-edge technology. However, there are devices employed in the laboratory that are not highly-technological but still are very expensive. This is the case of many automated devices, which perform tasks that could easily be done by simple and low-cost devices like simple robotic arms, universal (open-source) electronic modules, or 3D printers.\n\nSo far, using such low-cost devices together with laboratory equipment was considered to be very difficult. However, it has been demonstrated that such low cost devices can substitute without problems the standard machines used in laboratory. It can be anticipated that more laboratories will take advantage of this new reality as low-cost automation is very attractive for laboratories.\n\nThe technology that enables the integration of any machine regardless of their brand is scripting, more specifically, scripting involving the control of mouse clicks and keyboard entries, like AutoIt. By timing clicks and keyboard inputs, different software interfaces controlling different devices can be perfectly synchronized.\n\nBenchtop automation consists in the use of machines of reduced size compared to large automation units found in the most resource-rich laboratories. Benchtop automation are often flexible, meaning that they can deal with many different tasks. Since many laboratories do not need to employ full-scale automation, benchtop automation can be an attractive solution for them. Also, the low-cost devices presented in the previous subsection could easily be employed as benchtop solutions in many cases.\n\nAutomation is the use of control systems and information technologies to reduce the need \nfor human work in the production of goods and services\n\nLaboratory automation is the use of instrument and specimen processing equipment to \nperform clinical assay with only minimal involvement the technologist\n\nThere are several individual steps in the analysis process as a whole in a\nlaboratory such as:\n\n1. Identifying the patient\n2. Getting the correct sample\n3. Identifying and proper labeling of the sample\n4. Delivery of sample in proper storage condition and within time\n5. Preparation of sample for test\n6. Sample loading/aspirating\n7. Analysis\n8. Reporting—Entering the result manually \n9. Entering in register\n\nAutomation has a lot of benefits for the laboratory personnel.\n1. Reduces the workload\n2. Increases turnaround time (Saves time used per analysis)\n3. Increases total number of tests done in less time\n4. Eliminates repetition and monotony from human life so decreases human\nerror, improves accuracy\n5. Improves reproducibility (repeatability)\n6. Uses minimum amount of sample and reagent\n\n", "id": "5670694", "title": "Laboratory automation"}
{"url": "https://en.wikipedia.org/wiki?curid=871903", "text": "Laboratory robotics\n\nLaboratory robotics is the act of using robots in biology or chemistry labs. For example, pharmaceutical companies employ robots to move biological or chemical samples around to synthesize novel chemical entities or to test pharmaceutical value of existing chemical matter. Advanced laboratory robotics can be used to completely automate the process of science, as in the Robot Scientist project.\n\nLaboratory processes are suited for robotic automation as the processes are composed of repetitive movements (e.g. pick/place, liquid & solid additions, heating/cooling, mixing, shaking, testing). Many laboratory robots are commonly referred as autosamplers, as their main task is to provide continuous samples for analytical devices.\n\nThe first compact computer controlled robotic arms appeared in the early 1980s, and have continuously been employed in laboratories since then. These robots can be programmed to perform many different tasks, including sample preparation and handling.\n\nYet in the early 1980s, a group led by Dr. Masahide Sasaki, from Kochi Medical School, introduced the first fully automated laboratory employing several robotic arms working together with conveyor belts and automated analyzers. The success of Dr. Sasaki's pioneer efforts made other groups around the world to adopt the approach of Total Laboratory Automation (TLA).\n\nDespite the undeniable success of TLA, its multimillion-dollar cost prevented that most laboratories adopted it. Also, the lack of communication between different devices slowed down the development of automation solutions for different applications, while contributing to keeping costs high. Therefore, the industry attempted several times to develop standards that different vendors would follow in order to enable communication between their devices. However, the success of this approach has been only partial, as nowadays many laboratories still do not employ robots for many tasks due to their high costs.\n\nRecently, a different solution for the problem became available, enabling the use of inexpensive devices, including open-source hardware, to perform many different tasks in the laboratory. This solution is the use of scripting languages that can control mouse clicks and keyboard inputs, like AutoIt. This way, it is possible to integrate any device by any manufacturer as long as they are controlled by a computer, which is often the case.\n\nAnother important development in robotics which has important potential implications for laboratories is the arrival of robots that do not demand special training for their programming, like Baxter, the robot.\n\nThe high cost of many laboratory robots has inhibited their adoption. However, currently there are many robotic devices that have very low cost, and these could be employed to do some jobs in a laboratory. For example, a low-cost robotic arm was employed to perform several different kinds of water analysis, without loss of performance compared to much more expensive autosamplers. Alternatively, the autosampler of a device can be used with another device, thus avoiding the need for purchasing a different autosampler or hiring a technician for doing the job. The key aspects to achieve low-cost in laboratory robotics are 1) the use of low-cost robots, which become more and more common, and 2) the use of scripting, which enables compatibility between robots and other analytical equipment.\n\nBiological and chemical samples, in either liquid or solid state, are stored in vials, plates or tubes. Often, they need to be frozen and/or sealed to avoid contamination or to retain their biological and/or chemical properties. Specifically, the life science industry has standardized on a plate format, known as the microtiter plate, to store such samples.\n\nThe microtiter plate standard was formalized by the Society for Biomolecular Screening in 1996. It typically has 96, 384 or even 1536 sample wells arranged in a 2:3 rectangular matrix. The standard governs well dimensions (e.g. diameter, spacing and depth) as well as plate properties (e.g. dimensions and rigidity).\n\nA number of companies have developed robots to specifically handle SBS microplates. Such robots may be liquid handlers which aspirates or dispenses liquid samples from and to these plates, or \"plate movers\" which transport them between instruments.\n\nOther companies have pushed integration even further: on top of interfacing to the specific consumables used in biology, some robots (Andrew by Andrew Alliance, see picture) have been designed with the capability of interfacing to volumetric pipettes used by biologists and technical staff. Essentially, all the manual activity of liquid handling can be performed automatically, allowing humans spending their time in more conceptual activities.\n\nInstrument companies have designed plate readers which can carry out detect specific biological, chemical or physical events in samples stored in these plates. These readers typically use optical and/or computer vision techniques to evaluate the contents of the microtiter plate wells.\n\nOne of the first applications of robotics in biology was peptide and oligonucleotide synthesis. One early example is the polymerase chain reaction (PCR) which is able to amplify DNA strands using a thermal cycler to micromanage DNA synthesis by adjusting temperature using a pre-made computer program. Since then, automated synthesis has been applied to organic chemistry and expanded into three categories: reaction-block systems, robot-arm systems, and non-robotic fluidic systems. The primary objective of any automated workbench is high-throughput processes and cost reduction. This allows a synthetic laboratory to operate with a fewer number of people working more efficiently.\n\nOne major area where automated synthesis has been applied is structure determination in pharmaceutical research. Processes such as NMR and HPLC-MS can now have sample preparation done by robotic arm. Additionally, structural protein analysis can be done automatically using a combination of NMR and X-ray crystallography. Crystallization often takes hundreds to thousands of experiments to create a protein crystal suitable for X-ray crystallography. An automated micropipet machine can allow nearly a million different crystals to be created at once, and analyzed via X-ray crystallography.\n\nRobotics have applications with Combinatorial Chemistry which has great impact on the pharmaceutical industry. The use of robotics has allowed for the use of much smaller reagent quantities and mass expansion of chemical libraries. The \"parallel synthesis\" method can be improved upon with automation. The main disadvantage to \"parallel-synthesis\" is the amount of time it takes to develop a library, automation is typically applied to make this process more efficient.\n\nThe main types of automation are classified by the type of solid-phase substrates, the methods for adding and removing reagents, and design of reaction chambers. Polymer resins may be used as a substrate for solid-phase. It is not a true combinatorial method in the sense that \"split-mix\" where a peptide compound is split into different groups and reacted with different compounds. This is then mixed back together split into more groups and each groups is reacted with a different compound. Instead the \"parallel-synthesis\" method does not mix, but reacts different groups of the same peptide with different compounds and allows for the identification of the individual compound on each solid support. A popular method implemented is the reaction block system due to its relative low cost and higher output of new compounds compared to other \"parallel-synthesis\" methods. Parallel-Synthesis was developed by Mario Geysen and his colleagues and is not a true type of combinatorial synthesis, but can be incorporated into a combinatorial synthesis. This group synthesized 96 peptides on plastic pins coated with a solid support for the solid phase peptide synthesis. This method uses a rectangular block moved by a robot so that reagents can be pipetted by a robotic pipetting system. This block is separated into wells which the individual reactions take place. These compounds are later cleaved from the solid-phase of the well for further analysis. Another method is the closed reactor system which uses a completely closed off reaction vessel with a series of fixed connections to dispense. Though the produce fewer number of compounds than other methods, its main advantage is the control over the reagents and reaction conditions. Early closed reaction systems were developed for peptide synthesis which required variations in temperature and a diverse range of reagents. Some closed reactor system robots have a temperature range of 200 °C and over 150 reagents.\n\nSimulated distillation, a type of gas chromatography testing method used in the petroleum, can be automated via robotics. An older method used a system called ORCA (Optimized Robot for Chemical Analysis)was used for the analysis of petroleum samples by simulated distillation (SIMDIS). ORCA has allowed for shorter analysis times and has reduced maximum temperature needed to elute compounds. One major advantage of automating purification is the scale at which separations can be done. Using microprocessors, ion-exchange separation can be conducted on a nanoliter scale in a short period of time.\n\nRobotics have been implemented in liquid-liquid extraction (LLE) to streamline the process of preparing biological samples using 96-well plates. This is an alternative method to solid-phase extraction methods and protein precipitation, which has the advantage of being more reproducible and robotic assistance has made LLE comparable in speed to solid phase extraction. The robotics used for LLE can perform an entire extraction with quantities in the microliter scale and performing the extraction in as little as ten minutes.\n\nOne of the advantages to automation is faster processing, but it is not necessarily faster than a human operator. Repeatability and reproducibility are improved as automated systems as less likely to have variances in reagent quantities and less likely to have variances in reaction conditions. Typically productivity is increased since human constraints, such as time constraints, are no longer a factor. Efficiency is generally improved as robots can work continuously and reduce the amount of reagents used to perform a reaction. Also there is a reduction in material waste. Automation can also establish safer working environments since hazardous compounds do not have to be handled. Additionally automation allows staff to focus on other tasks that are not repetitive.\n\nTypically the cost of a single synthesis or sample assessment are expensive to set up and start up cost for automation can be expensive (but see above \"Low-cost laboratory robotics\"). Many techniques have not been developed for automation yet. Additionally there is difficultly automating instances where visual analysis, recognition, or comparison is required such as color changes. This also leads to the analysis being limited by available sensory inputs. One potential disadvantage is an increases job shortages as automation may replace staff members who do tasks easily replicated by a robot. Some systems require the use of programming languages such as C++ or Visual Basic to run more complicated tasks.\n", "id": "871903", "title": "Laboratory robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=184570", "text": "Microbotics\n\nMicrobotics (or microrobotics) is the field of miniature robotics, in particular mobile robots with characteristic dimensions less than 1 mm. The term can also be used for robots capable of handling micrometer size components.\n\nMicrobots were born thanks to the appearance of the microcontroller in the last decade of the 20th century, and the appearance of miniature mechanical systems on silicon (MEMS), although many microbots do not use silicon for mechanical components other than sensors. The earliest research and conceptual design of such small robots was conducted in the early 1970s in (then) classified research for U.S. intelligence agencies. Applications envisioned at that time included prisoner of war rescue assistance and electronic intercept missions. The underlying miniaturization support technologies were not fully developed at that time, so that progress in prototype development was not immediately forthcoming from this early set of calculations and concept design. As of 2008, the smallest microrobots use a Scratch Drive Actuator.\n\nThe development of wireless connections, especially Wi-Fi (i.e. in domotic networks) has greatly increased the communication capacity of microbots, and consequently their ability to coordinate with other microbots to carry out more complex tasks. Indeed, much recent research has focused on microbot communication, including a 1,024 robot swarm at Harvard University that assembles itself into various shapes; and manufacturing microbots at SRI International for DARPA's \"MicroFactory for Macro Products\" program that can build lightweight, high-strength structures.\n\nWhile the 'micro' prefix has been used subjectively to mean small, standardizing on length scales avoids confusion. Thus a nanorobot would have characteristic dimensions at or below 1 micrometer, or manipulate components on the 1 to 1000 nm size range. A microrobot would have characteristic dimensions less than 1 millimeter, a millirobot would have dimensions less than a cm, a minirobot would have dimensions less than , and a small robot would have dimensions less than .\n\nDue to their small size, microbots are potentially very cheap, and could be used in large numbers (swarm robotics) to explore environments which are too small or too dangerous for people or larger robots. It is expected that microbots will be useful in applications such as looking for survivors in collapsed buildings after an earthquake, or crawling through the digestive tract. What microbots lack in brawn or computational power, they can make up for by using large numbers, as in swarms of microbots.\n\nThe way microrobots move around is a function of their purpose and necessary size. At submicron sizes, the physical world demands rather bizarre ways of getting around. The Reynolds number for airborne robots is close to unity; the viscous forces dominate the inertial forces, so “flying” could use the viscosity of air, rather than Bernoulli's principle of lift. Robots moving through fluids may require rotating flagella like the motile form of E. coli. Hopping is stealthy and energy-efficient; it allows the robot to negotiate the surfaces of a variety of terrains. Pioneering calculations (Solem 1994) examined possible behaviours based on physical realities.\n\nOne of the major challenges in developing a microrobot is to achieve motion using a very limited power supply. The microrobots can use a small lightweight battery source like a coin cell or can scavenge power from the surrounding environment in the form of vibration or light energy. Microrobots are also now using biological motors as power sources, such as flagellated \"Serratia marcescens\", to draw chemical power from the surrounding fluid to actuate the robotic device. These biorobots can be directly controlled by stimuli such as chemotaxis or galvanotaxis with several control schemes available. A popular alternative to an on-board battery is to power the robots using externally induced power. Examples include the use of electromagnetic fields, ultrasound and light to activate and control micro robots.\n", "id": "184570", "title": "Microbotics"}
{"url": "https://en.wikipedia.org/wiki?curid=1050195", "text": "Evolutionary robotics\n\nEvolutionary robotics (ER) is a methodology that uses evolutionary computation to develop controllers for autonomous robots. Algorithms in ER frequently operate on populations of candidate controllers, initially selected from some distribution. This population is then repeatedly modified according to a fitness function. In the case of genetic algorithms (or \"GAs\"), a common method in evolutionary computation, the population of candidate controllers is repeatedly grown according to crossover, mutation and other GA operators\nand then culled according to the fitness function. The candidate controllers used in ER applications may be drawn from some subset of the set of artificial neural networks, although some applications (including SAMUEL, developed at the Naval Center for Applied Research in Artificial Intelligence) use collections of \"IF THEN ELSE\" rules as the constituent parts of an individual controller. It is theoretically possible to use any set of symbolic formulations of a control law (sometimes called a policy in the machine learning community) as the space of possible candidate controllers. Artificial neural networks can also be used for robot learning outside the context of evolutionary robotics. In particular, other forms of reinforcement learning can be used for learning robot controllers.\n\nDevelopmental robotics is related to, but differs from, evolutionary robotics. ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time.\n\nThe foundation of ER was laid with work at the national research council in Rome in the 90s, but the initial idea of encoding a robot control system into a genome and have artificial evolution improve on it dates back to the late 80s.\n\nIn 1992 and 1993 three research groups, one surrounding Floreano and Mondada at the EPFL in Lausanne and a second involving Cliff, Harvey, and Husbands from COGS at the University of Sussex and a third from the University of Southern California involved M. Anthony Lewis and Andrew H Fagg reported promising results from experiments on artificial evolution of autonomous robots. The success of this early research triggered a wave of activity in labs around the world trying to harness the potential of the approach.\n\nLately, the difficulty in \"scaling up\" the complexity of the robot tasks has shifted attention somewhat towards the theoretical end of the field rather than the engineering end.\n\nEvolutionary robotics is done with many different objectives, often at the same time. These include creating useful controllers for real-world robot tasks, exploring the intricacies of evolutionary theory (such as the Baldwin effect), reproducing psychological phenomena, and finding out about biological neural networks by studying artificial ones. Creating controllers via artificial evolution requires a large number of evaluations of a large population. This is very time consuming, which is one of the reasons why controller evolution is usually done in software. Also, initial random controllers may exhibit potentially harmful behaviour, such as repeatedly crashing into a wall, which may damage the robot. Transferring controllers evolved in simulation to physical robots is very difficult and a major challenge in using the ER approach. The reason is that evolution is free to explore all possibilities to obtain a high fitness, including any inaccuracies of the simulation . This need for a large number of evaluations, requiring fast yet accurate computer simulations, is one of the limiting factors of the ER approach .\n\nIn rare cases, evolutionary computation may be used to design the physical structure of the robot, in addition to the controller. One of the most notable examples of this was Karl Sims' demo for Thinking Machines Corporation.\n\nMany of the commonly used machine learning algorithms require a set of training examples consisting of both a hypothetical input and a desired answer. In many robot learning applications the desired answer is an action for the robot to take. These actions are usually not known explicitly a priori, instead the robot can, at best, receive a value indicating the success or failure of a given action taken. Evolutionary algorithms are natural solutions to this sort of problem framework, as the fitness function need only encode the success or failure of a given controller, rather than the precise actions the controller should have taken. An alternative to the use of evolutionary computation in robot learning is the use of other forms of reinforcement learning, such as q-learning, to learn the fitness of any particular action, and then use predicted fitness values indirectly to create a controller.\n\n\n\n\n\n", "id": "1050195", "title": "Evolutionary robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=1921364", "text": "Frankenstein complex\n\nIn Isaac Asimov's robot novels, Frankenstein complex is a term that he coined for the fear of mechanical men.\n\nSome of Asimov's science fiction short stories and novels predict that this phobia will become strongest and most widespread when being directed against \"mechanical men\" that most-closely resemble human beings (\"see android\"), but it is also present on a lower level against robots that are plainly electromechanical automatons. \n\nThe \"Frankenstein Complex\" is similar in many respects to Masahiro Mori's uncanny valley hypothesis. \n\nThe name, \"Frankenstein Complex\", derives from the name of Victor Frankenstein in the novel \"Frankenstein; or, The Modern Prometheus\" by Mary Shelley in about the year 1818. In Shelley's story, Frankenstein created an intelligent, somewhat superhuman being. He finds that his creation is horrifying to behold, and he abandons it. This ultimately leads to Victor's death at the conclusion of a vendetta between himself and his embittered creation.\n\nThe general attitude of the public towards robots in much of Dr. Asimov's fiction is fear and suspicion: ordinary people fear that robots will either replace them or dominate them, although dominance is impossible under the specifications of the Three Laws of Robotics, the first of which is:\nThe fictitious earthly public does not generally listen to this logic, but rather they listen to their fears. \"I, Robot\"s short story \"Little Lost Robot\" is an example of the \"fear of robots\" that Asimov described.\n\nIn Asimov's robot novels, the \"Frankenstein Complex\" is a major problem for roboticists and robot manufacturers. They do all they can to calm the public and show that robots are harmless, sometimes even hiding the truth because they think that the public would misunderstand it and take it to the extreme. The fear by the public and the response of the manufacturers is an example of the theme of paternalism, the dread of paternalism, and the conflicts that arise from it in Asimov's fiction. \n\n\n", "id": "1921364", "title": "Frankenstein complex"}
{"url": "https://en.wikipedia.org/wiki?curid=266747", "text": "Wired intelligence\n\nThe term wired intelligence refers to a robot that has no programmed microprocessor. Instead, the robot has a particular connection of wires and analog electronics between its sensors and motors that gives it seemingly intelligent actions. These actions can be complex enough to create a quadruped robot that seeks out the brightest light source.\n\nThe groundwork for these theories is laid in \"Vehicles: Experiments in Synthetic Psychology\" by Valentino Braitenberg.\n\n", "id": "266747", "title": "Wired intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=27792533", "text": "Bowler Communications System\n\nThe Bowler Communications System is an open protocol developed by Neuron Robotics for simplified communications between components in cyber-physical systems.\n\n", "id": "27792533", "title": "Bowler Communications System"}
{"url": "https://en.wikipedia.org/wiki?curid=24666570", "text": "Morphogenetic robotics\n\nMorphogenetic robotics generally refers to the methodologies that address challenges in robotics inspired by biological morphogenesis. \n\nMorphogenetic robotics is related to, but differs from, epigenetic robotics. The main difference between morphogenetic robotics and epigenetic robotics is that the former focuses on self-organization, self-reconfiguration, self-assembly and self-adaptive control of robots using genetic and cellular mechanisms inspired from biological early morphogenesis (\"activity-independent development\"), during which the body and controller of the organisms are developed simultaneously, whereas the latter emphasizes the development of robots' cognitive capabilities, such as language, emotion and social skills, through experience during the lifetime (\"activity-dependent development\"). Morphogenetic robotics is closely connected to developmental biology and systems biology, whilst epigenetic robotics is related to developmental cognitive neuroscience emerged from cognitive science, developmental psychology and neuroscience.\n\nMorphogenetic robotics includes, but is not limited to the following main topics:\n\n\n", "id": "24666570", "title": "Morphogenetic robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=10640172", "text": "Robotics conventions\n\nThere are many conventions used in the robotics research field. This article summarises these conventions.\n\nLines are very important in robotics because:\n\nA line formula_1 is completely defined by the ordered set of two vectors:\n\nEach point formula_5 on the line is given a parameter value formula_6 that satisfies:\nformula_7. The parameter t is unique once formula_2 and formula_4 are chosen. The representation formula_1 is not minimal, because it uses six parameters for only four degrees of freedom. The following two constraints apply:\n\nArthur Cayley and Julius Plücker introduced an alternative representation using two free vectors. This representation was finally named after Plücker.\n\nA line representation is minimal if it uses four parameters, which is the minimum needed to represent all possible lines in the Euclidean Space (E³).\n\nJaques Denavit and Richard S. Hartenberg presented the first minimal representation for a line which is now widely used. The common normal between two lines was the main geometric concept that allowed Denavit and Hartenberg to find a minimal representation. Engineers use the Denavit–Hartenberg convention(D–H) to help them describe the positions of links and joints unambiguously. Every link gets its own coordinate system. There are a few rules to consider in choosing the coordinate system:\n\nOnce the coordinate frames are determined, inter-link transformations are uniquely described by the following four parameters:\n\nThe Hayati–Roberts line representation, denoted formula_45, is another minimal line representation, with parameters:\n\nThis representation is unique for a directed line. The coordinate singularities are different from the DH singularities: it has singularities if the line becomes parallel to either the formula_48 or formula_49 axis of the world frame.\n\n\n\n", "id": "10640172", "title": "Robotics conventions"}
{"url": "https://en.wikipedia.org/wiki?curid=32678", "text": "Vocoder\n\nA vocoder (, a portmanteau of \"voice encoder\") is a category of voice codec that analyzes and synthesizes the human voice signal for audio data compression, multiplexing, voice encryption, voice transformation, etc.\n\nThe earliest type of vocoder, the \"channel vocoder\", was originally developed as a speech coder for telecommunications applications in the 1930s, the idea being to code speech in order to reduce bandwidth (i.e. audio data compression) for multiplexing transmission. In the channel vocoder algorithm, among the two components of an analytic signal, considering only the amplitude component and simply ignoring the phase component tends to result in an unclear voice; on methods for rectifying this, see phase vocoder.\n\nIn the encoder, the input is passed through a multiband filter, then each band is passed through an envelope follower, and the control signals from the envelope followers are transmitted to the decoder. The decoder applies these (amplitude) control signals to corresponding filters for re-synthesis. Since these control signals change only slowly compared to the original speech waveform, the bandwidth required to transmit speech can be reduced. This allows more speech channels to share a single communication channel, such as a radio channel or a submarine cable (i.e. multiplexing).\n\nBy encrypting the control signals, voice transmission can be secured against interception. Its primary use in this fashion is for secure radio communication. The advantage of this method of encryption is that none of the original signal is sent, only envelopes of the bandpass filters. The receiving unit needs to be set up in the same filter configuration to re-synthesize a version of the original signal spectrum.\n\nThe vocoder has also been used extensively as an electronic musical instrument (see Uses in music). The decoder portion of the vocoder, called a voder, can be used independently for speech synthesis (see History).\n\nThe human voice consists of sounds generated by the opening and closing of the glottis by the vocal cords, which produces a periodic waveform with many harmonics. This basic sound is then filtered by the nose and throat (a complicated resonant piping system) to produce differences in harmonic content (formants) in a controlled way, creating the wide variety of sounds used in speech. There is another set of sounds, known as the unvoiced and plosive sounds, which are created or modified by the mouth in different fashions.\n\nThe vocoder examines speech by measuring how its spectral characteristics change over time. This results in a series of signals representing these modified frequencies at any particular time as the user speaks. In simple terms, the signal is split into a number of frequency bands (the larger this number, the more accurate the analysis) and the level of signal present at each frequency band gives the instantaneous representation of the spectral energy content.\nThus, the vocoder dramatically reduces the amount of information needed to store speech, from a complete recording to a series of numbers. To recreate speech, the vocoder simply reverses the process, processing a broadband noise source by passing it through a stage that filters the frequency content based on the originally recorded series of numbers.\nInformation about the instantaneous frequency of the original voice signal (as distinct from its spectral characteristic) is discarded; it was not important to preserve this for the purposes of the vocoder's original use as an encryption aid. It is this \"dehumanizing\" aspect of the vocoding process that has made it useful in creating special voice effects in popular music and audio entertainment.\n\nSince the vocoder process sends only the parameters of the vocal model over the communication link, instead of a point-by-point recreation of the waveform, the bandwidth required to transmit speech can be reduced significantly.\n\nAnalog vocoders typically analyze an incoming signal by splitting the signal into a number of tuned frequency bands or ranges. A modulator and carrier signal are sent through a series of these tuned bandpass filters. In the example of a typical robot voice, the modulator is a microphone and the carrier is noise or a sawtooth waveform. There are usually between eight and 20 bands.\n\nThe amplitude of the modulator for each of the individual analysis bands generates a voltage that is used to control amplifiers for each of the corresponding carrier bands. The result is that frequency components of the modulating signal are mapped onto the carrier signal as discrete amplitude changes in each of the frequency bands. (See Modulation.)\n\nOften there is an unvoiced band or sibilance channel. This is for frequencies that are outside the analysis bands for typical speech but are still important in speech. Examples are words that start with the letters s, f, ch or any other sibilant sound. These can be mixed with the carrier output to increase clarity. The result is recognizable speech, although somewhat \"mechanical\" sounding. Vocoders often include a second system for generating unvoiced sounds, using a noise generator instead of the fundamental frequency.\n\nThe development of a vocoder was started in 1928 by Bell Labs engineer Homer Dudley, who was granted patents for it, on March 21, 1939, and on Nov 16, 1937.\n\nThen, to show the speech synthesis ability of its decoder part, the Voder (Voice Operating Demonstrator, ), was introduced to the public at the AT&T building at the 1939–1940 New York World's Fair. The Voder consisted of a switchable pair of electronic oscillator and noise generator as a sound source of pitched tone and hiss, 10-band resonator filters with variable-gain amplifiers as a vocal tract, and the manual controllers including a set of pressure-sensitive keys for filter control, and a foot pedal for pitch control of tone. The filters controlled by keys convert the tone and the hiss into vowels, consonants, and inflections. This was a complex machine to operate, but with a skilled operator could produce recognizable speech.\n\nDudley's vocoder was used in the SIGSALY system, which was built by Bell Labs engineers in 1943. SIGSALY was used for encrypted high-level voice communications during World War II. Later work in this field was conducted by James Flanagan.\n\n\nEven with the need to record several frequencies, and additional unvoiced sounds, the compression of vocoder systems is impressive. Standard speech-recording systems capture frequencies from about 500 Hz to 3,400 Hz, where most of the frequencies used in speech lie, typically using a sampling rate of 8 kHz (slightly greater than the Nyquist rate). The sampling resolution is typically at least 12 or more bits per sample resolution (16 is standard), for a final data rate in the range of 96–128 kbit/s, but a good vocoder can provide a reasonably good simulation of voice with as little as 2.4 kbit/s of data.\n\n\"Toll quality\" voice coders, such as ITU G.729, are used in many telephone networks. G.729 in particular has a final data rate of 8 kbit/s with superb voice quality. G.723 achieves slightly worse quality at data rates of 5.3 kbit/s and 6.4 kbit/s. Many voice vocoder systems use lower data rates, but below 5 kbit/s voice quality begins to drop rapidly.\n\nSeveral vocoder systems are used in NSA encryption systems:\n\nVocoders are also currently used in developing psychophysics, linguistics, computational neuroscience and cochlear implant research.\n\nModern vocoders that are used in communication equipment and in voice storage devices today are based on the following algorithms:\n\nSince the late 1970s, most non-musical vocoders have been implemented using linear prediction, whereby the target signal's spectral envelope (formant) is estimated by an all-pole IIR filter. In linear prediction coding, the all-pole filter replaces the bandpass filter bank of its predecessor and is used at the encoder to \"whiten\" the signal (i.e., flatten the spectrum) and again at the decoder to re-apply the spectral shape of the target speech signal.\n\nOne advantage of this type of filtering is that the location of the linear predictor's spectral peaks is entirely determined by the target signal, and can be as precise as allowed by the time period to be filtered. This is in contrast with vocoders realized using fixed-width filter banks, where spectral peaks can generally only be determined to be within the scope of a given frequency band. LP filtering also has disadvantages in that signals with a large number of constituent frequencies may exceed the number of frequencies that can be represented by the linear prediction filter. This restriction is the primary reason that LP coding is almost always used in tandem with other methods in high-compression voice coders.\n\nWaveform-Interpolative (WI) vocoder was developed in AT&T Bell Laboratories around 1995 by W.B. Kleijn, and subsequently a low- complexity version was developed by AT&T for the DoD secure vocoder competition. Notable enhancements to the WI coder were made at the University of California, Santa Barbara. AT&T holds the core patents related to WI, and other institutes hold additional patents. Using these patents as a part of WI coder implementation requires licensing from all IPR holders.\n\nFor musical applications, a source of musical sounds is used as the carrier, instead of extracting the fundamental frequency. For instance, one could use the sound of a synthesizer as the input to the filter bank, a technique that became popular in the 1970s.\n\nWerner Meyer-Eppler, a German scientist with a special interest in electronic voice synthesis, published a thesis in 1948 on electronic music and speech synthesis from the viewpoint of sound synthesis. Later he was instrumental in the founding of the Studio for Electronic Music of WDR in Cologne, in 1951.\nOne of the first attempts to use a vocoder in creating music was the \"Siemens Synthesizer\" at the Siemens Studio for Electronic Music, developed between 1956 and 1959.\n\nIn 1968, Robert Moog developed one of the first solid-state musical vocoders for the electronic music studio of the University at Buffalo.\n\nIn 1968, Bruce Haack built a prototype vocoder, named \"Farad\" after Michael Faraday. It was first featured on \"The Electronic Record For Children\" released in 1969 and then on his rock album \"The Electric Lucifer\" released in 1970.\n\nIn 1970, Wendy Carlos and Robert Moog built another musical vocoder, a ten-band device inspired by the vocoder designs of Homer Dudley. It was originally called a spectrum encoder-decoder, and later referred to simply as a vocoder. The carrier signal came from a Moog modular synthesizer, and the modulator from a microphone input. The output of the ten-band vocoder was fairly intelligible, but relied on specially articulated speech. Later improved vocoders use a high-pass filter to let some sibilance through from the microphone; this ruins the device for its original speech-coding application, but it makes the \"talking synthesizer\" effect much more intelligible.\n\nCarlos and Moog's vocoder was featured in several recordings, including the soundtrack to Stanley Kubrick's \"A Clockwork Orange\", in which the vocoder sang the vocal part of Beethoven's \"Ninth Symphony\". Also in the soundtrack was a piece called \"Timesteps\", which featured the vocoder in two sections. \"Timesteps\" was originally intended as merely an introduction to vocoders for the \"timid listener\", but Kubrick chose to include the piece on the soundtrack, much to the surprise of Wendy Carlos.\n\nKraftwerk's \"Autobahn\" (1974) was one of the first successful albums to feature vocoder vocals. Another of the early songs to feature a vocoder was \"The Raven\" on the 1976 album \"Tales of Mystery and Imagination\" by progressive rock band The Alan Parsons Project; the vocoder was also used on later albums such as \"I Robot\". Following Alan Parsons' example, vocoders began to appear in pop music in the late 1970s, for example, on disco recordings. Jeff Lynne of Electric Light Orchestra used the vocoder in several albums, such as \"Time\" (featuring the Roland VP-330 Plus MkI). ELO songs such as \"Mr. Blue Sky\" and \"Sweet Talkin' Woman\", both from \"Out of the Blue\" (1977), use the vocoder extensively, as does \"The Diary of Horace Wimp\" from the album \"Discovery\" (1979). Featured on the album are the EMS Vocoder 2000W MkI, and the EMS Vocoder (-System) 2000 (W or B, MkI or II).\nGiorgio Moroder made extensive use of the vocoder on the 1975 album \"Einzelgänger\" and 1977 album \"From Here to Eternity\".\n\nAnother example of its use is Pink Floyd's song \"Dogs\", from their album \"Animals\" (1977), where the band put the sound of a barking dog through the device.\n\nA vocoder was used by Jo Partridge to produce the Martian's unearthly exultations of \"Ulla\" in the 1978 concept album \"Jeff Wayne's Musical Version of The War of the Worlds\".\n\nSince 1979, a vocoder has been used at the start and end of the Main Street Electrical Parade at Disneyland and Walt Disney World.\n\nPhil Collins used a vocoder to provide a vocal effect for his 1981 international hit single \"In the Air Tonight\".\n\nVocoders are often used to create the sound of a robot talking, as in the Styx song \"Mr. Roboto\" (1983).\n\nRoger Taylor of Queen used the Vocoder on two songs on Queen's eleventh studio album \"The Works\", \"Radio Ga Ga\" and \"Machines (Or 'Back to Humans')\". He also used the device on the song \"I Cry For You\" from his solo album \"Strange Frontier\".\n\nVocoders have appeared on pop recordings from time to time ever since, most often simply as a special effect rather than a featured aspect of the work. However, many experimental electronic artists of the new-age music genre often utilize vocoder in a more comprehensive manner in specific works, such as Jean Michel Jarre (on \"Zoolook\", 1984) and Mike Oldfield (on \"QE2\", 1980 and \"Five Miles Out\", 1982).\n\nVocoder module and use by M. Oldfield can be clearly seen on his \"Live At Montreux 1981\" DVD (Track \"Sheba\").\n\nThere are also some artists who have made vocoders an essential part of their music, overall or during an extended phase. Examples include the German synthpop group Kraftwerk, Stevie Wonder (\"Send One Your Love\", \"A Seed's a Star\") and jazz/fusion keyboardist Herbie Hancock during his late 1970s period. In 1982 Neil Young used a Sennheiser Vocoder VSM201 on six of the nine tracks on \"Trans\". Perhaps the most heard, yet often unrecognized, example of the use of a vocoder in popular music, is on Michael Jackson's 1982 album \"Thriller\", in the song \"P.Y.T. (Pretty Young Thing)\". During the first few seconds of the song, the background voicings \"ooh-ooh, ooh, ooh\", behind his spoken words, exemplify the heavily modulated sound of his voice through a Vocoder. The bridge features a vocoder as well (\"Pretty young thing/You make me sing\"), courtesy of session musician Michael Boddicker.\n\nColdplay have used a vocoder in some of their songs. For example, in \"Major Minus\" and \"Hurts Like Heaven\", both from the album \"Mylo Xyloto\" (2011), Chris Martin's vocals are mostly vocoder-processed. \"Midnight\", from \"Ghost Stories\" (2014), also features Martin singing through a vocoder. The hidden track \"X Marks The Spot\" from \"A Head Full of Dreams\" has also been recorded through a vocoder.\n\nNoisecore band Atari Teenage Riot have used vocoders in variety of their songs and live performances such as Live at the Brixton Academy (2002) alongside other digital audio technology both old and new.\n\nAmong the most consistent uses of vocoder in emulating the human voice are Daft Punk, who have used this instrument from their first album \"Homework\" (1997) to their latest work \"Random Access Memories\" (2013) and consider the convergence of technological and human voice \"the identity of their musical project\". For instance, the lyrics of \"Around the World\" (1997) are integrally vocoder-processed, \"Get Lucky\" (2013) features a mix of natural and processed human voices, and \"Instant Crush\" (2013) features Julian Casablancas singing into a vocoder.\n\n\"Robot voices\" became a recurring element in popular music during the 20th century. Apart from vocoders, several other methods of producing variations on this effect include: the Sonovox, Talk box, and Auto-Tune, linear prediction vocoders, speech synthesis, ring modulation and comb filter.\nVocoders are used in television production, filmmaking and games, usually for robots or talking computers.\nThe robot voices of the Cylons in \"Battlestar Galactica\" were created with an EMS Vocoder 2000. The 1980 version of the \"Doctor Who\" theme, as arranged and recorded by Peter Howell, has a section of the main melody generated by a Roland SVC-350 Vocoder. A vocoder was also used to create the iconic voice of Soundwave, a character from the Transformers series.\n\nIn 1967 the Supermarionation series \"Captain Scarlet and the Mysterons\" to supply the deep, eerie threatening voice of the disembodied Mysterons and well as the bass tones for the Spectrum agent Captain Black when he is seized under their telepathic control. It was also used in the closing credits theme, of the first 13 episodes to provide the synthetic repetition of the words \"Captain Scarlet\".\n\nIn 1972, Isao Tomita's first electronic music album \"Electric Samurai: Switched on Rock\" was an early attempt at applying speech synthesis technique in electronic rock and pop music. The album featured electronic renditions of contemporary rock and pop songs, while utilizing synthesized voices in place of human voices. In 1974, he utilized synthesized voices in his popular classical music album \"Snowflakes are Dancing\", which became a worldwide success and helped to popularize electronic music. Emerson, Lake and Palmer used it for the album \"Brain Salad Surgery\" (1973).\n\n\n", "id": "32678", "title": "Vocoder"}
{"url": "https://en.wikipedia.org/wiki?curid=508896", "text": "Behavior-based robotics\n\nBehavior-based robotics or behavioral robotics is an approach in robotics that focuses on robots that are able to exhibit complex-appearing behaviors despite little internal variable state to model its immediate environment, mostly gradually correcting its actions via sensory-motor links.\n\nBehavior-based robotics sets itself apart from traditional artificial intelligence by using biological systems as a model. Classic artificial intelligence typically uses a set of steps to solve problems, it follows a path based on internal representations of events compared to the behavior-based approach. Rather than use preset calculations to tackle a situation, behavior-based robotics relies on adaptability. This advancement has allowed behavior-based robotics to become commonplace in researching and data gathering.\n\nMost behavior-based systems are also reactive, which means they need no programming of internal representations of what a chair looks like, or what kind of surface the robot is moving on. Instead all the information is gleaned from the input of the robot's sensors. The robot uses that information to gradually correct its actions according to the changes in immediate environment.\n\nBehavior-based robots (BBR) usually show more biological-appearing actions than their computing-intensive counterparts, which are very deliberate in their actions. A BBR often makes mistakes, repeats actions, and appears confused, but can also show the anthropomorphic quality of tenacity. Comparisons between BBRs and insects are frequent because of these actions. BBRs are sometimes considered examples of weak artificial intelligence, although some have claimed they are models of all intelligence.\n\nMost behavior-based robots are programmed with a basic set of features to start them off. They are given a behavioral repertoire to work with dictating what behaviors to use and when, obstacle avoidance and battery charging can provide a foundation to help the robots learn and succeed. Rather than build world models, behavior-based robots simply react to their environment and problems within that environment. They draw upon internal knowledge learned from their past experiences combined with their basic behaviors to resolve problems.\n\nThe school of behavior-based robots owes much to work undertaken in the 1980s at the Massachusetts Institute of Technology by Rodney Brooks, who with students and colleagues built a series of wheeled and legged robots utilizing the subsumption architecture. Brooks' papers, often written with lighthearted titles such as \"\"Planning is just a way of avoiding figuring out what to do next\"\", the anthropomorphic qualities of his robots, and the relatively low cost of developing such robots, popularized the behavior-based approach.\n\nBrooks' work builds -whether by accident or not- on two prior milestones in the behavior-based approach. In the 1950s, W. Grey Walter, an English scientist with a background in neurological research, built a pair of vacuum tube-based robots that were exhibited at the 1951 Festival of Britain, and which have simple but effective behavior-based control systems.\n\nThe second milestone is Valentino Braitenberg's 1984 book, \"\"Vehicles – Experiments in Synthetic Psychology\"\" (MIT Press). He describes a series of thought experiments demonstrating how simply wired sensor/motor connections can result in some complex-appearing behaviors such as fear and love.\n\nLater work in BBR is from the BEAM robotics community, which has built upon the work of Mark Tilden. Tilden was inspired by the reduction in the computational power needed for walking mechanisms from Brooks' experiments (which used one microcontroller for each leg), and further reduced the computational requirements to that of logic chips, transistor-based electronics, and analog circuit design.\n\nA different direction of development includes extensions of behavior-based robotics to multi-robot teams. The focus in this work is on developing simple generic mechanisms that result in coordinated group behavior, either implicitly or explicitly.\n\n\n\n", "id": "508896", "title": "Behavior-based robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=30878100", "text": "Robot Interaction Language\n\nThe Robot Interaction Language (ROILA) is the first spoken language created specifically for talking to robots. ROILA is being developed by the Department of Industrial Design at Eindhoven University of Technology. The major goals of ROILA are that it should be easily learnable by the user, and optimized for efficient recognition by robots. ROILA has a syntax that allows it to be useful for many different kinds of robots, including the Roomba, and Lego Mindstorms NXT. ROILA is free for anybody to use and to contribute to, as the team has released all documentation and tools under a Creative Commons license.\n\nROILA was developed due to the need for a unified language for humans to speak to robots. The designers performed research into the ability of robots to recognize and interpret natural languages. They discovered that natural languages can be very confusing for robots to interpret sometimes, due to elements such as homophones and tenses. Based on this research, the team set out to create a genetic algorithm that would generate an artificial vocabulary in a way that would be easy for a human to pronounce. The algorithm used the most common phonemes from the most popular natural languages and created easy to pronounce words. The team took the results of this algorithm and formed the ROILA vocabulary.\n\nROILA has an isolating grammar, meaning that it doesn't have suffixes or prefixes added to words to change their meanings. Instead, these changes are constructed by adding word markers that specify what the changes are, such as the tense of the previous verb. For example, in English the suffix “ed” is added to a word to show that it is in the past tense, but in ROILA the marker word “jifi” is placed after the verb.\n\nBelow is the list of all letters and sounds used in ROILA:\nThe vocabulary of ROILA was generated by an algorithm designed to create a vocabulary with the least confusion amongst words. Each word generated by this algorithm was assigned a basic meaning, as taken from Basic English. The words from Basic English that are used the most frequently are assigned to the shortest ROILA words generated by the algorithm. A short list of words in ROILA is included below, along with their English meaning.\nROILA was designed to have a regular grammar, with no exceptions to anything. All rules apply to all words in a part of speech. Due to the simple isolating type grammar of ROILA whole word markers are added following parts of speech to show the grammatical category. For example, a word marker placed after a verb type would apply a tense, while a word marker applied after a noun type would apply plurality. ROILA has five parts of speech: nouns, verbs, adverbs, adjectives, and pronouns. The only pronouns are I, you, he, and she. Sentences follow a subject–verb–object word order.\n\nThe following examples attempt to show what the syntax of the language looks like in various uses.\nROILA is currently only available for the Lego Mindstorms NXT. It uses the CMU Sphinx speech recognition library to interpret spoken commands to the NXT, and transform them into ROILA commands.\n", "id": "30878100", "title": "Robot Interaction Language"}
{"url": "https://en.wikipedia.org/wiki?curid=31719313", "text": "Sociorobotics\n\nSociorobotics is a field of research studying the implications, complexities and subsequent design of artificial social, spatial, cultural and haptic behaviours, protocols and interactions of robots with each other and with humans in equal measure. Intrinsically taking into account the structured and unstructured spaces of habitation, including industrial, commercial, healthcare, eldercare and domestic environments. This emergent perspective to robotic research encompasses and surpasses the conventions of Social robotics and Artificial society/social systems research. Which do not appear to acknowledge that numerous robots (humanoid and non-humanoid) and humans are increasingly inhabiting the same spaces which require similar performances and agency of social behaviour, particularly regarding the commercial emergence of workplace, personal and companion robotics.\n\nRobots in the near future will be required to behave socially and spatially intelligent in all contexts of artificial social interaction, recognising sophisticated cues from both humans and other robots alike, similar in some respects to the required social awareness and generally expected social behaviour of our animal companions. Furthermore, sociorobotics as a research platform also fully accepts that robots can be modular hardware, controlled by pre-programmed disembodied software agency (ubiquitous digital characters) or even 'Wizard of Oz' style teleoperation requiring the same social intelligence, awareness and protocols. Sociorobotics attempts to go further by observing and incorporating the aesthetic and intricacies of social, spatial, cultural & haptic complexities to study the creation of artificial social behaviours and the subsequent protocol design considerations for contemporary and future robotics. Sociorobotics also recognises the critical interdisciplinarity of contemporary and future robot design; acquiring theoretical foundations and research methodologies from HRI, Human Geography, Interaction Design, Actor-network theory and Social Psychology. These foundations bring together intimate concepts of space, haptics and social/cultural criteria of heterogeneous human-robot interactions (i.e. Real Dolls, Lars and the Real Girl), engendering a more comprehensive and cognizant research approach to successful robot design.\n\nProfessor Arvin Agah first coined the term 'sociorobotics' in 1993/4 to describe the study of robot team and colony behaviours but omitted to include himself and his colleagues as members and legitimate study subjects or actants of these sociorobotic assemblages. His and all robotocist's interactions with robot teams or colony members, including human controllers/creators/programmers and indeed, robot to robot, constitute this undefined term of study that optimally represents and describes this emergent field of research of the implications, complexities and design of spatial/social protocols and interactions of multiple robots, both with humans and each other.\n\n\"So who is pulling the strings? Well, the puppets do in addition to their puppeteers. It does not mean that puppets are controlling their handlers – this would be simply reversing the order of causality – and of course no dialectic will do the trick either. It simply means that the interesting question at this point is not to decide who is acting and how but to shift from a certainty about action to an uncertainty about action – but to decide what is acting and how.\", (Bruno Latour, 2005, p. 60)\n\n\n", "id": "31719313", "title": "Sociorobotics"}
{"url": "https://en.wikipedia.org/wiki?curid=31976229", "text": "Automated restaurant\n\nAutomated restaurant or robotic restaurant is a restaurant that uses robots to do tasks such as delivering food and drinks to the tables and/or to cook the food.\n\nRestaurant automation means the use of restaurant management system to automate the major operations of a restaurant establishment. Even in the early 1970s, a number of restaurants served foods solely through vending machines. Called automats, or in Japan shokkenki, customers ordered their foods directly through the machines.\n\nMore recently, restaurants are opening that have completely or partially automated their services. These may include taking of orders, preparing of food, serving and billing. A few fully automated restaurants operate without any human intervention whatsoever. Robots are designed to help and sometimes replace human labour (such as waiters and chefs). Automation of restaurants also allows the option for greater customization of an order.\n\nAutomated restaurants have been opening in many countries. Examples include Fritz's Railroad Restaurant in Kansas City, Missouri, Výtopna (Railway Restaurant), a franchise of various restaurants and coffeehouses in Czech Republic, Bagger's Restaurant in Nuremberg, Germany, FuA-Men Restaurant, a ramen restaurant located in Nagoya, Japan, Hajime Robot Restaurant, a Japanese restaurant in Bangkok, Thailand, the Dalu Robot Restaurant in Jinan, China Haohai Robot Restaurant in Harbin, China and Robot Kitchen Restaurant in Hong Kong.\nRobot Chacha, the first robot restaurant of India is due to open in capital city, New Delhi. Across Europe, McDonald's has already begun implementing 7,000 touch screen kiosks that will handle cashiering duties.\n\n\n", "id": "31976229", "title": "Automated restaurant"}
{"url": "https://en.wikipedia.org/wiki?curid=19637", "text": "Molecular nanotechnology\n\nMolecular nanotechnology (MNT) is a technology based on the ability to build structures to complex, atomic specifications by means of mechanosynthesis. This is distinct from nanoscale materials. Based on Richard Feynman's vision of miniature factories using nanomachines to build complex products (including additional nanomachines), this advanced form of nanotechnology (or \"molecular manufacturing\") would make use of positionally-controlled mechanosynthesis guided by molecular machine systems. MNT would involve combining physical principles demonstrated by biophysics, chemistry, other nanotechnologies, and the molecular machinery of life with the systems engineering principles found in modern macroscale factories.\n\nWhile conventional chemistry uses inexact processes obtaining inexact results, and biology exploits inexact processes to obtain definitive results, molecular nanotechnology would employ original definitive processes to obtain definitive results. The desire in molecular nanotechnology would be to balance molecular reactions in positionally-controlled locations and orientations to obtain desired chemical reactions, and then to build systems by further assembling the products of these reactions.\n\nA roadmap for the development of MNT is an objective of a broadly based technology project led by Battelle (the manager of several U.S. National Laboratories) and the Foresight Institute. The roadmap was originally scheduled for completion by late 2006, but was released in January 2008. The Nanofactory Collaboration is a more focused ongoing effort involving 23 researchers from 10 organizations and 4 countries that is developing a practical research agenda specifically aimed at positionally-controlled diamond mechanosynthesis and diamondoid nanofactory development. In August 2005, a task force consisting of 50+ international experts from various fields was organized by the Center for Responsible Nanotechnology to study the societal implications of molecular nanotechnology.\n\nOne proposed application of MNT is so-called smart materials. This term refers to any sort of material designed and engineered at the nanometer scale for a specific task. It encompasses a wide variety of possible commercial applications. One example would be materials designed to respond differently to various molecules; such a capability could lead, for example, to artificial drugs which would recognize and render inert specific viruses. Another is the idea of self-healing structures, which would repair small tears in a surface naturally in the same way as self-sealing tires or human skin.\n\nA MNT nanosensor would resemble a smart material, involving a small component within a larger machine that would react to its environment and change in some fundamental, intentional way. A very simple example: a photosensor might passively measure the incident light and discharge its absorbed energy as electricity when the light passes above or below a specified threshold, sending a signal to a larger machine. Such a sensor would supposedly cost less and use less power than a conventional sensor, and yet function usefully in all the same applications — for example, turning on parking lot lights when it gets dark.\n\nWhile smart materials and nanosensors both exemplify useful applications of MNT, they pale in comparison with the complexity of the technology most popularly associated with the term: the replicating nanorobot.\n\nMNT nanofacturing is popularly linked with the idea of swarms of coordinated nanoscale robots working together, a popularization of an early proposal by K. Eric Drexler in his 1986 discussions of MNT, but superseded in 1992. In this early proposal, sufficiently capable nanorobots would construct more nanorobots in an artificial environment containing special molecular building blocks.\n\nCritics have doubted both the feasibility of self-replicating nanorobots and the feasibility of control if self-replicating nanorobots could be achieved: they cite the possibility of mutations removing any control and favoring reproduction of mutant pathogenic variations. Advocates address the first doubt by pointing out that the first macroscale autonomous machine replicator, made of Lego blocks, was built and operated experimentally in 2002. While there are sensory advantages present at the macroscale compared to the limited sensorium available at the nanoscale, proposals for positionally controlled nanoscale mechanosynthetic fabrication systems employ dead reckoning of tooltips combined with reliable reaction sequence design to ensure reliable results, hence a limited sensorium is no handicap; similar considerations apply to the positional assembly of small nanoparts. Advocates address the second doubt by arguing that bacteria are (of necessity) evolved to evolve, while nanorobot mutation could be actively prevented by common error-correcting techniques. Similar ideas are advocated in the Foresight Guidelines on Molecular Nanotechnology, and a map of the 137-dimensional replicator design space recently published by Freitas and Merkle provides numerous proposed methods by which replicators could, in principle, be safely controlled by good design.\n\nHowever, the concept of suppressing mutation raises the question: How can design evolution occur at the nanoscale without a process of random mutation and deterministic selection? Critics argue that MNT advocates have not provided a substitute for such a process of evolution in this nanoscale arena where conventional sensory-based selection processes are lacking. The limits of the sensorium available at the nanoscale could make it difficult or impossible to winnow successes from failures. Advocates argue that design evolution should occur deterministically and strictly under human control, using the conventional engineering paradigm of modeling, design, prototyping, testing, analysis, and redesign.\n\nIn any event, since 1992 technical proposals for MNT do not include self-replicating nanorobots, and recent ethical guidelines put forth by MNT advocates prohibit unconstrained self-replication.\n\nOne of the most important applications of MNT would be medical nanorobotics or nanomedicine, an area pioneered by Robert Freitas in numerous books and papers. The ability to design, build, and deploy large numbers of medical nanorobots would, at a minimum, make possible the rapid elimination of disease and the reliable and relatively painless recovery from physical trauma. Medical nanorobots might also make possible the convenient correction of genetic defects, and help to ensure a greatly expanded lifespan. More controversially, medical nanorobots might be used to augment natural human capabilities. One study has reported on the conditions like tumors, arteriosclerosis, blood clots leading to stroke, accumulation of scar tissue and localized pockets of infection can be possibly be addressed by employing medical nanorobots.\n\nAnother proposed application of molecular nanotechnology is \"utility fog\" — in which a cloud of networked microscopic robots (simpler than assemblers) would change its shape and properties to form macroscopic objects and tools in accordance with software commands. Rather than modify the current practices of consuming material goods in different forms, utility fog would simply replace many physical objects.\n\nYet another proposed application of MNT would be phased-array optics (PAO). However, this appears to be a problem addressable by ordinary nanoscale technology. PAO would use the principle of phased-array millimeter technology but at optical wavelengths. This would permit the duplication of any sort of optical effect but virtually. Users could request holograms, sunrises and sunsets, or floating lasers as the mood strikes. PAO systems were described in BC Crandall's \"Nanotechnology: Molecular Speculations on Global Abundance\" in the Brian Wowk article \"Phased-Array Optics.\"\n\nMolecular manufacturing is a potential future subfield of nanotechnology that would make it possible to build complex structures at atomic precision. Molecular manufacturing requires significant advances in nanotechnology, but once achieved could produce highly advanced products at low costs and in large quantities in nanofactories weighing a kilogram or more. When nanofactories gain the ability to produce other nanofactories production may only be limited by relatively abundant factors such as input materials, energy and software.\n\nThe products of molecular manufacturing could range from cheaper, mass-produced versions of known high-tech products to novel products with added capabilities in many areas of application. Some applications that have been suggested are advanced smart materials, nanosensors, medical nanorobots and space travel. Additionally, molecular manufacturing could be used to cheaply produce highly advanced, durable weapons, which is an area of special concern regarding the impact of nanotechnology. Being equipped with compact computers and motors these could be increasingly autonomous and have a large range of capabilities.\n\nAccording to Chris Phoenix and Mike Treder from the Center for Responsible Nanotechnology as well as Anders Sandberg from the Future of Humanity Institute molecular manufacturing is the application of nanotechnology that poses the most significant global catastrophic risk. Several nanotechnology researchers state that the bulk of risk from nanotechnology comes from the potential to lead to war, arms races and destructive global government. Several reasons have been suggested why the availability of nanotech weaponry may with significant likelihood lead to unstable arms races (compared to e.g. nuclear arms races): (1) A large number of players may be tempted to enter the race since the threshold for doing so is low; (2) the ability to make weapons with molecular manufacturing will be cheap and easy to hide; (3) therefore lack of insight into the other parties' capabilities can tempt players to arm out of caution or to launch preemptive strikes; (4) molecular manufacturing may reduce dependency on international trade, a potential peace-promoting factor; (5) wars of aggression may pose a smaller economic threat to the aggressor since manufacturing is cheap and humans may not be needed on the battlefield.\n\nSince self-regulation by all state and non-state actors seems hard to achieve, measures to mitigate war-related risks have mainly been proposed in the area of international cooperation. International infrastructure may be expanded giving more sovereignty to the international level. This could help coordinate efforts for arms control. International institutions dedicated specifically to nanotechnology (perhaps analogously to the International Atomic Energy Agency IAEA) or general arms control may also be designed. One may also jointly make differential technological progress on defensive technologies, a policy that players should usually favour. The Center for Responsible Nanotechnology also suggest some technical restrictions. Improved transparency regarding technological capabilities may be another important facilitator for arms-control.\n\nA grey goo is another catastrophic scenario, which was proposed by Eric Drexler in his 1986 book \"Engines of Creation\", has been analyzed by Freitas in \"Some Limits to Global Ecophagy by Biovorous Nanoreplicators, with Public Policy Recommendations\" and has been a theme in mainstream media and fiction. This scenario involves tiny self-replicating robots that consume the entire biosphere using it as a source of energy and building blocks. Nanotech experts including Drexler now discredit the scenario. According to Chris Phoenix a \"So-called grey goo could only be the product of a deliberate and difficult engineering process, not an accident\". With the advent of nano-biotech, a different scenario called green goo has been forwarded. Here, the malignant substance is not nanobots but rather self-replicating biological organisms engineered through nanotechnology.\n\nNanotechnology (or molecular nanotechnology to refer more specifically to the goals discussed here) will let us continue the historical trends in manufacturing right up to the fundamental limits imposed by physical law. It will let us make remarkably powerful molecular computers. It will let us make materials over fifty times lighter than steel or aluminium alloy but with the same strength. We'll be able to make jets, rockets, cars or even chairs that, by today's standards, would be remarkably light, strong, and inexpensive. Molecular surgical tools, guided by molecular computers and injected into the blood stream could find and destroy cancer cells or invading bacteria, unclog arteries, or provide oxygen when the circulation is impaired.\n\nNanotechnology will replace our entire manufacturing base with a new, radically more precise, radically less expensive, and radically more flexible way of making products. The aim is not simply to replace today's computer chip making plants, but also to replace the assembly lines for cars, televisions, telephones, books, surgical tools, missiles, bookcases, airplanes, tractors, and all the rest. The objective is a pervasive change in manufacturing, a change that will leave virtually no product untouched. Economic progress and military readiness in the 21st Century will depend fundamentally on maintaining a competitive position in nanotechnology.\n\nDespite the current early developmental status of nanotechnology and molecular nanotechnology, much concern surrounds MNT's anticipated impact on economics and on law. Whatever the exact effects, MNT, if achieved, would tend to reduce the scarcity of manufactured goods and make many more goods (such as food and health aids) manufacturable.\n\nMNT should make possible nanomedical capabilities able to cure any medical condition not already cured by advances in other areas. Good health would be common, and poor health of any form would be as rare as smallpox and scurvy are today. Even cryonics would be feasible, as cryopreserved tissue could be fully repaired.\n\nMolecular nanotechnology is one of the technologies that some analysts believe could lead to a technological singularity.\nSome feel that molecular nanotechnology would have daunting risks. It conceivably could enable cheaper and more destructive conventional weapons. Also, molecular nanotechnology might permit weapons of mass destruction that could self-replicate, as viruses and cancer cells do when attacking the human body. Commentators generally agree that, in the event molecular nanotechnology were developed, its self-replication should be permitted only under very controlled or \"inherently safe\" conditions.\n\nA fear exists that nanomechanical robots, if achieved, and if designed to self-replicate using naturally occurring materials (a difficult task), could consume the entire planet in their hunger for raw materials, or simply crowd out natural life, out-competing it for energy (as happened historically when blue-green algae appeared and outcompeted earlier life forms). Some commentators have referred to this situation as the \"grey goo\" or \"ecophagy\" scenario. K. Eric Drexler considers an accidental \"grey goo\" scenario extremely unlikely and says so in later editions of \"Engines of Creation\".\n\nIn light of this perception of potential danger, the Foresight Institute (founded by K. Eric Drexler to prepare for the arrival of future technologies) has drafted a set of guidelines for the ethical development of nanotechnology. These include the banning of free-foraging self-replicating pseudo-organisms on the Earth's surface, at least, and possibly in other places.\n\nThe feasibility of the basic technologies analyzed in \"Nanosystems\" has been the subject of a formal scientific review by U.S. National Academy of Sciences, and has also been the focus of extensive debate on the internet and in the popular press.\n\nIn 2006, U.S. National Academy of Sciences released the report of a study of molecular manufacturing as part of a longer report, \"A Matter of Size: Triennial Review of the National Nanotechnology Initiative\" The study committee reviewed the technical content of \"Nanosystems\", and in its conclusion states that no current theoretical analysis can be considered definitive regarding several questions of potential system performance, and that optimal paths for implementing high-performance systems cannot be predicted with confidence. It recommends experimental research to advance knowledge in this area:\n\nA section heading in Drexler's \"Engines of Creation\" reads \"Universal Assemblers\", and the following text speaks of multiple types of assemblers which, collectively, could hypothetically \"build almost anything that the laws of nature allow to exist.\" Drexler's colleague Ralph Merkle has noted that, contrary to widespread legend, Drexler never claimed that assembler systems could build absolutely any molecular structure. The endnotes in Drexler's book explain the qualification \"almost\": \"For example, a delicate structure might be designed that, like a stone arch, would self-destruct unless all its pieces were already in place. If there were no room in the design for the placement and removal of a scaffolding, then the structure might be impossible to build. Few structures of practical interest seem likely to exhibit such a problem, however.\"\n\nIn 1992, Drexler published \"Nanosystems: Molecular Machinery, Manufacturing, and Computation\", a detailed proposal for synthesizing stiff covalent structures using a table-top factory. Diamondoid structures and other stiff covalent structures, if achieved, would have a wide range of possible applications, going far beyond current MEMS technology. An outline of a path was put forward in 1992 for building a table-top factory in the absence of an assembler. Other researchers have begun advancing tentative, alternative proposed paths for this in the years since Nanosystems was published.\n\nIn 2004 Richard Jones wrote Soft Machines (nanotechnology and life), a book for lay audiences published by Oxford University. In this book he describes radical nanotechnology (as advocated by Drexler) as a deterministic/mechanistic idea of nano engineered machines that does not take into account the nanoscale challenges such as wetness, stickness, Brownian motion, and high viscosity. He also explains what is soft nanotechnology or more appropriatelly biomimetic nanotechnology which is the way forward, if not the best way, to design functional nanodevices that can cope with all the problems at a nanoscale. One can think of soft nanotechnology as the development of nanomachines that uses the lessons learned from biology on how things work, chemistry to precisely engineer such devices and stochastic physics to model the system and its natural processes in detail.\n\nSeveral researchers, including Nobel Prize winner Dr. Richard Smalley (1943–2005), attacked the notion of universal assemblers, leading to a rebuttal from Drexler and colleagues, and eventually to an exchange of letters. Smalley argued that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction. Drexler and colleagues, however, noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in \"Nanosystems\". Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states. Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible. However, Drexler addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent/enzyme reaction could ever be. It is noteworthy that, contrary to Smalley's opinion that enzymes require water, \"Not only do enzymes work vigorously in anhydrous organic media, but in this unnatural milieu they acquire remarkable properties such as greatly enhanced stability, radically altered substrate and enantiomeric specificities, molecular memory, and the ability to catalyse unusual reactions.\"\n\nFor the future, some means have to be found for MNT design evolution at the nanoscale which mimics the process of biological evolution at the molecular scale. Biological evolution proceeds by random variation in ensemble averages of organisms combined with culling of the less-successful variants and reproduction of the more-successful variants, and macroscale engineering design also proceeds by a process of design evolution from simplicity to complexity as set forth somewhat satirically by John Gall: \"A complex system that works is invariably found to have evolved from a simple system that worked. . . . A complex system designed from scratch never works and can not be patched up to make it work. You have to start over, beginning with a system that works.\" A breakthrough in MNT is needed which proceeds from the simple atomic ensembles which can be built with, e.g., an STM to complex MNT systems via a process of design evolution. A handicap in this process is the difficulty of seeing and manipulation at the nanoscale compared to the macroscale which makes deterministic selection of successful trials difficult; in contrast biological evolution proceeds via action of what Richard Dawkins has called the \"blind watchmaker\"\ncomprising random molecular variation and deterministic reproduction/extinction.\n\nAt present in 2007 the practice of nanotechnology embraces both stochastic approaches (in which, for example, supramolecular chemistry creates waterproof pants) and deterministic approaches wherein single molecules (created by stochastic chemistry) are manipulated on substrate surfaces (created by stochastic deposition methods) by deterministic methods comprising nudging them with STM or AFM probes and causing simple binding or cleavage reactions to occur. The dream of a complex, deterministic molecular nanotechnology remains elusive. Since the mid-1990s, thousands of surface scientists and thin film technocrats have latched on to the nanotechnology bandwagon and redefined their disciplines as nanotechnology. This has caused much confusion in the field and has spawned thousands of \"nano\"-papers on the peer reviewed literature. Most of these reports are extensions of the more ordinary research done in the parent fields.\n\nThe feasibility of Drexler's proposals largely depends, therefore, on whether designs like those in \"Nanosystems\" could be built in the absence of a universal assembler to build them and would work as described. Supporters of molecular nanotechnology frequently claim that no significant errors have been discovered in \"Nanosystems\" since 1992. Even some critics concede that \"Drexler has carefully considered a number of physical principles underlying the 'high level' aspects of the nanosystems he proposes and, indeed, has thought in some detail\" about some issues.\n\nOther critics claim, however, that \"Nanosystems\" omits important chemical details about the low-level 'machine language' of molecular nanotechnology. They also claim that much of the other low-level chemistry in \"Nanosystems\" requires extensive further work, and that Drexler's higher-level designs therefore rest on speculative foundations. Recent such further work by Freitas and Merkle is aimed at strengthening these foundations by filling the existing gaps in the low-level chemistry.\n\nDrexler argues that we may need to wait until our conventional nanotechnology improves before solving these issues: \"Molecular manufacturing will result from a series of advances in molecular machine systems, much as the first Moon landing resulted from a series of advances in liquid-fuel rocket systems. We are now in a position like that of the British Interplanetary Society of the 1930s which described how multistage liquid-fueled rockets could reach the Moon and pointed to early rockets as illustrations of the basic principle.\" However, Freitas and Merkle argue that a focused effort to achieve diamond mechanosynthesis (DMS) can begin now, using existing technology, and might achieve success in less than a decade if their \"direct-to-DMS approach is pursued rather than a more circuitous development approach that seeks to implement less efficacious nondiamondoid molecular manufacturing technologies before progressing to diamondoid\".\n\nTo summarize the arguments against feasibility: First, critics argue that a primary barrier to achieving molecular nanotechnology is the lack of an efficient way to create machines on a molecular/atomic scale, especially in the absence of a well-defined path toward a self-replicating assembler or diamondoid nanofactory. Advocates respond that a preliminary research path leading to a diamondoid nanofactory is being developed.\n\nA second difficulty in reaching molecular nanotechnology is design. Hand design of a gear or bearing at the level of atoms might take a few to several weeks. While Drexler, Merkle and others have created designs of simple parts, no comprehensive design effort for anything approaching the complexity of a Model T Ford has been attempted. Advocates respond that it is difficult to undertake a comprehensive design effort in the absence of significant funding for such efforts, and that despite this handicap much useful design-ahead has nevertheless been accomplished with new software tools that have been developed, e.g., at Nanorex.\n\nIn the latest report \"A Matter of Size: Triennial Review of the National Nanotechnology Initiative\" put out by the National Academies Press in December 2006 (roughly twenty years after Engines of Creation was published), no clear way forward toward molecular nanotechnology could yet be seen, as per the conclusion on page 108 of that report: \"Although theoretical calculations can be made today, the eventually attainable\nrange of chemical reaction cycles, error rates, speed of operation, and thermodynamic\nefficiencies of such bottom-up manufacturing systems cannot be reliably\npredicted at this time. Thus, the eventually attainable perfection and complexity of\nmanufactured products, while they can be calculated in theory, cannot be predicted\nwith confidence. Finally, the optimum research paths that might lead to systems\nwhich greatly exceed the thermodynamic efficiencies and other capabilities of\nbiological systems cannot be reliably predicted at this time. Research funding that\nis based on the ability of investigators to produce experimental demonstrations\nthat link to abstract models and guide long-term vision is most appropriate to\nachieve this goal.\" This call for research leading to demonstrations is welcomed by groups such as the Nanofactory Collaboration who are specifically seeking experimental successes in diamond mechanosynthesis. The \"Technology Roadmap for Productive Nanosystems\" aims to offer additional constructive insights.\n\nIt is perhaps interesting to ask whether or not most structures consistent with physical law can in fact be manufactured. Advocates assert that to achieve most of the vision of molecular manufacturing it is not necessary to be able to build \"any structure that is compatible with natural law.\" Rather, it is necessary to be able to build only a sufficient (possibly modest) subset of such structures—as is true, in fact, of any practical manufacturing process used in the world today, and is true even in biology. In any event, as Richard Feynman once said, \"It is scientific only to say what's more likely or less likely, and not to be proving all the time what's possible or impossible.\"\n\nThere is a growing body of peer-reviewed theoretical work on synthesizing diamond by mechanically removing/adding hydrogen atoms and depositing carbon atoms (a process known as mechanosynthesis). This work is slowly permeating the broader nanoscience community and is being critiqued. For instance, Peng et al. (2006) (in the continuing research effort by Freitas, Merkle and their collaborators) reports that the most-studied mechanosynthesis tooltip motif (DCB6Ge) successfully places a C carbon dimer on a C(110) diamond surface at both 300 K (room temperature) and 80 K (liquid nitrogen temperature), and that the silicon variant (DCB6Si) also works at 80 K but not at 300 K. Over 100,000 CPU hours were invested in this latest study. The DCB6 tooltip motif, initially described by Merkle and Freitas at a Foresight Conference in 2002, was the first complete tooltip ever proposed for diamond mechanosynthesis and remains the only tooltip motif that has been successfully simulated for its intended function on a full 200-atom diamond surface.\n\nThe tooltips modeled in this work are intended to be used only in carefully controlled environments (e. g., vacuum). Maximum acceptable limits for tooltip translational and rotational misplacement errors are reported in Peng et al. (2006) -- tooltips must be positioned with great accuracy to avoid bonding the dimer incorrectly. Peng et al. (2006) reports that increasing the handle thickness from 4 support planes of C atoms above the tooltip to 5 planes decreases the resonance frequency of the entire structure from 2.0 THz to 1.8 THz. More importantly, the vibrational footprints of a DCB6Ge tooltip mounted on a 384-atom handle and of the same tooltip mounted on a similarly constrained but much larger 636-atom \"crossbar\" handle are virtually identical in the non-crossbar directions. Additional computational studies modeling still bigger handle structures are welcome, but the ability to precisely position SPM tips to the requisite atomic accuracy has been repeatedly demonstrated experimentally at low temperature, or even at room temperature constituting a basic existence proof for this capability.\n\nFurther research to consider additional tooltips will require time-consuming computational chemistry and difficult laboratory work.\n\nA working nanofactory would require a variety of well-designed tips for different reactions, and detailed analyses of placing atoms on more complicated surfaces. Although this appears a challenging problem given current resources, many tools will be available to help future researchers: Moore's law predicts further increases in computer power, semiconductor fabrication techniques continue to approach the nanoscale, and researchers grow ever more skilled at using proteins, ribosomes and DNA to perform novel chemistry.\n\n\n\n\n\n", "id": "19637", "title": "Molecular nanotechnology"}
{"url": "https://en.wikipedia.org/wiki?curid=1673867", "text": "Blobotics\n\nBlobotics is a term describing research into chemical based computer processors based on ions rather than electrons. Andrew Adamatzky, a computer scientist at the University of the West of England in Bristol used the term in an article in New Scientist March 28, 2005 . \n\nThe aim is to create 'liquid logic gates' which would be 'infinitely reconfigurable and self-healing'. The process relies on the Belousov-Zhabotinsky reaction, a repeating cycle of three separate sets of reactions. Such a processor could form the basis of a robot which, using artificial sensors, interact with its surroundings in a way which mimics living creatures. \n\nThe coining of the term was featured by ABC radio in Australia . \n\n", "id": "1673867", "title": "Blobotics"}
{"url": "https://en.wikipedia.org/wiki?curid=10579736", "text": "Rapid prototyping\n\nRapid prototyping is a group of techniques used to quickly fabricate a scale model of a physical part or assembly using three-dimensional computer aided design (CAD) data.\nConstruction of the part or assembly is usually done using 3D printing or \"additive layer manufacturing\" technology.\n\nThe first methods for rapid prototyping became available in the late 1980s and were used to produce models and prototype parts. Today, they are used for a wide range of applications and are used to manufacture production-quality parts in relatively small numbers if desired without the typical unfavorable short-run economics. This economy has encouraged online service bureaus. Historical surveys of RP technology start with discussions of simulacra production techniques used by 19th-century sculptors. Some modern sculptors use the progeny technology to produce exhibitions. The ability to reproduce designs from a dataset has given rise to issues of rights, as it is now possible to interpolate volumetric data from one-dimensional images.\n\nAs with CNC subtractive methods, the computer-aided-design – computer-aided manufacturing CAD -CAM workflow in the traditional Rapid Prototyping process starts with the creation of geometric data, either as a 3D solid using a CAD workstation, or 2D slices using a scanning device. For Rapid prototyping this data must represent a valid geometric model; namely, one whose boundary surfaces enclose a finite volume, contain no holes exposing the interior, and do not fold back on themselves. In other words, the object must have an \"inside\". The model is valid if for each point in 3D space the computer can determine uniquely whether that point lies inside, on, or outside the boundary surface of the model. CAD post-processors will approximate the application vendors' internal CAD geometric forms (e.g., B-splines) with a simplified mathematical form, which in turn is expressed in a specified data format which is a common feature in additive manufacturing: STL (stereolithography) a de facto standard for transferring solid geometric models to SFF machines. To obtain the necessary motion control trajectories to drive the actual SFF, rapid prototyping, 3D printing or additive manufacturing mechanism, the prepared geometric model is typically sliced into layers, and the slices are scanned into lines (producing a \"2D drawing\" used to generate trajectory as in CNC's toolpath), mimicking in reverse the layer-to-layer physical building process.\n\nElectric cars can be built and tested in one year with 3D production systems.\n\nRapid prototyping is commonly applied in software engineering to try out new business models and application architectures.\n\nIn the 1970s, Joseph Henry Condon and others at Bell Labs developed the Unix Circuit Design System (UCDS), automating the laborious and error-prone task of manually converting drawings to fabricate circuit boards for the purposes of research and development.\n\nIn the year 1980s U.S. policy makers and industrial managers were forced to take note that America's dominance in the field of machine tool manufacturing evaporated, in what was named the machine tool crisis. Numerous projects sought to counter these trends in the traditional CNC CAM area, which had begun in the US. Later when Rapid Prototyping Systems moved out of labs to be commercialized, it was recognized that developments were already international and U.S. rapid prototyping companies would not have the luxury of letting a lead slip away. The National Science Foundation was an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, Defense Advanced Research Projects Agency (DARPA), and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations. One such report was the 1997 \"Rapid Prototyping in Europe and Japan Panel Report\" in which Joseph J. Beaman founder of DTM Corporation [DTM RapidTool pictured] provides a historical perspective:\n\nThe technologies referred to as Solid Freeform Fabrication are what we recognize today as rapid prototyping, 3D printing or additive manufacturing: Swainson (1977), Schwerzel (1984) worked on polymerization of a photosensitive polymer at the intersection of two computer controlled laser beams. Ciraud (1972) considered magnetostatic or electrostatic deposition with electron beam, laser or plasma for sintered surface cladding. These were all proposed but it is unknown if working machines were built. Hideo Kodama of Nagoya Municipal Industrial Research Institute was the first to publish an account of a solid model fabricated using a photopolymer rapid prototyping system (1981). Even at that early date the technology was seen as having a place in manufacturing practice. A low resolution, low strength output had value in design verification, mould making, production jigs and other areas. Outputs have steadily advanced toward higher specification uses.\n\nInnovations are constantly being sought, to improve speed and the ability to cope with mass production applications. A dramatic development which RP shares with related CNC areas is the freeware open-sourcing of high level applications which constitute an entire CAD-CAM toolchain. This has created a community of low res device manufacturers. Hobbyists have even made forays into more demanding laser-effected device designs.\n\n\n\n", "id": "10579736", "title": "Rapid prototyping"}
{"url": "https://en.wikipedia.org/wiki?curid=3186372", "text": "Human–robot interaction\n\nHuman–robot interaction is the study of interactions between humans and robots. It is often referred as HRI by researchers. Human–robot interaction is a multidisciplinary field with contributions from human–computer interaction, artificial intelligence, robotics, natural language understanding, design, and social sciences.\n\nHuman–robot interaction has been a topic of both science fiction and academic speculation even before any robots existed. Because HRI depends on a knowledge of (sometimes natural) human communication, many aspects of HRI are continuations of human communications topics that are much older than robotics per se.\n\nThe origin of HRI as a discrete problem was stated by 20th-century author Isaac Asimov in 1941, in his novel \"I, Robot\". He states the Three Laws of Robotics as,\n\nThese three laws of robotics determine the idea of safe interaction. The closer the human and the robot get and the more intricate the relationship becomes, the more the risk of a human being injured rises. Nowadays in advanced societies, manufacturers employing robots solve this issue by not letting humans and robots share the workspace at any time. This is achieved by defining safe zones using lidar sensors or physical cages. Thus the presence of humans is completely forbidden in the robot workspace while it is working.\n\nWith the advances of artificial intelligence, the autonomous robots could eventually have more proactive behaviors, planning their motion in complex unknown environments. These new capabilities keep safety as the primary issue and efficiency as secondary. To allow this new generation of robot, research is being conducted on human detection, motion planning, scene reconstruction, intelligent behavior through task planning and compliant behavior using force control (impedance or admittance control schemes).\n\nThe goal of HRI research is to define models of humans' expectations regarding robot interaction to guide robot design and algorithmic development that would allow more natural and effective interaction between humans and robots. Research ranges from how humans work with remote, tele-operated unmanned vehicles to peer-to-peer collaboration with anthropomorphic robots.\n\nMany in the field of HRI study how humans collaborate and interact and use those studies to motivate how robots should interact with humans.\n\nRobots are artificial agents with capacities of perception and action in the physical world often referred by researchers as workspace. Their use has been generalized in factories but nowadays they tend to be found in the most technologically advanced societies in such critical domains as search and rescue, military battle, mine and bomb detection, scientific exploration, law enforcement, entertainment and hospital care.\n\nThese new domains of applications imply a closer interaction with the user. The concept of closeness is to be taken in its full meaning, robots and humans share the workspace but also share goals in terms of task achievement. This close interaction needs new theoretical models, on one hand for the robotics scientists who work to improve the robots utility and on the other hand to evaluate the risks and benefits of this new \"friend\" for our modern society.\n\nWith the advance in AI, the research is focusing on one part towards the safest physical interaction but also on a socially correct interaction, dependent on cultural criteria. The goal is to build an intuitive, and easy communication with the robot through speech, gestures, and facial expressions.\n\nDautenhahn refers to friendly Human–robot interaction as \"Robotiquette\" defining it as the \"social rules for robot behaviour (a ‘robotiquette’) that is comfortable and acceptable to humans\" The robot has to adapt itself to our way of expressing desires and orders and not the contrary. But every day environments such as homes have much more complex social rules than those implied by factories or even military environments. Thus, the robot needs perceiving and understanding capacities to build dynamic models of its surroundings. It needs to categorize objects, recognize and locate humans and further their emotions. The need for dynamic capacities pushes forward every sub-field of robotics.\n\nFurthermore, by understanding and perceiving social cues, robots can enable collaborative scenarios with humans. For example, with the rapid rise of personal fabrication machines such as desktop 3d printers, laser cutters, etc., entering our homes, scenarios may arise where robots can collaboratively share control, co-ordinate and achieve tasks together. Industrial robots have already been integrated into industrial assembly lines and are collaboratively working with humans. The social impact of such robots have been studied and has indicated that workers still treat robots and social entities, rely on social cues to understand and work together.\n\nOn the other end of HRI research the cognitive modelling of the \"relationship\" between human and the robots benefits the psychologists and robotic researchers the user study are often of interests on both sides. This research endeavours part of human society. For effective \"human – humanoid robot\" interaction numerous communication skills and related features should be implemented in the design of such artificial agents/systems.\n\nHRI research spans a wide range of field, some general to the nature of HRI.\n\nMost methods intend to build a 3D model through vision of the environment. The proprioception sensors permit the robot to have information over its own state. This information is relative to a reference.\n\nMethods for perceiving humans in the environment are based on sensor information. Research on sensing components and software led by Microsoft provide useful results for extracting the human kinematics (see Kinect). An example of older technique is to use colour information for example the fact that for light skinned people the hands are lighter than the clothes worn. In any case a human modelled a priori can then be fitted to the sensor data. The robot builds or has (depending on the level of autonomy the robot has) a 3D mapping of its surroundings to which is assigned the humans locations.\n\nA speech recognition system is used to interpret human desires or commands. By combining the information inferred by proprioception, sensor and speech the human position and state (standing, seated).\n\nMotion planning in dynamic environment is a challenge that is for the moment only achieved for 3 to 10 degrees of freedom robots. Humanoid robots or even 2 armed robots that can have up to 40 degrees of freedom are unsuited for dynamic environments with today's technology. However lower-dimensional robots can use potential field method to compute trajectories avoiding collisions with human.\n\nHumans exhibit negative social and emotional responses as well as decreased trust toward some robots that closely, but imperfectly, resemble humans; this phenomenon has been termed the \"Uncanny Valley.\" However recent research in telepresence robots has established that mimicking human body postures and expressive gestures has made the robots likeable and engaging in a remote setting. Further, the presence of a human operator was felt more strongly when tested with an android or humanoid telepresence robot than with normal video communication through a monitor.\n\nWhile there is a growing body of research about users perceptions and emotions towards robots, we are still far from a complete understanding. Only additional experiments will determine a more precise model. \n\nBased on past research we have some indications about current user sentiment and behavior around robots:\n\n\n\nA large body of work in the field of human-robot interaction has looked at how humans and robots may better collaborate. The primary social cue for humans while collaborating is the shared perception of an activity, to this end researchers have investigated anticipatory robot control through various methods including: monitoring the behaviors of human partners using eye tracking, making inferences about human task intent, and proactive action on the part of the robot. The studies revealed that the anticipatory control helped users perform tasks faster than with reactive control alone.\n\nA common approach to program social cues into robots is to first study human-human behaviors and then transfer the learning. For example, coordination mechanisms in human-robot collaboration are based on work in neuroscience which examined how to enable joint action in human-human configuration by studying perception and action in a social context rather than in isolation. These studies have revealed that maintaining a shared representation of the task is crucial for accomplishing tasks in groups. For example, the authors have examined the task of driving together by separating responsibilities of acceleration and braking i.e., one person is responsible for accelerating and the other for braking; the study revealed that pairs reached the same level of performance as individuals only when they received feedback about the timing of each other’s actions. Similarly, researchers have studied the aspect of human-human handovers with household scenarios like passing dining plates in order to enable an adaptive control of the same in human-robot handovers. Most recently, researchers have studied a system that automatically distributes assembly tasks among co-located workers to improve co-ordination.\n\nIn addition to general HRI research, researchers are currently exploring application areas for human-robot interaction systems. Application-oriented research is used to help bring current robotics technologies to bear against problems that exist in today's society. While human-robot interaction is still a rather young area of interest, there is active development and research in many areas.\n\nThe Human-Robot Interaction Operating System(HRI/OS), \"provides a structured software framework for building human-robot teams, supports a variety of user interfaces, enables humans and robots to engage in task-oriented dialogue, and facilitates integration of robots through an extensible API\".\n\nFirst responders face great risks in search and rescue (SAR) settings, which typically involve environments that are unsafe for a human to travel. In addition, technology offers tools for observation that can greatly speed-up and improve the accuracy of human perception. Robots can be used to address these concerns . Research in this area includes efforts to address robot sensing, mobility, navigation, planning, integration, and tele-operated control.\n\nSAR robots have already been deployed to environments such as the Collapse of the World Trade Center.\n\nOther application areas include:\n\n\n\nBartneck and Okada suggest that a robotic user interface can be described by the following four properties:\n\nThe International Conference on Social Robotics is a conference for scientists, researchers, and practitioners to report and discuss the latest progress of their forefront research and findings in social robotics, as well as interactions with human beings and integration into our society.\n\n\nThis symposium is organized in collaboration with the Annual Convention of the Society for the Study of Artificial Intelligence and Simulation of Behaviour.\n\nThe IEEE International Symposium on Robot and Human Interactive Communication ( RO-MAN ) was founded in 1992 by Profs. Toshio Fukuda, Hisato Kobayashi, Hiroshi Harashima and Fumio Hara. Early workshop participants were mostly Japanese, and the first seven workshops were held in Japan. Since 1999, workshops have been held in Europe and the United States as well as Japan, and participation has been of international scope.\n\nThis conference is amongst the best conferences in the field of HRI and has a very selective reviewing process. The average acceptance rate is 26% and the average attendance is 187. Around 65% of the contributions to the conference come from the USA and the high level of quality of the submissions to the conference becomes visible by the average of 10 citations that the HRI papers attracted so far.\n\n\nThere are many conferences that are not exclusively HRI, but deal with broad aspects of HRI, and often have HRI papers presented.\n\n\nThere are currently two dedicated HRI Journals\n\nand there are several more general journals in which one will find HRI articles.\n\n", "id": "3186372", "title": "Human–robot interaction"}
{"url": "https://en.wikipedia.org/wiki?curid=32985157", "text": "Robot economics\n\nRobot economics is the study of the market for robots. Robot markets function through the interaction of robot makers and robot users. As (in part) a factor of production, robots are complements and/or substitutes for other factors, such as labor and (non-robot) capital goods. Another part of robot economics considers the effects of the introduction of robots on the markets for those other factors and on the products that robots help produce.\n\nRobots are spreading throughout the economy, in fields such as agriculture, medicine, business and the home.\n\nThe use of robots (or \"robots\") began with automated milking systems. More recently, agricultural robots have begun to be used in harvesting, pruning, seeding, spraying and materials handling.\n\nIndustrial robots offer improved productivity, reduction in scrap losses/rework, lower labor costs and quality improvement. Between 2009 and ???, the global market for industrial robot rose by 60%. Globally 50% of industrial robots are located in Asia, 33% in Europe, and 17% in North America. The Chinese market grew by 41% annually and became the biggest market.\n\nMedical robots can be categorized in five segments: surgery, rehabilitation, non-invasive radiosurgery, hospital and pharmacy and others. Robot-assisted surgery can improve accuracy. However, medical robots can increase costs.\n\nThe market was valued at $1,781 million in 2013. Hospital and pharmacy robots segment grew fastest.\n\nMajor participants operating in the global robotic market include Samsung Electronics, iRobot, Toyota Motor Corporation, AB Electrolux, Hanool Robotics, Fujitsu Frontech Limited, LG Electronics, Fujitsu, Sony Corporation, Yujin Robot and GeckoSystems.\n", "id": "32985157", "title": "Robot economics"}
{"url": "https://en.wikipedia.org/wiki?curid=12342665", "text": "Autonomous logistics\n\nAutonomous logistics describes systems that provide unmanned, autonomous transfer of equipment, baggage, people, information or resources from point-to-point with minimal human intervention. Autonomous logistics is a new area being researched and currently there are few papers on the topic, with even fewer systems developed or deployed. With web enabled cloud software there are companies focused on developing and deploying such systems which will begin coming online in 2018.\n\nThere are several subclasses of autonomous logistics vehicles:\n\nAnother example is the TerraMax autonomous truck based on Oshkosh’s Medium Tactical Vehicle Replacement (MTVR) military truck platform. Most recently, TerraMax competed in the 2007 Darpa Urban Challenge. The MTVR was designed for the U.S. Marine Corps with a 70% off-road mission profile. TerraMax's unmanned ground vehicle kit does not interfere with the conventional operation of the vehicle. A robust sensor suite allows for 360-degree situational awareness around TerraMax. Elements of the autonomous navigation kit could be used to enhance driver awareness. The complete kit could be used in applications such as snow removal on airport runways.\n\n\n\n\nShipping containers handle most of today’s intercontinental transport of packaged goods. Managing them in terms of planning and scheduling is a challenging task due to the complexity and dynamics of the involved processes. Hence, recent developments show an increasing trend towards autonomous control with software agents acting on behalf of the logistic objects. Despite of the high degree of autonomy it is still necessary to cooperate in order to achieve certain goals.\n\nThe current trends and recent changes in logistics lead to new, complex and partially conflicting requirements for logistic planning and control systems. Due to the distributed nature of logistics, the usage of agent technology is promising. Due to the mobile nature of logistics, the usage of mobile agent technology is promising as well. Scenarios of usage of mobile agents in logistics has been envisioned.\n\n\n", "id": "12342665", "title": "Autonomous logistics"}
{"url": "https://en.wikipedia.org/wiki?curid=19148519", "text": "Glossary of robotics\n\nRobotics is the branch of technology that deals with the design, construction, operation, structural disposition, manufacture and application of robots. Robotics is related to the sciences of electronics, engineering, mechanics, and software.\n\nThe following is a list of common definitions related to the Robotics field.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnline Robotics glossary repositories:\n", "id": "19148519", "title": "Glossary of robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=24684701", "text": "Outline of robotics\n\nThe following outline is provided as an overview of and topical guide to robotics:\n\nRobotics is a branch of mechanical engineering, electrical engineering and computer science that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behaviour, and or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics.\n\nThe word \"robot\" was introduced to the public by Czech writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920. The term \"robotics\" was coined by Isaac Asimov in his 1941 science fiction short-story \"Liar!\"\n\nRobotics can be described as:\n\n\n\nRobotics incorporates aspects of many disciplines including electronics, engineering, mechanics, software and arts. The design and control of robots relies on many fields knowledge, including:\n\n\n\nAutonomous robot- robots that are not controlled by humans.\n\nMobile robots may be classified by:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistory of robots\n\nFuture of robotics\n\n\n\n\n\nRobot competition\n\n\n\n\n\n", "id": "24684701", "title": "Outline of robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=21175462", "text": "Open-source robotics\n\nOpen-source robotics (OSR) is where the physical artifacts of the subject are offered by the open design movement. This branch of robotics makes use of open-source hardware and free and open source software providing blueprints, schematics, and source code. The term usually means that information about the hardware is easily discerned so that others can make it from standard commodity components and tools — coupling it closely to the maker movement and open science.\n\nMiddleware are reusable hardware and software components that can be used in many different robotics projects.\n\nBy far the most common standard software are the interconnected,\n\nOther systems include,\n\nMany open source robots make extensive use of general open-source hardware (such as Arduino, Raspberry Pi, RISCV) as well as robotics-specific sensing and control components which include:\n\n\n\n\nA first sign of the increasing popularity of building robots yourself can be found with the DIY community. What began with small competitions for remote operated vehicles (e.g. Robot combat), soon developed to the building of autonomous telepresence robots as Sparky and then true robots (being able to take decisions themselves) as the Open Automaton Project and Leaf Project. Certain commercial companies now also produce kits for making simple robots.\n\nA recurring problem in the community has been projects, especially on Kickstarter, promiding to fully open-source their hardware and then reneging on this promise once funded, in order to profit from being the sole manufacturer and seller.\n\nPopular applications include:\n\n\nwww.osrfoundation.org - Open Source Robotics Foundation\n\nwww.robotarm.org - Open source robot arm building community\n\n", "id": "21175462", "title": "Open-source robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=37120802", "text": "Educational robotics\n\nEducational robotics is a broad term that refers to a collection of activities, instructional programs, physical platforms, educational resources and pedagogical philosophy. There are many schools which are using the robot teacher. \n\nThe primary of objective of educational robotics is to provide a set of experience to facilitate the student's development of knowledge, skills and attitudes for the design, analysis, application and operation of robots. The term robot here is used quite broadly and may include articulated robots, mobile robots or autonomous vehicles of any scale. The rigor of the approach can be scaled based on the background of the target audience and may be suitable for students across the entire educational spectrum—from elementary school or graduate programs.\n\nAn alternate, or secondary, objective is to use robotics as a tangible and exciting application to motivate and facilitate the instruction other, often foundational, topics such as computer programming, artificial intelligence or engineering design.\n\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence and robotics among students. First-year computer science courses at several universities now include programming of a robot in addition to traditional software engineering-based coursework.\n\nSince 2014, companies like Cytron Technologies has been making inroads into schools and learning centers with their rero reconfigurable robot. Designed to be easy and safe to assemble and easy to program, robotics became very accessible to young children with no programming skills and even up to advance users at tertiary level. Robotics education was heavily promoted via roadshows, science fairs, exhibitions, workshops, camps and co-sponsored classes, bringing robotics education to the masses.\n\nFrom approximately 1960 though 2005, robotics education at post-secondary institutions took place through elective courses, thesis experiences and design projects offered as part of degree programs in traditional academic disciplines, such as mechanical engineering, electrical engineering, industrial engineering or computer science.\n\nSince 2005, more universities have begun granting degrees in robotics as a discipline in its own right, often under the name \"Robotic Engineering\". Based on a 2015 web-based survey of robotics educators, the degree programs and their estimates annual graduates are listed alphabetically below. Note that only official degree programs where the word \"robotics\" appears on the transcript or diploma are listed here; whereas degree programs in traditional disciplines with course concentrations or thesis topics related to robotics are deliberately omitted.\n\nThe Robotics Certification Standards Alliance (RCSA) is an international robotics certification authority that confers various industry- and educational-related robotics certifications.\n\nSeveral summer camp programs include robotics as part of their core curriculum. In addition, youth summer robotics programs are frequently offered by celebrated museums such as the American Museum of Natural History and The Tech Museum of Innovation in Silicon Valley, CA, just to name a few.\n\nMany schools across the country are beginning to add robotics programs to their after school curriculum. Two main programs for afterschool robotics are botball and FIRST Robotics.\n\n", "id": "37120802", "title": "Educational robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=360066", "text": "Robopsychology\n\nRobopsychology is the study of the personalities and behavoiur of intelligent machines. The term was coined by Isaac Asimov in the short stories collected in \"I, Robot\", which featured robopsychologist Dr. Susan Calvin, and whose plots largely revolved around the protagonist solving problems connected with intelligent robot behaviour.\n\nAndrea Kuszewski, a self-described robopsychologist gives the following examples of potential responsibilities for a robopsychologist in Discover.\n\nThere is a robospychology research division at Ars Electronica Futurelab\n\nThe stories also introduced Asimov's famous Three Laws of Robotics. Another robopsychologist mentioned by name is Clinton Madarian, who is introduced as being Susan Calvin's successor in the story \"Feminine Intuition\".\n\nAs described by Asimov, robopsychology appears to be a mixture of detailed mathematical analysis and traditional psychology, applied to robots. Human psychology is also a part, covering human interaction with robots. This includes the \"Frankenstein complex\" – the irrational fear that robots (or other creations) will turn on their creator.\n\n", "id": "360066", "title": "Robopsychology"}
{"url": "https://en.wikipedia.org/wiki?curid=38590885", "text": "Holonomic (robotics)\n\nA robot is holonomic if all the constraints that it is subjected to are integrable into positional constraints of the form:\n\nThe variables formula_2 are the system coordinates. When a system contains constraints that cannot be written in this form, it is to be nonholonomic.\n\nIn simpler terms, a holonomic system is when the number of controllable degrees of freedom is equal to the total degrees of freedom.\n\nConsider a mobile robot such as the one depicted to the right, moving in the two-dimensional plane. Imagine that three omnidirectional wheels are mounted on the frame of the robot. Each wheel formula_3 is described by its coordinates formula_4, so that a configuration of the robot can be given by the six scalars formula_5. Also, each wheel formula_3 can impulse a velocity formula_7 to the robot. However, because all three wheels are connected by the rigid robot frame, their relative velocities are zero (unless the frame breaks):\n\nThese velocity constraints integrate into positional constraints\n\nwhere formula_14 are scalar constants. The system is thus holonomic.\n\nLet us finally look at the degree of freedom of the robot. We initially used six coordinates formula_5 to describe a configuration of the robot. Yet, each of the positional constraints \"consumes\" a degree of freedom. For instance, formula_16 implies that formula_17, i.e., formula_18. The coordinate formula_19 can then be replaced by the appropriate root of this quadratic polynomial. Repeating the process thrice leaves us with three irreducible coordinates, corresponding to the three degrees of freedom of the system.\n\nNote that the simplest generalized coordinates for this system are formula_20, where formula_21 and formula_22 denote translation along the plane axes, and formula_23 is the orientation of the robot.\n\nThe tricycle may look like a similar robotic system, however it is nonholonomic due to the parallel parking problem.\n", "id": "38590885", "title": "Holonomic (robotics)"}
{"url": "https://en.wikipedia.org/wiki?curid=40417621", "text": "Scout (autonomous boat)\n\nScout is an autonomous robotic boat designed to complete the first autonomous transatlantic crossing. The project was started by Tiverton students Dylan Rodriguez and Max Kramers in 2010 with the goal of creating an autonomous craft to make the journey from Rhode Island to Sanlucar de Barrameda, Spain. After several iterations, Scout's first transatlantic attempt was launched from Sakonnet Point on June 29, 2013 but unfavourable weather conditions forced the team to recover the craft the same day. A second launch was made July 4, 2013 but after two days a technical failure forced another recovery effort and a redesign of parts of the vessel. The third attempt was launched during the early morning of August 24, 2013 and is currently in progress but has already earned the record for distance of an unmanned Atlantic naval voyage. \n\nScout is based on a custom carbon fiber hull with a Divinycell foam core and measures 12.8 feet long, and 25 inches wide and weighs 160 pounds. The vessel design incorporates a bulb keel to right the craft in heavy seas, and propulsion is provided by an electric trolling motor powered by a bank of solar-charged lithium iron phosphate batteries. On-board control and navigation is provided by two Arduino microcontrollers and a GPS receiver, and telemetry data is sent back to the team using the Iridium satellite constellation and provided live on the World Wide Web. \n\n", "id": "40417621", "title": "Scout (autonomous boat)"}
{"url": "https://en.wikipedia.org/wiki?curid=40655539", "text": "Maillardet's automaton\n\nMaillardet's automaton (or Draughtsman-Writer, sometime also known as Maelzel's Juvenile Artist or Juvenile Artist) is an automaton built in London circa 1800 by a Swiss mechanician, Henri Maillardet. It is currently part of the collections at The Franklin Institute in Philadelphia.\n\nIn November 1928 the Franklin Institute received the pieces of a brass machine. It came from the descendants of John Penn Brock, a family who knew that at some time it had been able to write and draw pictures. Having been in a fire, its restoration involved a considerable amount of work. The Brock family believed that the machine had been made in France by an inventor named Maelzel. \nThe original writing instrument, either a quill or a brush, was replaced with a stylographic fountain pen. Once repaired the automaton began to produce elaborate sketches and poems. In the border surrounding the final poem, the automaton wrote, \"\"Ecrit par L'Automate de Maillardet\"\", translating to \"\"Written by the automaton of Maillardet\"\".\n\nRestorer and paper engineer Andrew Baron spent about 70 hours in 2007 repairing the Maillardet automaton to bring it back to working order.\n\nDuring the early nineteenth century, Maillardet exhibited this automaton and other automata that he created throughout England, and to other countries in Europe as far as Saint Petersburg, Russia. The following is a partial list of known exhibitions of the Maillardet's automaton. The automaton was mostly known as the \"Juvenile Artist\" in those exhibitions:\n\nThe Maillardet's automaton is a highlight of the \"Amazing Machine\" permanent exhibit at The Franklin Institute. The exhibition includes more than two dozen rarely displayed machines with exploded views of the machines to show their components such as gears, cams, pulleys and linkages. The Maillardet's automaton is always on display but the staff operate it only a few times a year for public demonstrations. The demonstrations show all seven drawings of the automaton as seen in the followings:\n\nThe pictures of Maillardet's automaton are among other artifacts in the collections of Computer History Museum in Mountain View, California. The museum has a collection of machines in pre-computer era. Particularly for Maillardet's, the museum highlights the use of its cams in similar function to read-only memory (ROM) of computer to store pre-defined data for retrieval at a later time. In Maillardet's automaton case, it stores motion data necessary to write a poem on paper. It is believed to be the largest cam-based memory of any automaton of the era. The information capacity of the automaton to hold 7 images within the machine was calculated to be 299,040 points, almost 300 kilobits of storage.\n\nThe Massachusetts Historical Society holds a drawing created in 1835 by an automaton in the collections of papers and artwork of the Minot family. The drawing is untitled with a depiction of Cupid. The writing on the picture indicated that it was drawn by Maelzel's automaton which was known at the time as the \"Maelzel's Juvenile Artist\". It is believed that at least one member of the Minot family witnessed the drawing by the Juvenile Artist at the exhibition of Johann Nepomuk Maelzel's automata in Boston on April 29, 1835. The Massachusetts Historical Society theorizes that the Juvenile Artist may in fact be the Maillardet's automaton. An explanation is that Maelzel might have acquired the automaton in the 1830s. As in other automata in his collection, they were known at the time as Maelzel's. After Maelzel's death, the automaton could have been auctioned off in Philadelphia to pay his debts. The new owner then attributed it to Maelzel.\n\n", "id": "40655539", "title": "Maillardet's automaton"}
{"url": "https://en.wikipedia.org/wiki?curid=40773258", "text": "Rajko Tomović\n\nRajko Tomović (1919-2001) was a Serbian scientist and he worked programs in robotics, information technologies in medicine, bio-medical engineering, rehabilitation engineering, artificial organs, and many other important disciplines that are reaching maturity today. The first artificial hand with five fingers in the world was made in 1963. years, precisely in Belgrade. It was made an academician Rajko Tomović associates. He was a member of Serbian Academy of Sciences and Arts (SANU).\n\nRajko Tomović was born in Baja, Hungary, in 1919. He started his undergraduate education at the Department for Electro-Mechanical Engineering of the Technical University of Belgrade in 1938. World War II dramatically changed his life, but he found the strength and graduated in 1946 with excellence. After graduation he started his extremely productive career in developing and fine-tuning scientific and cultural collaboration. With his extraordinary language skills, and excellent over-all education he built a communication at the highest scientific level with the colleagues from Soviet Union, European countries, and North America. Rajko Tomović received a doctorate in technical sciences from the Academy of Sciences of Serbia in 1952. Rajko Tomović started developing his science in the Institute for Nuclear Sciences Vinča in 1950. His main interests during the \"Vinca period\" were analog computers, and he greatly contributed to first analog, digital and hybrid computers ever built in Vinča.\n\nHis interests and energy moved him to the Institute “Mihajlo Pupin” in 1960. While working at the Pupin Institute, he started a very strong research in the field of anthropomorphic robotics. In 1962 Rajko Tomović joined the Faculty of Electrical Engineering in Belgrade at the Department for Automatic Control. While at the University Rajko started programs in robotics, information technologies in medicine, bio-medical engineering, rehabilitation engineering, artificial organs, and many other important disciplines that are reaching maturity today. The first artificial hand with five fingers in the world was made in 1963. year, precisely in Belgrade. It was made an academician Rajko Tomovic associates. This so-called Belgrade's hands now in the Museum of robotics in Boston, and the Museum of robotics in Moscow is the first machine designed for walking disability in 1972., also in Belgrade. It is the so-called external skeleton. Handicraft is academics Miomira Vukobratović and his team. Rajko Tomović retired from off- cia1 duties at the Faculty of Electrical Engineering, yet he never stopped being engaged in various projects, student supervisions at both the Faculty of Electrical Engineering and in the Academy of Sciences and Arts of Serbia SANU developing new initiatives, and educating young researchers.\nRajko Tomović spent time in the U.S. contributing greatly to the development of new views and methods in robotics, biomedical engineering, and computer sciences. During this period he built very strong lifelong lasting cooperation with scientists from the University of California, Los Angeles (UCLA), University of Southern California (USC), Los Angeles, Ohio State University, Columbus, Ohio, and many other research institutions. His contacts and communication spread over Canada, the U.S., the Soviet Union, Poland, Czechoslovakia, Hungary, Bulgaria, France, Germany, Greece, Italy, and many other countries. It is striking that anybody who has contacts with people from the World of Science would be asked about Rajko Tomović when mentioning Engineering and Belgrade University. It is difficult to find a research institution involved in robotics, rehabilitation engineering, and computer sciences that in some way was not connected to Rajko Tomović. He was a chief of project CER-10, the first Serbian digital computer from 1960.\n\nBased in his research results and contribution to the science he was elected to be a member of the Vojvodina Academy of Science and Arts (VANU), and after integration a member\nof Serbian Academy of Sciences and Arts (SANU). He was also elected to be an external member of the Slovenian Academy of Academy of Sciences and arts (SAZU). Rajko Tomović\nwas decorated with the highest state medals and awards for his specific, yet also general contributions. He received the National 7Ih July Award, the AVNOJ Award, and much recognition for his research and society work.\n\nIn parallel with his scientific work Rajko Tomović was very active and effective in his social and cultural activities. He was one of the founders of the Yugoslav Committee for Electronics, Telecommunication, Automatics and Nuclear Sciences (ETAN), Yugoslav Society for Computers, Yugoslav Society for Biomedical Engineering, Center for Multidisciplinary Studies of the University of Belgrade, Center for Strategic Social and Technical Planning in Novi Sad, and many other institutions. He was member of various high-level state organizations and recognized member of the only political party in Yugoslavia.\n\nDuring the period 1955 to 1990 he succeeded to start and finalize several original, pioneering sciantific and technological developments. Many of these activities received\nvery strong attention around the world. His book about High-speed Analog Computers (with Walter J.Karplus) published in 1962 introduced the electronic device systems to constitute a repetitive (analog) computer facility and was the first monograph ever in the world. It was published in a short interval in French, English, and Russian languages.\nHe is the author of the first monograph dealing with sensitivity theory together with Dr. Miomir Vukobratović. Rajko Tomović is the author of the text-book Nonlinear Systems (1983) with Prof. Srdjan Stanković and the book about Limitations of the Formal System Theory (1979) with Walter J. Karplus. In parallel, he was always trying to understand better the nature, and apply what he learned. One of the best-known results that came from this thinking was the multifunctional hand prosthesis, developed in collaboration with Prof. Miodrag Rakić that is now exposed in the Museum of Robotics in Boston although built in sixties. In the eighties together with the colleagues from.the Universities of Novi Sad and Belgrade, Rajko Tomović promoted the Belgrade NSC robot hand being at the time one of the most power robot grasping devices. In the early eighties Rajko Tomović was leading a project for +e Veterans Administration Center, New York City, that resulted in the only powered and externally controlled self-contained transfemoral prosthesis that reached the world market as a product of the biggest world manufacturers of artificial legs. Rajko Tomović participated and contributed in many designs and development projects including the first analog and digital computers for the Yugoslav army and defense systems. As a world-recognized expert and leader in the field, Rajko Tomović was frequently invited to chair, teach, instruct, consult, and contribute in other ways at various universities, international meetings, specialized workshop, major funding agencies, etc. \nRajko Tomović was constantly involved in the organization of scientific and engineering meetings, summer schools, and workshops that took place in Yugoslavia. These meetings were for a longtime a unique opportunity for East and West to meet. Yugoslavian meetings became places where scientists from the Soviet Union and other eastern countries met with their colleagues from North America and Western Europe. The series of 10 triennial international meetings \"Advances in External Control of Humans Extremities\", known as\nDubrovnik meetings, resulted in 10 historic Proceedings that are used as the basic reference for the work in the rehabilitation of humans with an impact on movement.\nThe best proofs of the extraordinary abilities of Rajko Tomović are his students and collaborators. He supervised many master and doctoral students. The students of Rajko Tomović became world known experts in their fields, and they are spread at almost every point of the planet. The Automatic Control of Belgrade is well recognized by the peers, and much of this recognition is linked to the name Tomović. In 1984, Dr. Norman Kaplan, Director of the National Science Foundation, Washington, D.C, gave an interesting, very valid depiction of the Rajko's personality; he said that Rajko is a renaissance scientist. Dr. Tomović authored more than 150 scientific papers; most of those in peer reviewed journals. He was and is frequently cited, and some of his works will be cited for a long time because they opened some important developments. He (with Drs.Dejan B. Popović and Richard B. Stein) published the first monograph in the world about nonanalytic methods for motor control in 1995. He wrote many invited chapters in the encyclopedia and other books dealing with robotics, biomedical engineering, and automatics.\n\n\nDr. Tomović was an active person in the political life of Yugoslavia, yet mostly working as an advisor and consultant. Throughout his life Dr. Tomović never compromised his cosmopolitan ideas on integration of things that are good and positive. There is no doubt that the last decade of his life and of the 20th century affected his life, and this\nshort biography of the Academician Professor Dr. Rajko Tomović would be incomplete if it would not include one totally different yet important dimension. Rajko was always able to find a right measure between his scientific and other activities that are characteristic only for good teachers and philanthropist. Rajko succeeded to prolong the day and night; thus, find the time for studying arts and philosophy. The piano and his collection of CDs were Rajko's runaway places; he was obsessed with good sounds of the classic music. The little pillow in front of the CD player and old gramophone, large collection of CDs and LP records, and headphones that can eliminate daily noise were inevitable parts of the living room furniture. Reading of the mathematics and philosophy classics was the source of much inspiration that Rajko contributed to the world. It was always interesting to see and talk to Rajko in the morning after he just finished another chapter or article written by some pioneer in mathematics or computer science, physics, philosophy, neuroscience, or some other visionary discipline. In parallel, Rajko was passionate in sports. Rajko was always involved actively in playing sports, especially the rowing and tennis. Rajko had the second best result in rowing in his age group for several years, and he constantly wanted to improve his performance while rowing with professional youngsters that could be his grand children on the Sava River around his famous Ada Ciganlija Island. Rajkos' love was tennis. It was intriguing to listen Rajkos' comments about the required motor skills for this great game. Although he was in his eighties he was still trying to improve his play, service, and base line shuts. Many times, Rajko was instructing much younger partners that they have to work on their fitness if they want to play with him. His roots are from Bosnia and Herzegovina, village Dobromani, where he owned house, and found his peace. He was a born competitor; thus he was always enjoying the game. The last days of May in 2001 he left emptiness in the hearts and souls of Rajko's wife Ljubica, his daughter Vesna, and many friends.\n\nEighty-one years are a short time for history, but these eighty-one years are a history on its own activities, yet he succeeded to maintain his positive actions and non-compromised fight for the best.\n", "id": "40773258", "title": "Rajko Tomović"}
{"url": "https://en.wikipedia.org/wiki?curid=41939036", "text": "Termite-inspired robots\n\nTermite-inspired robots or TERMES robots are biomimetic autonomous robots capable of building complex structures without a central controller. A prototype team of termite-inspired robots was demonstrated by Harvard University researchers in 2014, following four years of development. Their engineering was inspired by the complex mounds that termites build, and was accomplished by developing simple rules to allow the robots to navigate and move building blocks in their environment. By following these simple rules, the robots could construct complex structures through a process called \"stigmergy\", without requiring constant human instruction or supervision.\n\nSocial insects such as termites are capable of constructing elaborate structures such as mounds with complex tunnel systems. They are capable of doing so without an overall plan for the design of a structure, without directly communicating information with each other about how to construct a structure, and without a leader to guide them in constructing a specific structure. Roboticists have been interested in the problem of whether robots can be designed with the limited sensory and motor capabilities of insects such as termites and yet, by following simple rules, construct elaborate and complex structures.\n\nAs part of the TERMES project, the Harvard team designed each robot to perform a few simple behaviors: to move forward, move backward, turn, move up or down a step the size of a specially designed brick, and to move while carrying a brick on top of it. To perform these locomotor behaviors, each robot was equipped with whegs. To detect features of its environment and other robots, each robot was equipped with the following sensors: seven infrared sensors to detect black and white patterns for navigation; an accelerometer to detect tilt angle for climbing; and five ultrasound sonar detectors for determining distance from the perimeter and to nearby robots. To lift, lower, and place the specially designed bricks, each robot had an arm with a spring-loaded gripper to hold a brick while carried. The project's specially designed construction bricks were 21.5 cm × 21.5 cm × 4.5 cm, which was bigger than the footprint (17.5 cm × 11.0 cm) of the robots.\n\nDuring the Harvard trials, groups of termite-inspired robots constructed structures by stigmergy. That is, instead of directly communicating with each other, the robots detected features of their environment and followed simple rules for moving and placing bricks in response to the configuration of bricks that existed at a given time. The rules for constructing a given structure were designed to guarantee that different structures emerged depending on the number of robots involved and on the initial placement of bricks.\n\nThe TERMES robots constitute a proof of concept for the development of relatively simple teams of robots that are capable of constructing complex structures in, for example, remote or hostile locations. The Harvard researchers who built the initial termite-inspired robots have speculated that future robots could be designed to build a base for human habitation on Mars in preparation for the arrival of human astronauts. More immediate possibilities include teams of termite-inspired robots that are capable of moving sandbags and building levees in flood zones.\n\n", "id": "41939036", "title": "Termite-inspired robots"}
{"url": "https://en.wikipedia.org/wiki?curid=25781", "text": "Robot\n\nA robot is a machine—especially one programmable by a computer— capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed to take on human form but most robots are machines designed to perform a task with no regard to how they look.\n\nRobots can be autonomous or semi-autonomous and range from humanoids such as Honda's \"Advanced Step in Innovative Mobility\" (ASIMO) and TOSY's \"TOSY Ping Pong Playing Robot\" (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed \"swarm\" robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous Things are expected to proliferate in the coming decade, with home robotics and the autonomous car as some of the main drivers.\n\nThe branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.\n\nFrom the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.\n\nThe term comes from a Czech word, \"robota\", meaning \"forced labor\"; the word 'robot' was first used to denote a fictional humanoid in a 1920 play \"R.U.R.\" by the Czech writer, Karel Čapek but it was Karel's brother Josef Čapek who was the word's true inventor. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen. The first commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.\n\nRobots have replaced humans in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.\n\nThe word \"robot\" can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior, especially behavior which mimics humans or other animals. Closely related to the concept of a \"robot\" is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.\n\nThe idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a \"speaking\" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the \"Lie Zi\".\n\nMany ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.\n\nIn ancient Greece, the Greek engineer Ctesibius (c. 270 BC) \"applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures.\" In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called \"The Pigeon\". Hero of Alexandria , a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.\nThe 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka. \n\nIn ancient China, the 3rd century text of the \"Lie Zi\" describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the \"Han Fei Zi\" and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (\"ma yuan\") that could successfully fly. In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.\nThe beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours. His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.\n\n\"Samarangana Sutradhara\", a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.\n\nIn Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his \"Vitruvian Man\". It is not known whether he attempted to build it.\n\nIn Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century \"Karakuri zui\" (\"Illustrated Machinery\", 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the \"Butai karakuri\", which were used in theatre, the \"Zashiki karakuri\", which were small and used in homes, and the \"Dashi karakuri\" which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.\n\nIn France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.\n\nRemotely operated vehicles were demonstrated in the late 19th century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).\n\nThe Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it \"the world's first \"practical\" guided missile\". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by \"Hertzian\" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.\n\nArchibald Low, known as the \"father of radio guidance systems\" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.\n\n'Robot' was first applied as a term for artificial automata in a 1920 play \"R.U.R.\" by the Czech writer, Karel Čapek. However, Josef Čapek was named by his brother Karel as the true inventor of the term robot. The word 'robot' itself was not new, having been in Slavic language as \"robota\" (forced laborer), a term which classified those peasants obligated to compulsory service under the feudal system widespread in 19th century Europe (see: Robot Patent).\nČapek's fictional story postulated the technological creation of artificial human bodies without souls, and the old theme of the feudal \"robota\" class eloquently fit the imagination of a new class of manufactured, artificial workers.\n\nIn 1928, one of the first humanoid robots, Eric, was exhibited at the annual exhibition of the Model Engineers Society in London, where it delivered a speech. Invented by W. H. Richards, the robot's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control. Both Eric and his \"brother\" George toured the world.\n\nWestinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.\n\nThe first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors – essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named \"Elmer\" and \"Elsie\", were constructed between 1948 and 1949 and were often described as \"tortoises\" due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\n\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's \"turtles\" may be found in the form of BEAM robotics.\nThe first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol's patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.\n\nThe first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.\n\nCommercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.\n\nVarious techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent \"generation\" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.\n\nAs robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a \"Windows for robots\" system with its Robotics Developer Studio, which has been available since 2007.\n\nJapan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.\n\nMany future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.\nAs early as 1982 people were confident that someday robots would:\n1. clean parts by removing molding flash\n2. spray paint automobiles with absolutely no human presence\n3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes\n4. make electrical cable harness\n5. load trucks with boxes—a packing problem\n6. handle soft goods, such as garments and shoes\n7. shear sheep\n8. prosthesis\n9. cook fast food and work in other service industries\n10. household robot.\n\nGenerally such predictions are overly optimistic in timescale.\n\nIn 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator. Many analysts believe that self-driving trucks may eventually revolutionize logistics. By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. Some analysts believe that within the next few decades, most trucks will be self-driving.\n\nA literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.\n\nBaxter is a new robot introduced in 2012 which learns by guidance. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. Sawyer was added in 2015 for smaller, more precise tasks.\n\nThe word \"robot\" was introduced to the public by the Czech interwar writer Karel Čapek in his play \"R.U.R. (Rossum's Universal Robots)\", published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called \"robots.\" The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).\n\nKarel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the \"Oxford English Dictionary\" in which he named his brother, the painter and writer Josef Čapek, as its actual originator.\n\nIn an article in the Czech journal \"Lidové noviny\" in 1933, he explained that he had originally wanted to call the creatures \"laboři\" (\"workers\", from Latin \"labor\"). However, he did not like the word, and sought advice from his brother Josef, who suggested \"roboti\". The word \"robota\" means literally \"corvée\", \"serf labor\", and figuratively \"drudgery\" or \"hard work\" in Czech and also (more general) \"work\", \"labor\" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as \"robot\" in Hungarian). Traditionally the \"robota\" (Hungarian \"robot\") was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) \"rabota\" \"servitude\" (\"work\" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root \"*orbh-\". \"Robot\" is cognate with the German root \"Arbeit\" (work).\n\nThe word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the \"\"Three Laws of Robotics\"\" which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. \"People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical,\" said Dr. Joanna Bryson of the University of Bath.)\n\nMobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the \"automated guided vehicle\" or \"automatic guided vehicle\" (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.\n\nMobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.\n\nMobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.\n\nIndustrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.\n\nThe International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:\n\n\"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications.\"\n\nThis definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.\n\nMost commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term \"service robot\" is less well-defined. The International Federation of Robotics has proposed a tentative definition, \"A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations.\"\n\nRobots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.\n\nThere are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.\n\nThere have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.\n\nModular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These \"ANAT robots\" can be designed with \"n\" DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.\n\nModular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.\n\nA \"collaborative robot\" or \"cobot\" is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.\n\nThe collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.\n\nRethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. , 190 companies in the US have bought Baxters and they are being used commercially in the UK.\n\nRoughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.\n\nAs robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called \"Plug & Pray\".\n\nVernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this \"the Singularity\". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.\n\nIn 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns. In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.\n\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots. The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.\n\nOne robot in particular, the EATR, has generated public concerns over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.\n\nManuel De Landa has noted that \"smart missiles\" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.\n\nFor centuries, people have predicted that machines would make workers obsolete and increase unemployment, although the causes of unemployment are usually thought to be due to social policy.\n\nA recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.\n\nLawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to improve redundancy laws.\n\nKevin J. Delaney said \"Robots are taking human jobs. But Bill Gates believes that governments should tax companies’ use of them, as a way to at least temporarily slow the spread of automation and to fund other types of employment.\" The robot tax would also help pay a guaranteed living wage to the displaced workers.\n\nAt present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.\n\nRobots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. All robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.\n\nGeneral-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.\n\nOver the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.\n\nIndustrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.\n\nMass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.\n\nMobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.\n\nLimited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.\n\nDeveloped to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.\n\nSuch as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.\n\nThere are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.\n\nAlmost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.\n\nTeleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).\n\nRobots are used to automate picking fruit on orchards at a cost lower than that of human pickers.\n\nDomestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.\n\nMilitary robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.\n\nUnmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.\n\nThe AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.\n\nSome have suggested a need to build \"Friendly AI\", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as \"Robot Legal Studies.\" Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.\n\nMining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia. Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.\n\nDrilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.\n\nRobots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.\n\nRobots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.\n\nThe population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.\n\nFRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.\n\nScript Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient's medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it's the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.\n\nMcKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.\n\nWhile most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.\n\nOne approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.\n\nNanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10 meters). Also known as \"nanobots\" or \"nanites\", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into \"grey goo\", while others argue that this hypothetical outcome is nonsense.\n\nA few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.\n\nRobots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.\n\nInspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.\n\nRobotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called \"haptic interfaces\", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of \"virtual\" objects, which users can experience through their sense of touch.\n\nRobotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also \"bionic men/women\", or humans with significant mechanical enhancements) have become a staple of science fiction.\n\nThe first reference in Western literature to mechanical servants appears in Homer's \"Iliad\". In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, \"Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods.\" The words \"robot\" or \"android\" are not used to describe them, but they are nevertheless mechanical devices human in appearance. \"The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)\". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).\n\nPossibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story \"Runaround\", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: \"A robot may not harm humanity, or, by inaction, allow humanity to come to harm\"; the rest of the laws are modified sequentially to acknowledge this.\n\nAccording to the \"Oxford English Dictionary,\" the first passage in Asimov's short story \"Liar!\" (1941) that mentions the First Law is the earliest recorded use of the word \"robotics\". Asimov was not initially aware of this; he assumed the word already existed by analogy with \"mechanics,\" \"hydraulics,\" and other similar terms denoting branches of applied knowledge.\n\nRobots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the \"Star Wars\" franchise.\n\nThe concept of humanoid sex robots has elicited both public attention and concern. Opponents of the concept have stated that the development of sex robots would be morally wrong. They argue that the introduction of such devices would be socially harmful, and demeaning to women and children.\n\nFears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race.\n\"Frankenstein\" (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.\n\nOther works with similar themes include \"The Mechanical Man\", \"The Terminator, Runaway, RoboCop\", the Replicators in \"Stargate\", the Cylons in \"Battlestar Galactica\", the Cybermen and Daleks in \"Doctor Who\", \"The Matrix\", \"Enthiran\" and \"I, Robot\". Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are \"\", \"Red Planet\" and \"Enthiran\". \n\nThe 2017 game Horizon Zero Dawn explores themes of robotics in warfare, robot ethics, and the AI control problem, as well as the positive or negative impact such technologies could have on the environment.\n\nAnother common theme is the reaction, sometimes called the \"uncanny valley\", of unease and even revulsion at the sight of robots that mimic humans too closely.\n\nMore recently, fictional representations of artificially intelligent robots in films such as \"A.I. Artificial Intelligence\" and \"Ex Machina\" and the 2016 TV adaptation of \"Westworld\" have engaged audience sympathy for the robots themselves.\n\n\n\n\n\n\n", "id": "25781", "title": "Robot"}
{"url": "https://en.wikipedia.org/wiki?curid=27547393", "text": "For Inspiration and Recognition of Science and Technology\n\nFor Inspiration and Recognition of Science and Technology (FIRST) is an international youth organization that operates the FIRST Robotics Competition, FIRST LEGO League, FIRST LEGO League Jr., and FIRST Tech Challenge competitions.\nFounded by Dean Kamen and Woodie Flowers in 1989, its expressed goal is to develop ways to inspire students in engineering and technology fields. Its philosophy is expressed by the organization as \"coopertition\" and \"gracious professionalism\".\nFIRST also operates FIRST Place, a research facility at FIRST headquarters in Manchester, New Hampshire, where it holds educational programs and day camps for students and teachers.\n\nFIRST operates as a non-profit public charity corporation. It licenses qualified teams, usually affiliated with schools or other youth organizations, to participate in its competitions. The teams in turn pay a fee to \"FIRST\"; these fees, the majority of which are redistributed to pay for teams' kit of parts and other services, consist of the majority of \"FIRST\"'s revenue.\n\nThe supreme body of FIRST is its board of directors, which includes corporate executives and former government officials. FIRST also has an executive advisory board and several senior advisors; these advisors include engineers, involved volunteers, and other senior organizers. Day-to-day operations are run by a senior management team, consisting of a president and five vice presidents.\n\nThe first and highest-scale program developed through \"FIRST\" is the \"FIRST\" Robotics Competition (FRC), which is designed to inspire high school students to become engineers by giving them real world experience working with engineers to develop a robot. The inaugural FIRST Robotics Competition was held in 1992 in the Manchester Memorial High School gymnasium. , over 3,000 high school teams totaling over 46,000 students from Australia, Brazil, Canada, Turkey, Israel, Mexico, the Netherlands, the United States, the United Kingdom, and more compete in the annual competition.\n\nThe competition challenge changes each year, and the teams can only reuse certain components from previous years. The robots weigh at most , without batteries and bumpers. The kit issued to each team contains a base set of parts. Registration and the kit of parts together cost about US$6,000. In addition to that, teams are allowed to spend another $3,500 on their robot. The purpose of this rule is to lessen the influence of money on teams' competitiveness. Details of the game have been released on the first Saturday in January (except when that Saturday falls on January 1 or 2), and the teams have been given six weeks to construct a robot that can accomplish the game's tasks.\n\nIn 2011, teams participated in 48 regional and district competitions throughout March in an effort to qualify for the \"FIRST\" Championship in St. Louis in April. Previous years' Championships have been held in Atlanta, Georgia, Houston, Texas and at Walt Disney World's Epcot. On October 7, 2009, FIRST announced that the Championship Event will be held in St. Louis, Missouri for 2011 through 2013.\nEach year the \"FIRST\" Robotics Competition has scholarships for the participants in the program. In 2011, there were over $14 million worth of scholarships from more than 128 colleges and universities, associations, and corporations.\n\nThe district competition system was introduced in Michigan and as of 2017 has expanded to include districts in the Pacific Northwest, the Mid-Atlantic, the Washington DC area, New England, Georgia, North Carolina, Ontario, and Israel. When they were created in 2017, the Ontario and Israel districts became the first districts outside of the United States. The district competition system changed the traditional \"regional\" events by allowing teams to compete in multiple smaller events and using an associated ranking algorithm to determine which teams would advance to the next level of the competition. In general, there have been pushes to move more regions to the districts system; California, Texas, and New York have especially been pushed to move to the district system.\n\nThe FIRST Tech Challenge (FTC), formerly \"FIRST\" Vex Challenge (FVC), is a mid-level robotics competition announced by \"FIRST\" on March 22, 2005. According to \"FIRST\", this competition was designed to be a more accessible and affordable option for schools. FIRST has also said that the FTC program was created for those of an intermediate skill level. FIRST Tech Challenge robots are approximately one-third the scale of their FRC counterparts. The FTC competition is meant to provide a transition for students from the FLL competition to the FRC competition. FTC was developed for the Vex Robotics Design System, which is available commercially.\n\nThe 2005 FVC pilot season featured a demonstration of the \"FIRST\" Vex Challenge using a 1/3 linear scale mock-up of the 2004 FRC Competition, . For their 2005-2006 Pilot Season, FVC teams played the Half-Pipe Hustle game using racquet balls and ramps.\n\nFor the 2006-2007 FTC Season, the \"FIRST\" Tech Challenge teams competed in the Hangin'-A-Round challenge using softballs, rotating platforms, a hanging bar, and a larger 'Atlas' ball which is significantly larger than most Vex robots and harder to manipulate. Competitions were held around the United States, Canada, and Mexico.\n\nFor the 2008-2009 FTC season, a new kit was introduced, as \"FIRST\" moved away from the VEX platform and worked with several different vendors to create a custom kit and control system for FTC known as Tetrix. Based around the LEGO Mindstorms NXT \"brain\" and including secondary specialized controllers to overcome the limitations of the NXT, teams use a Bluetooth link between the NXT and a laptop running FTC driver station software. A team's drivers then use either one or two USB gamepads to control their robots.\n\nFor the 2015-2016 FTC season, in a partnership with Qualcomm, the LEGO Mindstorms NXT was replaced as the \"brain\" of the robot by an android device which communicates to a separate \"driver station\" android device via Wifi Direct. In addition, students were allowed to use either MIT App Inventor or Android Studio (Java language) to program their robots.\n\nIn 1998, the \"FIRST\" LEGO League (FLL), a program similar to the \"FIRST\" Robotics Competition, was formed. It is aimed at 9 to 14-year-old students and utilizes LEGO Mindstorms sets (EV3, NXT, RCX) to build palm-sized LEGO robots, which are then programmed using either the ROBOLAB software (RCX-based systems) or Mindstorms NXT or EV3 software (for NXT or EV3-based systems respectively) to autonomously compete against other teams. The ROBOLAB software is based on National Instruments' LabVIEW industrial control engineering software. The combination of interchangeable LEGO parts, computer 'bricks', sensors, and the aforementioned software, provide preteens and teenagers with the capability to build simple models of real-life robotic systems. This competition also utilizes a research element that is themed with each year's game, and deals with a real-world situation for students to learn about through the season.\n\nThe simplistic nature of its games, its relatively low team startup costs, and its association with the Lego Group mean that it is the most extensive of all \"FIRST\" competitions, despite a lower profile and fewer sponsors than \"FIRST\" Tech Challenge or \"FIRST\" Robotics Competition. In 2009, 14,725 teams from 56 countries participated in local, regional, national, and international competitions, compared with around 1,600 teams in roughly 10 countries for FRC.\n\n\"FIRST\" LEGO League Jr. is a variation of the \"FIRST\" LEGO League, aimed towards elementary school children, in which kids ages 5 to 8 build LEGO models dealing with that year's FLL challenge. At least one part of a model has a moving component. The teams participate in exhibitions around the country, where they demonstrate and explain their models and research for award opportunities.\n\nThe \"FIRST\" Championship is the annual event which celebrates the finale of all of their programs by bringing them all together for their final rounds in the same event. The \"FIRST\" Championship is scheduled to be in St. Louis, Missouri through 2017. At the 2014 Championship, \"FIRST\" announced changes to the 2015 structure that will bring a more \"Olympic Village\" feeling, and involves a rearrangement of the programs around the city.\n\n\"FIRST\" itself is a self-supporting organization; however, individual teams typically rely on outside funding sources. It also takes significant outside funds to run regional events and the \"FIRST\" Championship. In 2010, \"FIRST\" was a recipient of a Google Project 10^100 grant.\n\nTeams may request that team members, whether mentors or students, contribute to the costs of running a team. For example, members may pay a fee or donate tools and facilities.\n\nTeams frequently give other teams support. This may mean providing funds, tools, or facilities. Gracious professionalism and Coopertition are core tenets of the \"FIRST\" philosophy.\n\nGracious Professionalism is a major belief in the \"FIRST\" community. At every regional and national competition, the judges look for teams to be graciously professional. What gracious professionalism is all about is \"competing on an even playing field\". That means that each team wants their competition at the best. The way the team system is set up is that every team is matched up with two other teams per match at random. Therefore, a team's ally in one match may become an opponent in the next match. Traditionally, outside of \"FIRST\", when one shares resources in a competition, one only does so with their allies.\n\nHowever, with the element of gracious professionalism, one would share resources with their opponent as well. For example, if a team needs a part or tool to fix their robot, it is expected that any team, even an opposing team would give that team a hand in order to compete. \nThis helps student learn that success is in learning and helping others no matter the circumstances. With this in mind, the judges give a Gracious Professionalism award at every \"FIRST\" Robotics Competition tournament, to a team that shows outstanding gracious professionalism.\n\nThe term \"Gracious Professionalism\" was created by Dr. Woodie Flowers, \"FIRST\" National Advisor and Pappalardo Professor Emeritus of Mechanical Engineering, Massachusetts Institute of Technology.\n\nThe most common method of monetary and resource sponsorship teams comes through the community surrounding the team. Since the majority of teams are based around a school or a school district, schools often provide the infrastructure needed to run a team. Local governments and individual citizens may provide funds and other support to teams. Local universities and colleges often give significant funds to teams.\n\nCorporate donations and grants usually provide the majority of a mature team's funds. Major donors include BAE Systems, Google, Raytheon, and National Instruments.\n\nEach year during his speech at the kickoff event, founder Dean Kamen gives the student participants a homework assignment. It often involves spreading the word about \"FIRST\" in various ways, such as increasing attendance at regionals (2005), mentoring rookie teams, making sure that \"FIRST\"-specific scholarships are applied for (2004), and researching the capabilities of motors and disseminating that information to other teams (2006). In 2007, Dean's homework was for each team to contact their government officials (e.g. mayors, legislators, governors, federal officials) and invite them to a \"FIRST\" regional or the championship to expose them to the competition and increase the level of political awareness of FIRST. In 2008, it was to inform the media more about \"FIRST\". In 2009, the homework was for each team to have all students, mentors, and other persons involved with their team (past or present) register with \"FIRST\". One goal of this registration process was to provide \"FIRST\" with data to demonstrate that many people had benefited from their experiences in \"FIRST\" robotics and to encourage more funding of robotics-related events.\n\nAt the World Championship in Atlanta, speakers have included former President of the United States George Herbert Walker Bush in 2008, and United States Secretary of Education Arne Duncan in 2010. In 2010, former U.S. Undersecretary of Commerce and Director of the U.S. Patent and Trademark Office Jon Dudas was selected to be the President of FIRST.\n\nAt the Championship in St. Louis, President of the United States Barack Obama has spoken via a pre-recorded message every year from 2011-2014.\n\n\"FIRST\" has received the attention of politicians in Canada as well. Ontario MPP Bob Delaney and Ontario MPP Victor Fedeli have made remarks in the Legislative Assembly of Ontario regarding their FRC experiences and showing their support.\n\nNASA, through its Robotics Alliance Project, is a major supporter of \"FIRST\".\n\n\"FIRST\" seeks to promote a philosophy of teamwork and collaboration among engineers and encourages competing teams to remain friendly, helping each other out when necessary. Terms frequently applied to this ethos are \"Gracious Professionalism\" and \"Coopertition\"; terms coined by Woodie Flowers and Kamen that support respect towards one's competitors and integrity in one's actions. The concept of Gracious Professionalism grew from a robotics class that Flowers taught at Massachusetts Institute of Technology. Coopertition is patented under US Patent 7,507,169 by Dean Kamen.\n\nNote: All years indicate the year that the championship for that game was held.\n\n", "id": "27547393", "title": "For Inspiration and Recognition of Science and Technology"}
{"url": "https://en.wikipedia.org/wiki?curid=43065875", "text": "Terrainability\n\nThe terrainability of a machine or robot is defined as its ability to negotiate terrain irregularities.\n\nTerrainability is a term coined in the research community and related to locomotion in the field of mobile robotics. Its various definitions generically describe the ability of the robot to handle various terrains in terms of their ground support, obstacle sizes and spacing, passive/dynamic stability, etc.\n", "id": "43065875", "title": "Terrainability"}
{"url": "https://en.wikipedia.org/wiki?curid=34846522", "text": "Ivar Mendez\n\nIvar Mendez, M.D., PhD, is the Fred H. Wigmore Professor and Chairman of Surgery at the University of Saskatchewan and the Unified Head of Surgery for the Province of Saskatchewan. He is internationally known for his work in cell transplantation for Parkinson's disease and the use of remote presence robotics in neurosurgery and primary health care.\n\nHe also holds an appointment at the Department of Psychology and Neuroscience at Dalhousie University and he is one of the founders of the Brain Repair Centre. He is also the president and founder of the Ivar Mendez International Foundation that is dedicated to providing health and educational assistance to children in the Bolivian Andes. Mendez is a photographer and sculptor and has published 4 books of photography.\n\nMendez was born in La Paz, Bolivia and immigrated with his family to Canada as a teenager. He obtained a BSc degree from the University of Toronto and then an M.D. from the University of Western Ontario (UWO). He did a neurosurgical residency training in London, and was certified in Neurosurgery from the Royal College of Physician and Surgeons of Canada in 1994 and from the American Board of Neurological Surgery in 1996. He became a fellow of the American College of Surgeons in 1998 and became a member of the College Board of Governors in 2015. His interest in regenerative medicine led him to obtain a PhD in Anatomy and Neurobiology from the UWO his PhD thesis was on “Neurotransmitter Interactions in Nigral Grafts”. He did a postdoctoral fellowship at the University of Lund in Sweden under the supervision of Anders Björklund, considered the “father” of cell transplantation in Parkinson's disease. In 2014, Saint Mary's University (Halifax) in Nova Scotia awarded Mendez a Doctor of Science (honoris causa) degree for his contribution to Neuroscience and he was inducted a Fellow to the Canadian Academy of Health Sciences. In 2016, Dr. Mendez received the Government of Canada Public Service Award of Excellence for the use of remote presence robotic technology to improve healthcare in the Canadian North.\n\nHe pioneered the technique of multiple grafts to restore dopamine input to the parkinsonian mammalian brain. This technique was translated into clinical trials in patients with Parkinson's disease and showed long-term survival of those grafts. He also pioneered the use of Glial Derived Neurotrophic Factor (GDNF) in combination with fetal cells in humans. Mendez invented a transplantation delivery system to inject cells into the human brain. With his team, he performed the first long-distance brain surgery robotic telementoring in the world by using a robotic arm to mentor neurosurgeons located 400 km away.\nIn 2015, Mendez and his team printed the first 3D brain for planning deep brain stimulation surgery. Research in 3D brain printing led in 2016 to the development of a virtual reality (VR) brain for medical education and surgical planning applications.\n\n\nMendez has established a Canadian charitable organization, the Ivar Mendez International Foundation, to provide nutrition, dental care and art program to children in remote locations of the Bolivian Andes.\n\n\nHe has published 4 books of photography and has had several exhibitions of his photography and sculpture \n\n\n", "id": "34846522", "title": "Ivar Mendez"}
{"url": "https://en.wikipedia.org/wiki?curid=43921696", "text": "Salvius\n\nSalvius () is the first open source humanoid robot to be built in the United States. Introduced in 2008, Salvius, whose name is derived from the word 'salvaged', has been constructed with an emphasis on using recycled components and materials to reduce the costs of designing and construction. The robot is designed to be able to perform a wide range of tasks by having a body structure that is similar to that of a human. The primary goal for Salvius is to create a robot that can function dynamically in a domestic environment.\n\nSalvius is a part of the open source movement which means that all of the robot's source code is freely available for others to use. Unlike other humanoid robots, Salvius benefits from the advantages of open source software such as allowing any problems to be quickly addressed by a community of developers. The open source nature of the robot's code also makes it possible for anyone to learn about how it works. Salvius has been used as a resource by STEM educators to enable students to learn about many subjects in science and technology.\n\nUnlike many robots, salvius does not use an acronym for a name. The name \"Salvius\" dates back to the time of the Roman Empire, however, it was chosen for this robot because of its similarity to the word \"salvage\". Names haven been a significant part of this robot's development. Salvius is tattooed with the names of the individuals and businesses that have contributed to the project's progress.\n\nSalvius is intended to be a resource for developers to experiment with machine learning and kinematic applications for humanoid robots. The robot is designed to allow new hardware features to be added or removed as needed using plug and play USB connections. Recent changes to the robots design have improved the robot's ability to connect to other devices so that developers can also investigate new ways that robots can interact with the Internet of Things (IoT).\n\nThe robots construction has been documented since 2010. Along with emphasis on recycling, any commercially available parts used on the robot were chosen with availability and economic affordability in mind. Much of the robot's hardware is also open source. Hardware items such as the Raspberry Pi and Arduino microcontrollers were selected because of their open source design and the support communities that exist for these components. The robot uses multiple Arduino microcontrollers which were chosen based on the versatility and popularity of the platform across communities.\n\nThe robot's computer runs Raspbian Linux and primarily uses open source software. Salvius is able to operate autonomously as well as being controlled remotely using an online interface. The reason behind making the robot open source was to provide a platform for students and enthusiasts to build their own robots. The robot's programming languages include: Python, Arduino, and JavaScript. Python was chosen because of its status as the supported language of the Raspberry Pi. C is used for programming the Arduino micro-controllers that the robot's main computer, a Raspberry Pi, communicates with. By sending tasks off to several other boards it allows the robot to do parallel processing and to distribute the work load. The [star network] topography of the robot's network also prevents a failure in one of the Arduino procession nodes from crippling the robot entirely.\n\nSalvius has an API which allows users to send and retrieve data from the robot. When the robot's wireless connection is turned on, the robot can be controlled web interface to see exactly what the robot is seeing and to direct its actions accordingly. Since all the software is installed on the robot the user only needs a device with a working internet connection and a browser.\n\nThe robot is controlled by a network of Raspberry Pi and Arduino microcontrollers. The Raspberry Pi acts as a server which allows [high level programming languages] to be used to control the robot. A combination of several computers allows various tasks to be processed in parallel so that they can be completed quicker. The robot uses Grove motor controllers to control a variety of motors. Most of the robots motors have been salvaged from a variety of sources and reused to construct the robot.\n\nThe robots design incorporates a variety of sensors which allow the robot to successfully interact with its environment. Sensors that have been used on the robot include: touch, sound, light, ultrasonic, and a PIR (Passive infrared sensor). The robot also has an ethernet-connected IP camera which serves as its primary optical input device.\n", "id": "43921696", "title": "Salvius"}
{"url": "https://en.wikipedia.org/wiki?curid=44777146", "text": "Frubber\n\nFrubber (from \"flesh rubber\") is a patented elastic form of rubber used in robotics. The spongy elastomer has been used by Hanson Robotics for the face of its android robots, including Einstein 3 and Sophia.\n", "id": "44777146", "title": "Frubber"}
{"url": "https://en.wikipedia.org/wiki?curid=45185419", "text": "Robotic sensors\n\nRobotic sensors are used to estimate a robot's condition and environment. These signals are passed to a controller to enable appropriate behavior.\n\nSensors in robots are based on the functions of human sensory organs. Robots require extensive information about their environment in order to function effectively.\n\nSensors provide analogs to human senses and can monitor other phenomena for which humans lack explicit sensors.\n\nSensors can measure physical properties, such as the distance between objects, the presence of light and the frequency of sound.\nThey can measure:\n\nMotion controllers, potentiometers, tacho-generators and encoder are used as joint sensors, whereas strain-gauge based sensing is used at the end-effector location for contact force control.\n\nIt is the part of the robot.Internal sensors measure the robot's internal state. They are used to measure position, velocity and acceleration of the robot joint or end effectors.\n\nPosition sensors measure the position of a joint (the degree to which the joint is extended). They include:\n\nA velocity or speed sensor measures consecutive position measurements at known intervals and computes the time rate of change in the position values.\n\nIn a parts feeder, a vision sensor can eliminate the need for an alignment pallet. Vision-enabled insertion robots can precisely perform fitting and insertion operations of machine parts.\n\n", "id": "45185419", "title": "Robotic sensors"}
{"url": "https://en.wikipedia.org/wiki?curid=44484413", "text": "Autonomous spaceport drone ship\n\nAn autonomous spaceport drone ship (ASDS) is an ocean-going vessel derived from a deck barge, outfitted with station-keeping engines and a large landing platform. Construction of such ships was commissioned by aerospace company SpaceX to allow for recovery of rocket first-stages at sea for high-velocity missions which do not carry enough fuel to return to the launch site after lofting spacecraft onto an orbital trajectory.\n\nSpaceX has two operational drone ships: Just Read the Instructions in the Pacific for launches from Vandenberg, and Of Course I Still Love You in the Atlantic for launches from Cape Canaveral. , 17 Falcon 9 flights have attempted to land on a drone ship, with 12 of them succeeding, the first vertical landing being the CRS-8 mission in April 2016.\n\nThe ASDS ships are a key component of the SpaceX reusable launch system development program which aims to significantly lower the price of space launch services through \"full and rapid reusability.\" Any flights going to geostationary orbit or exceeding escape velocity will require landing at sea, encompassing about half of SpaceX missions.\n\nIn 2009, SpaceX CEO Elon Musk articulated ambitions for \"creating a paradigm shift in the traditional approach for reusing rocket hardware.\"\n\nIn October 2014, SpaceX publicly announced that they had contracted with a Louisiana shipyard to build a floating landing platform for reusable orbital launch vehicles. Early information indicated that the platform would carry an approximately landing pad and would be capable of precision positioning so that the platform could hold its position for launch vehicle landing. On 22 November 2014 Musk released a photograph of the \"autonomous spaceport drone ship\" along with additional details of its construction and size.\n\nAs of December 2014, the first drone ship used, the McDonough Marine Service's \"Marmac 300\" barge, was based in Jacksonville, Florida, at the northern tip of the JAXPORT Cruise Terminal () where SpaceX built a stand to secure the Falcon stage during post-landing operations. The stand consists of four , tall and wide pedestal structures bolted to a concrete base. A mobile crane will lift the stage from the ship and place it on the stand. Tasks such as removing or folding back the landing legs prior to placing the stage in a horizontal position for trucking will occur here.\n\nThe ASDS landing location for the first landing test was in the Atlantic approximately northeast of the launch location at Cape Canaveral, and southeast of Charleston, South Carolina.\nOn 23 January 2015, during repairs to the ship following the unsuccessful first test, Musk announced that the ship was to be named \"Just Read the Instructions\", with a sister ship planned for west coast launches to be named \"Of Course I Still Love You\". On 29 January, SpaceX released a manipulated photo of the ship with the name illustrating how it would look once painted. Both ships are named after two GCUs (General Contact Units), spaceships commanded by autonomous artificial intelligences, that appear in \"The Player of Games\", a Culture novel by Iain M. Banks.\n\nThe first \"Just Read the Instructions\" was retired in May 2015 after approximately six months of service in the Atlantic, and its duties were assumed by \"Of Course I Still Love You\". The former ASDS was modified by removing the wings that had extended the barge surface and the equipment (thrusters, communications gear, etc.) that had been added to refit it as an ASDS; these items were saved for future reuse.\n\nSpaceX subsequently leased two additional deck barges—\"Marmac 303\" and \"Marmac 304\"—and initiated refit to construct two additional autonomous-operation-capable ASDS ships, built on the hulls of these Marmac barges.\n\nThe second ASDS barge, \"Of Course I Still Love You\" (\"OCISLY\"), had been under construction in a Louisiana shipyard since early 2015 using a different hull—\"Marmac 304\"—in order to service launches on the east coast. It was built as a replacement for the first \"Just Read the Instructions\" and entered operational service for Falcon 9 Flight 19 in late June 2015. As of June 2015, its home port was Jacksonville, Florida, but after December 2015, it was transferred further south, at Port Canaveral.\n\nWhile the dimensions of the ship are nearly identical to the original ASDS, several enhancements were made including a steel blast wall erected between the aft containers and the landing deck. The ship was in place for a first-stage landing test on the CRS-7 mission, which failed on launch on 28 June 2015.\n\nOn 8 April 2016 the first stage, which launched the Dragon CRS-8 spacecraft, successfully landed for the first time ever on \"OCISLY\", which is also the first ever drone ship landing.\n\nA third ASDS barge, using the \"Marmac 303\" barge, was built during 2015 in a Louisiana shipyard, and the barge transited the Panama Canal in June 2015 carrying its wing extensions as cargo on the deck because the ASDS, when complete, would be too wide to pass through the canal.\n\nThe home port for the \"Marmac 303\" is the Port of Los Angeles, at the AltaSea marine research and business campus in San Pedro's outer harbor. The landing platform and tender vessels began docking there in July 2015 in advance of the main construction of AltaSea which is scheduled for 2017.\n\nSpaceX announced that the \"Marmac 303\" would be the second ASDS to be named \"Just Read the Instructions\" (\"JRtI\") in January 2016, shortly before its first use as a landing platform for Falcon 9 Flight 21.\n\nOn 17 January 2016, \"JRtI\" was put to first use in an attempt to recover a Falcon 9 first-stage booster from the Jason-3 mission from Vandenberg. The booster successfully landed on the deck, however, a lockout collet failed to engage on one of the legs causing the rocket to tip over, exploding on impact with the deck. On January 14, 2017, SpaceX launched Falcon 9 Flight 29 from Vandenberg and landed the first stage on the \"JRtI\" that was located about downrange in the Pacific Ocean, making it the first successful landing in the Pacific.\n\nASDSes are autonomous vessels capable of precision positioning, originally stated to be within even under storm conditions, using GPS position information and four diesel-powered azimuth thrusters. In addition to the autonomous operating mode, the ships may also be telerobotically controlled.\n\nThe azimuth thrusters are hydraulic propulsion outdrive units with modular diesel-hydraulic-drive power units and a modular controller all manufactured by Thrustmaster, a marine equipment manufacturer. The returning rocket must not only land within the confines of the deck surface but must also deal with ocean swells and GPS errors.\n\nSpaceX equips the ships with a variety of sensor and measurement technology to gather data on the booster returns and landing attempts, including commercial off the shelf GoPro cameras.\n\nAt the center of the ASDS landing pads is a circle that encloses the SpaceX stylized \"X\" in an X-marks-the-spot landing point.\n\nThe two ASDS names used so far, Just Read the Instructions (\"JRtI\"), and Of Course I Still Love You (\"OCISLY\"), pay homage to the works of the late science fiction author Iain M. Banks, and are drawn from his \"Culture\" fictional universe. Both \"JRtI\" and \"OCISLY\" are names of enormous, sentient starships, which appeared in the novel \"The Player of Games\".\n\nThe landing platform of the upper deck of the first vessel named \"Just Read the Instructions\" was while the span of the Falcon 9 v1.1 landing legs was .\n\n\"Of Course I Still Love You\" was built as a refit of the \"Marmac 304\" barge. It is home ported in Florida at Port Canaveral since December 2015, after being ported for a year at the Port of Jacksonville during most of 2015.\n\n\"Just Read the Instructions\" was built as a refit of the deck barge \"Marmac 303\" in 2015 for service in the Pacific Ocean. It is home ported at the Port of Los Angeles in southern California.\n\nA tug, \"Elsbeth III\" on the east coast, is used to bring the ASDS to its oceanic position. During rocket landing operations a support ship, \"Go Quest\" on the east coast, is typically standing by some distance away from the crewless ASDS. Following landing, technicians and engineers will board the landing platform, and secure the rocket's landing legs to lock the vehicle in place for transport back to port. The rocket stage is secured to the deck of the drone ship with steel hold downs welded on to the feet of the landing legs. In June 2017, \"OCISLY\" started being deployed with a robot that drives under the rocket and grabs onto the hold-down clamps located on the outside of the Falcon 9's structure after landing. Fans call the robot ‘Optimus Prime’ or ‘Roomba,' the latter of which has been retroactively transformed into an acronym for 'Remotely Operated Orientation & Mass Balance Adjustment.'\n\nOne of the series of autonomous ships has been used for four flight tests, three in early 2015 (although only two actually attempted landings on an ASDS) plus one more in January 2016.\n\nThe first test was 10 January 2015\nwhen SpaceX conducted a controlled-descent flight test to land the first-stage of Falcon 9 Flight 14 on a solid surface after it was used to loft a contracted payload toward Earth orbit. SpaceX projected prior to the first landing attempt that the likelihood of successfully landing on the platform would be 50 percent or less.\n\nSpaceX attempted a landing on \"Just Read the Instructions\" on 10 January 2015. Many of the test objectives were achieved, including precision control of the rocket's descent to land on the platform at a specific point in the south Atlantic Ocean and a large amount of test data was obtained from the first use of grid fin control surfaces used for more precise reentry positioning. However the landing was a hard landing.\n\nThe SpaceX webcast indicated that the boostback burn and reentry burns for the descending first-stage occurred, and that the descending rocket then went \"below the horizon,\" as expected, which eliminated the live telemetry signal. Shortly thereafter, SpaceX released information that the rocket did get to the drone spaceport ship as planned, but \"landed hard ... Ship itself is fine. Some of the support equipment on the deck will need to be replaced.\"\n\n\"Just Read the Instructions\" was towed to sea once again in early February for the DSCOVR satellite launch on 11 February 2015 but, in the event, was not used for a landing attempt. Ocean conditions of -high waves interfered with the ASDS recovery duties for the landing, so the ship returned to port and no landing test occurred. SpaceX executed a soft landing in the sea to continue data gathering for future landing attempts. The soft landing was successful, Elon Musk tweeted that it landed with a lateral accuracy of away from the target and in a vertical position.\n\nOn 14 April 2015, SpaceX made a second and final attempt to land a Falcon first-stage on the \"Marmac 300\" drone ship. Early news from Elon Musk suggested that it made a hard landing. He later clarified that it appeared to have made a vertical landing on the ship, but then toppled over due to excessive remaining lateral momentum.\n\nIn order to prepare for Falcon 9 Flight 19 on 28 June 2015, the new ASDS, \"Of Course I Still Love You\", was towed out to sea to prepare for a third landing test. This was its first operational assignment. However, the Falcon launch rocket disintegrated before first-stage shutdown so the mission never progressed to the point where the controlled-descent test was to be initiated.\n\nSpaceX indicated on 7 January that there would be an attempt to land on the new west coast ASDS following the launch of Falcon 9 Flight 21 scheduled for 17 January 2016.\n\n\"Just Read the Instructions\" was located about downrange from the launch site in the Pacific Ocean. Elon Musk later reported that the first-stage did successfully soft-land on the ship, but a lockout latch on one of the landing legs failed to latch and the first-stage fell over, causing a breach of the propellant tanks and a deflagration on impact with the drone ship.\n\nDuring a launch of a heavy communications satellite on 4 March 2016, SpaceX did an experimental descent and landing attempt with very low propellant margins. For the first time, and in order to reduce the propellant required, SpaceX attempted the landing burn with three engines. SpaceX had indicated that the test was unlikely to result in a successful landing and recovery. In the event, one engine flamed out early, and the rocket hit the deck surface with considerable velocity, destroying the rocket and causing damage to the drone ship's deck. By March 21, 2016, the deck of the drone ship was nearly repaired.\n\nThe Falcon 9 first-stage performed a successful landing on \"Of Course I Still Love You\" in the Atlantic Ocean off the coast of Florida at T+9 minutes and 10 seconds after liftoff, the first-ever successful landing of the first-stage on an Autonomous Spaceport Drone Ship. The rocket was successfully affixed to the barge for the maritime transport portion of the journey back to port, and successfully completed its journey, entering Port Canaveral early in the morning on 12 April 2016.\n\nOn May 6, 2016, SpaceX landed the first stage of the Falcon 9 on \"OCISLY\" during the JCSat-14 mission, its second time successfully landing on a drone ship at sea, and its first time recovering a booster from a high-velocity (GTO) mission.\n\nOn May 27, 2016, SpaceX landed the first stage of a Falcon 9 on \"OCISLY\" during the Thaicom 8 mission, its third time successfully landing on a drone ship at sea.\n\nOn June 15, 2016, SpaceX failed to land the first stage of the Falcon 9 on \"OCISLY\" during the Asia Broadcast Satellite/Eutelsat mission. Elon Musk tweeted that one of the three engines had low thrust, and when the rocket was just off the deck, the engines ran out of oxidizer.\n\nSpaceX's Falcon 9 Flight 28 propelled the Japanese JCSAT-16 telecommunications satellite to a geosynchronous transfer orbit on August 14, 2016. The first stage re-entered the atmosphere and during the night landed vertically on \"Of Course I Still Love You\", positioned in the Atlantic Ocean nearly 400 miles from the Florida coastline; unlike previous successful landings, this landing-burn only used one engine, not three.\n\nOn January 14, 2017, the first-stage of a Falcon 9 landed on the Pacific ASDS \"JRtI\" during the Iridium NEXT-1 mission. This marked the first successful landing on \"JRtI\" and the first landing in the Pacific Ocean.\n\nOn March 30, 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You,\" during the SES-10 launch. This was the first successful launch and landing of a previously flown orbital booster.\n\nOn 23 June 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You,\" during the BulgariaSat-1 launch. This was the second successful launch and landing of a previously flown orbital booster. This was also the first booster to have landed on both drone ships. While the landing was considered a success, the first-stage was \"slammed sideways\" and suffered a 'hard landing' which resulted in 'most of the emergency crush core being used'.\n\nOn 25 June 2017, the first stage of a Falcon 9 landed on \"Just Read the Instructions,\" during the Iridium launch.\n\nOn 24 August 2017, the first-stage of a Falcon 9 landed on \"Just Read the Instructions\" during the FORMOSAT-5 launch.\n\nOn 9 October 2017, the first stage of a Falcon 9 landed on \"Just Read the Instructions,\" during the Iridium launch.\n\nOn 11 October 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You\" during the SES-11 launch.\n\nOn 30 October 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You\" during the Koreasat 5A mission.\n\n\n", "id": "44484413", "title": "Autonomous spaceport drone ship"}
{"url": "https://en.wikipedia.org/wiki?curid=45370555", "text": "LUTZ Pathfinder\n\nThe two-seater prototype pod has been built by Coventry-based RDM Group, and was first shown to the public in February 2015.\n\nThe LUTZ (Low-carbon Urban Transport Zone) Pathfinder pod is part of the UK Government's Transport Systems Catapult Autodrive project, a 20 million project.\n\nThree pods were tested initially in Milton Keynes during 2015 to ensure that they can comply with the Highway Code. A further trial, with a four seat vehicle developed from the LUTZ, commenced along a guided busway near Cambridge for possible use in an after hours service.\n\nThe pod is a two-seater electric car with space for luggage. It has a limited top speed of and has a range of or can run for eight hours. The self-driving equipment includes 19 sensors, cameras, radar and Lidar. Users can hail them by using a smartphone app.\n\nThe autonomous control software is developed by Mobile Robotics Group from University of Oxford.\n\nThe Lutz Pathfinder pod has been developed by the UK Automotive Council, Department for Business, Innovation and Skills and RDM Group.\n\nThe first trial of autonomous operation on a public road, with pedestrians, cycles and other vehicles, was conducted in Milton Keynes on 11 October 2016. The vehicles \"operated as expected.\"\n\nIn October 2017 a trial service, using a four-seat vehicle, between Trumpington Park and Ride and Cambridge railway station along the guided busway, for possible use in an after hours service. The £250,000 project was part funded by Innovate UK and delivered in partnership with Connecting Cambridgeshire and the Smart Cambridge Programme. If the trial is successful a ten seat vehicle will be used in routine service.\n\n", "id": "45370555", "title": "LUTZ Pathfinder"}
{"url": "https://en.wikipedia.org/wiki?curid=45533018", "text": "CougarTech\n\nCougarTech, FRC team 2228, is a FIRST Robotics Competition team that was founded in 2007, and is a school-based team from the Honeoye Falls-Lima Central School District in Honeoye Falls, New York. The team also represents the Rush–Henrietta Central School District in competition. CougarTech is a veteran team that prides itself on its coopertition with other teams as well as its teaching of younger teams. During the all important six weeks of build season, the team builds a robot to play the year's new game that FIRST designs each year. During the off season, the team focuses work on public outreach, recruitment, fundraising and sponsorship to support their team and FIRST. \n\n\n", "id": "45533018", "title": "CougarTech"}
{"url": "https://en.wikipedia.org/wiki?curid=189749", "text": "Automaton\n\nAn automaton (; plural: automata or automatons) is a self-operating machine, or a machine or control mechanism designed to automatically follow a predetermined sequence of operations, or respond to predetermined instructions. Some automata, such as bellstrikers in mechanical clocks, are designed to give the illusion to the casual observer that they are operating under their own power.\n\nThe word \"automaton\" is the latinization of the Greek , \"automaton\", (neuter) \"acting of one's own will\". This word was first used by Homer to describe automatic door opening, or automatic movement of wheeled tripods. It is more often used to describe non-electronic moving machines, especially those that have been made to resemble human or animal actions, such as the \"jacks\" on old public striking clocks, or the cuckoo and any other animated figures on a cuckoo clock.\n\nThere are many examples of automata in Greek mythology: Hephaestus created automata for his workshop; Talos was an artificial man of bronze; Daedalus used quicksilver to install voice in his moving statues; King Alkinous of the Phaiakians employed gold and silver watchdogs.\n\nThe automata in the Hellenistic world were intended as tools, toys, religious idols, or prototypes for demonstrating basic scientific principles. Numerous water powered automata were built by Ktesibios, a Greek inventor and the first head of the Great Library of Alexandria, for example he \"\"used water to sound a whistle and make a model owl move. He had invented the world's first \"cuckoo\" clock\"\". This tradition continued in Alexandria with inventors such as the Greek mathematician Hero of Alexandria (sometimes known as Heron), whose writings on hydraulics, pneumatics, and mechanics described siphons, a fire engine, a water organ, the aeolipile, and a programmable cart.\n\nComplex mechanical devices are known to have existed in Hellenistic Greece, though the only surviving example is the Antikythera mechanism, the earliest known analog computer. It is thought to have come originally from Rhodes, where there was apparently a tradition of mechanical engineering; the island was renowned for its automata; to quote Pindar's seventh Olympic Ode:\n\nHowever, the information gleaned from recent scans of the fragments indicate that it may have come from the colonies of Corinth in Sicily and implies a connection with Archimedes.\n\nAccording to Jewish legend, Solomon used his wisdom to design a throne with mechanical animals which hailed him as king when he ascended it; upon sitting down an eagle would place a crown upon his head, and a dove would bring him a Torah scroll. It's also said that when King Solomon stepped upon the throne, a mechanism was set in motion. As soon as he stepped upon the first step, a golden ox and a golden lion each stretched out one foot to support him and help him rise to the next step. On each side, the animals helped the King up until he was comfortably seated upon the throne.\n\nIn ancient China, a curious account of automata is found in the Lie Zi text, written in the 3rd century BC. Within it there is a description of a much earlier encounter between King Mu of Zhou (1023-957 BC) and a mechanical engineer known as Yan Shi, an 'artificer'. The latter proudly presented the king with a life-size, human-shaped figure of his mechanical handiwork:\nThe king stared at the figure in astonishment. It walked with rapid strides, moving its head up and down, so that anyone would have taken it for a live human being. The artificer touched its chin, and it began singing, perfectly in tune. He touched its hand, and it began posturing, keeping perfect time...As the performance was drawing to an end, the robot winked its eye and made advances to the ladies in attendance, whereupon the king became incensed and would have had Yen Shih [Yan Shi] executed on the spot had not the latter, in mortal fear, instantly taken the robot to pieces to let him see what it really was. And, indeed, it turned out to be only a construction of leather, wood, glue and lacquer, variously coloured white, black, red and blue. Examining it closely, the king found all the internal organs complete—liver, gall, heart, lungs, spleen, kidneys, stomach and intestines; and over these again, muscles, bones and limbs with their joints, skin, teeth and hair, all of them artificial...The king tried the effect of taking away the heart, and found that the mouth could no longer speak; he took away the liver and the eyes could no longer see; he took away the kidneys and the legs lost their power of locomotion. The king was delighted.\nOther notable examples of automata include Archytas's dove, mentioned by Aulus Gellius. Similar Chinese accounts of flying automata are written of the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban, who made artificial wooden birds (\"ma yuan\") that could successfully fly according to the \"Han Fei Zi\" and other texts.\n\nThe manufacturing tradition of automata continued in the Greek world well into the Middle Ages. On his visit to Constantinople in 949 ambassador Liutprand of Cremona described automata in the emperor Theophilos' palace, including \n\"lions, made either of bronze or wood covered with gold, which struck the ground with their tails and roared with open mouth and quivering tongue,\" \"a tree of gilded bronze, its branches filled with birds, likewise made of bronze gilded over, and these emitted cries appropriate to their species\" and \"the emperor’s throne\" itself, which \"was made in such a cunning manner that at one moment it was down on the ground, while at another it rose higher and was to be seen up in the air.\" \nSimilar automata in the throne room (singing birds, roaring and moving lions) were described by Luitprand's contemporary Constantine Porphyrogenitus, who later became emperor, in his book \"Περὶ τῆς Βασιλείου Τάξεως\".\n\nIn the mid-8th century, the first wind powered automata were built: \"statues that turned with the wind over the domes of the four gates and the palace complex of the Round City of Baghdad\". The \"public spectacle of wind-powered statues had its private counterpart in the 'Abbasid palaces where automata of various types were predominantly displayed.\" Also in the 8th century, the Muslim alchemist, Jābir ibn Hayyān (Geber), included recipes for constructing artificial snakes, scorpions, and humans that would be subject to their creator's control in his coded \"Book of Stones\". In 827, Caliph Al-Ma'mun had a silver and golden tree in his palace in Baghdad, which had the features of an automatic machine. There were metal birds that sang automatically on the swinging branches of this tree built by Muslim inventors and engineers. The Abbasid Caliph Al-Muqtadir also had a golden tree in his palace in Baghdad in 915, with birds on it flapping their wings and singing. In the 9th century, the Banū Mūsā brothers invented a programmable automatic flute player and which they described in their \"Book of Ingenious Devices\".\n\nAl-Jazari described complex programmable humanoid automata amongst other machines he designed and constructed in the \"Book of Knowledge of Ingenious Mechanical Devices\" in 1206. His automaton was a boat with four automatic musicians that floated on a lake to entertain guests at royal drinking parties. His mechanism had a programmable drum machine with pegs (cams) that bump into little levers that operate the percussion. The drummer could be made to play different rhythms and drum patterns if the pegs were moved around. According to Charles B. Fowler, the automata were a \"robot band\" which performed \"more than fifty facial and body actions during each musical selection.\"\n\nAl-Jazari constructed a hand washing automaton first employing the flush mechanism now used in modern toilets. It features a female automaton standing by a basin filled with water. When the user pulls the lever, the water drains and the automaton refills the basin. His \"peacock fountain\" was another more sophisticated hand washing device featuring humanoid automata as servants who offer soap and towels. Mark E. Rosheim describes it as follows: \"Pulling a plug on the peacock's tail releases water out of the beak; as the dirty water from the basin fills the hollow base a float rises and actuates a linkage which makes a servant figure appear from behind a door under the peacock and offer soap. When more water is used, a second float at a higher level trips and causes the appearance of a second servant figure — with a towel!\" Al-Jazari thus appears to have been the first inventor to display an interest in creating human-like machines for practical purposes such as manipulating the environment for human comfort.\n\n\"Samarangana Sutradhara\", a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.\n\nVillard de Honnecourt, in his 1230s sketchbook, show plans for animal automata and an angel that perpetually turns to face the sun. At the end of the thirteenth century, Robert II, Count of Artois built a pleasure garden at his castle at Hesdin that incorporated several automata as entertainment in the walled park. The work was conducted by local workmen and overseen by the Italian knight Renaud Coignet. It included monkey marionettes, a sundial supported by lions and \"wild men\", mechanized birds, mechanized fountains and a bellows-operated organ. The park was famed for its automata well into the fifteenth century before it was destroyed by English soldiers in the sixteenth.\n\nThe Chinese author Xiao Xun wrote that when the Ming Dynasty founder Hongwu (r. 1368–1398) was destroying the palaces of Khanbaliq belonging to the previous Yuan Dynasty, there were—among many other mechanical devices—automata found that were in the shape of tigers.\n\nThe Renaissance witnessed a considerable revival of interest in automata. Hero's treatises were edited and translated into Latin and Italian. Giovanni Fontana created mechanical devils and rocket-propelled animal automata. Numerous clockwork automata were manufactured in the 16th century, principally by the goldsmiths of the Free Imperial Cities of central Europe. These wondrous devices found a home in the cabinet of curiosities or \"Wunderkammern\" of the princely courts of Europe. Hydraulic and pneumatic automata, similar to those described by Hero, were created for garden grottoes.\n\nLeonardo da Vinci sketched a more complex automaton around the year 1495. The design of Leonardo's robot was not rediscovered until the 1950s. The robot could, if built successfully, move its arms, twist its head, and sit up.\n\nThe Smithsonian Institution has in its collection a clockwork monk, about high, possibly dating as early as 1560. The monk is driven by a key-wound spring and walks the path of a square, striking his chest with his right arm, while raising and lowering a small wooden cross and rosary in his left hand, turning and nodding his head, rolling his eyes, and mouthing silent obsequies. From time to time, he brings the cross to his lips and kisses it. It is believed that the monk was manufactured by Juanelo Turriano, mechanician to the Holy Roman Emperor Charles V.\n\nA new attitude towards automata is to be found in Descartes when he suggested that the bodies of animals are nothing more than complex machines - the bones, muscles and organs could be replaced with cogs, pistons and cams. Thus mechanism became the standard to which Nature and the organism was compared. France in the 17th century was the birthplace of those ingenious mechanical toys that were to become prototypes for the engines of the Industrial Revolution. Thus, in 1649, when Louis XIV was still a child, an artisan named Camus designed for him a miniature coach, and horses complete with footmen, page and a lady within the coach; all these figures exhibited a perfect movement. According to P. Labat, General de Gennes constructed, in 1688, in addition to machines for gunnery and navigation, a peacock that walked and ate. Athanasius Kircher produced many automata to create Jesuit shows, including a statue which spoke and listened via a speaking tube.\nThe world's first successfully-built biomechanical automaton is considered to be \"The Flute Player\", invented by the French engineer Jacques de Vaucanson in 1737. He also constructed the Digesting Duck, a mechanical duck that gave the false illusion of eating and defecating, seeming to endorse Cartesian ideas that animals are no more than machines of flesh.\n\nIn 1769, a chess-playing machine called the Turk, created by Wolfgang von Kempelen, made the rounds of the courts of Europe purporting to be an automaton. The Turk was operated from inside by a hidden human director, and was not a true automaton.\nOther 18th century automaton makers include the prolific Swiss Pierre Jaquet-Droz (see Jaquet-Droz automata) and his contemporary Henri Maillardet. Maillardet, a Swiss mechanic, created an automaton capable of drawing four pictures and writing three poems. Maillardet's Automaton is now part of the collections at the Franklin Institute Science Museum in Philadelphia. Belgian-born John Joseph Merlin created the mechanism of the Silver Swan automaton, now at Bowes Museum. A musical elephant made by the French clockmaker Hubert Martinet in 1774 is one of the highlights of Waddesdon Manor. Tipu's Tiger is another late-18th century example of automata, made for Tipu Sultan, featuring a European soldier being mauled by a tiger.\n\nAccording to philosopher Michel Foucault, Frederick the Great, king of Prussia from 1740 to 1786, was \"obsessed\" with automata. According to Manuel de Landa, \"he put together his armies as a well-oiled clockwork mechanism whose components were robot-like warriors\".\n\nJapan adopted automata during the Edo period (1603–1867); they were known as \"karakuri ningyō\".\n\nAutomata, particularly watches and clocks, were popular in China during the 18th and 19th centuries, and items were produced for the Chinese market. Strong interest by Chinese collectors in the 21st century brought many interesting items to market where they have had dramatic realizations.\n\nThe famous magician Jean Eugène Robert-Houdin (1805–1871) was known for creating automata for his stage shows.\nIn 1840, Italian inventor Innocenzo Manzetti constructed a flute-playing automaton, in the shape of a man, life-size, seated on a chair. Hidden inside the chair were levers, connecting rods and compressed air tubes, which made the automaton's lips and fingers move on the flute according to a program recorded on a cylinder similar to those used in player pianos. The automaton was powered by clockwork and could perform 12 different arias. As part of the performance it would rise from the chair, bow its head, and roll its eyes.\n\nThe period 1860 to 1910 is known as \"The Golden Age of Automata\". During this period many small family based companies of Automata makers thrived in Paris. From their workshops they exported thousands of clockwork automata and mechanical singing birds around the world. It is these French automata that are collected today, although now rare and expensive they attract collectors worldwide. The main French makers were Bontems, Lambert, Phalibois, Renou, Roullet & Decamps, Theroude and Vichy.\n\nContemporary automata continue this tradition with an emphasis on art, rather than technological sophistication. Contemporary automata are represented by the works of Cabaret Mechanical Theatre in the United Kingdom, Dug North and Chomick+Meder, Thomas Kuntz, Arthur Ganson, Joe Jones in the United States, Le Défenseur du Temps by French artist Jacques Monestier, and François Junod in Switzerland.\n\nSome mechanized toys developed during the 18th and 19th centuries were automata made with paper. Despite the relative simplicity of the material, paper automata require a high degree of technical ingenuity.\n\nThe potential educational value of mechanical toys in teaching transversal skills has been recognised by the European Union education project \"Clockwork objects, enhanced learning: Automata Toys Construction\" (CLOHE).\n\nExamples of automaton clocks include Chariot clock and Cuckoo Clocks. The Cuckooland Museum exhibits autonomous clocks.\n\n\n\n", "id": "189749", "title": "Automaton"}
{"url": "https://en.wikipedia.org/wiki?curid=2083415", "text": "Navigation mesh\n\nA navigation mesh, or navmesh, is an abstract data structure used in artificial intelligence applications to aid agents in pathfinding through complicated spaces. This approach has been known since at least the mid-1980s in robotics, where it has been called a meadow map, and was popularized in video game AI in 2000.\n\nA navigation mesh is a collection of two-dimensional convex polygons (a polygon mesh) that define which areas of an environment are traversable by agents. In other words, a character in a game could freely walk around within these areas unobstructed by trees, lava, or other barriers that are part of the environment. Adjacent polygons are connected to each other in a graph.\n\nPathfinding within one of these polygons can be done trivially in a straight line because the polygon is convex and traversable. Pathfinding between polygons in the mesh can be done with one of the large number of graph search algorithms, such as A*. Agents on a navmesh can thus avoid computationally expensive collision detection checks with obstacles that are part of the environment.\n\nRepresenting traversable areas in a 2D-like form simplifies calculations that would otherwise need to be done in the \"true\" 3D environment, yet unlike a 2D grid it allows traversable areas that overlap above and below at different heights. The polygons of various sizes and shapes in navigation meshes can represent arbitrary environments with greater accuracy than regular grids can.\n\nNavigation meshes can be created manually, automatically, or by some combination of the two. In video games, a level designer might manually define the polygons of the navmesh in a level editor. This approach can be quite labor intensive. Alternatively, an application could be created that takes the level geometry as input and automatically outputs a navmesh.\n\nIt is commonly assumed that the environment represented by a navmesh is static – it does not change over time – and thus the navmesh can be created offline and be immutable. However, there has been some investigation of online updating of navmeshes for dynamic environments.\n\nIn robotics, using linked convex polygons in this manner has been called \"meadow mapping\", coined in a 1986 technical report by Ronald C. Arkin.\n\nNavigation meshes in video game artificial intelligence are usually credited to Greg Snook's 2000 article \"Simplified 3D Movement and Pathfinding Using Navigation Meshes\" in \"Game Programming Gems\". In 2001, J.M.P. van Waveren described a similar structure with convex and connected 3D polygons, dubbed the \"Area Awareness System\", used for bots in \"Quake III Arena\".\n\n\n", "id": "2083415", "title": "Navigation mesh"}
{"url": "https://en.wikipedia.org/wiki?curid=28928489", "text": "Common normal (robotics)\n\nIn robotics the common normal of two non-intersecting joint axes is a line perpendicular to both axes.\n\nThe common normal can be used to characterize robot arm links, by using the \"common normal distance\" and the angle between the link axes in a plane perpendicular to the common normal. When two consecutive joint axes are parallel, the common normal is not unique and an arbitrary common normal may be used, usually one that passes through the center of a coordinate system.\n\nThe common normal is widely used in the representation of the frames of reference for robot joints and links, and the selection of minimal representations with the Denavit–Hartenberg parameters.\n\n", "id": "28928489", "title": "Common normal (robotics)"}
{"url": "https://en.wikipedia.org/wiki?curid=47133704", "text": "Care-O-bot\n\nCare-O-bot is the product vision of a mobile robot assistant to actively support humans e.g. in their daily life, in hotels, health care institutions or hospitals, developed by the Fraunhofer Institute for Manufacturing Engineering and Automation.\n\nThe fourth generation, Care-O-bot 4, was completed in January 2015. While its predecessors from 1998 onwards were used primarily in the development of technological fundamentals, Care-O-bot 4 is a modular product family providing the basis for commercial service robot solutions. In 2015, the product design of Care-O-bot 4 was awarded with the Red Dot Award: Product Design. As only 1,6 percent of all applications, Care-O-bot 4 received the recognition \"Best of the Best”.\n\nNumerous research institutions and universities around the world work with Care-O-bot to advance the areas of applications.\n\n", "id": "47133704", "title": "Care-O-bot"}
{"url": "https://en.wikipedia.org/wiki?curid=15046825", "text": "IOIO\n\nIOIO (pronounced \"yo-yo\") is a series of open source PIC microcontroller-based boards that allow Android mobile applications to interact with external electronics. The device was invented by Ytai Ben-Tsvi in 2011, and was first manufactured by SparkFun Electronics. The name \"IOIO\" is inspired by the function of the device, which enables applications to receive external input (\"I\") and produce external output (\"O\").\n\nThe IOIO board contains a single PIC MCU that acts as a USB host/USB slave and communicates with an Android app running on a connected Android device. The board provides connectivity via USB, USB-OTG or Bluetooth, and is controllable from within an Android application using the Java API.\n\nIn addition to basic digital input/output and analog input, the IOIO library also handles PWM, I2C, SPI, UART, Input capture, Capacitive sensing and advanced motor control. To connect to older Android devices that use USB 2.0 in slave mode, newer IOIO models use USB On-The-Go to act as a host for such devices. Some models also support the Google Open Accessory USB protocol.\n\nThe IOIO motor control API can drive up to 9 motors and any number of binary actuators in synchronization and cycle-accurate precision. Developers may send a sequence of high-level commands to the IOIO, which performs the low-level waveform generation on-chip. The IOIO firmware supports 3 different kinds of motors; stepper motors, DC motors and servo motors.\n\nDevice firmware may be updated on-site by the user. For first-generation devices updating is performed using an Android device and the \"IOIO Manager\" application available on Google Play. Second-generation IOIO-OTG devices must be updated using a desktop computer running the \"IOIODude\" application.\n\nThe IOIO supports both computers and Android devices as first-class hosts, and provides the exact API on both types of devices. First-generation devices can only communicate with PCs over Bluetooth, while IOIO-OTG devices can use either Bluetooth or USB. PC applications may use APIs for Java or C# to communicate with the board; Java being the official API.\n\nThe IOIO hardware and software is entirely open source, and enabled the creation of hundreds of DIY robotic projects around the world.\n\nThe board has been featured in various learning kits, which aim to help students write Android applications that can interact with the external world.\n\nThe Qualcomm Snapdragon Micro Rover is a 3D printed robot that leverages an Android smartphone and the IOIO to control the robot's motors and sensors. A team led by Israeli inventor Dr. Guy Hoffman created an emotionally-sensitive robot, that relies on the IOIO to control the robot's hardware.\n\nThe IOIO has been variously described as a \"geek's paradise\", \"an easy way to get I/O from an Android device’s USB connection\" and \"a USB I/O breakout board for Android smartphones which turns your handset into a super-Arduino of sorts\". It featured as a recommended \"gift for geeks\" in a Scientific Computing article.\n\nAccording to SlashGear, an online electronics magazine:\nAccording to SparkFun, the first manufacturer of the device:\nAccording to Ytai Ben-Tsvi, the inventor of the device:\n\nThe first-generation IOIO boards (known as \"IOIO V1\") contain the following on-board features: This generation only supports USB slave mode, and requires a USB master as the host (PC or newer Android phones).\n\nThe IOIO V1 is a 3.3 V logic level device, and features a 5 V DC/DC switching regulator and a 3.3V linear regulator. The 5 V regulator supports a 5–15 V input range and up to 1.5 A load. This facilitates charging a connected Android device as well as driving several small motors or similar loads.\n\nThe second-generation IOIO boards (known as \"IOIO-OTG\") contain the following on-board features: As the name suggests, a key feature of this generation is the introduction of USB-OTG, supporting USB master or slave mode. This enables the IOIO to connect to older Android phones that only support USB slave mode, in addition.\n\nThe IOIO-OTG is a 3.3 V logic level device, with some of the pins being 5 V tolerant. It features a 5 V DC/DC switching regulator and a 3.3 V linear regulator. The 5 V regulator supports a 5–15 V input range and up to 3 A load. This facilitates charging a connected Android device as well as driving several small motors or similar loads.\n\n", "id": "15046825", "title": "IOIO"}
{"url": "https://en.wikipedia.org/wiki?curid=47796548", "text": "Amaryllo\n\nAmaryllo International B.V., founded in Amsterdam, the Netherlands, pioneers in AI as a Service market: Amaryllo offers real-time data mining, patented camera robot, fast object recognition, secure 256-bit encrypted P2P network, and flexible cloud storage to B2G and B2B market. \n\nAmaryllo develops a new type of patented robotic cameras that can talk, hear, sense, recognize human faces, and auto-track intruders 360 degrees and even from behind. Company debuted world's first security robot based on WebRTC protocol, iCamPRO FHD, and won the 2015 CES Best of Innovation Award under Embedded Technology category. Amaryllo's home security robots employ 256-bit encryption technologies and run on WebRTC protocol. Amaryllo is ranked as the number-one smart home camera robot company. \n\nAmaryllo revealed its first smart home security products at Internationale Funkausstellung Berlin (IFA) 2013 with a Skype-enabled IP camera called iCam HD. iCam HD was the first smart home IP camera certified by Plugged-into-Skype program with 256-bit encryption. Amaryllo announced its second Skype-certified smart home product, iBabi HD, at CES 2014. Company was chosen as a Cool Vendor by Gartner in Connected Home 2014 for being the first company offering the highest possible security protection in consumers security market. Amaryllo introduced WebRTC-based smart home products after Microsoft terminated embedded Skype services in mid 2014.Company has been developing a series of camera robots with focus on its auto-tracking and facial recognition technologies since. The professional outdoor IP65 grade camera robots, ATOM AR3 and ATOM AR3S, were introduced in late 2016.\n\nAmaryllo debuted its facial recognition technologies on the new auto-tracking model, ATOM at IFA 2016. ATOM is designed to recognize human faces from learning faces. It takes 0.5 seconds to detect a human face and another 0.5 second to identify a person, totaling only 1 second to recognize a human face. It can recognize over 100 people simultaneously. Amaryllo is offering facial recognition to its B2B partners. ASUS SmartHome platform has integrated ATOM to its offering to take advantage of this new technology.\n\nUnlike conventional tracking technologies which employ remote computers to perform object tracking algorithm, resulting in higher implementation costs and bulkier systems, Amaryllo uses multi-core processor embedded in compact cameras to realize complete tracking systems in single units. Each camera must be embedded with sufficient computation power and memory to possess artificial intelligence to analyze real-time image comparison to track moving objects, to stream real-time full HD video, to encode 256-bit encryption, to upload recorded videos, etc. Amaryllo security drones act as individual security robots to track moving objects on their own without commands from remote computers. With a 1920 x 1080 full HD resolution, these drones are able to track intruders over 30 feet away. Infrared lights are aesthetically hidden by a mask and are activated when the environment is dark. This enables Amaryllo drones to track objects in a complete darkness.\n\nAmaryllo added speech functions to its advanced camera robots. Multiple languages are introduced including English, Spanish, Arabic, French, Japanese, and Chinese. When faces or sensors are detected, camera robots will talk in pre-selected languages and say \"Hello\", \"Good Morning\", \"Good Afternoon\", etc. Camera robots can report hourly time such as \"It's 4 PM\" and even check your email arrival. If there is new email, camera robots will say \"You've got mail\" to remind owners. \n\nCompany's patent-pending technologies further enable security robots to track objects 360 degrees by introducing multiple motion sensors around the drones, so once a sensor is triggered, embedded CPUs will guide the drones to turn to the spotted direction to follow objects even from behind. This innovative design eliminates the need to implement multiple cameras in a single unit to reduce cost. Amaryllo security robots can talk to intruders if they are spotted and track intruders. Amaryllo iCamPRO FHD won 2015 WebRTC World Product of the Year Award.\n\nAmaryllo develops cloud-based artificial intelligent with its camera robots to recognize any objects. By taking advantage of large cloud computation power, faces, human body, vehicle, animals, birds, air planes, etc. can be detected and recognized in seconds. Over 100 human faces can be detected and recognized in seconds. Amaryllo uses real-time picture frame analysis to identify faces. Once detected, security drones will send out face alerts to registered users instantly. Amaryllo uses real-time picture frame analysis to identify faces. Once a humanoid face is recognized, robots will deliver face snapshots to smart devices within seconds. This patent-pending solution can eliminate possible false alerts found in popular network cameras with alerts generated by Passive infrared sensor.\n\nAmaryllo robots are linked to Google Services. If consumers receive an email, Amaryllo robots will say \"You've got mail.\" If consumers have appointments at 3 PM, Amaryllo robots will say \"You have an appointment at 3 pm,\" minutes before the meeting to remind consumers. These robots can also say \"Hello\", \"Good Morning\", \"Good Afternoon\", etc. when they detect events. These events could be motion, audio, or face detection pre-determined by users. These robots are wirelessly connected to networks, so they are aware of local time and can report time on an hourly fashion. For example, it will say \"It's 5 PM,\" acting as a regular clock. More interactive voice communications are reported.\n\nTo eliminate common false alarm triggered by popular audio or PIR sensors, Amaryllo devises an option on app to allow consumers to define areas where they are ignored or they are monitored by their robots. If robots are led to a certain area where movement later stops, robots will rotate themselves to resume to their pre-defined \"home\" positions to safeguard the most important asset. The combination of the above, enables Amaryllo robots to reduce false alarm raised by conventional pixel-based sensors.\n\nAmaryllo was the first to establish global Peer-to-Peer (P2P) server based on WebRTC protocol in smart home service. Amaryllo Live is a plug-in-free H.264-based browser service allowing consumers to access their cameras anywhere anytime. It runs on WebRTC protocol and Firefox is the first browser company to support Amaryllo Live. Other browser companies have vowed to support WebRTC H.264-based codec. Amaryllo server measures each communications channel and dynamically adjusts the video format based on available bandwidth to effectively alleviate video latency and results in a better video communications experience. It also permits a true two-way audio talk.\n\nAmaryllo offers free and paid unlimited cloud storage plans available to its products. Unconventionally, Amaryllo cloud service includes video alerts allowing consumers to review urgent video messages from smart devices remotely. Video alerts are handy as brief recorded videos provide additional information to consumers in comparison against typical single frame picture alerts. Amaryllo launched Urgent Home Care Service with an introduction of iCare FHD. With a touch of a remote control, the patent-pending technologies deliver video alerts to registered members in seconds. iCare FHD can detect faces and sends out real-time face alert video to family members. This is the first urgent home care products employing WebRTC technologies.\n\n", "id": "47796548", "title": "Amaryllo"}
{"url": "https://en.wikipedia.org/wiki?curid=48081904", "text": "Translational drift\n\nTranslational drift also known as melty brain or tornado drive is a form of locomotion, notably found in certain combat robots.\n\nThe principle is applied to spinning robots, where the driving wheels are normally on for the whole revolution, resulting in an increased rotational energy, which is stored for destructive effect, but, given perfect symmetry, no net translational acceleration. The drive works by modulating the power to the wheel or wheels that spin the robot. The net application of force in one direction results in acceleration in the plane - it can't really be characterised as \"forward\", \"backward and so forth, as the whole robot is spinning. However, in a standard configuration an accelerometer is used to determine the speed of rotation, and a light emitting diode is turned on once per revolution, to give a nominal forward direction indicator to the operator. The internal controls implement the commands received from the remote control to modulate the drive to the wheels, typically by turning it off for part of a revolution.\n\nOpen Melt is an open source implementation of melty brain, the code being licensed under Creative Commons Attribution-Noncommercial-Share Alike licence.\n\n", "id": "48081904", "title": "Translational drift"}
{"url": "https://en.wikipedia.org/wiki?curid=80579", "text": "Droid (robot)\n\nA droid is a fictional robot possessing some degree of artificial intelligence in the \"Star Wars\" science fiction franchise. Coined by special effects artist John Stears, the term is a clipped form of \"android\", a word originally reserved for robots designed to look and act like a human. The word \"droid\" has been a registered trademark of Lucasfilm Ltd since 1977.\n\nThe franchise, which began with the 1977 film \"Star Wars\", features a variety of droids designed to perform specific functions.\n\nA protocol droid specializes in translation, etiquette and cultural customs, and is typically humanoid in appearance. The most notable example is C-3PO, introduced in \"Star Wars\" and featured in all sequels and prequels. 4-LOM is a protocol droid turned bounty hunter who responds to Darth Vader's call to capture the \"Millennium Falcon\" in \"The Empire Strikes Back\" (1980). TC-14 is a droid with feminine programming that appears in \"\" (1999), and ME-8D9 is an \"ancient protocol droid of unknown manufacture\" that resides and works as a translator at Maz Kanata’s castle on Takodana in the 2015 \"\".\n\nAn astromech droid is one of a series of \"versatile utility robots generally used for the maintenance and repair of starships and related technology\". These small droids usually possess \"a variety of tool-tipped appendages that are stowed in recessed compartments\". R2-D2 is an astromech droid introduced in 1977's \"Star Wars\" and featured in all subsequent films. The malfunctioning droid R5-D4 also makes a brief appearance in \"Star Wars\". U9-C4 is a timid droid sent on a mission with D-Squad, an all-droid special unit in \"\", C1-10P is an oft-repaired, \"outmoded\" astromech who is one of the \"Star Wars Rebels\" regular characters, and BB-8 is the astromech droid of X-wing fighter pilot Poe Dameron in \"The Force Awakens\".\n\nA battle droid is a class of military robot in the \"Star Wars\" prequel trilogy of films (1999–2005) and the \"\" TV series (2008–14), used as an easily controlled alternative to human soldiers. The tall, thin B1 model resembles the Geonosian race, who designed the droids, and are known to \"suffer programming glitches that manifest as personality quirks\". The droideka is a three-legged heavy infantry unit with twin blasters and the ability to generate a force shield and transform into a disk shape. Multiple other types of specialized battle droids have been featured in the films and the TV series.\n\nHK-47 is a humanoid soldier robot, designed as a violent killer, which first appeared in the 2003 video game \"\".\n\n\"Star Wars: The Clone Wars\" has featured WAC-47, a \"pit droid\" programmed as a pilot and sent on a mission with the all-droid special unit D-Squad, and AZI-3, a medical droid serving the cloners of Kamino who helps uncover the secret of Order 66. The 2015 young adult novel \"\" by Cecil Castellucci and Jason Fry introduces the droid PZ-4CO, to whom Leia Organa dictates her memoirs. PZ-4CO also appears in \"The Force Awakens\" (2015). In the 2016 film \"Rogue One\", K-2SO is an Imperial enforcer droid reprogrammed by the Rebel Alliance.\nThe term \"Droid\" was first used in the science fiction story \"Robots of the World! Arise!\" by Mari Wolf, published in \"If: Worlds of Science Fiction\" in July 1952.\nLucasfilm registered \"droid\" as a trademark in 1977.\n\nThe term \"Droid\" has been used by Verizon Wireless under licence from Lucasfilm, for their line of smartphones based on the Android operating system. Motorola's late-2009 Google Android-based cell phone is called the Droid. This line of phone has been expanded to include other Android-based phones released under Verizon, including the HTC Droid Eris, the HTC Droid Incredible, Motorola Droid X, Motorola Droid 2, and Motorola Droid Pro. The term was also used for the Lucasfilm projects EditDroid, a non-linear editing system, and SoundDroid, an early digital audio workstation.\n\n\n", "id": "80579", "title": "Droid (robot)"}
{"url": "https://en.wikipedia.org/wiki?curid=48508780", "text": "Robotic materials\n\nRobotic materials are composite materials that combine sensing, actuation, computation, and communication in a repeatable or amorphous pattern. Robotic materials can be considered \"computational metamaterials\" in that they extend the original definition of a metamaterial as \"macroscopic composites having a man-made, three-dimensional, periodic cellular architecture designed to produce an optimized combination, not available in nature, of two or more responses to specific excitation\" by being fully programmable. That is, unlike in a conventional metamaterial, the relationship between a specific excitation and response is governed by sensing, actuation, and a computer program that implements the desired logic.\n\nThe idea of creating materials that embed computation is closely related to the concept of programmable matter, a term coined in 1991 by Toffoli and Margolus, describing dense arrays of computing elements that could solve complex finite-element like simulations of material systems, and then later developed to describe a class of materials consisting of identical, mobile building blocks, also known as catoms that are fully reconfigurable, therefore allowing materials to arbitrarily change their physical properties.\n\nRobotic materials build up on the original concept of programmable matter, but focus on the structural properties of the embedding polymers without claim of universal property changes. Here the term \"robotic\" refers to the confluence of sensing, actuation, and computation.\n\nRobotic materials allow to off-load computation inside the material, most notably signal processing that arises during high-bandwidth sensing applications or feedback control that is required by fine-grained distributed actuation. Examples for such applications include camouflage, shape change, load balancing, and robotic skins as well as equipping robots with more autonomy by off-loading some of the signal processing and controls into the material.\n\nResearch in robotic materials ranges from the device-level and manufacturing to the distributed algorithms that equip robotic materials with intelligence. As such it intersects the fields of composite materials, sensor networks, distributed algorithms, and due to the scale of the involved computation, swarm intelligence. Unlike any individual field, the design of the structure, sensors, actuators, communication infrastructure, and distributed algorithms are tightly intertwined. For example, the material properties of the structural material will affect how signals to be sensed propagate through the material, at which distance computational elements need to be spaced, and what signal processing needs to be done. Similarly, structural properties are closely related to the actual embedding of computing and communication infrastructure. Capturing these effects therefore requires interdisciplinary collaboration between materials, computer science, and robotics.\n", "id": "48508780", "title": "Robotic materials"}
{"url": "https://en.wikipedia.org/wiki?curid=48834013", "text": "DAVI\n\nThe Dutch Automated Vehicle Initiative (DAVI) is a research and demonstration initiative developing automated vehicles for use on public roads.\n\nThe project is special in that, besides simply making driverless cars, it also focuses on having the automated vehicles share information amongst each other. The aim is to have the cars helping to avoid traffic congestion, by reducing the safety distance between the cars (from 2 seconds drive distance now to just 0.5 seconds) and avoiding sudden traffic slow-downs (due to manoeuvres undertaken by drivers).\n", "id": "48834013", "title": "DAVI"}
{"url": "https://en.wikipedia.org/wiki?curid=48805092", "text": "Alice Cares\n\nAlice Cares is a 2015 Dutch film directed by Sander Burger. It is a documentary film that explores how a new robot technology may be able to aid many senior citizens in the developing western world. The film premiered at the Vancouver International Film Festival in Vancouver, B.C. on September 27, 2015.\n", "id": "48805092", "title": "Alice Cares"}
{"url": "https://en.wikipedia.org/wiki?curid=49312388", "text": "Open Bionics\n\nOpen Bionics, founded in 2014, is a UK based start-up company developing low-cost bionic hands. It is based inside the Bristol Robotics Laboratory, the largest robotics research centre in Europe.\n\nOpen Bionics creates advanced bionic hands for amputees using 3D scanning and 3D printing technology. Although the company is pre-sales they have released a number of user trial videos showing how their technology works.\n\nOpen Bionics have released designs for several of their prototypes with an open source license. The company encourages engineers and developers to contribute to the development of bionic hands by joining their developer community. These hands provide a research platform for robotics or a test platform for prosthetics research. Each hand has been designed to be easy to build and repair.\n\nA 3D printed robotic hand developed by The Open Hand Project using desktop 3D printers.\n\nSecond version of the 3D printed robotic hand developed by Open Bionics using desktop 3D printers.\n\nThe latest version of Open Bionics' robotic hand. The Ada hand can be assembled in under one hour, takes 20 hours to print on a desktop 3D printer, and can be controlled via EMG sensors. The Ada hand has fingers that are individually actuated and controlled via muscle tensing. The hand has multiple grip modes including open / close, tripod, pinch, point, and power grip.\n\nThe company has released their design files and code on their website, Instructables, Thingiverse, and GitHub.\n\nOpen Bionics announced a royalty-free licensing deal with Disney in 2015 to create official Iron Man, Star Wars, and Disney Frozen bionic hands for young amputees. The company is yet to set a release date for these superhero inspired bionic hands, although the company has successfully trialled an Iron Man and a Star Wars bionic hand with two children.\n\nOpen Bionics has won multiple awards for engineering and innovation including:\n", "id": "49312388", "title": "Open Bionics"}
{"url": "https://en.wikipedia.org/wiki?curid=49321811", "text": "UAV-related events\n\nUnmanned aerial vehicles (UAVs) or drones have frequently been involved in military operations. Non-military UAVs have often been reported as causing hazards to aircraft, or to people or property on the ground.\n\n\nThe pilot of British Airways flight BA727 reported a collision with a UAV to Metropolitan Police. The Airbus A320 was approaching Heathrow Airport when the collision happened. None of the 132 passengers or 5 crew were injured. After an inspection by engineers, the aircraft was cleared to take off for its next flight.\n\nMembers of the British government as well as the Labour Party and BALPA called for urgent action, including a register of drone users and geo-fencing of airports.\n\nIn late April 2016 Transport Secretary Patrick McLoughlin told MPs that experts believed that the incident was not related to UAVs. There was no damage to the aircraft and it wasn't clear if it was a plastic bag rather than a UAV. An investigation by the Air Accidents Investigation Branch had been closed due to lack of evidence. Police had searched a \"wide area\" of Richmond and found nothing.\n\nA UAV collided with a Black Hawk helicopter in the evening over the eastern shore of Staten Island. The helicopter was one of two with the 82nd Airborne Division flying out of Fort Bragg on duty for the United Nations General Assembly. The helicopters were able to continue flying and landed at Linden Airport. Nobody was hurt, but part of the UAV was found at the bottom of the main rotor system.\n\nIn December 2017 the National Transportation Safety Board issued an accident report into the collision, finding the pilot of the UAV at fault. The UAV operator deliberately flew the UAV 2.5 miles away from himsef and was unaware of the helicopters' presence. The operator did not know of the collision until contacted by the NTSB and when interviewed by them showed only a general cursory awareness of the regulations. There was also a Temporary flight restriction in place, which allowed the Black Hawk but not the UAV. UAVs are prohibited from flying beyond line of sight under FAA regulations. \n\nA Beech King Air A100 of Skyjet Aviation collided with a UAV as the former was approaching Jean Lesage Airport near Quebec City. The aircraft landed safely despite being hit on the wing. Thei aircraft had flown from Rouyn-Noranda Airport to Rouyn-Noranda Airport with six passengers and two crew. A spokeswoman for Quebec City Police said that neither the UAV or operator had been found. It had been flying at 1500 feet - five times the maximum altitude that UAVs are permitted to fly in Canada. Regulations banning flying of UAVs near airports had been introduced earlier in 2017. Minister of Transport Marc Garneau released a statement saying \"Although the vast majority of drone operators fly responsibly, it was our concern for incidents like this that prompted me to take action and issue interim safety measures restricting where recreational drones could be flown. I would like to remind drone operators that endangering the safety of an aircraft is extremely dangerous and a serious offence.\"\n\n\n\n\n\nIn January 2017 a 23 year old UAV operator from Xiaoshan was detained because of footage taken from a drone flying near airliners descending to land at Hangzhou Xiaoshan International Airport. The incident came to light when footage was uploaded to QQ. This was an eight-second clip from a ten-minute recording taken from an altitude of 450m. The operator had flown it to photograph a sunset, but had also recorded several airliners flying past. The model used was a DJI Mavic and the manufacturer strongly condemned the incident.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUAVs have historically had a much higher loss rate than manned military aircraft. In addition to anti-aircraft weapons, UAVs are vulnerable to power and communications link losses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "49321811", "title": "UAV-related events"}
{"url": "https://en.wikipedia.org/wiki?curid=49548813", "text": "Robot leg\n\nA robot leg (or robotic leg) is a mechanical leg that performs the same functions that a human leg can. The robotic leg is typically programmed to execute similar functions as a human leg. A robotic leg is similar to a prosthetic leg. However, a robotic leg can be controlled electrically or mechanically. To have the robotic leg emulate human leg behaviors, surgeons must redirect the nerves that previously controlled some of the person’s lower-leg muscles to cause the thigh muscles to contract. Sensors embedded in the robotic leg measure the electrical pulses created by both a re-innervated muscle contraction, and the existing thigh muscle.\n\nA robotic Leg attaches to an individual who has had a lower extremity amputation—of a portion of a leg or foot. Doctors and technicians measure the remaining limb structure and of the person’s prosthesis to ideally fit the robotic leg. After they attach the robotic leg, they embed the sensors in the robotic leg that measure the electrical activity created by re-innervated muscle contraction, and existing thigh muscle.\n\nRobotic legs have become a popular target of research and development in the past few years. Robotic legs can be applied to people who have had limb amputating surgery caused by a major trauma, or a disease like cancer. Robotic legs have also seen much applicable use in military combat injuries. Massachusetts Institute of Technology (MIT) has been working to create a “bionic” or robotic leg that can function as if it had organic muscles.\n\n", "id": "49548813", "title": "Robot leg"}
{"url": "https://en.wikipedia.org/wiki?curid=49944357", "text": "Bricksmart\n\nBricksmart is a FIRST Lego League community team of students of ages 10–14 located in The Woodlands, Texas. Over the past five years Bricksmart has received numerous accolades in various Texas FLL tournaments.\n\n2011: The team was founded\n\n2011-2012 (Senior Solutions): Bricksmart placed third at the Woodlands Preparatory Qualifier and advanced to the South Texas Championship\n\n2013-2014 (Nature's Fury): Bricksmart placed second at the Woodlands Preparatory Qualifier and won second place for innovative design in the research project at the South Texas Championship. The team also placed fourth overall at the South Texas Championship.\n\n2014: Bricksmart placed 4th in the All-Earth Ecobot challenge.\n\n2014-2015 (World Class): The team placed second at the Huntsville Qualifier and placed first at the South Texas Championship. That advanced them to the FIRST World Festival in St. Louis, Missouri.\n\n2015-2016 (Trash Trek): Bricksmart placed first at the Texas Torque Qualifier and won Champion's Award for the second time at the South Texas Championship. From May 19–22, they competed at the Razorback FLL Invitational, held in Fayetteville, Arkansas. In 2016 Bricksmart founded a business, now defunct, called Ecosmartz. \n\n2016-2017 (Animal Allies): Bricksmart is competing in the Animal Allies challenge.\n\n2017-2018 The team switches from FLL competitions and now competes in FTC competitions. \n\nFor their 2016 project, the team started a business. To reduce trash in their community, Ecosmartz hauls compostable food waste from grocery stores and restaurants in The Woodlands, and delivers it to a local composting facility.\nAs of November 5, 2016 EcoSmartz has ceased operations and is defunct. There are no plans to open the business in the near future.\n", "id": "49944357", "title": "Bricksmart"}
{"url": "https://en.wikipedia.org/wiki?curid=49959510", "text": "Georges Giralt PhD Award\n\n]\nThe Georges Giralt PhD Award is a European scientific prize for extraordinary contributions in robotics. It is yearly awarded at the \"European Robotics Forum\" by \"euRobotics\" AISBL, a non-profit organisation based in Brussels with the objective to turn robotics beneficial for Europe’s economy and society.\n\nThe high reputation of the \"Georges Giralt PhD Award\" is based on the prominent role of the awarding institution \"euRobotics\". With more than 250 member organisations, euRobotics represents the academic and industrial robotics community in Europe. Moreover, it provides the European robotics community with a legal entity to engage in a public/private partnership with the European Commission.\n\nEntitled for participation for the \"Georges Giralt PhD Award\" are all robotics-related dissertations which have been successfully defended at a European university.\n\nThe US-American counterpart is the Dick Volz Award.\n\n", "id": "49959510", "title": "Georges Giralt PhD Award"}
{"url": "https://en.wikipedia.org/wiki?curid=49803819", "text": "Australian Research Council Centre of Excellence for Robotic Vision\n\nThe Australian Centre for Robotic Vision is an unincorporated collaborative venture with funding of $25.6m over seven years to pursue a research agenda tackling the critical and complex challenge of applying robotics in the real world.\n\nThe Center is made up of an interdisciplinary team from four leading Australian universities\nresearch organisations and international universities\n\nThe Centre aims to achieve breakthrough science and technology in robotic vision by addressing four key research objectives: robust vision, vision and action, semantic vision, and algorithms and architecture. Together the four research objectives form the Centre’s research themes, which serve as organisational groupings of the Centre’s research projects.\n\nWill develop new sensing technologies and robust algorithms that allow robots to use visual perception in all viewing conditions: night and day, rain or shine, summer or winter, fast moving or static.\n\nWill create new theory and methods for using image data for control of robotic systems that navigate through space, grasp objects, interact with humans and use motion to assist in seeing.\n\nWill produce novel learning algorithms that can both detect and recognise a large, and potentially ever increasing, number of object classes from robotically acquired images, with increasing reliability over time.\n\nWill create novel technologies and techniques to ensure that the algorithms developed across the themes can be run in real-time on robotic systems deployed in large-scale real-world applications.\n", "id": "49803819", "title": "Australian Research Council Centre of Excellence for Robotic Vision"}
{"url": "https://en.wikipedia.org/wiki?curid=20756967", "text": "Cyborg\n\nA cyborg (short for \"cybernetic organism\") is a being with both organic and biomechatronic body parts. The term was coined in 1960 by Manfred Clynes and Nathan S. Kline.\n\nThe term cyborg is not the same thing as bionic, biorobot or android; it applies to an organism that has restored function or enhanced abilities due to the integration of some artificial component or technology that relies on some sort of feedback. While cyborgs are commonly thought of as mammals, including humans, they might also conceivably be any kind of organism.\n\nD. S. Halacy's \"Cyborg: Evolution of the Superman\" in 1965 featured an introduction which spoke of a \"new frontier\" that was \"not merely space, but more profoundly the relationship between 'inner space' to 'outer space' – a bridge...between mind and matter.\"\n\nIn popular culture, some cyborgs may be represented as visibly mechanical (e.g., Cyborg from DC Comics, the Cybermen in the \"Doctor Who\" franchise or The Borg from \"Star Trek\" or Darth Vader from \"Star Wars\") or as almost indistinguishable from humans (e.g., the \"Human\" Cylons from the re-imagining of \"Battlestar Galactica\", etc.). Cyborgs in fiction often play up a human contempt for over-dependence on technology, particularly when used for war, and when used in ways that seem to threaten free will. Cyborgs are also often portrayed with physical or mental abilities far exceeding a human counterpart (military forms may have inbuilt weapons, among other things).\n\nAccording to some definitions of the term, the physical attachments humanity has with even the most basic technologies have already made them cyborgs. In a typical example, a human with an artificial cardiac pacemaker or implantable cardioverter-defibrillator would be considered a cyborg, since these devices measure voltage potentials in the body, perform signal prpocessing, and can deliver electrical stimuli, using this synthetic feedback mechanism to keep that person alive. Implants, especially cochlear implants, that combine mechanical modification with any kind of feedback response are also cyborg enhancements. Some theorists cite such modifications as contact lenses, hearing aids, or intraocular lenses as examples of fitting humans with technology to enhance their biological capabilities. As cyborgs currently are on the rise some theorists argue there is a need to develop new definitions of aging and for instance a bio-techno-social definition of aging has been suggested.\n\nThe term is also used to address human-technology mixtures in the abstract. This includes not only commonly used pieces of technology such as phones, computers, the Internet, etc. but also artifacts that may not popularly be considered technology; for example, pen and paper, and speech and language. When augmented with these technologies and connected in communication with people in other times and places, a person becomes capable of much more than they were before. An example is a computer, which gains power by using Internet protocols to connect with other computers. Another example, which is becoming more and more relevant is a bot-assisted human or human-assisted-bot, used to target social media with likes and shares. Cybernetic technologies include highways, pipes, electrical wiring, buildings, electrical plants, libraries, and other infrastructure that we hardly notice, but which are critical parts of the cybernetics that we work within.\n\nBruce Sterling in his universe of Shaper/Mechanist suggested an idea of alternative cyborg called Lobster, which is made not by using internal implants, but by using an external shell (e.g. a Powered Exoskeleton). Unlike human cyborgs that appear human externally while being synthetic internally (e.g. the Bishop type in the Alien franchise), Lobster looks inhuman externally but contains a human internally (e.g. Elysium, RoboCop). The computer game \"\" prominently featured cyborgs called Omar, where \"Omar\" is a Russian translation of the word \"Lobster\" (since the Omar are of Russian origin in the game).\n\nThe concept of a man-machine mixture was widespread in science fiction before World War II. As early as 1843, Edgar Allan Poe described a man with extensive prostheses in the short story \"The Man That Was Used Up\". In 1911, Jean de la Hire introduced the Nyctalope, a science fiction hero who was perhaps the first literary cyborg, in \"Le Mystère des XV\" (later translated as \"The Nyctalope on Mars\"). Edmond Hamilton presented space explorers with a mixture of organic and machine parts in his novel \"The Comet Doom\" in 1928. He later featured the talking, living brain of an old scientist, Simon Wright, floating around in a transparent case, in all the adventures of his famous hero, Captain Future. He uses the term explicitly in the 1962 short story, \"After a Judgment Day,\" to describe the \"mechanical analogs\" called \"Charlies,\" explaining that \"[c]yborgs, they had been called from the first one in the 1960s...cybernetic organisms.\" In the short story \"No Woman Born\" in 1944, C. L. Moore wrote of Deirdre, a dancer, whose body was burned completely and whose brain was placed in a faceless but beautiful and supple mechanical body.\n\nThe term was coined by Manfred E. Clynes and Nathan S. Kline in 1960 to refer to their conception of an enhanced human being who could survive in extraterrestrial environments:\n\nTheir concept was the outcome of thinking about the need for an intimate relationship between human and machine as the new frontier of space exploration was beginning to open up. A designer of physiological instrumentation and electronic data-processing systems, Clynes was the chief research scientist in the Dynamic Simulation Laboratory at Rockland State Hospital in New York.\n\nThe term first appears in print five months earlier when \"The New York Times\" reported on the Psychophysiological Aspects of Space Flight Symposium where Clynes and Kline first presented their paper.\n\nA book titled \"Cyborg: Digital Destiny and Human Possibility in the Age of the Wearable Computer\" was published by Doubleday in 2001. Some of the ideas in the book were incorporated into the 35 mm motion picture film \"Cyberman\".\n\nCyborg tissues structured with carbon nanotubes and plant or fungal cells have been used in artificial tissue engineering to produce new materials for mechanical and electrical uses.\nThe work was presented by Di Giacomo and Maresca at MRS 2013 Spring conference on Apr, 3rd, talk number SS4.04. The cyborg obtained is inexpensive, light and has unique mechanical properties. It can also be shaped in desired forms. Cells combined with MWCNTs co-precipitated as a specific aggregate of cells and nanotubes that formed a viscous material. Likewise, dried cells still acted as a stable matrix for the MWCNT network. When observed by optical microscopy the material resembled an artificial \"tissue\" composed of highly packed cells. The effect of cell drying is manifested by their \"ghost cell\" appearance. A rather specific physical interaction between MWCNTs and cells was observed by electron microscopy suggesting that the cell wall (the most outer part of fungal and plant cells) may play a major active role in establishing a CNTs network and its stabilization. This novel material can be used in a wide range of electronic applications from heating to sensing and has the potential to open important new avenues to be exploited in electromagnetic shielding for radio frequency electronics and aerospace technology. In particular using Candida albicans cells cyborg tissue materials with temperature sensing properties have been reported.\n\nIn current prosthetic applications, the C-Leg system developed by Otto Bock HealthCare is used to replace a human leg that has been amputated because of injury or illness. The use of sensors in the artificial C-Leg aids in walking significantly by attempting to replicate the user's natural gait, as it would be prior to amputation. Prostheses like the C-Leg and the more advanced iLimb are considered by some to be the first real steps towards the next generation of real-world cyborg applications. Additionally cochlear implants and magnetic implants which provide people with a sense that they would not otherwise have had can additionally be thought of as creating cyborgs.\n\nIn vision science, direct brain implants have been used to treat non-congenital (acquired) blindness. One of the first scientists to come up with a working brain interface to restore sight was private researcher William Dobelle.\nDobelle's first prototype was implanted into \"Jerry\", a man blinded in adulthood, in 1978. A single-array BCI containing 68 electrodes was implanted onto Jerry's visual cortex and succeeded in producing phosphenes, the sensation of seeing light. The system included cameras mounted on glasses to send signals to the implant. Initially, the implant allowed Jerry to see shades of grey in a limited field of vision at a low frame-rate. This also required him to be hooked up to a two-ton mainframe, but shrinking electronics and faster computers made his artificial eye more portable and now enable him to perform simple tasks unassisted.\n\nIn 1997, Philip Kennedy, a scientist and physician, created the world's first human cyborg from Johnny Ray, a Vietnam veteran who suffered a stroke. Ray's body, as doctors called it, was \"locked in\". Ray wanted his old life back so he agreed to Kennedy's experiment. Kennedy embedded an implant he designed (and named \"neurotrophic electrode\") near the part of Ray's brain so that Ray would be able to have some movement back in his body. The surgery went successfully, but in 2002, Johnny Ray died.\n\nIn 2002, Canadian Jens Naumann, also blinded in adulthood, became the first in a series of 16 paying patients to receive Dobelle's second generation implant, marking one of the earliest commercial uses of BCIs. The second generation device used a more sophisticated implant enabling better mapping of phosphenes into coherent vision. Phosphenes are spread out across the visual field in what researchers call the starry-night effect. Immediately after his implant, Naumann was able to use his imperfectly restored vision to drive slowly around the parking area of the research institute.\n\nIn contrast to replacement technologies, in 2002, under the heading Project Cyborg, a British scientist, Kevin Warwick, had an array of 100 electrodes fired into his nervous system in order to link his nervous system into the internet to investigate enhancement possibilities. With this in place Warwick successfully carried out a series of experiments including extending his nervous system over the internet to control a robotic hand, also receiving feedback from the fingertips in order to control the hand's grip. This was a form of extended sensory input. Subsequently, he investigated ultrasonic input in order to remotely detect the distance to objects. Finally, with electrodes also implanted into his wife's nervous system, they conducted the first direct electronic communication experiment between the nervous systems of two humans.<ref name=\"doi10.1001/archneur.60.10.1369|noedit\"></ref>\n\nSince 2004, British artist Neil Harbisson, has had a cyborg antenna implanted in his head that allows him to extend his perception of colors beyond the human visual spectrum through vibrations in his skull. His antenna was included within his 2004 passport photograph which has been claimed to confirm his cyborg status. In 2012 at TEDGlobal, Harbisson explained that he started to feel cyborg when he noticed that the software and his brain had united and given him an extra sense. Neil Harbisson is the first person to be officially recognized as a cyborg by a government; he is a and co-founder of the Cyborg Foundation (2004)\n\nFurthermore many cyborgs with multifunctional microchips injected into their hand are known to exist. With the chips they are able swipe cards, open or unlock doors, operate devices such as printers or, with some using a cryptocurrency, buy products, such as drinks, with a wave of the hand.\n\nbodyNET is an application of human-electronic interaction currently in development by researchers from Stanford University. The technology is based on stretchable semiconductor materials (Elastronic). According to their article in Nature (journal), the technology is composed of smart devices, screens, and a network of sensors that can be implanted into the body, woven into the skin or worn as clothes. It has been suggested, that this platform can potentially replace the smartphone in the future.\n\nThe US-based company Backyard Brains released what they refer to as \"The world's first commercially available cyborg\" called the RoboRoach. The project started as a University of Michigan biomedical engineering student senior design project in 2010 and was launched as an available beta product on 25 February 2011. The RoboRoach was officially released into production via a TED talk at the TED Global conference, and via the crowdsourcing website Kickstarter in 2013, the kit allows students to use microstimulation to momentarily control the movements of a walking cockroach (left and right) using a bluetooth-enabled smartphone as the controller. Other groups have developed cyborg insects, including researchers at North Carolina State University, UC Berkeley, and Nanyang Technological University, Singapore, but the RoboRoach was the first kit available to the general public and was funded by the National Institute of Mental Health as a device to serve as a teaching aid to promote an interest in neuroscience. Several animal welfare organizations including the RSPCA and PETA have expressed concerns about the ethics and welfare of animals in this project.\n\nIn medicine, there are two important and different types of cyborgs: the restorative and the enhanced. Restorative technologies \"restore lost function, organs, and limbs\". The key aspect of restorative cyborgization is the repair of broken or missing processes to revert to a healthy or average level of function. There is no enhancement to the original faculties and processes that were lost.\n\nOn the contrary, the enhanced cyborg \"follows a principle, and it is the principle of optimal performance: maximising output (the information or modifications obtained) and minimising input (the energy expended in the process)\". Thus, the enhanced cyborg intends to exceed normal processes or even gain new functions that were not originally present.\n\nAlthough prostheses in general supplement lost or damaged body parts with the integration of a mechanical artifice, bionic implants in medicine allow model organs or body parts to mimic the original function more closely. Michael Chorost wrote a memoir of his experience with cochlear implants, or bionic ear, titled \"Rebuilt: How Becoming Part Computer Made Me More Human.\" Jesse Sullivan became one of the first people to operate a fully robotic limb through a nerve-muscle graft, enabling him a complex range of motions beyond that of previous prosthetics. By 2004, a fully functioning artificial heart was developed. The continued technological development of bionic and nanotechnologies begins to raise the question of enhancement, and of the future possibilities for cyborgs which surpass the original functionality of the biological model. The ethics and desirability of \"enhancement prosthetics\" have been debated; their proponents include the transhumanist movement, with its belief that new technologies can assist the human race in developing beyond its present, normative limitations such as aging and disease, as well as other, more general incapacities, such as limitations on speed, strength, endurance, and intelligence. Opponents of the concept describe what they believe to be biases which propel the development and acceptance of such technologies; namely, a bias towards functionality and efficiency that may compel assent to a view of human people which de-emphasizes as defining characteristics actual manifestations of humanity and personhood, in favor of definition in terms of upgrades, versions, and utility.\n\nA brain-computer interface, or BCI, provides a direct path of communication from the brain to an external device, effectively creating a cyborg. Research of Invasive BCIs, which utilize electrodes implanted directly into the grey matter of the brain, has focused on restoring damaged eyesight in the blind and providing functionality to paralyzed people, most notably those with severe cases, such as Locked-In syndrome. This technology could enable people who are missing a limb or are in a wheelchair the power to control the devices that aide them through neural signals sent from the brain implants directly to computers or the devices. It is possible that this technology will also eventually be used with healthy people.\n\nDeep brain stimulation is a neurological surgical procedure used for therapeutic purposes. This process has aided in treating patients diagnosed with Parkinson's disease, Alzheimer's disease, Tourette syndrome, epilepsy, chronic headaches, and mental disorders. After the patient is unconscious, through anesthesia, brain pacemakers or electrodes, are implanted into the region of the brain where the cause of the disease is present. The region of the brain is then stimulated by bursts of electric current to disrupt the oncoming surge of seizures. Like all invasive procedures, deep brain stimulation may put the patient at a higher risk. However, there have been more improvements in recent years with deep brain stimulation than any available drug treatment.\n\nRetinal implants are another form of cyborgization in medicine. The theory behind retinal stimulation to restore vision to people suffering from retinitis pigmentosa and vision loss due to aging (conditions in which people have an abnormally low number of ganglion cells) is that the retinal implant and electrical stimulation would act as a substitute for the missing ganglion cells (cells which connect the eye to the brain.)\n\nWhile work to perfect this technology is still being done, there have already been major advances in the use of electronic stimulation of the retina to allow the eye to sense patterns of light. A specialized camera is worn by the subject, such as on the frames of their glasses, which converts the image into a pattern of electrical stimulation. A chip located in the user's eye would then electrically stimulate the retina with this pattern by exciting certain nerve endings which transmit the image to the optic centers of the brain and the image would then appear to the user. If technological advances proceed as planned this technology may be used by thousands of blind people and restore vision to most of them.\n\nA similar process has been created to aide people who have lost their vocal cords. This experimental device would do away with previously used robotic sounding voice simulators. The transmission of sound would start with a surgery to redirect the nerve that controls the voice and sound production to a muscle in the neck, where a nearby sensor would be able to pick up its electrical signals. The signals would then move to a processor which would control the timing and pitch of a voice simulator. That simulator would then vibrate producing a multitonal sound which could be shaped into words by the mouth.\n\nAn article published in \"Nature Materials\" in 2012 reported a research on \"cyborg tissues\" (engineered human tissues with embedded three-dimensional mesh of nanoscale wires), with possible medical implications.\n\nIn 2014, researchers from the University of Illinois at Urbana-Champaign and Washington University in St. Louis had developed a device that could keep a heart beating endlessly. By using 3D printing and computer modeling these scientist developed an electronic membrane that could successfully replace pacemakers. The device utilizes a \"spider-web like network of sensors and electrodes\" to monitor and maintain a normal heart-rate with electrical stimuli. Unlike traditional pacemakers that are similar from patient to patient, the elastic heart glove is made custom by using high-resolution imaging technology. The first prototype was created to fit a rabbit's heart, operating the organ in an oxygen and nutrient-rich solution. The stretchable material and circuits of the apparatus were first constructed by Professor John A. Rogers in which the electrodes are arranged in a s-shape design to allow them to expand and bend without breaking. Although the device is only currently used as a research tool to study changes in heart rate, in the future the membrane may serve as a safeguard from heart attacks.\n\nMilitary organizations' research has recently focused on the utilization of cyborg animals for the purposes of a supposed tactical advantage. DARPA has announced its interest in developing \"cyborg insects\" to transmit data from sensors implanted into the insect during the pupal stage. The insect's motion would be controlled from a Micro-Electro-Mechanical System (MEMS) and could conceivably survey an environment or detect explosives and gas. Similarly, DARPA is developing a neural implant to remotely control the movement of sharks. The shark's unique senses would then be exploited to provide data feedback in relation to enemy ship movement or underwater explosives.\n\nIn 2006, researchers at Cornell University invented a new surgical procedure to implant artificial structures into insects during their metamorphic development. The first insect cyborgs, moths with integrated electronics in their thorax, were demonstrated by the same researchers. The initial success of the techniques has resulted in increased research and the creation of a program called Hybrid-Insect-MEMS, HI-MEMS. Its goal, according to DARPA's Microsystems Technology Office, is to develop \"tightly coupled machine-insect interfaces by placing micro-mechanical systems inside the insects during the early stages of metamorphosis\".\n\nThe use of neural implants has recently been attempted, with success, on cockroaches. Surgically applied electrodes were put on the insect, which were remotely controlled by a human. The results, although sometimes different, basically showed that the cockroach could be controlled by the impulses it received through the electrodes. DARPA is now funding this research because of its obvious beneficial applications to the military and other areas\n\nIn 2009 at the Institute of Electrical and Electronics Engineers (IEEE) Micro-electronic mechanical systems (MEMS) conference in Italy, researchers demonstrated the first \"wireless\" flying-beetle cyborg. Engineers at the University of California at Berkeley have pioneered the design of a \"remote controlled beetle\", funded by the DARPA HI-MEMS Program. Filmed evidence of this can be viewed here. This was followed later that year by the demonstration of wireless control of a \"lift-assisted\" moth-cyborg.\n\nEventually researchers plan to develop HI-MEMS for dragonflies, bees, rats and pigeons. For the HI-MEMS cybernetic bug to be considered a success, it must fly from a starting point, guided via computer into a controlled landing within of a specific end point. Once landed, the cybernetic bug must remain in place.\n\nIn 2016 the first cyborg Olympics were celebrated in Zurich Switzerland. Cybathlon 2016 were the first Olympics for cyborgs and the first worldwide and official celebration of cyborg sports. In this event, 16 teams of people with disabilities used technological developments to turn themselves into cyborg athletes. There were six different events and its competitors used and controlled advanced technologies such as powered prosthetic legs and arms, robotic exoskeletons, bikes and motorized wheelchairs.\n\nIf on one hand this was already a remarkable improvement, as it allowed disabled people to compete and showed the several technological enhancements that are already making a difference, on the other hand it showed that there is still a long way to go. For instance, the exoskeleton race still required its participants to stand up from a chair and sit down, navigate a slalom and other simple activities such as walk over stepping stones and climb up and down stairs. Despite the simplicity of these activities, 8 of the 16 teams that participated in the event drop of before the start.\n\nNonetheless, one of the main goals of this event and such simple activities is to show how technological enhancements and advanced prosthetic can make a difference in peoples' lives. The next Cybathlon is expected to occur in 2020\n\nThe concept of the cyborg is often associated with science fiction. However, many artists have tried to create public awareness of cybernetic organisms; these can range from paintings to installations. Some artists who create such works are Neil Harbisson, Moon Ribas, Patricia Piccinini, Steve Mann, Orlan, H. R. Giger, Lee Bul, Wafaa Bilal, Tim Hawkinson and Stelarc.\n\nStelarc is a performance artist who has visually probed and acoustically amplified his body. He uses medical instruments, prosthetics, robotics, virtual reality systems, the Internet and biotechnology to explore alternate, intimate and involuntary interfaces with the body. He has made three films of the inside of his body and has performed with a third hand and a virtual arm. Between 1976–1988 he completed 25 body suspension performances with hooks into the skin. For 'Third Ear' he surgically constructed an extra ear within his arm that was internet enabled, making it a publicly accessible acoustical organ for people in other places. He is presently performing as his avatar from his second life site.\n\nTim Hawkinson promotes the idea that bodies and machines are coming together as one, where human features are combined with technology to create the Cyborg. Hawkinson's piece \"Emoter\" presented how society is now dependent on technology.\n\nWafaa Bilal is an Iraqi-American performance artist who had a small 10 megapixel digital camera surgically implanted into the back of his head, part of a project entitled 3rd I. For one year, beginning 15 December 2010, an image is captured once per minute 24 hours a day and streamed live to and the Mathaf: Arab Museum of Modern Art. The site also displays Bilal's location via GPS. Bilal says that the reason why he put the camera in the back of the head was to make an \"allegorical statement about the things we don't see and leave behind.\" As a professor at NYU, this project has raised privacy issues, and so Bilal has been asked to ensure that his camera does not take photographs in NYU buildings.\n\nMachines are becoming more ubiquitous in the artistic process itself, with computerized drawing pads replacing pen and paper, and drum machines becoming nearly as popular as human drummers. This is perhaps most notable in generative art and music. Composers such as Brian Eno have developed and utilized software which can build entire musical scores from a few basic mathematical parameters.\n\nScott Draves is a generative artist whose work is explicitly described as a \"cyborg mind\". His Electric Sheep project generates abstract art by combining the work of many computers and people over the internet.\n\nArtists have explored the term cyborg from a perspective involving imagination. Some work to make an abstract idea of technological and human-bodily union apparent to reality in an art form utilizing varying mediums, from sculptures and drawings to digital renderings.\nArtists that seek to make cyborg-based fantasies a reality often call themselves cyborg artists, or may consider their artwork \"cyborg\". How an artist or their work may be considered cyborg will vary depending upon the interpreter's flexibility with the term.\nScholars that rely upon a strict, technical description of cyborg, often going by Norbert Wiener's cybernetic theory and Manfred E. Clynes and Nathan S. Kline's first use of the term, would likely argue that most cyborg artists do not qualify to be considered cyborgs. Scholars considering a more flexible description of cyborgs may argue it incorporates more than cybernetics. Others may speak of defining subcategories, or specialized cyborg types, that qualify different levels of cyborg at which technology influences an individual. This may range from technological instruments being external, temporary, and removable to being fully integrated and permanent. Nonetheless, cyborg artists are artists. Being so, it can be expected for them to incorporate the cyborg idea rather than a strict, technical representation of the term, seeing how their work will sometimes revolve around other purposes outside of cyborgism.\n\nAs medical technology becomes more advanced, some techniques and innovations are adopted by the body modification community. While not yet cyborgs in the strict definition of Manfred Clynes and Nathan Kline, technological developments like implantable silicon silk electronics, augmented reality and QR codes are bridging the disconnect between technology and the body. Hypothetical technologies such as digital tattoo interfaces would blend body modification aesthetics with interactivity and functionality, bringing a transhumanist way of life into present day reality.\n\nIn addition, it is quite plausible for anxiety expression to manifest. Individuals may experience pre-implantation feelings of fear and nervousness. To this end, individuals may also embody feelings of uneasiness, particularly in a socialized setting, due to their post-operative, technologically augmented bodies, and mutual unfamiliarity with the mechanical insertion. Anxieties may be linked to notions of otherness or a cyborged identity.\n\nCyborgs have become a well-known part of science fiction literature and other media. Although many of these characters may be technically androids, they are often referred to as cyborgs. Well-known examples from film and television include RoboCop, The Terminator, Evangelion, United States Air Force Colonel Steve Austin in both \"Cyborg\" and, as acted out by Lee Majors, \"The Six Million Dollar Man,\" Replicants from \"Blade Runner\", Daleks and Cybermen from \"Doctor Who,\" the Borg from \"Star Trek,\" Darth Vader and General Grievous from \"Star Wars\", Inspector Gadget, and Cylons from the 2004 \"Battlestar Galactica\" series. From comics, manga and anime are characters such as 8 Man (the inspiration for \"RoboCop\"), Kamen Rider, \"Ghost in the Shell's\" Motoko Kusanagi, as well as characters from western comic books like Tony Stark (after his Extremis and Bleeding Edge armor) and Victor \"Cyborg\" Stone. The \"Deus Ex\" videogame series deals extensively with the near-future rise of cyborgs and their corporate ownership, as does the \"Syndicate\" series. William Gibson's Neuromancer features one of the first female cyborgs, a \"Razorgirl\" named Molly Millions, who has extensive cybernetic modifications and is one of the most prolific cyberpunk characters in the science fiction canon.\n\nSending humans to space is a dangerous task in which the implementation of various cyborg technologies could be used in the future for risk mitigation. Stephen Hawking, a renowned physicist, stated \"Life on Earth is at the ever-increasing risk of being wiped out by a disaster such as sudden global warming, nuclear war... I think the human race has no future if it doesn't go into space.\" The difficulties associated with space travel could mean it might be centuries before humans ever become a multi-planet species. There are many effect of spaceflight on the human body. One major issue of space exploration is the biological need for oxygen. If this necessity was taken out of the equation, space exploration would be revolutionized. A theory proposed by Manfred E. Clynes and Nathan S. Kline is aimed at tackling this problem. The two scientists theorized that the use of an inverse fuel cell that is \"capable of reducing CO2 to its components with removal of the carbon and re-circulation of the oxygen...\" could make breathing unnecessary. Another prominent issue is radiation exposure. Yearly, the average human on earth is exposed to approximately 0.30 rem of radiation, while an astronaut aboard the International Space Station for 90 days is exposed to 9 rem. To tackle the issue, Clynes and Kline theorized a cyborg containing a sensor that would detect radiation levels and a Rose osmotic pump \"which would automatically inject protective pharmaceuticals in appropriate doses.\" Experiments injecting these protective pharmaceuticals into monkeys have shown positive results in increasing radiation resistance.\n\nAlthough the effects of spaceflight on our body is an important issue, the advancement of propulsion technology is just as important. With our current technology, it would take us about 260 days to get to Mars. A study backed by NASA proposes an interesting way to tackle this issue through deep sleep, or torpor. With this technique, it would \"reduce astronauts' metabolic functions with existing medical procedures\". So far experiments have only resulted in patients being in torpor state for one week. Advancements to allow for longer states of deep sleep would lower the cost of the trip to mars as a result of reduced astronaut resource consumption.\n\nTheorists such as Andy Clark suggest that interactions between humans and technology result in the creation of a cyborg system. In this model \"cyborg\" is defined as a part biological, part mechanical system which results in the augmentation of the biological component and the creation of a more complex whole. Clark argues that this broadened definition is necessary to an understanding of human cognition. He suggests that any tool which is used to offload part of a cognitive process may be considered the mechanical component of a cyborg system. Examples of this human and technology cyborg system can be very low tech and simplistic, such as using a calculator to perform basic mathematical operations or pen and paper to make notes, or as high tech as using a personal computer or phone. According to Clark, these interactions between a person and a form of technology integrate that technology into the cognitive process in a way which is analogous to the way that a technology which would fit the traditional concept a cyborg augmentation becomes integrated with its biological host. Because all humans in some way use technology to augment their cognitive processes, Clark comes to the conclusion that we are \"natural-born cyborgs\".\n\nIn 2010, the Cyborg Foundation became the world's first international organization dedicated to help humans become cyborgs. The foundation was created by cyborg Neil Harbisson and Moon Ribas as a response to the growing number of letters and emails received from people around the world interested in becoming a cyborg. The foundation's main aims are to extend human senses and abilities by creating and applying cybernetic extensions to the body, to promote the use of cybernetics in cultural events and to defend cyborg rights. In 2010, the foundation, based in Mataró (Barcelona), was the overall winner of the Cre@tic Awards, organized by Tecnocampus Mataró.\n\nIn 2012, Spanish film director Rafel Duran Torrent, created a short film about the Cyborg Foundation. In 2013, the film won the Grand Jury Prize at the Sundance Film Festival's Focus Forward Filmmakers Competition and was awarded with $100,000 USD.\n\nGiven the technical scope of current and future implantable sensory/telemetric devices, these devices will be greatly proliferated, and will have connections to commercial, medical, and governmental networks. For example, in the medical sector, patients will be able to login to their home computer, and thus visit virtual doctor’s offices, medical databases, and receive medical prognoses from the comfort of their own home from the data collected through their implanted telemetric devices. However, this online network presents huge security concerns because it has been proven by several U.S. universities that hackers could get onto these networks and shut down peoples’ electronic prosthetics. These sorts of technologies are already present in the U.S. workforce as a firm in River Falls, Wisconsin called Three Square Market partnered with a Swedish firm called Biohacks Technology to implant RFID microchips in the hands of its employees (which are about the size of a grain of rice) that allow employees to access offices, computers, and even vending machines. More than 50 of the firms 85 employees were chipped. It was confirmed that the U.S. Food and Drug Administration approved of these implantations . If these devices are to be proliferated within society, then the question that begs to be answered is what regulatory agency will oversee the operations, monitoring, and security of these devices? According to this case study of Three Square Market, it seems that the FDA is assuming the role in regulating and monitoring these devices.\n\n\n", "id": "20756967", "title": "Cyborg"}
{"url": "https://en.wikipedia.org/wiki?curid=50485535", "text": "Mind-controlled wheelchair\n\nA mind-controlled wheelchair is a mind-machine interfacing device that uses thought (neural impulses) to command the motorised wheelchair's motion. The first such device to reach production was designed by Diwakar Vaish, Head of Robotics and Research at A-SET Training & Research Institutes. The wheelchair is of great importance to patients suffering from locked-in syndrome (LIS), in which a patient is aware but cannot move or communicate verbally due to complete paralysis of nearly all voluntary muscles in the body except the eyes. Such wheelchairs can also be used in case of muscular dystrophy, a disease that weakens the musculoskeletal system and hampers locomotion (walking or moving).\n\nA mind-controlled wheelchair functions using a brain–computer interface: an electroencephalogram (EEG) worn on the user's forehead detects neural impulses that reach the scalp allowing the micro-controller on board to detect the user's thought process, interpret it, and control the wheelchair's movement.\n\nThe A-SET wheelchair comes standard with many different types of sensors, like temperature sensors, sound sensors and an array of distance sensors which detect any unevenness in the surface. The chair automatically avoids stairs and steep inclines. It also has a \"safety switch\": in case of danger, the user can close his eyes quickly to trigger an emergency stop.\n\n", "id": "50485535", "title": "Mind-controlled wheelchair"}
{"url": "https://en.wikipedia.org/wiki?curid=39129580", "text": "Vaimos\n\nVaimos (Voilier Autonome Instrumenté pour Mesures Océanographiques de Surface) is an autonomous sailing boat with embedded instrumentation for ocean surface measurements.\n\nIts goal is to collect measurements at the surface of the ocean. This robot is the result of a collaboration between ENSTA Bretagne and IFREMER. ENSTA-Bretagne (OSM Team) develops control algorithms and the software architecture, IFREMER (LPO+RDT) builds the mechanics, the embedded instrumentation.\n\nBrest-Douarnenez. One of the longest trips the robot has done is Brest-Douarnenez where Vaimos has done more than 100 km in an autonomous mode. Since Vaimos has made trips more than 350 km long.\n\nWRSC. In 2013, Vaimos participated to the World Robotic Sailing Championship WRSC 2013 in Brest, France \n\nThe robot follows a desired trajectory which is a sequence of lines. When following a line, the robot has two modes. \n\nTwo nested control loops are implemented in the computer of Vaimos. The first loop corresponds to a low level controller which tunes the rudder and the sail provided in order to have a desired heading. The second loop generates the desired heading to make the line to follow attractive. With this two loop control strategy, we have the guarantee that the robot will always stay inside the required corridor. This is illustrated the spiral experiment\nwhere the robot has to follow a square spiral\n\n. The proof that the robot will always stay inside its corridor can be performed using interval analysis and set inversion\n\n. The proof assumes that the robot obeys to some uncertain state equations which is not always the case in practice.\n", "id": "39129580", "title": "Vaimos"}
{"url": "https://en.wikipedia.org/wiki?curid=50647426", "text": "Soft robotics\n\nSoft Robotics is the specific subfield of robotics dealing with constructing robots from highly compliant materials, similar to those found in living organisms.\n\nSoft robotics draws heavily from the way in which living organisms move and adapt to their surroundings. In contrast to robots built from rigid materials, soft robots allow for increased flexibility and adaptability for accomplishing tasks, as well as improved safety when working around humans. These characteristics allow for its potential use in the fields of medicine and manufacturing.\n\nThe bulk of the field of soft robotics is based upon the design and construction of robots made completely from compliant materials, with the end result being similar to invertebrates like worms and octopi. The motion of these robots is difficult to model, as continuum mechanics apply to them, and they are sometimes referred to as continuum robots. Soft Robotics is the specific sub-field of robotics dealing with constructing robots from highly compliant materials, similar to those found in living organisms. Similarly, soft robotics also draws heavily from the way in which these living organisms move and adapt to their surroundings. This allows scientists to use soft robots to understand biological phenomena using experiments that cannot be easily performed on the original biological counterparts. In contrast to robots built from rigid materials, soft robots allow for increased flexibility and adaptability for accomplishing tasks, as well as improved safety when working around humans. These characteristics allow for its potential use in the fields of medicine and manufacturing. However, there exist rigid robots that are also capable of continuum deformations, most notably the snake-arm robot.\n\nAlso, certain soft robotic mechanics may be used as a piece in a larger, potentially rigid robot. Soft robotic end effectors exist for grabbing and manipulating objects, and they have the advantage of producing a low force that is good for holding delicate objects without breaking them.\n\nIn addition, hybrid soft-rigid robots may be built using an internal rigid framework with soft exteriors for safety. The soft exterior may be multifunctional, as it can act as both the actuators for the robot, similar to muscles in vertebrates, and as padding in case of a collision with a person.\n\nPlant cells can inherently produce hydrostatic pressure due to a solute concentration gradient between the cytoplasm and external surroundings (osmotic potential). Further, plants can adjust this concentration through the movement of ions across the cell membrane. This then changes the shape and volume of the plant as it responds to this change in hydrostatic pressure. This pressure derived shape evolution is desirable for soft robotics and can be emulated to create pressure adaptive materials through the use of fluid flow. The following equation models the cell volume change rate:\n\nThis principle has been leveraged in the creation of pressure systems for soft robotics. These systems are composed of soft resins and contain multiple fluid sacs with semi-permeable membranes. The semi-permeability allows for fluid transport that then leads to pressure generation. This combination of fluid transport and pressure generation then leads to shape and volume change.\nAnother biologically inherent shape changing mechanism is that of hygroscopic shape change. In this mechanism, plant cells react to changes in humidity. When the surrounding atmosphere has a high humidity, the plant cells swell, but when the surrounding atmosphere has a low humidity, the plant cells shrink. This volume change has been observed in pollen grains and pine cone scales.\n\nConventional manufacturing techniques, such as subtractive techniques like drilling and milling, are unhelpful when it comes to constructing soft robots as these robots have complex shapes with deformable bodies. Therefore, more advanced manufacturing techniques have been developed. Those include Shape Deposition Manufacturing (SDM), the Smart Composite Microstructure (SCM) process, and 3D multimaterial printing.\n\nSDM is a type of rapid prototyping whereby deposition and machining occur cyclically. Essentially, one deposits a material, machines it, embeds a desired structure, deposits a support for said structure, and then further machines the product to a final shape that includes the deposited material and the embedded part. Embedded hardware includes circuits, sensors, and actuators, and scientists have successfully embedded controls inside of polymeric materials to create soft robots, such as the Stickybot and the iSprawl.\n\nSCM is a process whereby one combines rigid bodies of carbon fiber reinforced polymer (CFRP) with flexible polymer ligaments. The flexible polymer act as joints for the skeleton. With this process, an integrated structure of the CFRP and polymer ligaments is created through the use of laser machining followed by lamination. This SCM process is utilized in the production of mesoscale robots as the polymer connectors serve as low friction alternatives to pin joints.\n\n3D printing can now produce shape morphing materials whose shape is photosensitive, thermally activated, or water responsive. Essentially, these polymers can automatically change shape upon interaction with water, light, or heat. One such example of a shape morphing material was created through the use of light reactive ink-jet printing onto a polystyrene target. Additionally, shape memory polymers have been rapid prototyped that comprise two different components: a skeleton and a hinge material. Upon printing, the material is heated to a temperature higher than the glass transition temperature of the hinge material. This allows for deformation of the hinge material, while not affecting the skeleton material. Further, this polymer can be continually reformed through heating.\n\nAll soft robots require some system to generate reaction forces, to allow the robot to move in and interact with its environment. Due to the compliant nature of these robots, this system must be able to move the robot without the use of rigid materials to act as the bones in organisms, or the metal frame in rigid robots. However, several solutions to this engineering problem exist and have found use, each possessing advantages and disadvantages.\n\nOne of these systems uses Dielectric Elastomeric Actuators (DEAs), materials that change shape through the application of a high-voltage electric field. These materials can produce high forces, and have high specific power (W/kg). However, these materials are best suited for applications in rigids robots, as they become inefficient when they do not act upon a rigid skeleton. Additionally, the high-voltages required can become a limiting factor in the potential practical applications for these robots.\n\nAnother system uses springs made of shape-memory alloy. Although made of metal, a traditionally rigid material, the springs are made from very thin wires and are just as compliant as other soft materials. These springs have a very high force-to-mass ratio, but stretch through the application of heat, which is inefficient energy-wise.\n\nPneumatic artificial muscles are yet another method used for controlling soft robots. By changing the pressure inside a flexible tube, it will act as a muscle, contracting and extending, and applying force to what it’s attached to. Through the use of valves, the robot may maintain a given shape using these muscles with no additional energy input. However, this method generally requires an external source of compressed air to function.\n\nSoft robots can be implemented in the medical profession, specifically for invasive surgery. Soft robots can be made to assist surgeries due to their shape changing properties. Shape change is important as a soft robot could navigate around different structures in the human body by adjusting its form. This could be accomplished through the use of fluidic actuation.\nSoft robots may also be used for the creation of flexible exosuits, for rehabilitation of patients, assisting the elderly, or simply enhancing the user’s strength. A team from Harvard created an exosuit using these materials in order to give the advantages of the additional strength provided by an exosuit, without the disadvantages that come with how rigid materials restrict a person’s natural movement.\n\nTraditionally, manufacturing robots have been isolated from human workers due to safety concerns, as a rigid robot colliding with a human could easily lead to injury due to the fast-paced motion of the robot. However, soft robots could work alongside humans safely, as in a collision the compliant nature of the robot would prevent or minimize any potential injury.\n\nThere is a company, Soft Robotics Inc., www.softroboticsinc.com, in Cambridge, MA that has commercialized soft robotics systems for industrial and collaborative robotics applications. These systems are in use in food packaging, consumer goods manufacturing, and retail logistics applications.\n\n\n\nThe 2014 Disney film Big Hero 6 revolved around a soft robot, Baymax, originally designed for use in the healthcare industry. In the film, Baymax is portrayed as a large yet unintimidating robot with an inflated vinyl exterior surrounding a mechanical skeleton. The basis of the Baymax concept come from real life research on applications of soft robotics in the healthcare field, such as roboticist Chris Atkeson's work at Carnegie Mellon's Robotics Institute.\n\n", "id": "50647426", "title": "Soft robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=50393660", "text": "Robotic prosthesis control\n\nRobotic prosthesis control is a method for controlling a prosthesis in such a way that the controlled robotic prosthesis restores a biologically accurate gait to a person with a loss of limb. This is special branch of control that has an emphasis on the interaction between humans and robotics.\n\nBack in the 1970 several researchers developed a tethered electrohydraulic transfemoral prosthesis. It only included a hydraulically actuated knee joint controlled by off-board electronics using a type of control called echo control. Echo control tries to take the kinematics from the sound leg and control the prosthetic leg to match the intact leg when it reaches that part of the gait cycle. In 1988 a battery-powered active knee joint powered by DC motors and controlled by a robust position tracking control algorithm was created by Popovic and Schwirtlich. Tracking control is a common method of control used to force a particular state, such as position, velocity, or torque, to track a particular trajectory. These are just two examples of previous work that has been done in this field.\n\nThis form of control is an approach used to control the dynamic interactions between the environment and a manipulator. This works by treating the environment as an admittance and the manipulator as the impedance. The relationship this imposes for robotic prosthesis the relationship in between force production in response to the motion imposed by the environment. This translates into the torque required at each joint during a single stride, represented as a series of passive impedance functions piece wise connected over a gait cycle. Impedance control doesn't regulate force or position independently, instead it regulates the relationship between force and position and velocity. To Design an impedance controller, a regression analysis of gait data is used to parameterize an impedance function. For lower limb prosthesis the impedance function looks similar to the following equation.\n\nformula_1The terms k (spring stiffness), θ (equilibrium angle), and b (dampening coefficient) are all parameters found through regression and tuned for different parts of the gait cycle and for a specific speed. This relationship is then programmed into a micro controller to determine the required torque at different parts of the walking phase.\n\nElectromyography (EMG) is a technique used for evaluating and recording the electrical activity produced by skeletal muscles. Advanced pattern recognition algorithms can take these recordings and decode the unique EMG signal patterns generated by muscles during specific movements. The patterns can be used to determine the intent of the user and provide control for a prosthetic limb. For lower limb robotic prosthesis it is important to be able to determine if the user wants to walk on level ground, up a slope, or up stairs. Currently this is where myoelectric control comes intro play. During transitions between these different modes of operation EMG signal becomes highly variable and can be used to compliment information from mechanical sensors to determine the intended mode of operation. Each patient that uses a robotic prosthesis that is tuned for this type of control has to have there system trained for them specifically. This is done by have them go through the different modes of operation and using that data to train there pattern recognition algorithm.\n\nThe speed-adaption mechanism is a mechanism used to determine the required torque from the joints at different moving speeds. During the stance phase it has been seen that quasistiffness, which is the derivative of the torque angle relationship with respect the angle, changes constantly as a function of walking speed. This means that over the stance phase, depending on the speed the subject is moving, there is a derivable torque angle relationship that can be used to control a lower limb prosthesis. During the swing phase joint torque increases proportionally to walking speed and the duration of the swing phase decreases proportionally to the stride time. These properties allow for trajectories to be derived that can be controlled around that accurately describe the angle trajectory over the swing phase. Because these two mechanism remain constant from person to person this method removes the speed and patient specific tuning required by most lower limb prosthetic controllers.\n\nWalking gait is classified as hybrid system, meaning that it has split dynamics. With this unique problem, a set of solutions to hybrid systems that undergo impacts was developed called Rapid Exponentially Stabilizing Control Lyapunov Functions(RES-CLF). Control Lyapunov function are used to stabilize a nonlinear system to a desired set of states. RES-CLFs can be realized using quadratic programs that take in several inequality constraints and return an optimal output. One problem with these are that they require a model of the system to develop the RES-CLFs. To remove the need of tuning to specific individuals Model Independent Quadratic Programs (MIQP) were used to derive CLFs. These CLFs are only focused on reducing the error in the desired output without any knowledge of what the desired torque should be. To provide this information an impedance control is added to provide a feed forward term that allows the MIQP to gather information about the system it is controlling without having a full model of the system.\n\nThis section is currently being left blank with the intent that other users will fill in this information.\n", "id": "50393660", "title": "Robotic prosthesis control"}
{"url": "https://en.wikipedia.org/wiki?curid=50493193", "text": "Robotic non-destructive testing\n\nRobotic non-destructive testing (NDT) is a method of inspection used to assess the structural integrity of petroleum, natural gas, and water installations. Crawler-based robotic tools are commonly used for in-line inspection (ILI) applications in pipelines that cannot be inspected using traditional intelligent pigging tools (or unpiggable pipelines).\n\nRobotic NDT tools can also be used for mandatory inspections in inhospitable areas (e.g., tank interiors, subsea petroleum installations) to minimize danger to human inspectors, as these tools are operated remotely by a trained technician or NDT analyst. These systems transmit data and commands via either a wire (typically called an umbilical cable or tether) or wirelessly (in the case of battery-powered tetherless crawlers).\n\nRobotic NDT tools help pipeline operators and utility companies complete required structural integrity data sets for maintenance purposes in the following applications:\n\n\nPipeline conditions that may prevent or hinder a flow-driven pig inspection include:\n\n\nRobotic NDT tools also offer safety advantages in inhospitable areas:\n\n\nUsing robotics for infrastructure inspections can save the Department of Transportation millions in lane closures and heavy equipment rentals. By updating the 50year old methods currently in place for infrastructure inspections, the Department of Fransportation will get better results, allowing them to better allocate resources. \n\nPost-tension tendons that hold up large concrete structures worldwide (e.g., bridges) are still mostly inspected manually. Robotic inspection devices can peer through concrete and steel and take the guesswork out of post-tension tendon inspections. \n\nLightweight, portable without lane closures or heavy equipment can save money and prevent traffic interruptions and deadly accidents.\n\nReplacing the manual subjective inspection with robotics within the same budget is the key to fixing and maintaining a strong infrastructure. The information provided by robotic inspections will help extend the service life of valuable infrastructure assets, keep the public safe and save billions in untimely replacements.\n\nTethered robotic inspection tools have an umbilical cable attached to them, which provides power and control commands to the tool while relaying sensor data back to the technician. Tethered crawlers have the following advantages over untethered crawlers:\n\n\nTethered crawlers have the following disadvantages against untethered crawlers:\n\n\nUntethered robotic ILI crawlers are powered by onboard batteries; these tools transmit sensor data wirelessly to the tool operator or store the data for downloading upon tool retrieval. Untethered crawlers have the following advantages over tethered crawlers:\n\n\nUntethered crawlers have the following disadvantages against tethered crawlers:\n\n\nRobotic NDT tools employ suites of inspection sensors. This section describes common sensor types; most tools combine several types of sensor depending on factors such as robot size, design, and application.\n\nMain article – Electromagnetic acoustic transducers\n\nElectromagnetic acoustic transducers (EMAT) induce ultrasonic waves into uniformly-milled metal inspection objects (e.g., pipe walls, tank floors). Technicians can assess metal condition and detect anomalies based on the reflections of these waves – when the transducer passes over an anomaly, a new reflection appears between the initial pulse and the normal reflection.\n\nDirect beam EMAT, where the tool induces ultrasonic waves into the metal at a 0° angle (or perpendicular to the metal surface), is the most common inspection method. Direct beam inspections determine metal thickness as well as detect and measure the following defects:\n\n\nAngle beam inspections, where the tool induces ultrasonic waves into the metal at an angle relative to the metal surface, can be performed concurrently with direct beam inspections to confirm anomaly detections. An angle beam transducer only registers echoes from anomalies or reflectors that fall into the beam path; unlike direct beam, it does not receive reflections from the opposite wall of normal steel.\n\nThe combination of angle beam and direct beam methods may find additional anomalies and increase inspection accuracy. However, the angle beam method has a lower tolerance for surface debris than the direct beam method. Angle beam inspections discover crack-like anomalies parallel to the pipe axis and metal loss defects that are too small to detect via direct beam, including the following:\n\n\nBesides its uses in unpiggable pipelines, the non-contact nature of EMAT tools makes this method ideal for dry applications where liquid couplant requirements may make traditional UT tools undesirable (e.g., natural gas lines).\n\nWeld integrity is a crucial component of pipeline safety, especially girth welds (or the circumferential welds that join each section of pipe together). However, unlike the consistent molecular structure of milled steel, welds and their heat-affected zones (HAZs) have an anisotropic grain structure that attenuates ultrasonic signals and creates wave velocity variances that are difficult for ILI tools to analyze.\n\nOne angle-beam EMAT method employs a set of nine frequency-time (FT) scans on each side of the girth weld, where each frequency corresponds to a different input wave angle. The following figure shows a diagram of the inspection area covered by this method, where the green area represents the propagation of shear waves in the weld and surrounding metal.\n\nThe tool merges each set of FT scans into a single frequency-time matrix scan to display weld conditions, with anomalies color-coded by severity. This method of girth weld scanning is designed to detect the following weld defects:\n\n\nMain article – Magnetic flux leakage\n\nMagnetic flux leakage (MFL) tools use a sensor sandwiched between multiple powerful magnets to create and measure the flow of magnetic flux in the pipe wall. Structurally-sound steel has a uniform structure that allows regular flow of the magnetic flux, while anomalies and features interrupt the flow of flux in identifiable patterns; the sensor registers these flow interruptions and records them for later analysis. The following figure illustrates the principle of a typical MFL inspection tool; the left side of the diagram shows how an MFL tool works in structurally sound pipe, while the right side shows how the tool detects and measures a metal loss defect.\n\nMFL tools are used primarily to detect pitting corrosion, and some tool configurations can detect weld defects. One advantage of MFL tools over ultrasonic tools is the ability to maintain reasonable sensitivity through relatively thick surface coatings (e.g., paint, pipe liners).\n\nMain article – video inspection\n\nRobotic NDT tools employ cameras to provide technicians an optimal view of the inspection area. Some cameras provide specific views of the pipeline (e.g., straight forward, sensor contact area on the metal) to assist in controlling the tool, while other cameras are used to take high-resolution photographs of inspection findings.\n\nSome tools exist solely to perform video inspection; many of these tools include a mechanism to aim the camera to completely optimize technicians’ field of vision, and the lack of other bulky ILI sensor packages makes these tools exceptionally maneuverable. Cameras on multipurpose ILI tools are usually placed in locations that maximize technicians’ ability to analyze findings as well as optimally control the tool.\n\nMain article – surface metrology\n\nLaser profilometers project a shape onto the object surface. Technicians configure the laser (both angle of incidence and distance from the object) to ensure the shape is uniform on normal metal. Superficial anomalies (e.g., pitting corrosion, dents) distort the shape, allowing the inspection technicians to measure the anomalies using proprietary software programs. Photographs of these laser distortions provide visual evidence that improves the data analysis process and contributes to structural integrity efforts.\n\nMain article – Pulsed-eddy current\n\nPulsed-eddy current (PEC) tools use a probe coil to send a pulsed magnetic field into a metal object. The varying magnetic field induces eddy currents on the metal surface. The tool processes the detected eddy current signal and compares it to a reference signal set before the tool run; the material properties are eliminated to give a reading for the average wall thickness within the area covered by the magnetic field. The tool logs the signal for later analysis. The following diagram illustrates the principle of a typical PEC inspection tool.\n\nPEC tools can inspect accurately with a larger gap between the transducer and the inspection object than other tools, making it ideal for inspecting metal through non-metal substances (e.g., pipe coatings, insulation, marine growth).\n\nUnited States federal law requires baseline inspections to establish pipeline as-built statistics and subsequent periodic inspections to monitor asset deterioration. Pipeline operators also are responsible to designate high-consequence areas (HCAs) in all pipelines, perform regular assessments to monitor pipeline conditions, and develop preventive actions and response plans.\n\nState regulations for inspecting pipelines vary based on the level of public safety concerns. For example, a 2010 natural gas pipeline explosion in a San Bruno residential neighborhood led the California Public Utilities Commission to require safety enhancement plans from California natural gas transmission operators. The safety plan included numerous pipeline replacements and in-line inspections.\n\nThe federal Pipeline and Hazardous Materials Safety Administration (PHMSA) does not permit use of tetherless crawlers in HCAs due to the risk of getting stuck. Excavating buried pipelines to retrieve stuck tools beneath freeway crossings, river crossings or dense urban areas would impact the community infrastructure too greatly. Natural gas and oil pipeline operators therefore rely on tethered robotic ILI crawlers to inspect unpiggable pipelines.\n\nWilliams used a tethered robotic ILI crawler to inspect an unpiggable section of the Transco Pipeline in New Jersey in 2015. The pipeline system ran beneath the Hudson River; construction of a new condominium development nearby created a new HCA, requiring Williams to create an integrity management program per PHMSA regulations.\n\nAlyeska Pipeline Service Company inspected Pump Station 3 on the Trans-Alaska Pipeline System after an oil leak was discovered in an underground oil pipeline at Pump Station 1 in 2011. The spill resulted in a consent agreement between Alyeska and PHMSA requiring Alyeska to remove all liquid-transport piping from its system that could not be assessed using ILI tools or a similar suitable inspection technique. Because other ILI tools could not navigate the pipeline geometry common to each of the eleven pump stations along the pipeline, Alyeska received approval to use a tethered robotic ILI crawler manufactured by Diakont to complete an inspection project at Pump Station 3. This tool allowed Alyeska to only remove a few small aboveground fittings to permit crawler entry into the piping, saving the time and expense necessary to excavate hundreds of feet of pipe (some of which was also encased in concrete vaults) to inspect by hand.\n\nNuclear power plants in the United States are subject to unique integrity management mandates per the Nuclear Energy Institute (NEI) NEI 09-14, Guideline for the Management of Buried Piping Integrity.\n\n\nNatural gas pipeline operators can use tetherless robotic ILI crawlers for smaller distribution pipelines that are not located beneath critical infrastructure elements (e.g., freeway crossings).\n\n\nRobotic NDT tools have the following advantages over other NDT methods:\n\n\n\nRobotic tools have the following disadvantages against other NDT methods:\n\n\n\n", "id": "50493193", "title": "Robotic non-destructive testing"}
{"url": "https://en.wikipedia.org/wiki?curid=50780831", "text": "Prismizer\n\nPrismizer () is an audio effect using multiple different audio codecs that sounds similar to a vocoder in that it combines pitch and frequency characteristics from multiple sources (most frequently used on the human voice) but to a different, choral effect. It utilizes the Antares Harmony Engine plugin and recalls the sound of Auto-Tune. The name combines the two words, \"prism\" and \"harmonizer,\" in reference to the polyharmonic pitch, which provides saturation akin to the dispersion of light through a prism. \n\nWhile the Antares Harmony Engine plugin has been used in many albums, the 'Prismizer,' which references a specific configuration, debuted on the Chance the Rapper mixtape \"Coloring Book.\" He has stated that it will appear on the new Kanye West album, \"Turbo Grafx 16.\" It can be heard on Frank Ocean's sophomore album, \"Blonde,\" the Bon Iver album \"22, A Million,\" and \"Farewell, Starlite!,\" the debut album of its inventor, Francis Starlite of Francis and the Lights.\n\nThe live instrument version of the Prismizer was developed by Chris Messina, studio manager for Justin Vernon, and is known as \"The Messina.\". The Messina isn't an instrument, but rather a laptop running the \"Prismizer\" connected to a MIDI controller with some other form of outboard gear.\n\nFrancis Starlite of Francis and the Lights perfected the method of reaching bright polyphony sounds without using a vocoder or Auto-Tune. It works like light that passes through a prism area and, naturally, splits into a color spectrum. Something similar happens with the vocals — it is not monotonous, but scattered and voluminous, giving a choir effect to the sound. Chance the Rapper spoke on Prismizer in an interview to Zane Lowe after its debut on \"Coloring Book\":\nThat’s the one perpetual thing. I’ve heard a lot of songs with harmonizer, right? One of my favorite things that I see on the Internet is people commenting on the album, whenever people talk about this vocal sound that he’s created, they call it Auto-Tune, or they say this sounds like a lot like Bon Iver - Justin, who Francis worked with and showed a lot of this musical styling to, uses a very similar harmonizer effect - but there’s this very special thing that Francis does that he calls Prismizer. I love it, and it’s going to be Kanye’s album, and it’s on Frank’s album, but, this Prismizer thing that he does, he sings or he’ll take a vocal and then, very similar to a vocoder, instead of it being singular keys though, he builds chordal sound around it, so it sounds like a choir, so, when you first hear Ye’s vocal come in, it sounds like 15 cyborgs, all singing in Auto-Tune, but really it’s one vocal with Francis saying, ‘OK, this is the 3rd and the 5th and the 7th,’ and he just builds a full choir around it. This whole sound is my favorite thing because it resembles the choir sound that I’ve been trying to get forever, the choral sound, but it also kind of makes it futurist at the same time. That’s like, I think, one of the all-encompassing ideas of the whole project. Francis is credited on “Summer Friends” but he also did all of the Kanye vocals on it. He also does it at the end of “Same Drugs,” and, even though it’s not him playing on “How Great,” My Cousin Nicole came in and sang this lead vocal for “How Great Is Our God,” and my homie Peter came in and did Francis’s Prismizer with his same harmonizer and preset that we learned from him and really makes it sound like a whole new thing.\n", "id": "50780831", "title": "Prismizer"}
{"url": "https://en.wikipedia.org/wiki?curid=50952451", "text": "Czech Institute of Informatics, Robotics and Cybernetics\n\nThe Czech Institute of Informatics, Robotics and Cybernetics (Český institut informatiky, robotiky a kybernetiky, CIIRC)] was established as a part of CTU on July 1, 2013. Its mission is to become an internationally respected research institute, which participates in education of students and is a place responsible for a technology transfer into the field of industry. In the premises of the CTU in Dejvice there has been two new buildings for CIIRC built since November 2013. The construction works should be finished and the buildings prepared for its inhabitants - employees of CIIRC at the end of summer 2016. CIIRC will reside in a new building, which used to serve as a Technical University canteen and one of the biggest Billa hypermarkets.\n\nCIIRC consists of eight research departments, CYPHY: Cyber-physical systems, INTSYS: Intelligent systems, IIG: Industrial informatics, RMP: Robotics and machine perception, IPA: Industrial production and automation, COGSYS: Cognitive systems and neurosciences, BEAT: Biomedical engineering and Assistive technologies, PLAT: Research Management of Platforms. Research departments are further divided into groups. The head of CIIRC is Vladimír Mařík.\n\n", "id": "50952451", "title": "Czech Institute of Informatics, Robotics and Cybernetics"}
{"url": "https://en.wikipedia.org/wiki?curid=21488059", "text": "Sex robot\n\nSex robots or sexbots are anthropomorphic robot sex dolls. , no functioning sex robots exist. There is controversy as to whether developing them would be morally justifiable.\n\nIn 2014, David Levy, the chess champion and author of \"Love and Sex with Robots\" said in an interview with \"Newsweek\" that \"I believe that loving sex robots will be a great boon to society ... There are millions of people out there who, for one reason or another, cannot establish good relationships.\" He estimates that this will take place by the mid-21st century.\n\nThere are ongoing attempts to make sex dolls socially interactive. In 2010, a sex doll called Roxxxy that had the capacity to play back pre-recorded speech cues was demonstrated at a trade show. In 2015, Matt McMullen, the creator of the RealDoll stated that he intended to create sex dolls with the capacity to hold conversations. Sexbots with a male design may be referred to as \"malebots\" or \"manbots\".\n\nBarcelona based Dr. Sergi Santos developed sex robot Samantha; the robot can switch between a sex setting (which can include Samantha simulating an orgasm) and a family mode. It can also can tell jokes and discuss philosophy.\n\nIn September 2015, Kathleen Richardson of De Montfort University and Erik Billing of the University of Skövde created the \"\", calling for a ban on the creation of anthropomorphic sex robots. In a journal article Richardson is critical of Levy and argues that the introduction of such devices would be socially harmful, and demeaning to women and children.\n\nIn September 2015, the Japanese company SoftBank, the makers of the \"Pepper\" robot, included a ban on robot sex. The robots user agreement states: \"The policy owner must not perform any sexual act or other indecent behaviour\".\n\nNoel Sharkey and Aimee van Wynsberghe of the Foundation for Responsible Robotics released a consultation report presenting a summary of the issues and various opinions about what could be society's intimate association with robots. The report includes an examination of how such robots could be employed as a sexual therapy tool for rapists or paedophiles. Sharkey warns that this could be \"problematic\" in terms of sex dolls resembling children.\n\nThe \"First International Congress on Love and Sex with Robots\" was held in Funchal, Madeira in November 2014. In October 2015 a second conference scheduled for November 2015 in Malaysia was declared illegal by the Malaysian Inspector-General of Police. The second conference was eventually held in December 2016 chaired by Dr. Kate Devlin at Goldsmiths, University of London in the United Kingdom. Devlin also founded the UK's first ever sex tech hackathon, also held in 2016 at Goldsmiths.\n\nIn 2016, a discussion of these issues was held at the 12th IFIP TC9 Human Choice & Computers Conference, entitled \"Technology and Intimacy: Choice or Coercion?\".\n\n", "id": "21488059", "title": "Sex robot"}
{"url": "https://en.wikipedia.org/wiki?curid=51775967", "text": "Real-time path planning\n\nPath planning and navigation play a significant role in robot motion planning and simulated virtual environments. Computing collision-free paths, addressing clearance, and designing dynamic representations and re-planning strategies are examples of important problems with roots in computational geometry and discrete artificial intelligence search methods, and which are being re-visited with innovative new perspectives from researchers in computer graphics and animation.\n", "id": "51775967", "title": "Real-time path planning"}
{"url": "https://en.wikipedia.org/wiki?curid=51925440", "text": "Crazyflie 2.0\n\nThe Crazyflie 2.0 is a lightweight, open source flying development platform based on a nano quadcopter.\n\nCrazyflie 2.0 is the second iteration of the open source Crazyflie nano quadcopter released in 2013 by Marcus Eliasson, Arnaud Taffanel, and Tobias Antonsson. The Crazyflie platform specifications are open source and available to anyone through the Bitcraze wiki and the Bitcraze Github repo\n\nThe Crazyflie 2.0 is a palm sized quadcopter weighing 27 grams supporting wireless control over radio and Bluetooth Low Energy (LE). It has a flight time of 7 minutes and a charge time of 40. As an open source project, its code and design specifications are available to anyone and the design was created with modification in mind. It's extensible with additional boards called \"decks\" - similar to Arduino shields - that are connected through two rows of pin headers.\n\nThe following decks are currently available:\n\n\nThe Crazyflie firmware is written in C++ and based on FreeRTOS. On the client side several languages can be used including Python, C++, C#, Ruby, NodeJS, Scala, Java and Ada/SPARK. There are also mobile clients for iOS an Android available.\n\n\n", "id": "51925440", "title": "Crazyflie 2.0"}
{"url": "https://en.wikipedia.org/wiki?curid=7909383", "text": "Programmable matter\n\nProgrammable matter is matter which has the ability to change its physical properties (shape, density, moduli, conductivity, optical properties, etc.) in a programmable fashion, based upon user input or autonomous sensing. Programmable matter is thus linked to the concept of a material which inherently has the ability to perform information processing.\n\nProgrammable matter is a term originally coined in 1991 by Toffoli and Margolus to refer to an ensemble of fine-grained computing elements arranged in space. Their paper describes a computing substrate that is composed of fine-grained compute nodes distributed throughout space which communicate using only nearest neighbor interactions. In this context, programmable matter refers to compute models similar to cellular automata and lattice gas automata. The CAM-8 architecture is an example hardware realization of this model. This function is also known as \"digital referenced areas\" (DRA) in some forms of self-replicating machine science.\n\nIn the early 1990s, there was a significant amount of work in reconfigurable modular robotics with a philosophy similar to programmable matter.\n\nAs semiconductor technology, nanotechnology, and self-replicating machine technology have advanced, the use of the term programmable matter has changed to reflect the fact that\nit is possible to build an ensemble of elements which can be \"programmed\" to change their physical properties in reality, not just in simulation. Thus, programmable matter has come to mean \"any bulk substance which can be programmed to change its physical properties.\"\n\nIn the summer of 1998, in a discussion on artificial atoms and programmable matter, Wil McCarthy and G. Snyder coined the term \"quantum wellstone\" (or simply \"wellstone\") to describe this hypothetical but plausible form of programmable matter. McCarthy has used the term in his fiction.\n\nIn 2002, Seth Goldstein and Todd Mowry started the claytronics project at Carnegie Mellon University to investigate the underlying hardware and software mechanisms necessary to realize programmable matter.\n\nIn 2004, the DARPA Information Science and Technology group (ISAT) examined the potential of programmable matter. This resulted in the 2005–2006 study \"Realizing Programmable Matter\", which laid out a multi-year program for the research and development of programmable matter.\n\nIn 2007, programmable matter was the subject of a DARPA research solicitation and subsequent program.\n\n In one school of thought the programming could be external to the material and might be achieved by the \"application of light, voltage, electric or magnetic fields, etc.\" . For example, a liquid crystal display is a form of programmable matter. A second school of thought is that the individual units of the ensemble can compute and the result of their computation is a change in the ensemble's physical properties. An example of this more ambitious form of programmable matter is claytronics.\nThere are many proposed implementations of programmable matter. Scale is one key differentiator between different forms of programmable matter. At one end of the spectrum reconfigurable modular robotics pursues a form of programmable matter where the individual units are in the centimeter size range.\nAt the nanoscale end of the spectrum there are a tremendous number of different bases for programmable matter, ranging from shape changing molecules to quantum dots. Quantum dots are in fact often referred to as artificial atoms. In the micrometer to sub-millimeter range examples include MEMS-based units, cells created using synthetic biology, and the utility fog concept.\n\nAn important sub-group of programmable matter are robotic materials, which combine the structural aspects of a composite with the affordances offered by tight integration of sensors, actuators, computation and communication, while foregoing reconfiguration by particle motion.\n\nThere are many conceptions of programmable matter, and thus many discrete avenues of research using the name. Below are some specific examples of programmable matter.\n\nThese include materials that can change their properties based on some input, but do not have the ability to do complex computation by themselves.\n\nThe physical properties of several complex fluids can be modified by applying a current or voltage, as is the case with liquid crystals.\n\nMetamaterials are artificial composites that can be controlled to react in ways that do not occur in nature. One example developed by David Smith and then by John Pendry and David Schuri is of a material that can have its index of refraction tuned so that it can have a different index of refraction at different points in the material. If tuned properly this could result in an \"invisibility cloak.\"\n\nA further example of programmable -mechanical- metamaterial is presented by Bergamini et al. Here, a pass band within the phononic bandgap is introduced, by exploiting variable stiffness of piezoelectric elements linking aluminum stubs to the aluminum plate to create a phononic crystal as in the work of Wu et al. The piezoelectric elements are shunted to ground over synthetic inductors. Around the resonance frequency of the LC circuit formed by the piezoelectric and the inductors, the piezoelectric elements exhibit near zero stiffness, thus effectively disconnecting the stubs from the plate. This is considered an example of programmable mechanical metamaterial.\n\nAn active area of research is in molecules that can change their shape, as well as other properties, in response to external stimuli. These molecules can be used individually or en masse to form new kinds of materials. For example, J Fraser Stoddart's group at UCLA has been developing molecules that can change their electrical properties.\n\nAn electropermanent magnet is a type of magnet which consists of both an electromagnet and a dual material permanent magnet, in which the magnetic field produced by the electromagnet is used to change the magnetization of the permanent magnet. The permanent magnet consists of magnetically hard and soft materials, of which only the soft material can have its magnetization changed. When the magnetically soft and hard materials have opposite magnetizations the magnet has no net field, and when they are aligned the magnet displays magnetic behaviour.\n\nThey allow creating controllable permanent magnets where the magnetic effect can be maintained without requiring a continuous supply of electrical energy. For these reasons, electropermanent magnets are essential components of the research studies aiming to build programmable magnets that can give rise to self-building structures.\n\nSelf-reconfiguring modular robotics is a field of robotics in which a group of basic robot modules work together to dynamically form shapes and create behaviours suitable for many tasks. Like Programmable matter SRCMR aims to offer significant improvement to any kind of objects or system by introducing many new possibilities for example: 1. Most important is the incredible flexibility that comes from the ability to change the physical structure and behavior of a solution by changing the software that controls modules. 2. The ability to self-repair by automatically replacing a broken module will make SRCMR solution incredibly resilient. 3. Reducing the environmental foot print by reusing the same modules in many different solutions. Self-Reconfiguring Modular Robotics enjoys a vibrant and active research community.\n\nClaytronics is an emerging field of engineering concerning reconfigurable nanoscale robots ('claytronic atoms', or \"catoms\") designed to form much larger scale machines or mechanisms. The catoms will be sub-millimeter computers that will eventually have the ability to move around, communicate with other computers, change color, and electrostatically connect to other catoms to form different shapes.\n\nCellular automata are a useful concept to abstract some of the concepts of discrete units interacting to give a desired overall behavior.\n\nQuantum wells can hold one or more electrons. Those electrons behave like artificial atoms which, like real atoms, can form covalent bonds, but these are extremely weak. Because of their larger sizes, other properties are also widely different.\n\nSynthetic biology is a field that aims to engineer cells with \"novel biological functions.\" Such cells are usually used to create larger systems (e.g., biofilms) which can be \"programmed\" utilizing synthetic gene networks such as genetic toggle switches, to change their color, shape, etc. Such bioinspired approaches to materials production has been demonstrated, using self-assembling bacterial biofilm materials that can be programmed for specific functions, such as substrate adhesion, nanoparticle templating, and protein immobilization.\n\n", "id": "7909383", "title": "Programmable matter"}
{"url": "https://en.wikipedia.org/wiki?curid=52914225", "text": "Sharp INTELLOS automated unmanned ground vehicle (A-UGV)\n\nSharp INTELLOS automated unmanned ground vehicle (A-UGV) is a type of robotics system in the category called Automated unmanned ground vehicle (A-UGV) known for using a navigation surveillance platform developed by Sharp. The system combines automation, mobility and a variety of monitoring and detection capabilities to extend the impact of a traditional security force.\nIn Greek mythology, a giant man of bronze, known as either Talos or Talon, circled Crete's shores three times daily, protecting Europa from pirates and invaders. \"INTELLOS\" combines the concept of intelligence with both technology and security.\n\nThe product launched at the ASIS International 62nd Annual Seminar and Exhibits (ASIS 2016) in Orlando, Florida on September 12, 2016. \nThe Sharp INTELLOS A-UGV provides data that can enhance outdoor surveillance, security, safety, and maintenance inspections. The system offers those services through a durable cost-effective multi-terrain, mobile sensor platform that can capture video, audio and environmental data. Robots generally work best at repetitive tasks (tedious to humans). This type of automated robot is ideal for dull, dirty, and/or dangerous outdoor patrols, which may be unsafe or unsuitable for existing field personnel.\nRobots can serve to extend the capacity and reach of traditional guarding, by integrating with existing security infrastructures and using technology to help lower security operating costs for routine tasks. They can also be tied into current networks and systems, extending their capabilities, while avoiding incremental implementation costs.\n\nOrganizations may have areas around their property that are not covered by cameras or foot patrols. These vehicles monitor, record, and provide visual surveillance in and between these uncovered areas. \n\nAutomated unmanned ground vehicles can be used for video investigation missions. Optional mobile sensors can gather information around a property to be analyzed and reviewed by personnel. These robots follow predetermined or changeable routes within fenced areas and can be used to inspect for industries, such as public utilities, refineries and storage yards. They can navigate patrol paths, along waypoints, recording and securely transmitting to security personnel allowing them to decide when human inspection or intervention is necessary.\n\nThe Sharp INTELLOS A-UGV can also provide a visible deterrent, while helping to protect people, infrastructure and assets. The vehicle has a ruggedized design intended to withstand variations in temperature and terrains.\n\nThe propulsion (drive) system is powered by two lithium ION battery packs. The vehicle can operate on either one or two battery packs. These batteries are housed in user-replaceable cartridges that can be charged in an off-line charger.\n\nThe Sharp INTELLOS A-UGV has standard cameras and an optional thermal camera. The FLIR Thermal camera, which is mounted atop the Sharp INTELLOS A-UGV's adjustable-height boom, illuminates objects in environments without any ambient light, enabling nighttime surveillance as well as showing differences in temperature. The integrated FLIR Thermal camera helps personnel detect human, vehicular, and animal intrusions, which can be viewed and recorded with the AXIS P56 PTZ dome network series camera.\n\nThe Intellos is designed to carry optional MultiRAE gas and radiation sensors as well as a siren, flashing lights and voice announcements (including two-way communications). Its video, audio and optional sensor signals are transmitted by wireless mesh network or cellular network. Other functions include a PC/Server with preinstalled command and control software, navigation system using GPS (Global Positioning System) and RTK (Real Time Kinematic) to provide accurate positioning.\n\nThe Sharp INTELLOS CCAP (Command and Control Application Software) PC/Server is located in the Security Command Center and is used to control and monitor the vehicles patrol path operations. The software allows an authorized administrator to create and schedule patrols as well as determine waypoint actions to undertake. These actions include the activation of sensors at the appropriate time and location.\n\nIn the case of cellular, the unit includes a communication router that can automatically switch between the wireless network to cellular as a fail over in the event a wireless signal cannot be acquired. Transition to cellular requires a data plan on a public carrier's cellular network.\n\nSharp INTELLOS A-UGV was named one of the Top 30 Technology Innovations of 2016 by Security Sales & Integration and in 2017 INTELLOS was named the winner of the Law Enforcement / Guarding Systems category of the Security Industry Associaton's New Product Showcase.\n\nA U.S. federal trademark registration was filed for \"INTELLOS\" by Sharp Corporation. The United States Patent and Trademark Office has given the \"INTELLOS\" trademark serial number 86886871.\n\n", "id": "52914225", "title": "Sharp INTELLOS automated unmanned ground vehicle (A-UGV)"}
{"url": "https://en.wikipedia.org/wiki?curid=42148003", "text": "Underwater robotics\n\nUnderwater robotics is a branch of robotics. Underwater robots can be autonomous, or they can be remotely operated. This is an emerging science, which has become more popular with evolving technology. There are many applications of underwater robotics such as scientific exploration, military use, and hobbies.\nBesides capability of swimming an underwater robot also has multi DOF manipulators and end effectors on these arms of various types to perform underwater tasks such as construction, salvage, rescue and repair. They can also help in collecting items that are deeply submerged inside the sea, used by the military and scientists mostly.\n", "id": "42148003", "title": "Underwater robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=49580171", "text": "Passenger drone\n\nA passenger drone (also known as a drone taxi, flying taxi, or pilotless helicopter) is a type of unmanned aerial vehicle (UAV) that carries passengers. The first passenger drone was introduced at the Consumer Electronics Show (CES) 2016 by Chinese entrepreneurs and is called the Ehang 184.\n\nThe use of UAVs, or drones, has been popular in recent years. Once used primarily for recreation by hobbyists, drones are now used in military operations and for conducting research. More recently, commercial companies have explored using drones to transport merchandise. Since 2011, several commercial developers and amateur builders have conducted short manned flights on experimental electric multi-rotor craft. In January 2016, the first commercially produced drone capable of carrying a human was introduced by Chinese entrepreneurs at CES 2016.\n\nRadio controlled model airplanes have been a popular hobby since the 1970s. Drones, especially electric powered multi-rotor craft, have only emerged among hobbyists in the past ten to fifteen years. Drones differ from model airplanes in that they implement a measure of autonomy in their operation. Aerial drones have been used by militaries since World War II. Military drone capability expanded rapidly at the end of the twentieth century. Military drones have seen extensive use during campaigns in Iraq and Afghanistan.\n\nThroughout the twentieth century and more recently, designers have proposed and developed many radical ideas for personal flight. Among these are the personal jetpack introduced in the 1960s, the Aeromobil flying car concept of the early 1990s, and the Terrafugia flying vehicle concept of 2006. While these are steered by the pilot, and thus are not technically considered drones, they nevertheless serve as inspirational precursors to the flying passenger drones being developed today.\n\nThe future of passenger drones remains uncertain since this technology is so new. Innovation in aerial drone technology, and in aerial traffic coordination, control, and collision-avoidance could result in rapid proliferation of passenger drones for civilian travel. Several companies are exploring the use of passenger drones as air-taxis and for air-ambulance services. Passenger drone developers are working to overcome many challenges, including noise, small useful load, short flight times, airspace regulations, and scarce data on both safety and general operations.\n\n", "id": "49580171", "title": "Passenger drone"}
{"url": "https://en.wikipedia.org/wiki?curid=53382545", "text": "Robot tax\n\nLeaders of the industrial and scientific communities argue that, to offset the social costs created by automation’s displacement effects, either robots should pay income tax, or their owners should pay a tax for replacing a worker with a robot. And this \"robot tax\" should be used to finance a universal basic income or guaranteed living wage. Bill Gates said the solution to the problem is simple, the government should start taxing robots i.e. make the robot owners cough up the money needed to re-establish the defunct workforce. But EU Commissioner Andrus Ansip, tasked with bringing Europe to the digital age, says no. He explained that a robot tax would only mean someone else would take the leading position and leave Europe behind.\n\n\n", "id": "53382545", "title": "Robot tax"}
{"url": "https://en.wikipedia.org/wiki?curid=53534260", "text": "Robotic governance\n\nRobotic governance provides a regulatory framework to deal with autonomous and intelligent machines. This includes research and development activities as well as handling of these machines. The idea is related to the concepts of corporate governance, technology governance and IT-governance, which provide a framework for the management of organizations or the focus of a global IT infrastructure.\n\nRobotic governance describes the impact of robotics, automation technology and artificial intelligence on society from a holistic, global perspective, considers implications and provides recommendations for actions in a Robot Manifesto. This is realized by the Robotic Governance Foundation, an international non-profit organization.\n\nThe robotic governance approach is based on the German research on discourse ethics. Therefore, the discussion should involve all Stakeholders, including scientists, society, religion, politics, industry as well as labor unions in order to reach a consensus on how to shape the future of robotics and artificial intelligence. The compiled framework, the so-called Robot Manifesto, will provide voluntary guidelines for a self-regulation in the fields of research, development as well as use and sale of autonomous and intelligent systems.\n\nThe concept does not only appeal on the responsibility of researchers and robot manufacturers, but like with child labor and sustainability, also means a raising of opportunity costs. The greater public awareness and pressure will become concerning this topic, the harder it will get for companies to conceal or justify violations. Therefore, from a certain point it will be cheaper for organizations to invest in sustainable technologies and accepted.\n\nThe idea to set ethical standards for intelligent machines is not a new one and undoubtedly has its roots in science fiction literature. Even older is the discussion about ethics of intelligent, man-made creatures in general. Some of the earliest recorded examples can be found in Ovid's \"Metamorphoses\", in Pygmalion, in the Jewish golem mysticism (12th century) as well as in the idea of Homunkulus (Latin: little man) arisen from the alchemy of the Late Middle Ages.\n\nThe fundamental and philosophical question of these literary works is what will happen, if humans presume to create autonomous, conscious or even godlike creatures, machines, robots or androids. While most of the older works broach the issue of the act of creation, if it is morally appropriate and which dangers could arise, Isaac Asimov was the first to realize the necessity to restrict and regulate the freedom of action of machines. He wrote the first Three Laws of Robotics.\n\nAt least since the use of drones equipped with air-to-ground missiles in 1995 that can be used against ground targets, like e.g. the General Atomics MQ-1, and the resulting collateral damage, the discussion on the international regulation of remote controlled, programmable and autonomous machines attracted public attention. Nowadays, this discussion covers the entire range of programmable, intelligent and/or autonomous machines, drones as well as automation technology combined with Big Data and artificial intelligence. Lately, well-known visionaries like Stephen Hawking, Elon Musk and Bill Gates brought the topic to the focus of public attention and awareness. Due to the increasing availability of small and cheap systems for public service as well as commercial and private use, the regulation of robotics in all social dimensions gained a new significance. \n\nRobotic governance was first mentioned in the scientific community within a dissertation project at the Technical University of Munich, supervised by Professor Dr. emeritus Klaus Mainzer. The topic has been the subject of several scientific workshops, symposia and conferences ever since, including the Sensor Technologies & the Human Experience 2015, the Robotic Governance Panel at the We Robots 2015 Conference, a keynote at the 10th IEEE International Conference on Self-Adaptive and Self-Organizing Systems (SASO), a full day workshop on Autonomous Technologies and their Societal Impact as part of the 2016 IEEE International Conference on Prognostics and Health Management (PHM’16), a keynote at the 2016 IEEE International Conference on Cloud and Autonomic Computing (ICCAC), the FAS*W 2016: IEEE 1st International Workshops on Foundations and Applications of Self* Systems, the 2016 IEEE International Conference on Emerging Technologies and Innovative Business Practices for the Transformation of Societies (IEEE EmergiTech 2016) and the IEEE Global Humanitarian Technology Conference (GHTC 2016).\n\nSince 2015 IEEE even holds an own forum on robotic governance at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE IROS): the first and second \"Annual IEEE IROS Futurist Forum\", which brought together worldwide renowned experts from a wide range of specialities to discuss the future of robotics and the need for regulation in 2015 and 2016. In 2016 Robotic governance has also been the topic of a plenary keynote presentation on the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2016) in Daejeon, South Korea.\n\nSeveral video statements and interviews on robotic governance, responsible use of robotics, automation technology and artificial intelligence as well as self-regulation in a world of Robotic Natives, with internationally recognized experts from research, economy and politics are published on the website of the Robotic Governance Foundation. Max Levchin, co-founder and former CTO of PayPal emphasized the need for robotic governance in the course of his Q&A session on the South by Southwest Festival (SXSW) 2016 in Austin and referred to the comments of his friend and colleague Elon Musk on this subject. Gerd Hirzinger, former head of the Institute of Robotics and Mechatronics of the German Aerospace Center, showed during his keynote speech at the IROS Futurist Forum 2015 the possibility of machines being so intelligent that it would be inevitable, one day, to prevent certain behavior. At the same event, Oussama Khatib, American roboticist and director of the stanford robotics lab, advocated to emphasize the user acceptance when producing intelligent and autonomous machines. Bernd Liepert, president of the euRobotics aisbl – the most important robotics community in Europe – recommended to establish robotic governance worldwide and underlined his wish for Europe taking the lead in this discussion, during his plenary keynote at the IEEE IROS 2015 in Hamburg. Hiroshi Ishiguro, inventor of the Geminoid and head of the Intelligent Robotics Laboratory at the University of Osaka, showed during the RoboBusiness Conference 2016 in Odense that it is impossible to stop technical progress. Therefore, it is necessary to accept the responsibility and to think about regulation. In the course of the same conference, Henrik I. Christensen, author of the U.S. Robotic Roadmap, underlined the importance of ethical and moral values in robotics and the suitability of robotic governance to create a regulatory framework.\n\n", "id": "53534260", "title": "Robotic governance"}
{"url": "https://en.wikipedia.org/wiki?curid=53810462", "text": "Autonomous things\n\nAutonomous things, abbreviated AuT, or the Internet of autonomous things, abbreviated as IoAT, is an emerging term for the technological developments that are expected to bring computers into the physical environment as autonomous entities without human direction, freely moving and interacting with humans and other objects. \n\nSelf-navigating drones are the first AuT technology in (limited) deployment. It is expected that the first mass-deployment of AuT technologies will be the autonomous car, generally expected to be available around 2020. Other currently expected AuT technologies include home robotics (e.g., machines that provide care for the elderly, infirm or young), and military robots (air, land or sea autonomous machines with information-collection or target-attack capabilities).\n\nAuT technologies share many common traits, which justify the common notation. They are all based on recent breakthroughs in the domains of (deep) machine learning and artificial intelligence. They all require extensive and prompt regulatory developments to specify the requirements from them and to license and manage their deployment (see the further reading below). And they all require unprecedented levels of safety (e.g., automobile safety) and security, to overcome concerns about the potential negative impact of the new technology.\n\nAs an example, the autonomous car both addresses the main existing safety issues and creates new issues. It is expected to be much safer than existing vehicles, by eliminating the single most dangerous elementthe driver. The US's National Highway Traffic Safety Administration estimates 94 percent of US accidents were the result of human error and poor decision-making, including speeding and impaired driving, and the Center for Internet and Society at Stanford Law School claims that \"Some ninety percent of motor vehicle crashes are caused at least in part by human error\". So while safety standards like the ISO 26262 specify the required safety, there is still a burden on the industry to demonstrate acceptable safety.\n\nWhile car accidents claim every year 35,000 lives in the US, and 1.25 million worldwide, some believe that even \"a car that's 10 times as safe, which means 3,500 people die on the roads each year [in the US alone]\" would not be accepted by the public. The acceptable level may be closer to the current figures on aviation accidents and incidents, with under a thousand worldwide deaths in most yearsthree orders of magnitude lower than cars. This underscores the unprecedented nature of the safety requirements that will need to be met for cars, with similar levels of safety expected for other Autonomous Things.\n\n", "id": "53810462", "title": "Autonomous things"}
{"url": "https://en.wikipedia.org/wiki?curid=50835878", "text": "Sensing floor\n\nA sensing floor is a floor with embedded sensors. \nDepending on their construction, these floors are either monobloc (e.g. structures made of a single frame, carpets).\nor modular (e.g. tiled floors, floors made of stripes of sensors). \nThe first sensing floor prototypes were developed in the 1990s, mainly for human gait analysis. \nSuch floors are usually used as a source of sensing information for an ambient intelligence.\nDepending on the type of sensors employed, sensing floors can measure load (pressure), proximity (to detect, track, and recognize humans), as well as the magnetic field (for detecting metallic objects like robots using magnetometers).\n\nSensing floors have a variety of usages:\n\nMore than 30 distinct sensing floor prototypes have been developed between 1990 and 2015.\nNotable examples of sensing floors have been developed by Oracle, MIT, and Inria \nAs of 2015, few sensing floors are available as commercial products, mainly targeting healthcare facilities (e.g. the GAITRite surface pressure sensing floor, and the SensFloor ).\n", "id": "50835878", "title": "Sensing floor"}
{"url": "https://en.wikipedia.org/wiki?curid=395851", "text": "International Conference on Intelligent Robots and Systems\n\nThe International Conference on Intelligent Robots and Systems (or IROS) is a conference about robotics and automation that is held in various countries and is sponsored by the IEEE. It started as the IEEE International Workshop on Intelligent Robots and Systems in 1988 and had been held annually ever since\n", "id": "395851", "title": "International Conference on Intelligent Robots and Systems"}
{"url": "https://en.wikipedia.org/wiki?curid=53609258", "text": "Taurob tracker\n\nTaurob tracker is a mobile robot, manufactured by taurob GmbH in Austria. It has been originally developed as a remote controlled reconnaissance platform for fire departments.\n\nSince 2013 a ATEX certified variant (Zone 1), called taurob tracker Ex is available, which is able to drive in explosive atmospheres safely.\n\nA taurob tracker version with sensors for environments with extreme smoke (e.g. fires in tunnels or subway stations) is currently being developed in the EU funded \"SmokeBot\" project\n\nCompared to most other mobile robots, the taurob tracker has a unique track geometry which allows it to climb over obstacles with just one pair of tracks. Due this geometry the tracks do not loose their tension when raising or lowering the front wheels. Further advantages include improved traction (thus the name of the robot) on uneven ground and a rapid track exchange mechanism.\n\nIn 2016 a taurob tracker platform was used in the RoboCup Rescue League by team Hector.\n\nIn 2017 a variant of taurob tracker called \"Argonaut\" has won the ARGOS Challenge organised by Total S.A.. It is the first fully autonomous, ATEX certified mobile inspection robot for Oil and Gas installations. According to Total it will be used on their industrial sites by 2020. \n", "id": "53609258", "title": "Taurob tracker"}
{"url": "https://en.wikipedia.org/wiki?curid=53815584", "text": "Dash Robotics, Inc\n\nDash Robotics, Inc. is a toy robotics startup company located in Hayward, California. Their main focus is on prototyping and manufacturing smart toys (sometimes called \"connected toys\"). The company is often referred to simply as \"Dash,\" and was founded in 2013. Although not officially affiliated with UC Berkeley, many of the company's members met there.\n\nThey are most widely known for creating Kamigami Robots, a biomimetic, foldable robot that was funded via Kickstarter in 2015. Kamigami received attention for their unique motion, which mimics that of a cockroach. The original concepts behind the robot's motion are attributed to Dr. Robert Full, a biologist at UC Berkeley, and can be seen outlined in his Ted Talk. Mattel licensed the Kamigami brand in spring of 2017, and says it will oversee a new release in Fall of 2017.\n\nPrevious products include Dash Beta and Dash VR. Dash Beta was a cardboard, app-controlled robot that required glue to be assembled, and acted as a predecessor to Kamigami. Dash Beta is no longer in production.\n\nMr. Dad Seal of Approval\n\nMaker Faire Editor's Choice\n\n", "id": "53815584", "title": "Dash Robotics, Inc"}
{"url": "https://en.wikipedia.org/wiki?curid=54305107", "text": "Kate Devlin\n\nKate Devlin, born Adela Katharine Devlin is a British computer scientist specialising in Artificial intelligence and Human–computer interaction (HCI). She is best known for her work on human sexuality and robotics and was co-chair of the annual Love and Sex With Robots convention in 2016 held in London and was founder of the UK's first ever sex tech hackathon held in 2016 at Goldsmiths, University of London.\nShe is a senior lecturer in the department of computing at Goldsmiths, part of the University of London and is the author of \"The Brightness of Things: An Adventure in Light and Time\" in addition to several academic papers.\n\nDevlin began her university career in the humanities and graduated from Queen's University Belfast in 1997 with a BA (Honours) degree in archaeology. After deciding that archaeology presented her with limited future prospects, she returned to Queen's University to study computer science, and in 1999 she was awarded an MSc in that subject. She then moved to The University of Bristol, where in 2004 she was awarded a PhD in computer science.\n\nIn 2003 Devlin began researching computer graphics in archaeology at Bristol University, rendering 3D computer models of archaeological sites such as at Pompeii with attention to realistically rendering lighting effects caused by the spectral composition of light sources available at the time period in history. This involved experimental archaeology, recreating light sources and analysing the spectral range for each type of candle or fuel lamp.\n\nSince 2007 Devlin has worked in the field of human-computer interaction and artificial intelligence at Goldsmiths, and is a senior lecturer in several areas of computer science, including programming, graphics and animation.\n\nIn 2015 Devlin spoke to news broadcasters in the UK about institutionalised sexism within science research and academia after comments made by Sir Tim Hunt regarding women scientists working in mixed laboratories. While Devlin, along with many other commentators, acknowledged the comments to be 'banter' she expressed the frustration that many women have with sexism in the fields of science, technology, engineering, and mathematics and jokingly tweeted that she couldn't chair a departmental meeting because she was \"too busy swooning and crying.\" Devlin also speaks publicly and writes to encourage more women to pursue technology careers.\n\nIn 2016 Devlin co-chaired the \"International Congress on Love and Sex With Robots\" held in London, UK, an annual conference held since 2014, co-founded by Adrian David Cheok and David Levy, writer of the book of the same name, Love and Sex with Robots.\n\nAlso, in 2016, Devlin founded the first UK sex technology (sex tech) hackathon, a conference where scientists, students, academics and other people in the sex tech industry meet to pool ideas and build projects in the field of sex and intimacy with artificial partners.\n\nIn 2016 Devlin appeared several times in the media debating ethical issues concerning sex robots with Kathleen Richardson, fellow of the ethics of robotics at De Montfort University, and founder of Campaign Against Sex Robots which seeks to ban sex robots on the grounds that they encourage isolation, perpetuate the idea of women as property and are dehumanising. Devlin has argued that not only would a ban be impractical, but as technology develops more women need to be involved in order to diversify a field which is dominated by men creating products for heterosexual men. She also points out that the technology can be used as therapy, citing the use of artificial intelligence to treat anxiety, and the possible application towards understanding the psychology of sex offenders.\n\nDevlin frequently speaks at conferences and her areas of scientific interest include: the social and ethical problems of integrating artificial intelligence into sexual experience with computer systems and robots, the human and social consequences of AI as it becomes more sophisticated, and improving human sexual relationships by moving away from a \"hetero-normative male view\" of sex and intimacy using sex toys, robots and computer software. She has raised issues which she believes need addressing as this technology develops. These concerns include: if robots gain self-awareness, will they be able to give informed consent and be entitled to make choices regarding their own desires, and should they be supplied to the elderly in residential care facilities for companionship and sex.\n\nDevlin was named one of London's most influential people 2017 by the Progress 1000, London Evening Standard.\n\n\n\nDevlin has written for the \"New Scientist\", \"The Conversation\" and has presented a TEDx talk entitled \"Sex Robots\".\n\nDevlin has spoken publicly about living with bipolar disorder and epilepsy and how stress can affect both her academic and professional life, as well as how important it is to bring mental health issues into public debate to reduce the stigma attached.\n\nDevlin is open about her consensually non-monogamous relationships and has written about her experiences of polyamory.\n\nShe is also interested in, and has researched, the life story of Adela Breton, the Victorian archaeologist and explorer, and contributed to the \"Raising Horizons\" exhibition of 'trowel-blazing' women throughout the history of archaeology and geology.\n\nShe is divorced and has a daughter.\n\n\n", "id": "54305107", "title": "Kate Devlin"}
{"url": "https://en.wikipedia.org/wiki?curid=54310332", "text": "Maarten Steinbuch\n\nMaarten Steinbuch is a serial entrepreneur and professor of Systems & Control at Eindhoven University of Technology. His research interests span from design and control of motion systems to robotics, automotive powertrains and fusion plasmas. He is most known for his work in the field of robotics.\n\nMaarten started his career at Philips Research during his postgraduate degree at Delft University of Technology in 1987. In 1998-1999 he had a year long stint at Philips CFT, after which he became a full professor of the Control Systems Group at the Mechanical Engineering department of Eindhoven University of Technology. During his academic career he received many awards, such as the KIVI Academic Society Award and the Simon Stevin Meester 2016, the highest Dutch award for scientific technological research.\n\nAside from teaching, Maarten has been involved in several companies as a (co-)founder. These include:\n", "id": "54310332", "title": "Maarten Steinbuch"}
{"url": "https://en.wikipedia.org/wiki?curid=54620481", "text": "FIRST Global\n\nFIRST Global (For Inspiration and Recognition of Science and Technology) is a trade name for a nonprofit organization, the International First Committee Association. It promotes STEM education and careers for youth through Olympics-style robotics competitions called the FIRST Global Challenge. It was founded by Dean Kamen in 2016 as an expansion of FIRST, an organization with similar objectives. \n\nFIRST Global is a trade name for the International First Committee Association, a nonprofit corporation based in Manchester, New Hampshire, with a 501(c)3 designation from the IRS.\n\nIt was founded by the co-founder of FIRST, Dean Kamen, with the objective of promoting STEM education and careers in the developing world through Olympics-style robotics competitions. \n\nThe 2017 FIRST Global Challenge was held in Washington, D.C., from July 17–19, and the challenge was the use of robots to separate different colored balls, representing clean water and impurities in water, symbolizing the Engineering Grand Challenge (based on the Millennium Development Goal) of improving access to clean water in the developing world. Around 160 teams composed of 15- to 18-year-olds from 157 countries participated, and around 60% of teams were created or led by young women. Six continental teams also participated.\n\nDuring the closing ceremony of the 2017 FIRST Global Challenge, entrepreneur Ricardo Salinas, a founding member of FIRST Global, announced that Mexico City would host the 2018 FIRST Global Challenge.\n\nThe Global STEM Corps is a FIRST Global initiative that connects qualified volunteer mentors with students in developing countries to prepare them for competitions.\n", "id": "54620481", "title": "FIRST Global"}
{"url": "https://en.wikipedia.org/wiki?curid=46530282", "text": "Robo-Taxi\n\nA Robo-Taxi, also known as a Robo-Cab, a self-driving taxi or a driverless taxi is an autonomous car (SAE Level 4 or 5) operated in an Autonomous Mobility on Demand (AMoD) e-hailing service. \n\nThe fact of eliminating the need for a human chauffeur, which represents a significant part of the operating costs of that type of services, could make it a very affordable solution for the customers and accelerate the spreading of Transportation-as-a-Service (TaaS) solutions as opposed to individual car ownership. However, it raises the issue of job destruction.\n\nSeveral studies highlighted that robo-taxis operated in an AMoD service could be one of the most rapidly adopted applications of autonomous cars at scale and a major mobility solution in the near future, especially in urban areas, providing the majority of vehicle miles in the United States within a decade of their first introduction. Moreover, they could have a very positive impact on road safety, pollution (since these services will most probably use electric cars and reduce the number of vehicles on the road), traffic congestion and parking.\n\nSeveral companies are testing robo-taxi services, especially in Asia and in the United-States. In most tests, there are human chauffeurs or \"safety drivers\" in these test cars to take control back in case of emergency. \n\nIn August 2016, MIT spinoff NuTonomy was the first company to make robo-taxis available to the public, starting to offer rides with a fleet of 6 modified Renault Zoes and Mitsubishi i-MiEVs in a limited area in Singapore. NuTonomy later signed three significant partnerships to develop its robo-taxi service: with Grab, Uber’s rival in Southeast Asia, with Groupe PSA, which is supposed to provide the company with Peugeot 3008 SUVs and the last one with Lyft to launch a robo-taxi service in Boston.\n\nIn September 2016, Uber started allowing a select group of users in Pittsburgh to order robo-taxis from a fleet of 14 modified Ford Fusions. The test extended to San Francisco with modified Volvo XC90s before being relocated to Tempe, Arizona in February 2017. In March 2017, one of Uber’s robo-taxis crashed in self-driving mode in Arizona, which led the company to suspend its tests before resuming them a few days later.\n\nIn early 2017, Waymo, the Google self-driving car project which became an independent company in 2016, started a large public robo-taxi test in Phoenix using 100 and then 500 more Chrysler Pacifica Hybrid minivans provided by Fiat Chrysler Automobiles as part of a partnership between the two companies. Waymo also signed a deal with Lyft to collaborate on self-driving cars in May 2017. In November 2017, Waymo revealed it had begun to operate some of its automated vehicles in Arizona without a safety driver behind the wheel.\n\nIn August 2017, Cruise Automation, a self-driving startup acquired by General Motors in 2016, launched the beta version of a robo-taxi service for its employees in San Francisco using a fleet of 46 Chevrolet Bolt EVs.\n\nMany automakers have announced their plans to develop robo-taxis before 2025 and specific partnerships have been signed between automakers, technology providers and service operators. Most significant disclosed information include: \n", "id": "46530282", "title": "Robo-Taxi"}
{"url": "https://en.wikipedia.org/wiki?curid=55013965", "text": "Robot Aviation\n\nRobot Aviation is an aerospace company headquartered in the Oslo area, Norway with development and production facilities in Warsaw, Poland and offices in Grand Forks in North Dakota and Phoenix in Arizona in the US.\n\nRobot Aviation designs, develops and manufactures complete unmanned aerial systems (UAS) for various industrial applications. The company's products include both unmanned helicopters and fixed-wing aircraft.\n\nRobot Aviation was founded in 2008 in Gjøvik in Oppland in Norway by Ole Vidar Homleid and Oddvar Kristiansen. Although the company's official address was at Gjøvik, development and production of the first SkyRobot prototype (FX450) started at Skotfoss in Skien, Telemark in cooperation with the company Aero-Design of Paweł Różański from Warsaw, Poland.\n\n", "id": "55013965", "title": "Robot Aviation"}
{"url": "https://en.wikipedia.org/wiki?curid=245926", "text": "Autonomous car\n\nAn autonomous car (also known as a driverless car, self-driving car, robotic car) and unmanned ground vehicle is a vehicle that is capable of sensing its environment and navigating without human input.\n\nAutonomous cars use a variety of techniques to detect their surroundings, such as radar, laser light, GPS, odometry and computer vision. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage. Autonomous cars must have control systems that are capable of analyzing sensory data to distinguish between different cars on the road.\n\nThe potential benefits of autonomous cars include reduced mobility and infrastructure costs, increased safety, increased mobility, increased customer satisfaction and reduced crime. Specifically a significant reduction in traffic collisions; the resulting injuries; and related costs, including less need for insurance. Autonomous cars are predicted to increase traffic flow; provide enhanced mobility for children, the elderly, disabled and the poor; relieve travelers from driving and navigation chores; lower fuel consumption; significantly reduce needs for parking space; reduce crime; and facilitate business models for transportation as a service, especially via the sharing economy. This shows the vast disruptive potential of the emerging technology. A frequently cited paper by Michael Osborne and Carl Benedikt Frey found that autonomous cars would make many jobs redundant.\n\nAmong the main obstacles to widespread adoption are technological challenges, disputes concerning liability; the time period needed to replace the existing stock of vehicles; resistance by individuals to forfeit control; consumer safety concerns; implementation of a workable legal framework and establishment of government regulations; risk of loss of privacy and security concerns, such as hackers or terrorism; concerns about the resulting loss of driving-related jobs in the road transport industry; and risk of increased suburbanization as travel becomes less costly and time-consuming. Many of these issues are due to the fact that autonomous objects, for the first time, allow computers to roam freely, with many related safety and security concerns.\n\nExperiments have been conducted on automating driving since at least the 1920s; promising trials took place in the 1950s. The first truly autonomous prototype cars appeared in the 1980s, with Carnegie Mellon University's Navlab and ALV projects in 1984 and Mercedes-Benz and Bundeswehr University Munich's EUREKA Prometheus Project in 1987. Since then, numerous companies and research organizations have developed prototypes. In 2015, the US states of Nevada, Florida, California, Virginia, and Michigan, together with Washington, D.C. allowed the testing of autonomous cars on public roads.\n\nIn 2017, Audi stated that its latest A8 would be autonomous at up to speeds of 60 km/h using its \"Audi AI\". The driver would not have to do safety checks such as frequently gripping the steering wheel. The Audi A8 was claimed to be the first production car to reach level 3 autonomous driving and Audi would be the first manufacturer to use laser scanners in addition to cameras and ultrasonic sensors for their system.\n\nIn November 2017, Waymo announced that it had begun testing driverless cars without a safety driver at the driver position, however; there is still an employee in the car.\n\n\"Autonomous\" means self-governance. Many historical projects related to vehicle autonomy have been \"automated\" (made to be \"automatic\") due to a heavy reliance on artificial hints in their environment, such as magnetic strips. Autonomous control implies satisfactory performance under significant uncertainties in the environment and the ability to compensate for system failures without external intervention.\n\nOne approach is to implement communication networks both in the immediate vicinity (for collision avoidance) and further away (for congestion management). Such outside influences in the decision process reduce an individual vehicle's autonomy, while still not requiring human intervention.\n\nWood et al. (2012) write \"This Article generally uses the term 'autonomous,' instead of the term 'automated.'\" The term \"autonomous\" was chosen \"because it is the term that is currently in more widespread use (and thus is more familiar to the general public). However, the latter term is arguably more accurate. 'Automated' connotes control or operation by a machine, while 'autonomous' connotes acting alone or independently. Most of the vehicle concepts (that we are currently aware of) have a person in the driver’s seat, utilize a communication connection to the Cloud or other vehicles, and do not independently select either destinations or routes for reaching them. Thus, the term 'automated' would more accurately describe these vehicle concepts\". As of 2017, most commercial projects focused on autonomous vehicles that did not communicate with other vehicles or an enveloping management regime.\n\nA classification system based on six different levels (ranging from fully manual to fully automated systems) was published in 2014 by SAE International, an automotive standardization body, as J3016, \"Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems\". This classification system is based on the amount of driver intervention and attentiveness required, rather than the vehicle capabilities, although these are very loosely related. In the United States in 2013, the National Highway Traffic Safety Administration (NHTSA) released a formal classification system, but abandoned this system in favor of the SAE standard in 2016. Also in 2016, SAE updated its classification, called J3016_201609.\n\nIn SAE's autonomy level definitions, \"driving mode\" means \"a type of driving scenario with characteristic dynamic driving task requirements (e.g., expressway merging, high speed cruising, low speed traffic jam, closed-campus operations, etc.)\"\nIn the formal SAE definition below, note in particular what happens in the shift from SAE 2 to from SAE 3: the human driver no longer has to monitor the environment. This is the final aspect of the ”dynamic driving task” that is now passed over from the human to the automated system. At SAE 3, the human driver still has the responsibility to intervene when asked to do so by the automated system. At SAE 4 the human driver is relieved of that responsibility and at SAE 5 the automated system will never need to ask for an intervention.\n\nModern self-driving cars generally use Bayesian Simultaneous localization and mapping (SLAM) algorithms, which fuse data from multiple sensors and an off-line map into current location estimates and map updates. SLAM with detection and tracking of other moving objects (DATMO), which also handles things such as cars and pedestrians, is a variant being developed at Google. Simpler systems may use roadside real-time locating system (RTLS) beacon systems to aid localisation. Typical sensors include lidar, stereo vision, GPS and IMU. Visual object recognition uses machine vision including neural networks. Udacity is developing an open-source software stack.\n\nAutonomous cars are being developed with deep learning, or neural networks. Deep neural networks have many computational stages, or levels in which neurons are simulated from the environment that activate the network . The neural network depends on an extensive amount of data extracted from real life driving scenarios. The neural network is activated and “learns” to perform the best course of action. Deep learning has been applied to answer to real life situations, and is used in the programming for autonomous cars. In addition, sensors, such as the LIDAR sensors already used in self-driving cars; cameras to detect the environment, and precise GPS navigation will be used in autonomous cars.\n\nTesting vehicles with varying degrees of autonomy can be done physically, in closed environments, on public roads (where permitted, typically with a license or permit or adhering to a specific set of operating principles) or virtually, i.e. in computer simulations.\n\nWhen driven on public roads, autonomous vehicles require a person to monitor their proper operation and \"take over\" when needed.\n\nSeveral companies are said to be testing autonomous technology in semi trucks. Otto, a self-driving trucking company that was acquired by Uber in August 2016, demoed their trucks on the highway before being acquired. In May 2017, San Francisco-based startup Embark announced a partnership with truck manufacturer Peterbilt to test and deploy autonomous technology in Peterbilt's vehicles. Google's Waymo has also said to be testing autonomous technology in trucks, however no timeline has been given for the project.\n\nIn Europe, cities in Belgium, France, Italy and the UK are planning to operate transport systems for autonomous cars, and Germany, the Netherlands, and Spain have allowed public testing in traffic. In 2015, the UK launched public trials of the LUTZ Pathfinder autonomous pod in Milton Keynes. Beginning in summer 2015 the French government allowed PSA Peugeot-Citroen to make trials in real conditions in the Paris area. The experiments were planned to be extended to other cities such as Bordeaux and Strasbourg by 2016. The alliance between French companies THALES and Valeo (provider of the first self-parking car system that equips Audi and Mercedes premi) is testing its own system. New Zealand is planning to use autonomous vehicles for public transport in Tauranga and Christchurch.\n\nTraffic collisions (and resulting deaths and injuries and costs), caused by human errors, such as delayed reaction time, tailgating, rubbernecking, and other forms of distracted or aggressive driving should be substantially reduced. Consulting firm McKinsey & Company estimated that widespread use of autonomous vehicles could \"eliminate 90% of all auto accidents in the United States, prevent up to in damages and health-costs annually and save thousands of lives.\"\n\nAutonomous cars could reduce labor costs; relieve travelers from driving and navigation chores, thereby replacing behind-the-wheel commuting hours with more time for leisure or work; and also would lift constraints on occupant ability to drive, distracted and texting while driving, intoxicated, prone to seizures, or otherwise impaired. For the young, the elderly, people with disabilities, and low-income citizens, autonomous cars could provide enhanced mobility. The removal of the steering wheel—along with the remaining driver interface and the requirement for any occupant to assume a forward-facing position—would give the interior of the cabin greater ergonomic flexibility. Large vehicles, such as motorhomes, would attain appreciably enhanced ease of use.\n\nAdditional advantages could include higher speed limits; smoother rides; and increased roadway capacity; and minimized traffic congestion, due to decreased need for safety gaps and higher speeds. Currently, maximum controlled-access highway throughput or capacity according to the U.S. Highway Capacity Manual is about 2,200 passenger vehicles per hour per lane, with about 5% of the available road space is taken up by cars. One study estimated that autonomous cars could increase capacity by 273% (~8,200 cars per hour per lane). The study also estimated that with 100% connected vehicles using vehicle-to-vehicle communication, capacity could reach 12,000 passenger vehicles per hour (up 445% from 2,200 pc/h per lane) traveling safely at with a following gap of about of each other. Currently, at highway speeds drivers keep between away from the car in front. These increases in highway capacity could have a significant impact in traffic congestion, particularly in urban areas, and even effectively end highway congestion in some places. The ability for authorities to manage traffic flow would increase, given the extra data and driving behavior predictability. combined with less need for traffic police and even road signage.\n\nSafer driving is expected to reduce the costs of vehicle insurance. Reduced traffic congestion and the improvements in traffic flow due to widespread use of autonomous cars will also translate into better fuel efficiency.\n\nBy reducing the (labor and other) cost of mobility as a service, autonomous cars could reduce the number of cars that are individually owned, replaced by taxi/pooling and other car sharing services. This could dramatically reduce the need for parking space, freeing scarce land for other uses. This would also dramatically reduce the size of the automotive production industry, with corresponding environmental and economic effects. Assuming the increased efficiency is not fully offset by increases in demand, more efficient traffic flow could free roadway space for other uses such as better support for pedestrians and cyclists.\n\nThe vehicles' increased awareness could aid the police by reporting on illegal passenger behavior, while possibly enabling other crimes, such as deliberately crashing into another vehicle or a pedestrian.\n\nIn spite of the various benefits to increased vehicle automation, some foreseeable challenges persist, such as disputes concerning liability, the time needed to turn the existing stock of vehicles from nonautonomous to autonomous, resistance by individuals to forfeit control of their cars, customer concern about the safety of driverless cars, and the implementation of legal framework and establishment of government regulations for self-driving cars. Other obstacles could be missing driver experience in potentially dangerous situations, ethical problems in situations where an autonomous car's software is forced during an unavoidable crash to choose between multiple harmful courses of action, and possibly insufficient Adaptation to Gestures and non-verbal cues by police and pedestrians.\n\nPossible technological obstacles for autonomous cars are:\n\nA direct impact of widespread adoption of autonomous vehicles is the loss of driving-related jobs in the road transport industry. There could be resistance from professional drivers and unions who are threatened by job losses. In addition, there could be job losses in public transit services and crash repair shops. The automobile insurance industry might suffer as the technology makes certain aspects of these occupations obsolete.\n\nPrivacy could be an issue when having the vehicle's location and position integrated into an interface in which other people have access to. In addition, there is the risk of automotive hacking through the sharing of information through V2V (Vehicle to Vehicle) and V2I (Vehicle to Infrastructure) protocols. There is also the risk of terrorist attacks. Self-driving cars could potentially be loaded with explosives and used as bombs.\n\nThe lack of stressful driving, more productive time during the trip, and the potential savings in travel time and cost could become an incentive to live far away from cities, where land is cheaper, and work in the city's core, thus increasing travel distances and inducing more urban sprawl, more fuel consumption and an increase in the carbon footprint of urban travel. There is also the risk that traffic congestion might increase, rather than decrease. Appropriate public policies and regulations, such as zoning, pricing, and urban design are required to avoid the negative impacts of increased suburbanization and longer distance travel.\n\nSome believe that once automation in vehicles reaches higher levels and becomes reliable, drivers will pay less attention to the road. Research shows that drivers in autonomous cars react later when they have to intervene in a critical situation, compared to if they were driving manually.\n\nEthical and moral reasoning come into consideration when programming the software that decides what action the car takes in an unavoidable crash; whether the autonomous car will crash into a bus, potentially killing people inside; or swerve elsewhere, potentially killing its own passengers or nearby pedestrians. A question that comes into play that programmers find difficult to answer is “what decision should the car make that causes the ‘smallest’ damage when it comes to people’s lives?” The ethics of autonomous vehicles is still in the process of being solved and could possibly lead to controversiality.\n\nIn 1999, Mercedes introduced Distronic, the first radar-assisted ACC, on the Mercedes-Benz S-Class (W220) and the CL-Class. The Distronic system was able to adjust the vehicle speed automatically to the car in front in order to always maintain a safe distance to other cars on the road.\n\nIn 2005, Mercedes refined the system (from this point called \"Distronic Plus\") with the Mercedes-Benz S-Class (W221) being the first car to receive the upgraded Distronic Plus system. Distronic Plus could now completely halt the car if necessary on E-Class and most Mercedes sedans. In an episode of \"Top Gear\", Jeremy Clarkson demonstrated the effectiveness of the cruise control system in the S-class by coming to a complete halt from motorway speeds to a round-about and getting out, without touching the pedals.\n\nBy 2017, Mercedes has vastly expanded its autonomous driving features on production cars: In addition to the standard Distronic Plus features such as an active brake assist, Mercedes now includes a steering pilot, a parking pilot, a cross-traffic assist system, night-vision cameras with automated danger warnings and braking assist (in case animals or pedestrians are on the road for example), and various other autonomous-driving features. In 2016, Mercedes also introduced its Active Brake Assist 4, which was the first emergency braking assistant with pedestrian recognition on the market.\n\nDue to Mercedes' history of gradually implementing advancements of their autonomous driving features that have been extensively tested, not many crashes that have been caused by it are known. One of the known crashes dates back to 2005, when German news magazine \"\"Stern\"\" was testing Mercedes' old Distronic system. During the test, the system did not always manage to brake in time. Ulrich Mellinghoff, then Head of Safety, NVH, and Testing at the Mercedes-Benz Technology Centre, stated that some of the tests failed due to the vehicle being tested in a metallic hall, which caused problems with the system's radar. Later iterations of the Distronic system have an upgraded radar and numerous other sensors, which are not susceptible to a metallic environment anymore. In 2008, Mercedes conducted a study comparing the crash rates of their vehicles equipped with Distronic Plus and the vehicles without it, and concluded that those equipped with Distronic Plus have an around 20% lower crash rate. In 2013, German Formula One driver Michael Schumacher was invited by Mercedes to try to crash a Mercedes C-Class vehicle, which was equipped with all safety features that Mercedes offered for its production vehicles at the time, which included the Active Blind Spot Assist, Active Lane Keeping Assist, Brake Assist Plus, Collision Prevention Assist, Distronic Plus with Steering Assist, Pre-Safe Brake, and Stop&Go Pilot. Due to the safety features, Schumacher was unable to crash the vehicle in realistic scenarios.\n\nIn midOctober 2015 Tesla Motors rolled out version 7 of their software in the U.S. that included Tesla Autopilot capability. On 9 January 2016, Tesla rolled out version 7.1 as an over-the-air update, adding a new \"summon\" feature that allows cars to self-park at parking locations without the driver in the car. Tesla's autonomous driving features can be classified as somewhere between level 2 and level 3 under the U.S. Department of Transportation’s National Highway Traffic Safety Administration (NHTSA) five levels of vehicle automation. At this level the car can act autonomously but requires the full attention of the driver, who must be prepared to take control at a moment's notice. Autopilot should be used only on limited-access highways, and sometimes it will fail to detect lane markings and disengage itself. In urban driving the system will not read traffic signals or obey stop signs. The system also does not detect pedestrians or cyclists.\n\nThe first fatal accident involving a vehicle being driven by itself took place in Williston, Florida on 7 May 2016 while a Tesla Model S electric car was engaged in Autopilot mode. The occupant was killed in a crash with an 18-wheel tractor-trailer. On 28 June 2016 the National Highway Traffic Safety Administration (NHTSA) opened a formal investigation into the accident working with the Florida Highway Patrol. According to the NHTSA, preliminary reports indicate the crash occurred when the tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway, and the car failed to apply the brakes. The car continued to travel after passing under the truck’s trailer. The NHTSA's preliminary evaluation was opened to examine the design and performance of any automated driving systems in use at the time of the crash, which involved a population of an estimated 25,000 Model S cars. On 8 July 2016, the NHTSA requested Tesla Motors provide the agency detailed information about the design, operation and testing of its Autopilot technology. The agency also requested details of all design changes and updates to Autopilot since its introduction, and Tesla's planned updates schedule for the next four months.\n\nAccording to Tesla, \"neither autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.\" The car attempted to drive full speed under the trailer, \"with the bottom of the trailer impacting the windshield of the Model S.\" Tesla also stated that this was Tesla’s first known autopilot death in over 130 million miles (208 million km) driven by its customers with Autopilot engaged. According to Tesla there is a fatality every 94 million miles (150 million km) among all type of vehicles in the U.S. However, this number also includes fatalities of the crashes, for instance, of motorcycle drivers with pedestrians.\n\nIn July 2016 the U.S. National Transportation Safety Board (NTSB) opened a formal investigation into the fatal accident while the Autopilot was engaged. The NTSB is an investigative body that has the power to make only policy recommendations. An agency spokesman said \"It's worth taking a look and seeing what we can learn from that event, so that as that automation is more widely introduced we can do it in the safest way possible.\". In January 2017, the NTSB released the report that concluded Tesla was not at fault; the investigation revealed that for Tesla cars, the crash rate dropped by 40 percent after Autopilot was installed.\n\nAccording to Tesla, starting 19 October 2016, all Tesla cars are built with hardware to allow full self-driving capability at the highest safety level (SAE Level 5). The hardware includes eight surround cameras and twelve ultrasonic sensors, in addition to the forward-facing radar with enhanced processing capabilities. The system will operate in \"shadow mode\" (processing without taking action) and send data back to Tesla to improve its abilities until the software is ready for deployment via over-the-air upgrades. After the required testing, Tesla hopes to enable full self-driving by the end of 2019 under certain conditions.\n\nIn August 2012, Alphabet (then Google) announced that their vehicles had completed over 300,000 autonomous-driving miles (500,000 km) accident-free, typically involving about a dozen cars on the road at any given time, and that they were starting to test with single drivers instead of in pairs. In late-May 2014, Alphabet revealed a new prototype that had no steering wheel, gas pedal, or brake pedal, and was fully autonomous. , Alphabet had test-driven their fleet in autonomous mode a total of . In December 2016, Alphabet Corporation announced that its technology would be spun-off to a new subsidiary called Waymo.\n\nBased on Alphabet's accident reports, their test cars have been involved in 14 collisions, of which other drivers were at fault 13 times, although in 2016 the car's software caused a crash.\n\nIn June 2015, Brin confirmed that 12 vehicles had suffered collisions as of that date. Eight involved rear-end collisions at a stop sign or traffic light, two in which the vehicle was side-swiped by another driver, one in which another driver rolled through a stop sign, and one where a Google employee was controlling the car manually. In July 2015, three Google employees suffered minor injuries when their vehicle was rear-ended by a car whose driver failed to brake at a traffic light. This was the first time that a collision resulted in injuries. On 14 February 2016 a Waymo vehicle attempted to avoid sandbags blocking its path. During the maneuver it struck a bus. Alphabet stated, \"In this case, we clearly bear some responsibility, because if our car hadn’t moved there wouldn’t have been a collision.\" Google characterized the crash as a misunderstanding and a learning experience.\n\nIn March 2017, an Uber test vehicle was involved in an accident in Arizona when another car failed to yield, flipping the Uber vehicle.\n\nIf fully autonomous cars become commercially available, they have the potential to be a disruptive innovation with major implications for society. The likelihood of widespread adoption is still unclear, but if they are used on a wide scale, policy makers face a number of unresolved questions about their effects.\n\nOne fundamental question is about their effect on travel behavior. Some people believe that they will increase car ownership and car use because it will become easier to use them and they will ultimately be more useful. This may in turn encourage urban sprawl and ultimately total private vehicle use. Others argue that it will be easier to share cars and that this will thus discourage outright ownership and decrease total usage, and make cars more efficient forms of transportation in relation to the present situation.\n\nPolicy-makers will have to take a new look at how infrastructure is to be built and how money will be allotted to build for autonomous vehicles. The need for traffic signals could potentially be reduced with the adoption of smart highways. Due to smart highways and with the assistance of smart technological advances implemented by policy change, the dependence on oil imports may be reduced because of less time being spent on the road by individual cars which could have an effect on policy regarding energy. On the other hand, autonomous vehicles could increase the overall number of cars on the road which could lead to a greater dependence on oil imports if smart systems are not enough to curtail the impact of more vehicles. However, due to the uncertainty of the future of autonomous vehicles, policy makers may want to plan effectively by implementing infrastructure improvements that can be beneficial to both human drivers and autonomous vehicles. Caution needs to be taken in acknowledgment to public transportation and that the use may be greatly reduced if autonomous vehicles are catered to through policy reform of infrastructure with this resulting in job loss and increased unemployment.\n\nOther disruptive effects will come from the use of autonomous vehicles to carry goods. s have the potential to make home deliveries significantly cheaper, transforming retail commerce and possibly making hypermarkets and supermarkets redundant. As of right now the U.S. Government defines automation into six levels, starting at level zero which means the human driver does everything and ending with level five, the automated system performs all the driving tasks. Also under the current law, manufacturers bear all the responsibility to self-certify vehicles for use on public roads. This means that currently as long as the vehicle is compliant within the regulatory framework, there are no specific federal legal barriers to a highly automated vehicle being offered for sale. Iyad Rahwan, an associate professor in the MIT Media lab said, \"Most people want to live in a world where cars will minimize casualties, but everyone wants their own car to protect them at all costs.\" Furthermore, industry standards and best practice are still needed in systems before they can be considered reasonably safe under real-world conditions.\n\nThe 1968 Vienna Convention on Road Traffic, subscribed to by over 70 countries worldwide, establishes principles to govern traffic laws. One of the fundamental prinicples of the Convention has been the concept that a driver is always fully in control and responsible for the behavior of a vehicle in traffic. The progress of technology that assists and takes over the functions of the driver is undermining this principle, implying that much of the groundwork must be rewritten.\nIn the United States, a non-signatory country to the Vienna Convention, state vehicle codes generally do not envisage — but do not necessarily prohibit — highly automated vehicles. To clarify the legal status of and otherwise regulate such vehicles, several states have enacted or are considering specific laws. In 2016, 7 states (Nevada, California, Florida, Michigan, Hawaii, Washington, and Tennessee), along with the District of Columbia, have enacted laws for autonomous vehicles. Incidents such as the first fatal accident by Tesla's Autopilot system have led to discussion about revising laws and standards for autonomous cars.\n\nIn September 2016, the US National Economic Council and Department of Transportation released federal standards that describe how automated vehicles should react if their technology fails, how to protect passenger privacy, and how riders should be protected in the event of an accident. The new federal guidelines are meant to avoid a patchwork of state laws, while avoiding being so overbearing as to stifle innovation.\n\nIn June 2011, the Nevada Legislature passed a law to authorize the use of autonomous cars. Nevada thus became the first jurisdiction in the world where autonomous vehicles might be legally operated on public roads. According to the law, the Nevada Department of Motor Vehicles (NDMV) is responsible for setting safety and performance standards and the agency is responsible for designating areas where autonomous cars may be tested. This legislation was supported by Google in an effort to legally conduct further testing of its Google driverless car. The Nevada law defines an autonomous vehicle to be \"a motor vehicle that uses artificial intelligence, sensors and global positioning system coordinates to drive itself without the active intervention of a human operator.\" The law also acknowledges that the operator will not need to pay attention while the car is operating itself. Google had further lobbied for an exemption from a ban on distracted driving to permit occupants to send text messages while sitting behind the wheel, but this did not become law. Furthermore, Nevada's regulations require a person behind the wheel and one in the passenger’s seat during tests.\n\nIn 2013, the government of the United Kingdom permitted the testing of autonomous cars on public roads. Before this, all testing of robotic vehicles in the UK had been conducted on private property.\n\nIn 2014 the Government of France announced that testing of autonomous cars on public roads would be allowed in 2015. 2000 km of road would be opened through the national territory, especially in Bordeaux, in Isère, Île-de-France and Strasbourg. At the 2015 ITS World Congress, a conference dedicated to intelligent transport systems, the very first demonstration of autonomous vehicles on open road in France was carried out in Bordeaux in early October 2015.\n\nIn 2015, a preemptive lawsuit against various automobile companies such as GM, Ford, and Toyota accused them of \"Hawking vehicles that are vulnerable to hackers who could hypothetically wrest control of essential functions such as brakes and steering.\"\n\nIn spring of 2015, the Federal Department of Environment, Transport, Energy and Communications in Switzerland (UVEK) allowed Swisscom to test a driverless Volkswagen Passat on the streets of Zurich.\n\nOn 19 February 2016, Assembly Bill No. 2866 was introduced in California that would allow completely autonomous vehicles to operate on the road, including those without a driver, steering wheel, accelerator pedal, or brake pedal. The Bill states the Department of Motor Vehicles would need to comply with these regulations by 1 July 2018 for these rules to take effect. This bill has yet to pass the house of origin.\n\nIn 2016, the Singapore Land Transit Authority in partnership with UK automotive supplier Delphi Automotive Plc will launch preparations for a test run of a fleet of automated taxis for an on-demand autonomous cab service to take effect in 2017.\n\nIn September 2016, the U.S. Department of Transportation released its Federal Automated Vehicles Policy, and California published discussions on the subject in October 2016.\n\nIn December 2016, the California Department of Motor Vehicles ordered Uber to remove its self-driving vehicles from the road in response to two red-light violations. Uber immediately blamed the violations on \"human-error\", and has suspended the drivers.\n\nIndividual vehicles may benefit from information obtained from other vehicles in the vicinity, especially information relating to traffic congestion and safety hazards. Vehicular communication systems use vehicles and roadside units as the communicating nodes in a peer-to-peer network, providing each other with information. As a cooperative approach, vehicular communication systems can allow all cooperating vehicles to be more effective. According to a 2010 study by the National Highway Traffic Safety Administration, vehicular communication systems could help avoid up to 79 percent of all traffic accidents.\n\nIn 2012, computer scientists at the University of Texas in Austin began developing smart intersections designed for autonomous cars. The intersections will have no traffic lights and no stop signs, instead using computer programs that will communicate directly with each car on the road.\n\nAmong connected cars, an unconnected one is the weakest link and will be increasingly banned from busy high-speed roads, predicted a Helsinki think tank in January 2016.\n\nIn a 2011 online survey of 2,006 US and UK consumers by Accenture, 49% said they would be comfortable using a \"driverless car\".\n\nA 2012 survey of 17,400 vehicle owners by J.D. Power and Associates found 37% initially said they would be interested in purchasing a fully autonomous car. However, that figure dropped to 20% if told the technology would cost $3,000 more.\n\nIn a 2012 survey of about 1,000 German drivers by automotive researcher Puls, 22% of the respondents had a positive attitude towards these cars, 10% were undecided, 44% were skeptical and 24% were hostile.\n\nA 2013 survey of 1,500 consumers across 10 countries by Cisco Systems found 57% \"stated they would be likely to ride in a car controlled entirely by technology that does not require a human driver\", with Brazil, India and China the most willing to trust autonomous technology.\n\nIn a 2014 US telephone survey by Insurance.com, over three-quarters of licensed drivers said they would at least consider buying a self-driving car, rising to 86% if car insurance were cheaper. 31.7% said they would not continue to drive once an autonomous car was available instead.\n\nIn a February 2015 survey of top auto journalists, 46% predict that either Tesla or Daimler will be the first to the market with a fully autonomous vehicle, while (at 38%) Daimler is predicted to be the most functional, safe, and in-demand autonomous vehicle.\n\nIn 2015 a questionnaire survey by Delft University of Technology explored the opinion of 5,000 people from 109 countries on automated driving. Results showed that respondents, on average, found manual driving the most enjoyable mode of driving. 22% of the respondents did not want to spend any money for a fully automated driving system. Respondents were found to be most concerned about software hacking/misuse, and were also concerned about legal issues and safety. Finally, respondents from more developed countries (in terms of lower accident statistics, higher education, and higher income) were less comfortable with their vehicle transmitting data. The survey also gave results on potential consumer opinion on interest of purchasing an automated car, stating that 37% of surveyed current owners were either \"definitely\" or \"probably\" interested in purchasing an automated car.\n\nIn 2016, a survey in Germany examined the opinion of 1,603 people, who were representative in terms of age, gender, and education for the German population, towards partially, highly, and fully automated cars. Results showed that men and women differ in their willingness to use them. Men felt less anxiety and more joy towards automated cars, whereas women showed the exact opposite. The gender difference towards anxiety was especially pronounced between young men and women but decreased with participants’ age.\n\nIn 2016, a PwC survey, in the United States, showing the opinion of 1,584 people, highlights that \"66 percent of respondents said they think autonomous cars are probably smarter than the average human driver\". People are still worried about safety and mostly the fact of having the car hacked. Nevertheless, only 13% of the interviewees see no advantages in this new kind of cars.\n\nWith the emergence of autonomous cars, there are various ethical issues arising. While morally, the introduction of autonomous vehicles to the mass market seems inevitable due to a reduction of crashes by up to 90% and their accessibility to disabled, elderly, and young passengers, there still remain some ethical issues that have not yet been fully solved. Those include, but are not limited to: the moral, financial, and criminal responsibility for crashes, the decisions a car is to make right before a (fatal) crash, privacy issues, and potential job loss.\n\nThere are different opinions on who should be held liable in case of a crash, in particular with people being hurt. Many experts see the car manufacturers themselves responsible for those crashes that occur due to a technical malfunction or misconstruction. Besides the fact that the car manufacturer would be the source of the problem in a situation where a car crashes due to a technical issue, there is another important reason why car manufacturers could be held responsible: it would encourage them to innovate and heavily invest into fixing those issues, not only due to protection of the brand image, but also due to financial and criminal consequences. However, there are also voices that argue those using or owning the vehicle should be held responsible since they know the risks involved in using such a vehicle. Experts suggest introducing a tax or insurances that would protect owners and users of autonomous vehicles of claims made by victims of an accident. Other possible parties that can be held responsible in case of a technical failure include software engineers that programmed the code for the autonomous operation of the vehicles, and suppliers of components of the AV.\n\nTaking aside the question of legal liability and moral responsibility, the question arises how autonomous vehicles should be programmed to behave in an emergency situation where either passengers or other traffic participants are endangered. A very visual example of the moral dilemma that a software engineer or car manufacturer might face in programming the operating software is described in an ethical thought experiment, the trolley problem: a conductor of a trolley has the choice of staying on the planned track and running over 5 people, or turn the trolley onto a track where it would kill only one person, assuming there is no traffic on it. There are two main considerations that need to be addressed. First, what moral basis would be used by an autonomous vehicle to make decisions? Second, how could those be translated into software code? Researchers have suggested, in particular, two ethical theories to be applicable to the behavior of autonomous vehicles in cases of emergency: deontology and utilitarianism. Asimov’s three laws of robotics are a typical example of deontological ethics. The theory suggests that an autonomous car needs to follow strict written-out rules that it needs to follow in any situation. Utilitarianism suggests the idea that any decision must be made based on the goal to maximize utility. This needs a definition of utility which could be maximizing the number of people surviving in a crash. Critics suggest that autonomous vehicles should adapt a mix of multiple theories to be able to respond morally right in the instance of a crash.\n\nPrivacy-related issues arise mainly from the interconnectivity of autonomous cars, making it just another mobile device that can gather any information about an individual. This information gathering ranges from tracking of the routes taken, voice recording, video recording, preferences in media that is consumed in the car, behavioral patterns, to many more streams of information.\n\nThe implementation of autonomous vehicles to the mass market might cost up to 5 million jobs in the US alone, making up almost 3% of the workforce. Those jobs include drivers of taxis, buses, vans, trucks, and e-hailing vehicles. Many industries, such as the auto insurance industry are indirectly affected. This industry alone generates an annual revenue of about $220 billions, supporting 277,000 jobs. To put this into perspective – this is about the number of mechanical engineering jobs. The potential loss of a majority of those jobs due to an estimated decline of accidents by up to 90% will have a tremendous impact on those individuals involved. Both India and China have placed bans on automated cars with the former citing protection of jobs.\n\n\n\nIntelligent or self-driving cars are a common theme in science fiction literature. Examples include:\n\n\n\n\n", "id": "245926", "title": "Autonomous car"}
{"url": "https://en.wikipedia.org/wiki?curid=1006293", "text": "Biorobotics\n\nBiorobotics is a term that loosely covers the fields of cybernetics, bionics and even genetic engineering as a collective study.\n\nBiorobotics is often used to refer to a real subfield of robotics: studying how to make robots that emulate or simulate living biological organisms mechanically or even chemically. \n\nThe term is also used in a reverse definition: making biological organisms as manipulatable and functional as robots, or making biological organisms as components of robots. In the latter sense, biorobotics can be referred to as a theoretical discipline of comprehensive genetic engineering in which organisms are created and designed by artificial means. The creation of life from non-living matter for example, would be biorobotics. The field is in its infancy and is sometimes known as synthetic biology or bionanotechnology.\n\nBio-inspired robotics is the practice of making robots that are inspired by real biological systems, while being simpler and more effective. In contrast, the resemblance of animatronics to biological organisms is usually only in general shape and form.\n\nOrel V.E. invented the device of mechanochemiemission microbiorobotics. The phenomenon of mechanochemiemission is related to the processes interconversion of mechanical, chemical, electromagnetic energy in the mitochondria. Microbiorobot may be used for treatment of cancer patients.\n\nA biological brain, grown from cultured neurons which were originally separated, has been developed as the neurological entity subsequently embodied within a robot body by Kevin Warwick and his team at University of Reading. The brain receives input from sensors on the robot body and the resultant output from the brain provides the robot's only motor signals. The biological brain is the only brain of the robot.\n\n", "id": "1006293", "title": "Biorobotics"}
{"url": "https://en.wikipedia.org/wiki?curid=58900", "text": "Unmanned aerial vehicle\n\nAn unmanned aerial vehicle (UAV), commonly known as a drone, is an aircraft without a human pilot aboard. UAVs are a component of an unmanned aircraft system (UAS); which include a UAV, a ground-based controller, and a system of communications between the two. The flight of UAVs may operate with various degrees of autonomy: either under remote control by a human operator or autonomously by onboard computers.<ref name=\"ICAO's circular 328 AN/190\"></ref>\n\nCompared to manned aircraft, UAVs were originally used for missions too \"dull, dirty or dangerous\" for humans. While they originated mostly in military applications, their use is rapidly expanding to commercial, scientific, recreational, agricultural, and other applications, such as policing, peacekeeping, and surveillance, product deliveries, aerial photography, agriculture, smuggling, and drone racing. Civilian UAVs now vastly outnumber military UAVs, with estimates of over a million sold by 2015, so they can be seen as an early commercial application of autonomous things, to be followed by the autonomous car and home robots.\n\nMultiple terms are used for unmanned aerial vehicles, which generally refer to the same concept.\n\nThe term drone, more widely used by the public, was coined in reference to the early remotely-flown target aircraft used for practice firing of a battleship's guns, and the term was first used with the 1920's Fairey Queen and 1930's de Havilland Queen Bee target aircraft. These two were followed in service by the similarly-named Airspeed Queen Wasp and Miles Queen Martinet, before ultimate replacement by the GAF Jindivik.\n\nThe term \"unmanned aircraft system\" (UAS) was adopted by the United States Department of Defense (DoD) and the United States Federal Aviation Administration in 2005 according to their Unmanned Aircraft System Roadmap 2005–2030. The International Civil Aviation Organization (ICAO) and the British Civil Aviation Authority adopted this term, also used in the European Union's Single-European-Sky (SES) Air-Traffic-Management (ATM) Research (SESAR Joint Undertaking) roadmap for 2020. This term emphasizes the importance of elements other than the aircraft. It includes elements such as ground control stations, data links and other support equipment. A similar term is an \"unmanned-aircraft vehicle system\" (UAVS) \"remotely piloted aerial vehicle\" (RPAV), \"remotely piloted aircraft system\" (RPAS). Many similar terms are in use.\n\nA UAV is defined as a \"powered, aerial vehicle that does not carry a human operator, uses aerodynamic forces to provide vehicle lift, can fly autonomously or be piloted remotely, can be expendable or recoverable, and can carry a lethal or nonlethal payload\". Therefore, missiles are not considered UAVs because the vehicle itself is a weapon that is not reused, though it is also unmanned and in some cases remotely guided.\n\nThe relation of UAVs to remote controlled model aircraft is unclear. UAVs may or may not include model aircraft. Some jurisdictions base their definition on size or weight, however, the US Federal Aviation Administration defines any unmanned flying craft as a UAV regardless of size. For recreational uses, a drone (as apposed to a UAV) is a model aircraft that has first person video, autonomous capabilities or both.\n\nIn 1849 Austria sent unmanned, bomb-filled balloons to attack Venice. UAV innovations started in the early 1900s and originally focused on providing practice targets for training military personnel.\n\nUAV development continued during World War I, when the Dayton-Wright Airplane Company invented a pilotless aerial torpedo that would explode at a preset time.\n\nThe earliest attempt at a powered UAV was A. M. Low's \"Aerial Target\" in 1916. Nikola Tesla described a fleet of unmanned aerial combat vehicles in 1915. Advances followed during and after World War I, including the Hewitt-Sperry Automatic Airplane. The first scaled remote piloted vehicle was developed by film star and model-airplane enthusiast Reginald Denny in 1935. More emerged during World War II – used both to train antiaircraft gunners and to fly attack missions. Nazi Germany produced and used various UAV aircraft during the war. Jet engines entered service after World War II in vehicles such as the Australian GAF Jindivik, and Teledyne Ryan Firebee I of 1951, while companies like Beechcraft offered their Model 1001 for the U.S. Navy in 1955. Nevertheless, they were little more than remote-controlled airplanes until the Vietnam War.\n\nIn 1959, the U.S. Air Force, concerned about losing pilots over hostile territory, began planning for the use of unmanned aircraft. Planning intensified after the Soviet Union shot down a U-2 in 1960. Within days, a highly classified UAV program started under the code name of \"Red Wagon\". The August 1964 clash in the Tonkin Gulf between naval units of the U.S. and North Vietnamese Navy initiated America's highly classified UAVs (Ryan Model 147, Ryan AQM-91 Firefly, Lockheed D-21) into their first combat missions of the Vietnam War. When the Chinese government showed photographs of downed U.S. UAVs via \"Wide World Photos\", the official U.S. response was \"no comment\".\n\nThe War of Attrition (1967–1970) featured the introduction of UAVs with reconnaissance cameras into combat in the Middle East.\n\nIn the 1973 Yom Kippur War Israel used UAVs as decoys to spur opposing forces into wasting expensive anti-aircraft missiles.\n\nIn 1973 the U.S. military officially confirmed that they had been using UAVs in Southeast Asia (Vietnam). Over 5,000 U.S. airmen had been killed and over 1,000 more were missing or captured. The USAF 100th Strategic Reconnaissance Wing flew about 3,435 UAV missions during the war at a cost of about 554 UAVs lost to all causes. In the words of USAF General George S. Brown, Commander, Air Force Systems Command, in 1972, \"The only reason we need (UAVs) is that we don't want to needlessly expend the man in the cockpit.\" Later that year, General John C. Meyer, Commander in Chief, Strategic Air Command, stated, \"we let the drone do the high-risk flying ... the loss rate is high, but we are willing to risk more of them ... they save lives!\"\n\nDuring the 1973 Yom Kippur War, Soviet-supplied surface-to-air missile batteries in Egypt and Syria caused heavy damage to Israeli fighter jets. As a result, Israel developed the first UAV with real-time surveillance. The images and radar decoys provided by these UAVs helped Israel to completely neutralize the Syrian air defenses at the start of the 1982 Lebanon War, resulting in no pilots downed. The first time UAVs were used as proof-of-concept of super-agility post-stall controlled flight in combat-flight simulations involved tailless, stealth technology-based, three-dimensional thrust vectoring flight control, jet-steering UAVs in Israel in 1987.\n\nWith the maturing and miniaturization of applicable technologies in the 1980s and 1990s, interest in UAVs grew within the higher echelons of the U.S. military. In the 1990s, the U.S. DoD gave a contract to AAI Corporation along with Israeli company Malat. The U.S. Navy bought the AAI Pioneer UAV that AAI and Malat developed jointly. Many of these UAVs saw service in the 1991 Gulf War. UAVs demonstrated the possibility of cheaper, more capable fighting machines, deployable without risk to aircrews. Initial generations primarily involved surveillance aircraft, but some carried armaments, such as the General Atomics MQ-1 Predator, that launched AGM-114 Hellfire air-to-ground missiles.\n\nCAPECON was a European Union project to develop UAVs, running from 1 May 2002 to 31 December 2005.\n\nAs of 2012, the USAF employed 7,494 UAVs – almost one in three USAF aircraft. The Central Intelligence Agency also operated UAVs.\n\nIn 2013 at least 50 countries used UAVs. China, Iran, Israel and others designed and built their own varieties.\n\nUAVs typically fall into one of six functional categories (although multi-role airframe platforms are becoming more prevalent):\nThe U.S. Military UAV tier system is used by military planners to designate the various individual aircraft elements in an overall usage plan.\n\nVehicles can be categorised in terms of range/altitude. The following has been advanced as relevant at industry events such as ParcAberporth Unmanned Systems forum:\nOther categories include:\nClassifications according to aircraft weight are quite simpler:\n\nManned and unmanned aircraft of the same type generally have recognizably similar physical components. The main exceptions are the cockpit and environmental control system or life support systems. Some UAVs carry payloads (such as a camera) that weigh considerably less than an adult human, and as a result can be considerably smaller. Though they carry heavy payloads, weaponized military UAVs are lighter than their manned counterparts with comparable armaments.\n\nSmall civilian UAVs have no life-critical systems, and can thus be built out of lighter but less sturdy materials and shapes, and can use less robustly tested electronic control systems. For small UAVs, the quadcopter design has become popular, though this layout is rarely used for manned aircraft. Miniaturization means that less-powerful propulsion technologies can be used that are not feasible for manned aircraft, such as small electric motors and batteries.\n\nControl systems for UAVs are often different than manned craft. For remote human control, a camera and video link almost always replace the cockpit windows; radio-transmitted digital commands replace physical cockpit controls. Autopilot software is used on both manned and unmanned aircraft, with varying feature sets.\n\nThe primary difference for planes is the absence of the cockpit area and its windows. Tailless quadcopters are a common form factor for rotary wing UAVs while tailed mono- and bi-copters are common for manned platforms.\n\nSmall UAVs mostly use lithium-polymer batteries (Li-Po), while larger vehicles rely on conventional airplane engines.\n\nBattery elimination circuitry (BEC) is used to centralize power distribution and often harbors a microcontroller unit (MCU). Costlier switching BECs diminish heating on the platform.\n\nUAV computing capability followed the advances of computing technology, beginning with analog controls and evolving into microcontrollers, then system-on-a-chip (SOC) and single-board computers (SBC).\n\nSystem hardware for small UAVs is often called the Flight Controller (FC), Flight Controller Board (FCB) or Autopilot.\n\nPosition and movement sensors give information about the aircraft state. Exteroceptive sensors deal with external information like distance measurements, while exproprioceptive ones correlate internal and external states.\n\nNon-cooperative sensors are able to detect targets autonomously so they are used for separation assurance and collision avoidance.\n\nDegrees of freedom (DOF) refer to both the amount and quality of sensors on-board: 6 DOF implies 3-axis gyroscopes and accelerometers (a typical inertial measurement unit – IMU), 9 DOF refers to an IMU plus a compass, 10 DOF adds a barometer and 11 DOF usually adds a GPS receiver.\n\nUAV actuators include digital electronic speed controllers (which control the RPM of the motors) linked to motors/engines and propellers, servomotors (for planes and helicopters mostly), weapons, payload actuators, LEDs and speakers.\n\nUAV software called the flight stack or autopilot. UAVs are real-time systems that require rapid response to changing sensor data. Examples include Raspberry Pis, Beagleboards, etc. shielded with NavIO, PXFMini, etc. or designed from scratch such as Nuttx, preemptive-RT Linux, Xenomai, Orocos-Robot Operating System or DDS-ROS 2.0.\n\nList of civil-use open-source stacks include:\n\nUAVs employ open-loop, closed-loop or hybrid control architectures.\n\nFlight control is one of the lower-layer system and is similar to manned aviation: plane flight dynamics, control and automation, helicopter flight dynamics and controls and multirotor flight dynamics were researched long before the rise of UAVs.\n\nAutomatic flight involves multiple levels of priority.\n\nUAVs can be programmed to perform aggressive manœuvres or landing/perching on inclined surfaces, and then to climb toward better communication spots. Some UAVs can control flight with varying flight modelisation, such as VTOL designs.\n\nUAVs can also implement perching on a flat vertical surface.\n\nMost UAVs use a radio frequency front-end that connects the antenna to the analog-to-digital converter and a flight computer that controls avionics (and that may be capable of autonomous or semi-autonomous operation).\n\nRadio allows remote control and exchange of video and other data. Early UAVs had only uplink. Downlinks (e.g., realtime video) came later.\n\nIn military systems and high-end domestic applications, downlink may convey payload management status. In civilian applications, most transmissions are commands from operator to vehicle. Downstream is mainly video. Telemetry is another kind of downstream link, transmitting status about the aircraft systems to the remote operator. UAVs use also satellite \"uplink\" to access satellite navigation systems.\n\nThe radio signal from the operator side can be issued from either:\n\nICAO classifies unmanned aircraft as either remotely piloted aircraft or fully autonomous. Actual UAVs may offer intermediate degrees of autonomy. E.g., a vehicle that is remotely piloted in most contexts may have an autonomous return-to-base operation.\n\nBasic autonomy comes from proprioceptive sensors. Advanced autonomy calls for situational awareness, knowledge about the environment surrounding the aircraft from exterioceptive sensors: sensor fusion integrates information from multiple sensors.\n\nOne way to achieve autonomous control employs multiple control-loop layers, as in hierarchical control systems. As of 2016 the low-layer loops (i.e. for flight control) tick as fast as 32,000 times per second, while higher-level loops may cycle once per second. The principle is to decompose the aircraft's behavior into manageable \"chunks\", or states, with known transitions. Hierarchical control system types range from simple scripts to finite state machines, behavior trees and hierarchical task planners. The most common control mechanism used in these layers is the PID controller which can be used to achieve hover for a quadcopter by using data from the IMU to calculate precise inputs for the electronic speed controllers and motors.\n\nExamples of mid-layer algorithms:\nEvolved UAV hierarchical task planners use methods like state tree searches or genetic algorithms.\n\nUAV manufacturers often build in specific autonomous operations, such as:\n\nFull autonomy is available for specific tasks, such as airborne refueling or ground-based battery switching; but higher-level tasks call for greater computing, sensing and actuating capabilities. One approach to quantifying autonomous capabilities is based on OODA terminology, as suggested by a 2002 US Air Force Research Laboratory, and used in the table below:\n\nReactive autonomy, such as collective flight, real-time collision avoidance, wall following and corridor centring, relies on telecommunication and situational awareness provided by range sensors: optic flow, lidars (light radars), radars, sonars.\n\nMost range sensors analyze electromagnetic radiation, reflected off the environment and coming to the sensor. The cameras (for visual flow) act as simple receivers. Lidars, radars and sonars (with sound mechanical waves) emit and receive waves, measuring the round-trip transit time. UAV cameras do not require emitting power, reducing total consumption.\n\nRadars and sonars are mostly used for military applications.\n\nReactive autonomy has in some forms already reached consumer markets: it may be widely available in less than a decade.\n\nSLAM combines odometry and external data to represent the world and the position of the UAV in it in three dimensions. High-altitude outdoor navigation does not require large vertical fields-of-view and can rely on GPS coordinates (which makes it simple mapping rather than SLAM).\n\nTwo related research fields are photogrammetry and LIDAR, especially in low-altitude and indoor 3D environments.\n\nRobot swarming refers to networks of agents able to dynamically reconfigure as elements leave or enter the network. They provide greater flexibility than multi-agent cooperation. Swarming may open the path to data fusion. Some bio-inspired flight swarms use steering behaviors and flocking.\n\nIn the military sector, American Predators and Reapers are made for counterterrorism operations and in war zones in which the enemy lacks sufficient firepower to shoot them down. They are not designed to withstand antiaircraft defenses or air-to-air combat. In September 2013, the chief of the US Air Combat Command stated that current UAVs were \"useless in a contested environment\" unless manned aircraft were there to protect them.[167] A 2012 Congressional Research Service (CRS) report speculated that in the future, UAVs may be able to perform tasks beyond intelligence, surveillance, reconnaissance and strikes; the CRS report listed air-to-air combat (\"a more difficult future task\") as possible future undertakings.[168] The Department of Defense's Unmanned Systems Integrated Roadmap FY2013-2038 foresees a more important place for UAVs in combat.[169] Issues include extended capabilities, human-UAV interaction, managing increased information flux, increased autonomy and developing UAV-specific munitions.[169] DARPA's project of systems of systems, or General Atomics work may augur future warfare scenarios, the latter disclosing Avenger swarms equipped with High Energy Liquid Laser Area Defense System (HELLADS).\n\nCognitive radio technology may have UAV applications.\n\nUAVs may exploit distributed neural networks.\n\nThe UAV global military market is dominated by pioneers United States and Israel. The US held a 60% military-market share in 2006. It operated over 9,000 UAVs in 2014. From 1985 to 2014, exported UAVs came predominantly from Israel (60.7%) and the United States (23.9%); top importers were The United Kingdom (33.9%) and India (13.2%). Northrop Grumman and General Atomics are the dominant manufacturers on the strength of the Global Hawk and Predator/Mariner systems.\n\nThe leading civil UAV companies are currently (Chinese) DJI with $500m global sales, (French) Parrot with $110m and (US) 3DRobotics with $21.6m in 2014. As of March 2017, more than 770,000 civilian UAVs were registered with the U.S. FAA, though it is estimated more than 1.1 million have been sold in the United States alone.\n\nUAV companies are also emerging in developing nations such as India for civilian use, although it is at a very nascent stage, a few early stage startups have received support and funding.\n\nSome universities offer research and training programs or degrees. Private entities also provide online and in-person training programs for both recreational and commercial UAV use.\n\nFlapping-wing ornithopters, imitating birds or insects, are a research field in microUAVs. Their inherent stealth recommends them for spy missions.\n\nThe Nano Hummingbird is commercially available, while sub-1g microUAVs inspired by flies, albeit using a power tether, can \"land\" on vertical surfaces.\n\nOther projects include unmanned \"beetles\" and other insects.\n\nResearch is exploring miniature optic-flow sensors, called ocellis, mimicking the compound insect eyes formed from multiple facets, which can transmit data to neuromorphic chips able to treat optic flow as well as light intensity discrepancies.\n\nUAV endurance is not constrained by the physiological capabilities of a human pilot.\n\nBecause of their small size, low weight, low vibration and high power to weight ratio, Wankel rotary engines are used in many large UAVs. Their engine rotors cannot seize; the engine is not susceptible to shock-cooling during descent and it does not require an enriched fuel mixture for cooling at high power. These attributes reduce fuel usage, increasing range or payload.\n\nHydrogen fuel cells, using hydrogen power, may be able to extend the endurance of small UAVs, up to several hours.\n\nMicro air vehicles endurance is so far best achieved with flapping-wing UAVs, followed by planes and multirotors standing last, due to lower Reynolds number.\n\nSolar-electric UAVs, a concept originally championed by the AstroFlight Sunrise in 1974, have achieved flight times of several weeks.\n\nSolar-powered atmospheric satellites (\"atmosats\") designed for operating at altitudes exceeding 20 km (12 miles, or 60,000 feet) for as long as five years could potentially perform duties more economically and with more versatility than low earth orbit satellites. Likely applications include weather monitoring, disaster recovery, earth imaging and communications.\n\nElectric UAVs powered by microwave power transmission or laser power beaming are other potential endurance solutions.\n\nAnother application for a high endurance UAV would be to \"stare\" at a battlefield for a long interval (ARGUS-IS, Gorgon Stare, Integrated Sensor Is Structure) to record events that could then be played backwards to track battlefield activities.\n\nReliability improvements target all aspects of UAV systems, using resilience engineering and fault tolerance techniques.\n\nIndividual reliability covers robustness of flight controllers, to ensure safety without excessive redundancy to minimize cost and weight. Besides, dynamic assessment of flight envelope allows damage-resilient UAVs, using non-linear analysis with ad-hoc designed loops or neural networks. UAV software liability is bending toward the design and certifications of manned avionics software.\n\nSwarm resilience involves maintaining operational capabilities and reconfiguring tasks given unita failures.\n\nThere are numerous civilian, commercial, military, and aerospace applications for UAVs. These include:\n\n\nUAVs are being developed and deployed by many countries around the world. Due to their wide proliferation, no comprehensive list of UAV systems exists.\n\nThe export of UAVs or technology capable of carrying a 500 kg payload at least 300 km is restricted in many countries by the Missile Technology Control Regime.\n\nUAVs can threaten airspace security in numerous ways, including unintentional collisions or other interference with other aircraft, deliberate attacks or by distracting pilots or flight controllers. The first incident of a drone-airplane collision occurred in mid-October 2017 in Quebec City, Canada.\n\nUAVs could be loaded with dangerous payloads, and crashed into vulnerable targets. Payloads could include explosives, chemical, radiologial or biological hazards. UAVs with generally non-lethal payloads could possibly be hacked and put to malicious purposes. Anti-UAV systems are being developed by states to counter this threat. This is, however, proving difficult. As Dr J. Rogers stated in an interview to A&T \"There is a big debate out there at the moment about what the best way is to counter these small UAVs, whether they are used by hobbyists causing a bit of a nuisance or in a more sinister manner by a terrorist actor.”\n\nBy 2017, drones were being used to drop contraband into prisons.\n\nThe interest in UAVs cyber security has been raised greatly after the Predator UAV video stream hijacking incident in 2009, where Islamic militants used cheap, off-the-shelf equipment to stream video feeds from a UAV. Another risk is the possibility of hijacking or jamming a UAV in flight. In recent years several security researchers have made public vulnerabilities for commercial UAVs, in some cases even providing full source code or tools to reproduce their attacks. At a workshop on UAVs and privacy in October 2016, researchers from the Federal Trade Commission showed they were able to hack into three different consumer quadcopters and noted that UAV manufacturers can make their UAVs more secure by the basic security measures of encrypting the Wi-Fi signal and adding password protection.\n\nIn the United States, flying close to a wildfire is punishable by a maximum $25,000 fine. Nonetheless, in 2014 and 2015, firefighting air support in California was hindered on several occasions, including at the Lake Fire and the North Fire. In response, California legislators introduced a bill that would allow firefighters to disable UAVs which invaded restricted airspace. The FAA later required registration of most UAVs.\n\nThe use of UAVs is also being investigated to help detect and fight wildfires, whether through observation or launching pyrotechnic devices to start backfires.\n\nEthical concerns and UAV-related accidents have driven nations to regulate the use of UAVs.\n\nThe Irish Aviation Authority (IAA) requires all UAVs over 1 kg must be registered with UAVs weighing 4 kg or more requiring a license to be issued by the IAA.\n\n, the Dutch police is testing trained bald eagles to intercept offending UAVs.\n\nIn 2016 Transport Canada proposed the implementation of new regulations that would require all UAVs over 250 grams to be registered and insured and that operators would be required to be a minimum age and pass an exam in order to get a license. These regulations are expected to be introduced in 2018. (http://www.gazette.gc.ca/rp-pr/p1/2017/2017-07-15/html/reg2-eng.php)\n\nIn April 2014, the South African Civil Aviation Authority announced that it would clamp down on the illegal flying of UAVs in South African airspace. \"Hobby drones\" with a weight of less than 7 kg at altitudes up to 500m with restricted visual line-of-sight below the height of the highest obstacle within 300m of the UAV are allowed. No license is required for such vehicles.\n\nThe ENAC (Ente Nazionale per l'Aviazione Civile), that is, the Italian Civil Aviation Authority for technical regulation, certification, supervision and control in the field of civil aviation, issued on May 31, 2016 a very detailed regulation for all UAV, determining which types of vehicles can be used, where, for which purposes, and who can control them. The regulation deals with the usage of UAV for either commercial and recreational use. Last version was published on December 22, 2016.\n\nFrom 21 December 2015 all hobby type UAV's between 250 grams and 25 kilograms needed to be registered with FAA no later than 19 February 2016.\n\nThe new FAA UAV registration process includes requirements for:\n\nOn May 19, 2017, in the case \"Taylor v. Huerta\", the U.S. Court of Appeals for the District of Columbia Circuit held that that the FAA's 2015 drone registration rules were in violation of the 2012 FAA Modernization and Reform Act. Under the court's holding, although commercial drone operators are required to register, recreational operators are not. On May 25, 2017, one week after the \"Taylor\" decision, Senator Diane Feinstein introduced S. 1272, the Drone Federalism Act of 2017, in Congress.\n\nOn 21 June 2016 the Federal Aviation Administration announced regulations for commercial operation of small UAS craft (sUAS), those between 0.55 and 55 pounds (about 250 gm to 25 kg) including payload. The rules, which exclude hobbyists, require the presence at all operations of a licensed Remote Pilot in Command. Certification of this position, available to any citizen at least 16 years of age, is obtained solely by passing a written test and then submitting an application. For those holding a sport pilot license or higher, and with a current flight review, a rule-specific exam can be taken at no charge online at the faasafety.gov website. Other applicants must take a more comprehensive examination at an aeronautical testing center. All licensees are required to take a review course every two years. At this time no ratings for heavier UAS are available.\n\nCommercial operation is restricted to daylight, line-of-sight, under 100 mph, under 400 feet, and Class G airspace only, and may not fly over people or be operated from a moving vehicle. Some organizations have obtained a waiver or Certificate of Authorization that allows them to exceed these rules. For example, CNN has obtained a waiver for UAVs modified for injury prevention to fly over people, and other waivers allow night flying with special lighting, or non-line-of-sight operations for agriculture or railroad track inspection.\n\nPrevious to this announcement, any commercial use required a full pilot's license and an FAA waiver, of which hundreds had been granted.\n\nThe use of UAVs for law-enforcement purposes is regulated at a state level.\n\nAs of 2015, UAV's under 300g are not controlled by the CAA guidances that include maintaining 50 meters from person, animal or property.\n\nThe UAV must still not go higher than 400 ft with a single pilot or 1000 ft with a pilot and spotter, however as with UAV's above 300g, if within 400 ft of a structure, you are allowed to go 400 ft higher than the structure.\n\n\n\n", "id": "58900", "title": "Unmanned aerial vehicle"}
{"url": "https://en.wikipedia.org/wiki?curid=20903754", "text": "Robotics\n\nRobotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electrical engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing.\n\nThese technologies are used to develop machines that can substitute for humans and replicate human actions. Robots can be used in any situation and for any purpose, but today many are used in dangerous environments (including bomb detection and de-activation), manufacturing processes, or where humans cannot survive. Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, and basically anything a human can do. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\n\nThe concept of creating machines that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (Science, Technology, Engineering, and Mathematics) as a teaching aid.\n\nRobotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with electronics, computer science, artificial intelligence, mechatronics, nanotechnology and bioengineering.\n\nScience-fiction author Isaac Asimov is often given credit for being the first person to use the term robotics in a short story composed in the 1940s. In the story, Asimov suggested three principles to guide the behavior of robots and smart machines. Asimov's Three Laws of Robotics, as they are called, have survived to the present:\n1. Robots must never harm human beings.\n2. Robots must follow instructions from humans without violating rule 1.\n3. Robots must protect themselves without violating the other rules.\n\nThe word \"robotics\" was derived from the word \"robot\", which was introduced to the public by Czech writer Karel Čapek in his play \"R.U.R. (Rossum's Universal Robots)\", which was published in 1920. The word \"robot\" comes from the Slavic word \"robota\", which means labour. The play begins in a factory that makes artificial people called \"robots\", creatures who can be mistaken for humans – very similar to the modern ideas of androids. Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the \"Oxford English Dictionary\" in which he named his brother Josef Čapek as its actual originator.\n\nAccording to the \"Oxford English Dictionary\", the word \"robotics\" was first used in print by Isaac Asimov, in his science fiction short story \"Liar!\", published in May 1941 in \"Astounding Science Fiction\". Asimov was unaware that he was coining the term; since the science and technology of electrical devices is \"electronics\", he assumed \"robotics\" already referred to the science and technology of robots. In some of Asimov's other works, he states that the first use of the word \"robotics\" was in his short story \"Runaround\" (Astounding Science Fiction, March 1942). However, the original publication of \"Liar!\" predates that of \"Runaround\" by ten months, so the former is generally cited as the word's origin.\n\nIn 1942, the science fiction writer Isaac Asimov created his Three Laws of Robotics.\n\nIn 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\n\nFully autonomous only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately and more reliably, than humans. They are also employed in some jobs which are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery, weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.\n\nThere are many types of robots; they are used in many different environments and for many different uses, although being very diverse in application and form they all share three basic similarities when it comes to their construction:\n\nAs more and more robots are designed for specific tasks this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed as \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labelled as \"heavy duty robots\".\n\nCurrent and potential applications include:\n\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries that are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need a fuel, require heat dissipation and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage. Potential power sources could be:\n\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement. By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\n\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\n\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator).\n\nA flexure is designed as part of the motor actuator, to improve safety and provide robust force control, energy efficiency, shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. The resultant lower reflected inertia can improve safety when a robot is interacting with humans or during collisions. It has been used in various robots, particularly advanced manufacturing robots and walking humanoid robots.\n\nPneumatic artificial muscles, also known as air muscles, are special tubes that expand(typically up to 40%) when air is forced inside them. They are used in some robot applications.\n\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material which contracts (under 5%) when electricity is applied. They have been used for some small robot applications.\n\nEAPs or EPAMs are a new plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots, and to enable new robots to float, fly, swim or walk.\n\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line. Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size. These motors are already available commercially, and being used on some robots.\n\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.\n\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information of the task it is performing.\n\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips. The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting robotic grip on held objects.\n\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one—allowing patients to write with it, type on a keyboard, play piano and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feeling in its fingertips.\n\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\n\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\n\nComputer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\n\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have their background in biology.\n\nOther common forms of sensing in robotics use lidar, radar, and sonar.\n\nRobots need to manipulate objects; pick up, modify, destroy, or otherwise have an effect. Thus the \"hands\" of a robot are often referred to as \"end effectors\", while the \"arm\" is referred to as a \"manipulator\". Most robot arms have replaceable effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator which cannot be replaced, while a few have one very general purpose manipulator, for example, a humanoid hand.\nLearning how to manipulate a robot often requires a close feedback between human to the robot, although there are several methods for remote manipulation of robots.\n\nOne of the most common effectors is the gripper. In its simplest manifestation, it consists of just two fingers which can open and close to pick up and let go of a range of small objects. Fingers can for example, be made of a chain with a metal wire run through it. Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand. Hands that are of a mid-level complexity include the Delft hand. Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\n\nVacuum grippers are very simple astrictive devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\n\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum grippers.\n\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS, and the Schunk hand. These are highly dexterous manipulators, with as many as 20 degrees of freedom and hundreds of tactile sensors.\n\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\n\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum. Many different balancing robots have been designed. While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.\n\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" that is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\". Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.\n\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball, or by rotating the outer shells of the sphere. These have also been referred to as an orb bot or a ball bot.\n\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\n\nTank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".\n\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University. Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct. Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Hybrids too have been proposed in movies such as \"I, Robot\", where they walk on two legs and switch to four (arms+legs) when going to a sprint. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\n\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over). However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory. ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\n\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself. Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults. A quadruped was also demonstrated which could trot, run, pace, and bound. For a full list of these robots, see the MIT Leg Lab Robots page.\n\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability. This technique was recently demonstrated by Anybots' Dexter Robot, which is so stable, it can even jump. Another example is the TU Delft Flame.\n\nPerhaps the most promising approach utilizes passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.\n\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing. Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, propelled by paddles, and guided by sonar.\n\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings. The Japanese ACM-R5 snake robot can even navigate both on land and in water.\n\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll. Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin, built by Dr. Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot and Stickybot. China's \"Technology Daily\" reported on November 15, 2008, that Dr. Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Dr. Li, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole..\n\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%. Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion. Notable examples are the Essex University Computer Science Robotic Fish G9, and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion. The Aqua Penguin, designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\n\nIn 2014 \"iSplash\"-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, \"iSplash\"-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.\n\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is \"Vaimos\" built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\n\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information. Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\n\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation.\n\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech. The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent. Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952. Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.\n\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium, making it necessary to develop the emotional component of robotic voice through various techniques.\n\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots. A great many systems have been developed to recognize human hand gestures.\n\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos). The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.\n\nArtificial emotions can also be generated, composed of a sequence of facial expressions and/or gestures. As can be seen from the movie , the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots.\n\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future. Nevertheless, researchers are trying to create robots which appear to have a personality: i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.\n\nThe Socially Intelligent Machines Lab of the Georgia Institute of Technology researches new concepts of guided teaching interaction with robots. The aim of the projects is a social robot that learns task and goals from human demonstrations without prior knowledge of high-level concepts. These new concepts are grounded from low-level continuous sensor data through unsupervised learning, and task goals are subsequently learned using a Bayesian approach. These concepts can be used to transfer knowledge to future tasks, resulting in faster learning of those tasks. The results are demonstrated by the robot \"Curi\" who can scoop some pasta from a pot onto a plate and serve the sauce on top.\n\nThe mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors) which move the mechanical.\n\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands. Sensor fusion may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction) is inferred from these estimates. Techniques from control theory convert the task into commands that drive the actuators.\n\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how they interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\n\nControl systems may also have varying levels of autonomy.\n\nAnother classification takes into account the interaction between human control and the machine motions.\n\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\n\nA first particular new innovation in robot design is the open sourcing of robot-projects. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. \"First generation\" robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the \"first generation\" robot would be incapable of learning, however, Moravec predicts that the \"second generation\" robot would be an improvement over the \"first\" and become available by 2020, with the intelligence maybe comparable to that of a mouse. The \"third generation\" robot should have the intelligence comparable to that of a monkey. Though \"fourth generation\" robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.\n\nThe second is evolutionary robots. This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots, and to explore the nature of evolution. Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, then tested on real robots once the evolved algorithms are good enough. Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.\n\nThe study of motion can be divided into kinematics and dynamics. Direct kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\n\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\n\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\n\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students. First-year computer science courses at some universities now include programming of a robot in addition to traditional software engineering-based coursework.\n\nUniversities offer bachelors, masters, and doctoral degrees in the field of robotics. Vocational schools offer robotics training aimed at careers in robotics.\n\nThe Robotics Certification Standards Alliance (RCSA) is an international robotics certification authority that confers various industry- and educational-related robotics certifications.\n\nSeveral national summer camp programs include robotics as part of their core curriculum. In addition, youth summer robotics programs are frequently offered by celebrated museums and institutions.\n\nThere are lots of competitions all around the globe. One of the most important competitions is the FLL or FIRST Lego League. The idea of this specific competition is that kids start developing knowledge and getting into robotics while playing with Legos since they are 9 years old. This competition is associated with Ni or National Instruments.\n\nMany schools across the country are beginning to add robotics programs to their after school curriculum. Some major programs for afterschool robotics include FIRST Robotics Competition, Botball and B.E.S.T. Robotics. Robotics competitions often include aspects of business and marketing as well as engineering and design.\n\nThe Lego company began a program for children to learn and get excited about robotics at a young age.\n\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising. The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long term investment for benefactors. A paper by Michael Osborne and Carl Benedikt Frey found that 47 per cent of US jobs are at risk to automation \"over some unspecified number of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment.\n\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).\n\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defence, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.\n\nDespite these advances, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programmes and trying to promote a safe and flexible co-operation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\n\nIn future, co-operation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised.\n\n\n\n", "id": "20903754", "title": "Robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=54982564", "text": "CloudMinds\n\nCloudMinds () is an operator of cloud-based systems for intelligent robots, founded in the early 2015. It concentrates on realizing a safe cloud computing network for intelligent robots, creating a large and mixed platform for machine learning, safe intelligent terminals, and researching the technologies of realizing robot controller in operations.\n\nFounded in 2015, CloudMinds has created a cloud-based intelligent robot architecture based on the cloud's brain and secure network. It is backed by SoftBank, Foxconn, Walden Venture Investments and Keytone Ventures. CloudMinds has developed in-depth research in cloud intelligence integration, high-speed security network, safe smart devices and robot control technology.\n\nCloudMinds built up Human Augmented Robotics Intelligence (HARI) platform and has applied it to some significant areas such as blind guiding. The operator has created the world's first cloud-based terminal, “DATA” series. And it is now working on constructing the backbone network worldwide (VBN).\n\nBased on these nuclear technologies, CloudMinds has succeeded in realizing the Mobile Intranet Cloud Services (MCS), which contains the function on improving information security of the cloud robot remote controlling. The technology has been widely used in some areas such as financial, medical, military, public security department and large-scale manufacturing industry.\n\nCloudMinds is the first cloud robot operator that builds the platform and services to make robots connect and learn possible. With the tendency that each household will own a robot by 2025, the company determines to develop ground breaking technology in AI, Robotic control, network and cloud security. Mobile Intranet Cloud Services (MCS) platform offers secure device to cloud connection over a high performance low latency private network. Besides, they target to partner with the best minds in AI and smart devices. Their first products include a blind guiding robot and an AI mobile device that can be used as the control unit for generic robots in order to stimulate an end-to-end ecosystem. The products and services are used to solve some complex problems in health care, manufacturing and finance industries.\n\nDATA is a cloud intelligent device that will serve as the controller between the robot and the cloud brain. It implemented hardware virtualization on Qualcomm platform. CloudMinds also implemented GPU virtualization on DATA. DATA uses a self-developed sandwich-like hardware extension architecture, which is based on special hardware interface that enables support for dual LTE.\n\nMETA is the first wearable device based on cloud framework of CloudMinds, which takes the shape of helmet for visually impaired people to support face recognition, object recognition, path planning and obstacle avoidance, etc.\n\nMobile-intranet Cloud Service (MCS) is made up of three parts, XaaS Cloud, VBN and terminal. The design on the terminal contains technologies on dual chips and virtualization, which applies the artificial intelligence to activate the robot through this AI Mobile.\nXaaS Cloud, which is based on the original HARI (Human - Augmented - Robotic - Intelligence) architecture to realize the integration of artificial Intelligence and Human Intelligence and provide image recognition, enterprise collaboration and different types of services. The XaaS Cloud has currently completed the first phase of construction, providing the Cloud service platform of MCS.\nSince August 2015, CloudMinds has begun to construct the backbone network worldwide, which can realize the network path optimization and protocol optimization to provide safe and high-speed Internet service through SDN (software defined network) and SDP (software defined boundary).\n", "id": "54982564", "title": "CloudMinds"}
{"url": "https://en.wikipedia.org/wiki?curid=1006597", "text": "Nanorobotics\n\nNanorobotics is an emerging technology field creating machines or robots whose components are at or near the scale of a nanometre (10 meters). More specifically, nanorobotics (as opposed to microrobotics) refers to the nanotechnology engineering discipline of designing and building nanorobots, with devices ranging in size from 0.1–10 micrometres and constructed of nanoscale or molecular components. The terms \"nanobot\", \"nanoid\", \"nanite\", \"nanomachine\", or \"nanomite\" have also been used to describe such devices currently under research and development.\n\nNanomachines are largely in the research and development phase, but some primitive molecular machines and nanomotors have been tested. An example is a sensor having a switch approximately 1.5 nanometers across, able to count specific molecules in a chemical sample. The first useful applications of nanomachines may be in nanomedicine. For example, biological machines could be used to identify and destroy cancer cells. Another potential application is the detection of toxic chemicals, and the measurement of their concentrations, in the environment. Rice University has demonstrated a single-molecule car developed by a chemical process and including Buckminsterfullerenes (buckyballs) for wheels. It is actuated by controlling the environmental temperature and by positioning a scanning tunneling microscope tip.\n\nAnother definition is a robot that allows precise interactions with nanoscale objects, or can manipulate with nanoscale resolution. Such devices are more related to microscopy or scanning probe microscopy, instead of the description of nanorobots as molecular machine. Using the microscopy definition, even a large apparatus such as an atomic force microscope can be considered a nanorobotic instrument when configured to perform nanomanipulation. For this viewpoint, macroscale robots or microrobots that can move with nanoscale precision can also be considered nanorobots.\n\nAccording to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical micromachines (see nanomachine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"\"swallow the surgeon\"\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSince nanorobots would be microscopic in size, it would probably be necessary for very large numbers of them to work together to perform microscopic and macroscopic tasks. These nanorobot swarms, both those unable to replicate (as in utility fog) and those able to replicate unconstrainedly in the natural environment (as in grey goo and its less common variants, such as synthetic biology or utility fog), are found in many science fiction stories, such as the Borg nanoprobes in \"Star Trek\" and \"The Outer Limits\" episode \"The New Breed\".\n\nSome proponents of nanorobotics, in reaction to the grey goo scenarios that they earlier helped to propagate, hold the view that nanorobots able to replicate outside of a restricted factory environment do not form a necessary part of a purported productive nanotechnology, and that the process of self-replication, were it ever to be developed, could be made inherently safe. They further assert that their current plans for developing and using molecular manufacturing do not in fact include free-foraging replicators.\n\nThe most detailed theoretical discussion of nanorobotics, including specific design issues such as sensing, power communication, navigation, manipulation, locomotion, and onboard computation, has been presented in the medical context of nanomedicine by Robert Freitas. Some of these discussions remain at the level of unbuildable generality and do not approach the level of detailed engineering.\n\nA document with a proposal on nanobiotech development using open design technology methods, as in open-source hardware and open-source software, has been addressed to the United Nations General Assembly. According to the document sent to the United Nations, in the same way that open source has in recent years accelerated the development of computer systems, a similar approach should benefit the society at large and accelerate nanorobotics development. The use of nanobiotechnology should be established as a human heritage for the coming generations, and developed as an open technology based on ethical practices for peaceful purposes. Open technology is stated as a fundamental key for such an aim.\n\nIn the same ways that technology research and development drove the space race and nuclear arms race, a race for nanorobots is occurring. There is plenty of ground allowing nanorobots to be included among the emerging technologies. Some of the reasons are that large corporations, such as General Electric, Hewlett-Packard, Synopsys, Northrop Grumman and Siemens have been recently working in the development and research of nanorobots; surgeons are getting involved and starting to propose ways to apply nanorobots for common medical procedures; universities and research institutes were granted funds by government agencies exceeding $2 billion towards research developing nanodevices for medicine; bankers are also strategically investing with the intent to acquire beforehand rights and royalties on future nanorobots commercialisation. Some aspects of nanorobot litigation and related issues linked to monopoly have already arisen. A large number of patents has been granted recently on nanorobots, done mostly for patent agents, companies specialized solely on building patent portfolios, and lawyers. After a long series of patents and eventually litigations, see for example the Invention of Radio, or the War of Currents, emerging fields of technology tend to become a monopoly, which normally is dominated by large corporations.\n\nManufacturing nanomachines assembled from molecular components is a very challenging task. Because of the level of difficulty, many engineers and scientists continue working cooperatively across multidisciplinary approaches to achieve breakthroughs in this new area of development. Thus, it is quite understandable the importance of the following distinct techniques currently applied towards manufacturing nanorobots:\n\nThe joint use of nanoelectronics, photolithography, and new biomaterials provides a possible approach to manufacturing nanorobots for common medical uses, such as surgical instrumentation, diagnosis, and drug delivery. This method for manufacturing on nanotechnology scale is in use in the electronics industry since 2008. So, practical nanorobots should be integrated as nanoelectronics devices, which will allow tele-operation and advanced capabilities for medical instrumentation.\n\nA \"nucleic acid robot\" (nubot) is an organic molecular machine at the nanoscale. DNA structure can provide means to assemble 2D and 3D nanomechanical devices. DNA based machines can be activated using small molecules, proteins and other molecules of DNA. Biological circuit gates based on DNA materials have been engineered as molecular machines to allow in-vitro drug delivery for targeted health problems. Such material based systems would work most closely to smart biomaterial drug system delivery, while not allowing precise in vivo teleoperation of such engineered prototypes.\n\nSeveral reports have demonstrated the attachment of synthetic molecular motors to surfaces. These primitive nanomachines have been shown to undergo machine-like motions when confined to the surface of a macroscopic material. The surface anchored motors could potentially be used to move and position nanoscale materials on a surface in the manner of a conveyor belt.\n\nNanofactory Collaboration, founded by Robert Freitas and Ralph Merkle in 2000 and involving 23 researchers from 10 organizations and 4 countries, focuses on developing a practical research agenda specifically aimed at developing positionally-controlled diamond mechanosynthesis and a diamondoid nanofactory that would have the capability of building diamondoid medical nanorobots.\n\nThis approach proposes the use of biological microorganisms, like the bacterium \"Escherichia coli\" and \"Salmonella typhimurium\".\nThus the model uses a flagellum for propulsion purposes. Electromagnetic fields normally control the motion of this kind of biological integrated device.\nChemists at the University of Nebraska have created a humidity gauge by fusing a bacterium to a silicone computer chip.\n\nRetroviruses can be retrained to attach to cells and replace DNA. They go through a process called reverse transcription to deliver genetic packaging in a vector. Usually, these devices are Pol – Gag genes of the virus for the Capsid and Delivery system. This process is called retroviral gene therapy, having the ability to re-engineer cellular DNA by usage of viral vectors. This approach has appeared in the form of retroviral, adenoviral, and lentiviral gene delivery systems. These gene therapy vectors have been used in cats to send genes into the genetically modified organism (GMO), causing it to display the trait.\n3D printing is the process by which a three-dimensional structure is built through the various processes of additive manufacturing. Nanoscale 3D printing involves many of the same process, incorporated at a much smaller scale. To print a structure in the 5-400 µm scale, the precision of the 3D printing machine is improved greatly. A two-steps process of 3D printing, using a 3D printing and laser etched plates method was incorporated as an improvement technique. To be more precise at a nanoscale, the 3D printing process uses a laser etching machine, which etches into each plate the details needed for the segment of nanorobot. The plate is then transferred to the 3D printer, which fills the etched regions with the desired nanoparticle. The 3D printing process is repeated until the nanorobot is built from the bottom up. This 3D printing process has many benefits. First, it increases the overall accuracy of the printing process. Second, it has the potential to create functional segments of a nanorobot. The 3D printer uses a liquid resin, which is hardened at precisely the correct spots by a focused laser beam. The focal point of the laser beam is guided through the resin by movable mirrors and leaves behind a hardened line of solid polymer, just a few hundred nanometers wide. This fine resolution enables the creation of intricately structured sculptures as tiny as a grain of sand. This process takes place by using photoactive resins, which are hardened by the laser at an extremely small scale to create the structure. This process is quick by nanoscale 3D printing standards. Ultra-small features can be made with the 3D micro-fabrication technique used in multiphoton photopolymerisation. This approach uses a focused laser to trace the desired 3D object into a block of gel. Due to the nonlinear nature of photo excitation, the gel is cured to a solid only in the places where the laser was focused while the remaining gel is then washed away. Feature sizes of under 100 nm are easily produced, as well as complex structures with moving and interlocked parts.\n\nPotential uses for nanorobotics in medicine include early diagnosis and targeted drug-delivery for cancer, biomedical instrumentation, surgery, pharmacokinetics, monitoring of diabetes, and health care.\n\nIn such plans, future medical nanotechnology is expected to employ nanorobots injected into the patient to perform work at a cellular level. Such nanorobots intended for use in medicine should be non-replicating, as replication would needlessly increase device complexity, reduce reliability, and interfere with the medical mission.\n\nNanotechnology provides a wide range of new technologies for developing customized means to optimize the delivery of pharmaceutical drugs. Today, harmful side effects of treatments such as chemotherapy are commonly a result of drug delivery methods that don't pinpoint their intended target cells accurately. Researchers at Harvard and MIT, however, have been able to attach special RNA strands, measuring nearly 10 nm in diameter, to nanoparticles, filling them with a chemotherapy drug. These RNA strands are attracted to cancer cells. When the nanoparticle encounters a cancer cell, it adheres to it, and releases the drug into the cancer cell. This directed method of drug delivery has great potential for treating cancer patients while avoiding negative effects (commonly associated with improper drug delivery). The first demonstration of nanomotors operating in living organism was carried out in 2014 at University of California, San Diego. MRI-guided nanocapsules are one potential precursor to nanorobots.\n\nAnother useful application of nanorobots is assisting in the repair of tissue cells alongside white blood cells. Recruiting inflammatory cells or white blood cells (which include neutrophil granulocytes, lymphocytes, monocytes, and mast cells) to the affected area is the first response of tissues to injury. Because of their small size, nanorobots could attach themselves to the surface of recruited white cells, to squeeze their way out through the walls of blood vessels and arrive at the injury site, where they can assist in the tissue repair process. Certain substances could possibly be used to accelerate the recovery.\n\nThe science behind this mechanism is quite complex. Passage of cells across the blood endothelium, a process known as transmigration, is a mechanism involving engagement of cell surface receptors to adhesion molecules, active force exertion and dilation of the vessel walls and physical deformation of the migrating cells. By attaching themselves to migrating inflammatory cells, the robots can in effect “hitch a ride” across the blood vessels, bypassing the need for a complex transmigration mechanism of their own.\n\n, in the United States, Food and Drug Administration (FDA) regulates nanotechnology on the basis of size.\n\n\n", "id": "1006597", "title": "Nanorobotics"}
{"url": "https://en.wikipedia.org/wiki?curid=1678822", "text": "Perceptual control theory\n\nPerceptual control theory (PCT) is a model of behavior based on the principles of negative feedback, but differing in important respects from engineering control theory. Results of PCT experiments have demonstrated that an organism controls neither its own behavior, nor external environmental variables, but rather its own perceptions of those variables. Actions are not controlled, they are varied so as to cancel the effects that unpredictable environmental disturbances would otherwise have on controlled perceptions. According to the standard catch-phrase of the field, \"behavior is the control of perception.\" PCT demonstrates circular causation in a negative feedback loop closed through the environment. This fundamentally contradicts the classical notion of linear causation of behavior by stimuli, in which environmental stimuli are thought to cause behavioral responses, mediated (according to cognitive psychology) by intervening cognitive processes.\n\nNumerous computer simulations of specific behavioral situations demonstrate its efficacy, with extremely high correlations to observational data (0.95 or better), which are vanishingly rare in the so-called 'soft' sciences. While the adoption of PCT in the scientific community has not been widespread, it has been applied not only in experimental psychology and neuroscience, but also in sociology, linguistics, and a number of other fields, and has led to a method of psychotherapy called the method of levels.\n\nA tradition from Aristotle through William James recognizes that behavior is \"purposeful\" rather than merely reactive. However, the only evidence for intentions was subjective. Behaviorists following Wundt, Thorndyke, Watson, and others rejected introspective reports as data for an objective science of psychology. Only observable behavior could be admitted as data.\n\nThere follows from this stance the assumption that environmental events (stimuli) cause behavioral actions (responses). This assumption persists in cognitive psychology, which interposes cognitive maps and other postulated information processing between stimulus and response, but otherwise retains the assumption of linear causation from environment to behavior.\n\nAnother, more specific reason for psychologists' rejecting notions of purpose or intention was that they could not see how a goal (a state that did not yet exist) could cause the behavior that led to it. PCT resolves these philosophical arguments about teleology because it provides a model of the functioning of organisms in which purpose has objective status without recourse to introspection, and in which causation is circular around feedback loops.\n\nPCT has roots in insights of Claude Bernard and 20th century control systems engineering and cybernetics. It was originated as such, and given its present form and experimental methodology, by William T. Powers.\n\nPowers recognized that to be purposeful implies control, and that the concepts and methods of engineered control systems could be applied to biological control systems. A key insight is that the variable that is controlled is not the output of the system (the behavioral actions), but its input, that is, a sensed and transformed function of some state of the environment that could be affected by the control system's output. Because some of these sensed and transformed inputs appear as consciously perceived aspects of the environment, Powers labelled the controlled variable \"perception\". The theory came to be known as \"Perceptual Control Theory\" or PCT rather than \"Control Theory Applied to Psychology\" because control theorists often assert or assume that it is the system's output that is controlled. In PCT it is the internal representation of the state of some variable in the environment—a \"perception\" in everyday language—that is controlled. The basic principles of PCT were first published by Powers, Clark, and MacFarland as a \"general feedback theory of behavior\" in 1960, with credits to cybernetic authors Wiener and Ashby, and has been systematically developed since then in the research community that has gathered around it. Initially, it received little general recognition, but is now better known.\n\nA simple negative feedback control system is a cruise control system for a car. A cruise control system has a sensor which \"perceives\" speed as the rate of spin of the drive shaft directly connected to the wheels. It also has a driver-adjustable 'goal' specifying a particular speed. The sensed speed is continuously compared against the specified speed by a device (called a \"comparator\") which subtracts the currently sensed input value from the stored goal value. The difference (the error signal) determines the throttle setting (the accelerator depression), so that the engine output is continuously varied to counter variations in the speed of the car. This type of classical negative feedback control was worked out by engineers in the 1930s and 1940s.\n\nIf the speed of the car starts to drop below the goal-speed, for example when climbing a hill, the small increase in the error signal, amplified, causes engine output to increase, which keeps the error very nearly at zero. If the speed exceeds the goal, e.g. when going down a hill, the engine is throttled back so as to act as a brake, so again the speed is kept from departing more than a barely detectable amount from the goal speed (brakes are needed only if the hill is too steep). The result is that the cruise control system maintains a speed close to the goal as the car goes up and down hills, and as other disturbances such as wind affect the car's speed. This is all done without any planning of specific actions, and without any blind reactions to stimuli.\n\nThe same principles of negative feedback control (including the ability to nullify effects of unpredictable external or internal disturbances) apply to living control systems. The thesis of PCT is that animals and people do not control their behavior; rather, they vary their behavior as their means for controlling their perceptions, with or without external disturbances. This directly contradicts the historical and still widespread assumption that behavior is the final result of stimulus inputs and cognitive plans.\n\nThe principal datum in PCT methodology is the controlled variable. The fundamental step of PCT research, the test for controlled variables, is the slow and gentle application of disturbing influences to the state of a variable in the environment which the researcher surmises is already under control by the observed organism. It is essential not to overwhelm the organism's ability to control, since that is what is being investigated. If the organism changes its actions just so as to prevent the disturbing influence from having the expected effect on that variable, that is strong evidence that the experimental action disturbed a controlled variable. It is crucially important to distinguish the perceptions and point of view of the observer from those of the observed organism. It may take a number of variations of the test to isolate just which aspect of the environmental situation is under control, as perceived by the observed organism.\n\nPCT employs a black box methodology. The controlled variable as measured by the observer corresponds quantitatively to a reference value for a perception that the organism is controlling. The controlled variable is thus an objective index of the purpose or intention of those particular behavioral actions by the organism—the goal which those actions consistently work to attain despite disturbances. With few exceptions, in the current state of neuroscience this internally maintained reference value is seldom directly observed as such (e.g. as a rate of firing in a neuron), since few researchers trace the relevant electrical and chemical variables by their specific pathways while a living organism is engaging in what we externally observe as behavior. However, when a working negative feedback system simulated on a digital computer performs essentially identically to observed organisms, then the well understood negative feedback structure of the simulation or model (the white box) is understood to demonstrate the unseen negative feedback structure within the organism (the black box).\n\nData for individuals are not aggregated for statistical analysis; instead, a generative model is built which replicates the data observed for individuals with very high fidelity (0.95 or better). To build such a model of a given behavioral situation requires careful measurements of three observed variables:\n\nA fourth value, the internally maintained reference \"r\" (a variable ′setpoint′), is deduced from the value at which the organism is observed to maintain \"q\", as determined by the test for controlled variables (described at the beginning of this section).\n\nWith two variables specified, the controlled input \"q\" and the reference \"r\", a properly designed control system, simulated on a digital computer, produces outputs \"q\" that almost precisely oppose unpredictable disturbances \"d\" to the controlled input. Further, the variance from perfect control accords well with that observed for living organisms. Perfect control would result in zero effect of the disturbance, but living organisms are not perfect controllers, and the aim of PCT is to model living organisms. When a computer simulation performs with >95% conformity to experimentally measured values, opposing the effect of unpredictable changes in \"d\" by generating (nearly) equal and opposite values of \"q\", it is understood to model the behavior and the internal control-loop structure of the organism.\n\nBy extension, the elaboration of the theory constitutes a general model of cognitive process and behavior. With every specific model or simulation of behavior that is constructed and tested against observed data, the general model that is presented in the theory is exposed to potential challenge that could call for revision or could lead to refutation.\n\nTo illustrate the mathematical calculations employed in a PCT simulation, consider a pursuit tracking task in which the participant keeps a mouse cursor aligned with a moving target on a computer monitor.\n\nThe model assumes that a perceptual signal within the participant represents the magnitude of the input quantity \"q\". (This has been demonstrated to be a rate of firing in a neuron, at least at the lowest levels.) In the tracking task, the input quantity is the vertical distance between the target position \"T\" and the cursor position \"C\", and the random variation of the target position acts as the disturbance \"d\" of that input quantity. This suggests that the perceptual signal \"p\" quantitatively represents the cursor position \"C\" minus the target position T, as expressed in the equation \"p\"=\"C\"–\"T\".\n\nBetween the perception of target and cursor and the construction of the signal representing the distance between them there is a delay of \"Τ\" milliseconds, so that the working perceptual signal at time \"t\" represents the target-to-cursor distance at a prior time, \"t\" – \"Τ\". Consequently, the equation used in the model is\n\n1. \"p\"(\"t\") = \"C\"(\"t–Τ\") – \"T\"(\"t–Τ\")\n\nThe negative feedback control system receives a reference signal \"r\" which specifies the magnitude of the given perceptual signal which is currently intended or desired. (For the origin of \"r\" within the organism, see under \"A hierarchy of control\", below.) Both \"r\" and \"p\" are input to a simple neural structure with \"r\" excitatory and \"p\" inhibitory. This structure is called a \"comparator\". The effect is to subtract \"p\" from \"r\", yielding an error signal \"e\" that indicates the magnitude and sign of the difference between the desired magnitude \"r\" and the currently input magnitude \"p\" of the given perception. The equation representing this in the model is:\n\n2. \"e\" = \"r–p\"\n\nThe error signal \"e\" must be transformed to the output quantity \"q\" (representing the participant's muscular efforts affecting the mouse position). Experiments have shown that in the best model for the output function, the mouse velocity \"V\" is proportional to the error signal \"e\" by a gain factor \"G\" (that is, \"V\" = \"G\"*\"e\"). Thus, when the perceptual signal \"p\" is smaller than the reference signal \"r\", the error signal \"e\" has a positive sign, and from it the model computes an upward velocity of the cursor that is proportional to the error.\n\nThe next position of the cursor \"C\" is the current position \"C\" plus the velocity \"V\" times the duration \"dt\" of one iteration of the program. By simple algebra, we substitute \"G\"*\"e\" (as given above) for \"V\", yielding a third equation:\n\n3. \"C\" = \"C\" + \"G\"*\"e\"*\"dt\"\n\nThese three simple equations or program steps constitute the simplest form of the model for the tracking task. When these three simultaneous equations are evaluated over and over with the same random disturbances \"d\" of the target position that the human participant experienced, the output positions and velocities of the cursor duplicate the participant's actions in the tracking task above within 4.0% of their peak-to-peak range, in great detail.\n\nThis simple model can be refined with a damping factor \"d\" which reduces the discrepancy between the model and the human participant to 3.6% when the disturbance \"d\" is set to maximum difficulty.\n\n3'. \"C\" = \"C\" + [(\"G\"*\"e\")–(\"d\"*\"C\")]*\"dt\"\n\nDetailed discussion of this model in (Powers 2008) includes both source and executable code, with which the reader can verify how well this simple program simulates real behavior. No consideration is needed of possible nonlinearities such as the Weber-Fechner law, potential noise in the system, continuously varying angles at the joints, and many other factors that could afflict performance if this were a simple linear model. No inverse kinematics or predictive calculations are required. The model simply reduces the discrepancy between input \"p\" and reference \"r\" continuously as it arises in real time, and that is all that is required—as predicted by the theory.\n\nIn the artificial systems that are specified by engineering control theory, the reference signal is considered to be an external input to the 'plant'. In engineering control theory, the reference signal or set point is public; in PCT, it is not, but rather must be deduced from the results of the test for controlled variables, as described above in the methodology section. This is because in living systems a reference signal is not an externally accessible input, but instead originates within the system. In the hierarchical model, error output of higher-level control loops, as described in the next section below, evokes \"r\" from synapse-local memory, and the strength of \"r\" is proportional to the (weighted) strength of the error signal or signals from one or more higher-level systems.\n\nIn engineering control systems, in the case where there are several such reference inputs, a 'Controller' is designed to manipulate those inputs so as to obtain the effect on the output of the system that is desired by the system's designer, and the task of a control theory (so conceived) is to calculate those manipulations so as to avoid instability and oscillation. The designer of a PCT model or simulation specifies no particular desired effect on the output of the system, except that it must be whatever is required to bring the input from the environment (the perceptual signal) into conformity with the reference. In Perceptual Control Theory, the input function for the reference signal is a weighted sum of internally generated signals (in the canonical case, higher-level error signals), and loop stability is determined locally for each loop in the manner sketched in the preceding section on the mathematics of PCT (and elaborated more fully in the referenced literature). The weighted sum is understood to result from reorganization.\n\nEngineering control theory is computationally demanding, but as the preceding section shows, PCT is not. For example, contrast the implementation of a model of an inverted pendulum in engineering control theory with the PCT implementation as a hierarchy of five simple control systems.\n\nPerceptions, in PCT, are constructed and controlled in a hierarchy of levels. For example, visual perception of an object is constructed from differences in light intensity or differences in sensations such as color at its edges. Controlling the shape or location of the object requires altering the perceptions of sensations or intensities (which are controlled by lower-level systems). This organizing principle is applied at all levels, up to the most abstract philosophical and theoretical constructs.\n\nThe Russian physiologist Nicolas Bernstein independently came to the same conclusion that behavior has to be multiordinal—organized hierarchically, in layers. A simple problem led to this conclusion at about the same time both in PCT and in Bernstein's work. The spinal reflexes act to stabilize limbs against disturbances. Why do they not prevent centers higher in the brain from using those limbs to carry out behavior? Since the brain obviously does use the spinal systems in producing behavior, there must be a principle that allows the higher systems to operate by incorporating the reflexes, not just by overcoming them or turning them off. The answer is that the reference value (setpoint) for a spinal reflex is not static; rather, it is varied by higher-level systems as their means of moving the limbs. This principle applies to higher feedback loops, as each loop presents the same problem to subsystems above it.\n\nWhereas an engineered control system has a reference value or setpoint adjusted by some external agency, the reference value for a biological control system cannot be set in this way. The setpoint must come from some internal process. If there is a way for behavior to affect it, any perception may be brought to the state momentarily specified by higher levels and then be maintained in that state against unpredictable disturbances. In a hierarchy of control systems, higher levels adjust the goals of lower levels as their means of approaching their own goals set by still-higher systems. This has important consequences for any proposed external control of an autonomous living control system (organism). At the highest level, reference values (goals) are set by heredity or adaptive processes.\n\nIf an organism controls inappropriate perceptions or controls some perceptions to inappropriate values, it is less likely to bring progeny to maturity, and may die. Consequently, by natural selection successive generations of organisms evolve so that they control those perceptions that, when controlled with appropriate setpoints, tend to maintain critical internal variables at optimal levels, or at least within non-lethal limits. Powers called these critical internal variables \"intrinsic variables\" (Ashby's \"essential variables\").\n\nThe mechanism that influences the development of structures of perceptions to be controlled is termed \"reorganization\", a process within the individual organism that is subject to natural selection just as is the evolved structure of individuals within a species.\n\nThis \"reorganization system\" is proposed to be part of the inherited structure of the organism. It changes the underlying parameters and connectivity of the control hierarchy in a random-walk manner. There is a basic continuous rate of change in intrinsic variables which proceeds at a speed set by the total error (and stops at zero error), punctuated by random changes in direction in a hyperspace with as many dimensions as there are critical variables. This is a more or less direct adaptation of Ashby's \"homeostat\", first adopted into PCT in the 1960 paper and then changed to use E. coli's method of navigating up gradients of nutrients, as described by Koshland (1980).\n\nReorganization may occur at any level when loss of control at that level causes intrinsic (essential) variables to deviate from genetically determined set points. This is the basic mechanism that is involved in trial-and-error learning, which leads to the acquisition of more systematic kinds of learning processes.\n\nIn a hierarchy of interacting control systems, different systems at one level can send conflicting goals to one lower system. When two systems are specifying different goals for the same lower-level variable, they are in conflict. Protracted conflict is experienced by human beings as many forms of psychological distress such as anxiety, obsession, depression, confusion, and vacillation. Severe conflict prevents the affected systems from being able to control, effectively destroying their function for the organism.\n\nHigher level control systems often are able to use known strategies (which are themselves acquired through prior reorganizations) to seek perceptions that don't produce the conflict. Normally, this takes place without notice. If the conflict persists and systematic \"problem solving\" by higher systems fails, the reorganization system may modify existing systems until they bypass the conflict or until they produce new reference signals (goals) that are not in conflict at lower levels.\n\nWhen reorganization results in an arrangement that reduces or eliminates the error that is driving it, the process of reorganization slows or stops with the new organization in place. (This replaces the concept of reinforcement learning.) New means of controlling the perceptions involved, and indeed new perceptual constructs subject to control, may also result from reorganization. In simplest terms, the reorganization process varies things until something works, at which point we say that the organism has learned. When done in the right way, this method can be surprisingly efficient in simulations.\n\nThe reorganization concept has led to a method of psychotherapy called the method of levels (MOL) currently being tested in England, the United States, and Australia.\n\nCurrently, no one theory has been agreed upon to explain the synaptic, neuronal or systemic basis of learning. Prominent since 1973, however, is the idea that long-term potentiation (LTP) of populations of synapses induces learning through both pre- and postsynaptic mechanisms (Bliss & Lømo, 1973; Bliss & Gardner-Medwin, 1973). LTP is a form of Hebbian learning, which proposed that high-frequency, tonic activation of a circuit of neurones increases the efficacy with which they are activated and the size of their response to a given stimulus as compared to the standard neurone (Hebb, 1949). These mechanisms are the principles behind Hebb's famously simple explanation: \"Those that fire together, wire together\" (Hebb, 1949).\n\nLTP has received much support since it was first observed by Terje Lømo in 1966 and is still the subject of many modern studies and clinical research. However, there are possible alternative mechanisms underlying LTP, as presented by Enoki, Hu, Hamilton and Fine in 2009, published in the journal \"Neuron\". They concede that LTP is the basis of learning. However, they firstly propose that LTP occurs in individual synapses, and this plasticity is graded (as opposed to in a binary mode) and bidirectional (Enoki et al., 2009). Secondly, the group suggest that the synaptic changes are expressed solely presynaptically, via changes in the probability of transmitter release (Enoki et al., 2009). Finally, the team predict that the occurrence of LTP could be age-dependent, as the plasticity of a neonatal brain would be higher than that of a mature one. Therefore, the theories differ, as one proposes an on/off occurrence of LTP by pre- and postsynaptic mechanisms and the other proposes only presynaptic changes, graded ability, and age-dependence.\n\nThese theories do agree on one element of LTP, namely, that it must occur through physical changes to the synaptic membrane/s, i.e. synaptic plasticity. Perceptual control theory encompasses both of these views. It proposes the mechanism of 'reorganisation' as the basis of learning. Reorganisation occurs within the inherent control system of a human or animal by restructuring the inter- and intraconnections of its hierarchical organisation, akin to the neuroscientific phenomenon of neural plasticity. This reorganisation initially allows the trial-and-error form of learning, which is seen in babies, and then progresses to more structured learning through association, apparent in infants, and finally to systematic learning, covering the adult ability to learn from both internally and externally generated stimuli and events. In this way, PCT provides a valid model for learning that combines the biological mechanisms of LTP with an explanation of the progression and change of mechanisms associated with developmental ability (Plooij 1984, 1987, 2003, Plooij & Plooij (1990), 2013).\n\nPowers (2008) produced a simulation of arm co-ordination. He suggested that in order to move your arm, fourteen control systems that control fourteen joint angles are involved, and they reorganise simultaneously and independently. It was found that for optimum performance, the output functions must be organised in a way so as each control system's output only affects the one environmental variable it is perceiving. In this simulation, the reorganising process is working as it should, and just as Powers suggests that it works in humans, reducing outputs that cause error and increasing those that reduce error. Initially, the disturbances have large effects on the angles of the joints, but over time the joint angles match the reference signals more closely due to the system being reorganised. Powers (2008) suggests that in order to achieve coordination of joint angles to produce desired movements, instead of calculating how multiple joint angles must change to produce this movement the brain uses negative feedback systems to generate the joint angles that are required. A single reference signal that is varied in a higher-order system can generate a movement that requires several joint angles to change at the same time.\n\nBotvinick (2008) proposed that one of the founding insights of the cognitive revolution was the recognition of hierarchical structure in human behavior. Despite decades of research, however, the computational mechanisms underlying hierarchically organized behavior are still not fully understood. Bedre, Hoffman, Cooney & D'Esposito (2009) propose that the fundamental goal in cognitive neuroscience is to characterize the functional organization of the frontal cortex that supports the control of action.\n\nRecent neuroimaging data has supported the hypothesis that the frontal lobes are organized hierarchically, such that control is supported in progressively caudal regions as control moves to more concrete specification of action. However, it is still not clear whether lower-order control processors are differentially affected by impairments in higher-order control when between-level interactions are required to complete a task, or whether there are feedback influences of lower-level on higher-level control (Bedre, Hoffman, Cooney & D'Esposito 2009).\n\nBotvinik (2008) found that all existing models of hierarchically structured behavior share at least one general assumption – that the hierarchical, part–whole organization of human action is mirrored in the internal or neural representations underlying it. Specifically, the assumption is that there exist representations not only of low-level motor behaviors, but also separable representations of higher-level behavioral units. The latest crop of models provides new insights, but also poses new or refined questions for empirical research, including how abstract action representations emerge through learning, how they interact with different modes of action control, and how they sort out within the prefrontal cortex (PFC).\n\nPerceptual control theory (PCT) can provide an explanatory model of neural organisation that deals with the current issues. PCT describes the hierarchical character of behavior as being determined by control of hierarchically organized perception. Control systems in the body and in the internal environment of billions of interconnected neurons within the brain are responsible for keeping perceptual signals within survivable limits in the unpredictably variable environment from which those perceptions are derived. PCT does not propose that there is an internal model within which the brain simulates behavior before issuing commands to execute that behavior. Instead, one of its characteristic features is the principled lack of cerebral organisation of behavior. Rather, behavior is the organism's variable means to reduce the discrepancy between perceptions and reference values which are based on various external and internal inputs (Cools, 1985). Behavior must constantly adapt and change for an organism to maintain its perceptual goals. In this way, PCT can provide an explanation of abstract learning through spontaneous reorganisation of the hierarchy. PCT proposes that conflict occurs between disparate reference values for a given perception rather than between different responses (Mansell 2011), and that learning is implemented as trial-and-error changes of the properties of control systems (Marken & Powers 1989), rather than any specific response being \"reinforced\". In this way, behavior remains adaptive to the environment as it unfolds, rather than relying on learned action patterns that may not fit.\n\nHierarchies of perceptual control have been simulated in computer models and have been shown to provide a close match to behavioral data. For example, Marken conducted an experiment comparing the behavior of a perceptual control hierarchy computer model with that of six healthy volunteers in three experiments. The participants were required to keep the distance between a left line and a centre line equal to that of the centre line and a right line. They were also instructed to keep both distances equal to 2 cm. They had 2 paddles in their hands, one controlling the left line and one controlling the middle line. To do this, they had to resist random disturbances applied to the positions of the lines. As the participants achieved control, they managed to nullify the expected effect of the disturbances by moving their paddles. The correlation between the behavior of subjects and the model in all the experiments approached .99. It is proposed that the organization of models of hierarchical control systems such as this informs us about the organization of the human subjects whose behavior it so closely reproduces.\n\nPCT has significant implications for Robotics and Artificial Intelligence. The architecture, of a hierarchy of perceptual controllers, is an ideal and comparatively simple implementation for artificial systems by avoiding the need to generate specific actions whether by complex models of the external world or the computation from input-output mappings. \nThis is in stark contrast to traditional methodologies, such as the computational approach and Behavior-based Robotics.\n\nThe application of perceptual control to Robotics was outlined in a seminal paper in the Artificial Life journal by Rupert Young of Perceptual Robots in 2017. The architecture has been applied to a number of real-world robotic systems including robotic rovers, balancing robot and robot arms.\n\nTraditional approaches to robotics, which generally depend upon the computation of actions in specific situations, result in inflexible, clumsy robots unable to cope with the dynamic nature of the world. PCT robots, on the other hand, demonstrate robots that inherently resist and counter the chaotic, unpredictable world.\n\nThe preceding explanation of PCT principles provides justification of how this theory can provide a valid explanation of neural organisation and how it can explain some of the current issues of conceptual models.\n\nPerceptual control theory currently proposes a hierarchy of 11 levels of perceptions controlled by systems in the human mind and neural architecture. These are: intensity, sensation, configuration, transition, event, relationship, category, sequence, program, principle, and system concept. Diverse perceptual signals at a lower level (e.g. visual perceptions of intensities) are combined in an input function to construct a single perception at the higher level (e.g. visual perception of a color sensation). The perceptions that are constructed and controlled at the lower levels are passed along as the perceptual inputs at the higher levels. The higher levels in turn control by telling the lower levels what to perceive: that is, they adjust the reference levels (goals) of the lower levels.\n\nWhile many computer demonstrations of principles have been developed, the proposed higher levels are difficult to model because too little is known about how the brain works at these levels. Isolated higher-level control processes can be investigated, but models of an extensive hierarchy of control are still only conceptual, or at best rudimentary.\n\nPerceptual control theory has not been widely accepted in mainstream psychology, but has been effectively used in a considerable range of domains in human factors, clinical psychology, and psychotherapy (the \"Method of Levels\"), and it has formed the conceptual foundation for the reference model used by a succession of NATO research study groups. It is the basis for a considerable body of research in sociology. It is being taught in several universities worldwide and is the subject of a number of PhD degrees.\n\n\n\n\n\n", "id": "1678822", "title": "Perceptual control theory"}
{"url": "https://en.wikipedia.org/wiki?curid=56002175", "text": "List of unmanned aerial vehicle applications\n\nUnmanned aerial vehicles are used across the world for civilian, commercial, as well as military applications. This is an incomplete list of those applications.\n\nAirlines and maintenance, repair, and operations contractors use UAVs for aircraft maintenance. In June 2015 EasyJet began testing UAVs in the maintenance of their Airbus A320s and in July 2016 at the Farnborough Airshow, Airbus (manufacturer of the A320), demonstrated the use of UAVs for the visual inspection of an aircraft. However, some aircraft maintenance professionals remain wary of the technology and its ability to properly catch potential dangers. \n\nA helicopter UAV has been proposed to accompany a future NASA Mars rover mission. Investigators believe a solar-powered craft would be able to fly for a few minutes at a time despite the thin atmosphere of the planet, and help the rover scout out interesting destinations.\n\nUAVs are used by a broad range of military forces, from Argentina to the US and also by Islamic State of Iraq and the Levant (ISIS).\n\nAs of January 2014, the U.S. military operated 7,362 RQ-11B Ravens; 145 AeroVironment RQ-12A Wasps; 1,137 AeroVironment RQ-20A Pumas; 306 RQ-16 T-Hawk small UAS; 246 Predators and MQ-1C Grey Eagles; 126 MQ-9 Reapers; 491 RQ-7 Shadows and 33 RQ-4 Global Hawk large systems. The MQ-9 Reaper costs $12 million while a manned F-22 costs over $120 million.\n\nISIS announced a \"Unmanned Aircraft of the Mujahideen\" unit in January 2017 and use drones for both reconnaissance and to drop bombs.\n\nThe Tu-141 \"Swift\" reusable Soviet reconnaissance UAV is intended for reconnaissance to a depth of several hundred kilometers from the front line at supersonic speeds. The Tu-123 \"Hawk\" is a supersonic long-range reconnaissance UAV intended for conducting photographic and signals intelligence to a distance of 3200 km; it was produced beginning in 1964. The La-17P (UAV) is a reconnaissance UAV produced since 1963. In 1945 the Soviet Union began producing \"doodlebug\". 43 Soviet/Russian UAV models are known.\n\nIn 2013, the U.S. Navy launched a UAV from a submerged submarine, the first step to \"providing mission intelligence, surveillance and reconnaissance capabilities to the U.S. Navy's submarine force.\"\n\nUAVs avoid potential diplomatic embarrassment when a manned aircraft is shot down and the pilots captured.\n\nMQ-1 Predator UAVs armed with Hellfire missiles have been used by the U.S. as platforms for hitting ground targets. Armed Predators were first used in late 2001, mostly aimed at assassinating high-profile individuals (terrorist leaders, etc.) inside Afghanistan.\n\nIslamic State of Iraq and the Levant have used drones, adapting UAVs bought online, to drop explosives, primarily using quadcopters.\n\nThe US armed forces have no defense against low-level UAV attack, but the Joint Integrated Air and Missile Defense Organization is working to repurpose existing systems. Two German companies are developing 40-kW lasers to damage UAVs. Other systems still include the OpenWorks Engineering Skywall and the Battelle DroneDefender.\n\nSince 1997, the US military has used more than 80 F-4 Phantoms converted into UAVs as aerial targets for combat training of human pilots. The F-4s were supplemented in September 2013 with F-16s as more realistically maneuverable targets.\n\nSince January 2016 British scientists are developing UAVs with advanced imaging technology to more cheaply and effectively map and speed up the clearing of minefields. The Find A Better Way charity, working since 2011 to advance technologies that will enable safer and more efficient clearance of landmines, teamed up with scientists at the University of Bristol to develop UAVs fit with hyperspectral imaging technology that can quickly identify landmines buried in the ground. John Fardoulis, project researcher from Bristol University states that \"the maps [their] UAVs will generate should help deminers focus on the places where mines are most likely to be found\". Their intended UAVs will be able to perform flyovers and gather images at various wavelengths which, according to Dr John Day from the University of Bristol, could indicate explosive chemicals seeping from landmines into the surrounding foliage as \"chemicals in landmines leak out and are often absorbed by plants, causing abnormalities\" which can be detected as \"living plants have a very distinctive reflection in the near infrared spectrum, just beyond human vision, which makes it possible to tell how healthy they are\".\n\nThe Dutch Mine Kafon project, led by designer Massoud Hassani is working on a UAV system that can quickly detect and clear land mines. The unmanned airborne de-mining system called Mine Kafon Drone uses a three step process to autonomously map, detect and detonate land mines. It flies above potentially dangerous areas, generating a 3D map, and uses a metal detector to pinpoint the location of mines. The UAV can then place a detonator above the mines using its robotic gripping arm, before retreating to a safe distance. The firm claims its UAV is safer, 20 times faster and up to 200 times cheaper than current technologies and might clear mines globally in 10 years. The project raised funds on the crowdfunding site Kickstarter with their goal set at €70,000 and receiving over €100,000 above it.\n\nCivil uses include aerial crop surveys, aerial photography, search and rescue, inspection of power lines and pipelines, counting wildlife, delivering medical supplies to otherwise inaccessible regions, and detection of illegal hunting, reconnaissance operations, cooperative environment monitoring, border patrol missions, convoy protection, forest fire detection and monitoring, surveillance, coordinating humanitarian aid, plume tracking, land surveying, fire and large-accident investigation, landslide measurement, illegal landfill detection, the construction industry, smuggling, and crowd monitoring.\n\nUS government agencies use UAVs such as the RQ-9 Reaper to patrol borders, scout property and locate fugitives. One of the first authorized for domestic use was the ShadowHawk in Montgomery County, Texas SWAT and emergency management offices.\n\nPrivate citizens and media organizations use UAVs for surveillance, recreation, news-gathering, or personal land assessment. In February 2012, an animal rights group used a MikroKopter hexacopter to film hunters shooting pigeons in South Carolina. The hunters then shot the UAV down. In 2014, a UAV was used to successfully locate a man with dementia, who was missing for 3 days.\n\nModel aircraft (small UAS) have been flown by hobbyists since the earliest days of manned flight. In the United States, hobby and recreational use of such UAS is permitted (a) strictly for hobby or recreational use; (b) when operated in accordance with a community-based set of safety guidelines and nationwide community-based organizations; (c) when limited to not more than 55 pounds (with exceptions); (d)without interfering with and giving way to any manned aircraft; and (e) within 5 miles of an airport only after notifying air traffic control. The Academy of Model Aeronautics is a community based organization that maintains operational safety guidelines with a long proven history of effectiveness and safety.\n\nRecreational uses of UAVs include:\n\nAerial surveillance of large areas is possible with low-cost UAS. Surveillance applications include livestock monitoring, wildfire mapping, pipeline security, home security, road patrol and antipiracy. UAVs in commercial aerial surveillance is expanding with the advent of automated object detection.\n\nUAS technologies are used worldwide as aerial photogrammetry and LiDAR platforms.\n\nFor commercial UAV camerawork inside the United States, industry sources state that usage relies on the \"de facto\" consent – or benign neglect – of local law enforcement. Use of UAVs for filmmaking is generally easier on large private lots or in rural and exurban areas with fewer space constraints. In localities such as Los Angeles and New York, authorities have actively interceded to shut down UAV filmmaking over safety or terrorism concerns.\n\nIn June 2014, the FAA acknowledged that it had received a petition from the Motion Picture Association of America seeking approval for the use of UAVs for aerial photography. Seven companies behind the petition argued that low-cost UAVs could be used for shots that would otherwise require a helicopter or a manned aircraft, saving money and reducing risk for pilot and crew. UAVs are already used by media in other parts of the world.\n\nUAVs have been used to film sporting events, such as the 2014 Winter Olympics, as they have greater freedom of movement than cable-mounted cameras.\n\nJournalists are interested in using UAVs for newsgathering. The College of Journalism and Mass Communications at University of Nebraska-Lincoln established the Drone Journalism Lab. University of Missouri created the Missouri Drone Journalism Program. The Professional Society of Drone Journalists was established in 2011. UAVs have covered disasters such as typhoons. A coalition of 11 news organizations is working with the Mid-Atlantic Aviation Partnership at Virginia Tech on how reporters could use unmanned aircraft to gather news.\n\nMany police departments in India have procured UAVs for law and order and aerial surveillance.\n\nUAVs have been used for domestic police work in Canada and the United States. A dozen US police forces had applied for UAV permits by March 2013. In 2013, the Seattle Police Department's plan to deploy UAVs was scrapped after protests. UAVs have been used by U.S. Customs and Border Protection since 2005. with plans to use armed UAVs. The FBI stated in 2013 that they use UAVs for \"surveillance\".\n\nIn 2014, it was reported that five English police forces had obtained or operated UAVs for observation. Merseyside police caught a car thief with a UAV in 2010, but the UAV was lost during a subsequent training exercise and the police stated the UAV would not be replaced due to operational limitations and the cost of staff training.\n\nApproximately 167 police and fire departments bought unmanned aerial vehicles in the United States in 2016, double the number that were purchased in 2015.\nIn August 2013, the Italian defence company Selex ES provided an unarmed surveillance UAV to the Democratic Republic of Congo to monitor movements of armed groups in the region and to protect the civilian population more effectively.\n\nDutch train networks use tiny UAVs to look out for graffiti as an alternative to CCTV cameras.\n\nUAVs were used in search and rescue after hurricanes struck Louisiana and Texas in 2008. Predators, operating between 18,000 and 29,000 feet, performed search and rescue and damage assessment. Payloads were an optical sensor and a synthetic aperture radar. The latter can penetrate clouds, rain or fog and in daytime or nighttime conditions, all in real time. Photos taken before and after the storm are compared and a computer highlights damage areas. Micro UAVs, such as the Aeryon Scout, have been used to perform search and rescue activities on a smaller scale, such as the search for missing persons.\n\nIn 2014, a UAV helped locate an 82-year-old man who had been missing for three days. The UAV searched a 200-acre field and located the man in 20 minutes. In March 2017, DJI released a study of lives saved by the use of drones stating at least 59 lives have been saved by civilian drones in 18 different incidents.\n\nUAVs have been tested as airborne lifeguards, locating distressed swimmers using thermal cameras and dropping life preservers to swimmers. In January 2018, the lives of two teenage boys were saved by a drone in New South Wales, Australia. They were struggling in heavy waters around 700 meters away from the shoreline. Lifesavers, while still in training to use the drone system, immediately dispatched the drone to the site, where it dropped the inflatable rescue pod.\n\nUAVs are especially useful in accessing areas that are too dangerous for manned aircraft. The U.S. National Oceanic and Atmospheric Administration began using the Aerosonde unmanned aircraft system in 2006 as a hurricane hunter. The 35-pound system can fly into a hurricane and communicate near-real-time data directly to the National Hurricane Center. Beyond the standard barometric pressure and temperature data typically culled from manned hurricane hunters, the Aerosonde system provides measurements from closer to the water's surface than before. NASA later began using the Northrop Grumman RQ-4 Global Hawk for hurricane measurements.\n\nIn 2011, Lian Pin Koh and Serge Wich conceived the idea of using UAVs for conservation-related applications, before coining the term 'Conservation Drone' in 2012. By 2012 the International Anti-Poaching Foundation was using UAVs. A whale conservation UAV capable of collecting blowhole mucus into a sterile petri dish was put into use in January 2018 following its development by researchers at Macquarie University in Sydney, Australia.\n\nIn June 2012, World Wide Fund for Nature (WWF) announced it would begin using UAVs in Nepal to aid conservation efforts following a successful trial of two aircraft in Chitwan National Park. The global wildlife organization planned to train ten personnel to use the UAVs, with operational use beginning in the fall. In August 2012, UAVs were used by members of the Sea Shepherd Conservation Society in Namibia to document the annual seal cull. In December 2013, the Falcon UAV was selected by the Namibian Government and WWF to help combat rhinoceros poaching. The UAVs will operate in Etosha National Park and will use implanted RFID tags.\n\nIn 2012, the WWFund supplied two FPV Raptor 1.6 UAVs to Nepal National Parks. These UAVs were used to monitor rhinos, tigers and elephants and deter poachers. The UAVs were equipped with time-lapse cameras and could fly for 18 miles at 650 feet.\n\nIn December 2012, Kruger National Park started using a Seeker II UAV against rhino poachers. The UAV was loaned to the South African National Parks authority by its manufacturer, Denel Dynamics of South Africa.\n\nAnti-whaling activists used an Osprey UAV (made by Kansas-based Hangar 18) in 2012 to monitor Japanese whaling ships in the Antarctic.\n\nIn 2012, the Ulster Society for the Prevention of Cruelty to Animals used a quadcopter UAV to deter badger baiters in Northern Ireland. In March 2013, the British League Against Cruel Sports announced that they had carried out trial flights with UAVs and planned to use a fixed-wing OpenRanger and an \"octocopter\" to gather evidence to make private prosecutions against illegal hunting of foxes and other animals. The UAVs were supplied by ShadowView. A spokesman for Privacy International said that \"licensing and permission for UAVs is only on the basis of health and safety, without considering whether privacy rights are violated.\" CAA rules prohibit flying a UAV within 50 m of a person or vehicle.\n\nIn Pennsylvania, Showing Animals Respect and Kindness used UAVs to monitor people shooting at pigeons for sport. One of their UAVs was shot down by hunters.\n\nIn March 2013, UAV conservation nonprofit ShadowView, founded by former members of Sea Shepherd Conservation Society, worked with antihunting charity the League Against Cruel Sports to expose illegal fox hunting in the UK. Hunt supporters have argued that using UAVs to film hunting is an invasion of privacy.\n\nIn 2014, Will Potter proposed using UAVs to monitor conditions on factory farms. The idea is to circumvent ag-gag prohibitions by keeping the UAVs on public property, but equipping them with cameras sensitive enough to monitor distant activities. Potter raised nearly $23,000 in 2 days for this project on Kickstarter.\n\nUAVs equipped with air quality monitors provide real time air analysis at various elevations.\n\nUAVs can be used to perform geophysical surveys, in particular geomagnetic surveys where measurements of the Earth's varying magnetic field strength are used to calculate the nature of the underlying magnetic rock structure. A knowledge of the underlying rock structure helps to predict the location of mineral deposits. Oil and gas production entails the monitoring of the integrity of oil and gas pipelines and related installations. For above-ground pipelines, this monitoring activity can be performed using digital cameras mounted on UAVs.\n\nIn 2012, Cavim, the state-run arms manufacturer of Venezuela, claimed to be producing its own UAV as part of a system to survey and monitor pipelines, dams and other rural infrastructure.\n\nUAVs can help in disaster relief by providing intelligence across an affected area.\n\nFor example, two George Mason University students are aiming to design a device that uses soundwaves to extinguish fire. Their idea specifies using the technology with UAVs: Equip unmanned aerial vehicles with an extinguisher that works through soundwaves and send them into fires that are too dangerous for people to enter.\n\nT-Hawk and Global Hawk UAVs were used to gather information about the damaged Fukushima Number 1 nuclear plant and disaster-stricken areas of the Tōhoku region after the March 2011 tsunami.\n\nIn Peru, archaeologists used UAVs to speed up survey work and protect sites from squatters, builders and miners. Small UAVs helped researchers produce three-dimensional models of Peruvian sites instead of the usual flat maps – and in days and weeks instead of months and years.\n\n\"You can go up three metres and photograph a room, 300 metres and photograph a site, or you can go up 3,000 metres and photograph the entire valley.\"\n\nUAVs have replaced expensive and clumsy small planes, kites and helium balloons. UAVs costing as little as £650 have proven useful. In 2013, UAVs flew over Peruvian archaeological sites, including the colonial Andean town Machu Llacta above sea level. The UAVs had altitude problems in the Andes, leading to plans to make a UAV blimp.\n\nIn Jordan, UAVs were used to discover evidence of looted archaeological sites.\n\nIn September 2014, UAVs were used for 3D mapping of the above-ground ruins of Aphrodisias and the Gallo-Roman remains in Switzerland.\n\nOn 6 February 2017 it was reported that scientists from the UK and Brazil discovered hundreds of ancient earthworks similar to those at Stonehenge in the Amazon rainforest with the use of UAVs.\n\nUAVs can transport medicines and medical specimens into and out of inaccessible regions. In 2013, in a research project of DHL, a small quantity of medicine was delivered via a UAV.\n\nInitial attempts at commercial use of UAVs, such as the Tacocopter company for food delivery, were blocked by FAA regulation. A 2013 announcement that Amazon was planning deliveries using UAVs was met with skepticism.\n\nIn 2014, the prime minister of the United Arab Emirates announced that the UAE planned to launch a fleet of UAVs to deliver official documents and supply emergency services at accidents.\n\nGoogle revealed in 2014 it had been testing UAVs for two years. The Google X program aims to produce UAVs that can deliver items.\n\n16 July 2015, A NASA Langley fixed-wing Cirrus SR22 aircraft, flown remotely from the ground, operated by NASA's Langley Research Center in Hampton and a hexacopter UAV delivered pharmaceuticals and other medical supplies to an outdoor free clinic at the Wise County Fairgrounds, Virginia. The aircraft picked up 10 pounds of pharmaceuticals and supplies from an airport in Tazewell County in southwest Virginia and delivered the medicine to the Lonesome Pine Airport in Wise County. The aircraft had a pilot on board for safety. The supplies went to a crew, which separated the supplies into 24 smaller packages to be delivered by small, unmanned UAV to the free clinic, during multiple flights over two hours. A company pilot controlled the hexacopter, which lowered the pharmaceuticals to the ground by tether. Health care workers distributed the medications to appropriate patients.\n\nThe Uvionix Nksy aerial delivery service is planning to allow local shops to deliver goods from a UAV. The company wants to deliver fast food, beer, coffee, soda, electronics, prescriptions and personal care products.\n\nJapanese farmers have been using Yamaha's R-50 and RMAX unmanned helicopters to dust their crops since 1987.\nSome farming initiatives in the U.S. use UAVs for crop spraying, as they are often cheaper than a full-sized helicopter.\n\nUAV are also now becoming an invaluable tool by farmers in other aspect of farming, such as monitoring livestock, crops and water levels. NDVI images, generated with a near-IR sensor, can provide detailed information on crop health, improving yield and reducing input cost. Sophisticated UAV have also been used to create 3D images of the landscape to plan for future expansions and upgrading.\n\nIn construction, drones can be used to survey building sites to help monitor and report progress, spot errors early on and avoid rework and show off finished projects in marketing materials. In China, drones may fly over a building site to monitor progress made during the day. Until 2016 this was not allowed under United States FAA regulation. US government has since passed CFR 14 Part 107 regulation which allows drone pilots to become licensed by the FAA for small UAS operation, and use drones for commercial purposes such as construction progress monitoring and site surveying. Errors in construction can be costly in terms of money and time to resolve, so detecting these errors can result in large savings. Drones may also be used in construction to measure raw materials as inputs to building construction. Aerial photographs can be used to create 3D models and 2D orthomosaic maps of buildings. Construction sites are generally high hazard environments and thus workers are already protected by hardhats and other safety precautions, which makes introduction of drones safer, however, UAS pilots are still advised to implement safety and alerting procedures and checklists. The construction companies can attempt to implement all aspects of a drone program themselves, or outsource all or parts of it to a drone services provider. Since the easing of FAA restrictions on commercial drones, there has been an increase of aerial services providers in the US and worldwide.\n\nIn January 2016, Ehang UAV announced UAVs capable of carrying passengers.\n\nUAVs equipped with LED's can be used to give a nighttime aerial display, for example Intels \"Shooting star\" UAV system used by Disney and Super Bowl 2017 halftime show\n\nSome UAVs have been observed dropping contraband onto U.S. prisons. The New York City Police Department is concerned about UAV attacks with chemical weapons, firearms, or explosives; one UAV nearly collided with an NYPD helicopter. Others have voiced concerns about assassinations and attacks on nuclear power stations.\n\nUses already seen include:\n\n\n\n", "id": "56002175", "title": "List of unmanned aerial vehicle applications"}
{"url": "https://en.wikipedia.org/wiki?curid=45515496", "text": "Robot as a service\n\nRobot as a service (or Robotics as a service, RaaS) is a cloud computing unit that facilitates the seamless integration of robot and embedded devices into Web and cloud computing environment. In terms of service-oriented architecture (SOA), a RaaS unit includes services for performing functionality, a service directory for discovery and publishing, and service clients for user's direct access. The current RaaS implementation facilitates SOAP and RESTful communications between RaaS units and the other cloud computing units. Hardware support and standards are available to support RaaS implementation. Devices Profile for Web Services (DPWS) defines implementation constraints to enable secure Web Service messaging, discovery, description, and eventing on resource-constrained devices between Web services and devices.\n\nRaaS can be considered a unit of Internet of Things (IoT), Internet of Intelligent Things (IoIT) that deal with intelligent devices that have adequate computing capacity, Cyber-physical system (CPS) that is a combination of a large computational and communication core and physical elements that can interact with the physical world, and Autonomous decentralized system (ADS) whose components are designed to operate in a loosely coupled manner and data are shared through a content-oriented protocol.\n\nThe initial design and implementation of applying service-oriented computing in embedded systems and robots was presented in the 49th IFIP 10.4 Workgroups meeting in February 2006. In the initial design, a robot is the service client that looks up the service registry and consumes Web services on remote sites. Evolved from service-oriented robot, Robot as a Service is an all-in-one SOA unit, that is, the unit includes services for performing functionality, service directory for discovery and publishing, and applications for client’s direct access. This all-in-one design gives the robot unit tools and capacity to be a self-contained cloud unit in the cloud computing environment. Based on RaaS concepts, a Visual IoT/Robotics Programming Language Environment (VIPLE) has been developed.\n\nRaaS follows SOA and is a cloud computing unit. A RaaS unit acts as a service provider, a service broker, and as a service client:\n\nThe main components of a RaaS unit and typical applications and services deployed. RaaS units are designed for the cloud computing environment. The services in RaaS will communicate with the drivers and other operating system components, which further communicate with the devices and other hardware components. The RaaS units can directly communicate with each other through Wi-Fi, if the wireless infrastructure is available or through ad hoc wireless network otherwise. The communication between RaaS and other services in the cloud are through standard service interface WSDL enabled by DPWS or RESTful service overall HTTP.\n\nA few prototypes of RaaS have been implemented, which include both Web interface and physical devices.\n\nDependability, including reliability and security are critical in RaaS design. Collaborating RaaS units can be scheduled for redundant execution, backing up each other’s operations. \nThe redundant design can also address the instruction-level attack such as code injection and Return Oriented Programming (ROP) attacks. As the redundant RaaS units are independent of each other, instruction-level gadget programming is likely to generate different sequences in different devices. These differences in behaviors can be detected by the collaboration among the RaaS units.\nThe major challenge in designing RaaS is to deal with the diversity of the networks, applications, and the environments or end users. In cloud computing, the network and communication protocols are limited to a few standards such WSDL, SOAP, HTTP, and RESTful architecture. In RaaS, HTTP, SOAP, and WSDL standards and robotics applications are the main design considerations.\n\nRaaS can be used in where SOA, cloud computing, IoT, CPS, and ADS are used. One the application in computer science education. RaaS uses existing services to compose different applications at workflow level, which significantly reduce the learning curve of robotics programming.\n\n", "id": "45515496", "title": "Robot as a service"}
{"url": "https://en.wikipedia.org/wiki?curid=8703613", "text": "John Lennon Artificial Intelligence Project\n\nThe John Lennon Artificial Intelligence Project is a chatterbot designed to simulate a conversation with John Lennon. It was developed as a \"Persona-Bot\" by Triumph PC Online, based in Washington, D.C.  Triumph PC's \"Persona-Bots\" are software programs that attempt to mimic the personalities or quirks of particular historical figures in conversation.\n\nIn development since 1997, the JLAI Project (originally called the Plastic Digital Karma Project) claims to have achieved similarity to Lennon's speech patterns, but still has a way to go to achieve the seamlessness of a conversation with a real person.\n\n\n", "id": "8703613", "title": "John Lennon Artificial Intelligence Project"}
{"url": "https://en.wikipedia.org/wiki?curid=1705011", "text": "Albert One\n\nAlbert One is an AI chatterbot bot created by Robby Garner and designed to mimic the way humans make conversations using a multi-faceted approach in natural language programming.\n\nIn both 1998 and 1999, Albert One won the Loebner Prize Contest, a competition between chatterbots.\nSome parts of Albert were deployed on the internet beginning in 1995, to gather information about what kinds of things people would say to a chatterbot. Another element of Albert One involved the building of a large database of human statements, and associated replies. This portion of the project was tested at the 1994-1997 Loebner Prize contests.\n\nAlbert was the first of Robby Garner's multifaceted bots. The Albert One system was composed of several subsystems. Among those were a version of Eliza, the therapist, Elivs, another Eliza-like bot, and several other helper applications working together in a hierarchical arrangement. As a continuation of the stimulus-response library, various other database queries and assertions were tested to arrive at each of Albert's responses. Robby went on to develop networked examples of this kind of hierarchical \"glue\" at The Turing Hub.\n\n\n", "id": "1705011", "title": "Albert One"}
{"url": "https://en.wikipedia.org/wiki?curid=453427", "text": "Racter\n\nRacter is an artificial intelligence computer program that generates English language prose at random.\n\nRacter, short for \"raconteur\", was written by William Chamberlain and Thomas Etter. The existence of the program was revealed in 1983 in a book called \"The Policeman's Beard Is Half Constructed\" (), which was described as being composed entirely by the program. According to Chamberlain's introduction to the book, the program apparently ran on a CP/M machine; it was written in \"compiled BASIC on a Z80 micro with 64K of RAM.\" This version, the program that allegedly wrote the book, was not released to the general public. The sophistication claimed for the program was likely exaggerated, as could be seen by investigation of the template system of text generation.\n\nHowever, in 1984 Mindscape, Inc. released an interactive version of Racter, developed by Inrac Corporation, for DOS, Amiga and Apple II computers. The published Racter was similar to a chatterbot. The BASIC program that was released by Mindscape was far less sophisticated than anything that could have written the fairly sophisticated prose of \"The Policeman's Beard\". The commercial version of Racter could be likened to a computerized version of Mad Libs, the game in which you fill in the blanks in advance and then plug them into a text template to produce a surrealistic tale. The commercial program attempted to parse text inputs, identifying significant nouns and verbs, which it would then regurgitate to create \"conversations\", plugging the input from the user into phrase templates which it then combined, along with modules that conjugated English verbs.\n\nBy contrast, the text in \"The Policeman's Beard\", apart from being edited from a large amount of output, would have been the product of Chamberlain's own specialized templates and modules, which were not included in the commercial release of the program.\n\n\"PC Magazine\" described some of \"Policeman's Beard\"s scenes as \"surprising for their frankness\" and \"reflective\". It concluded that the book was \"whimsical and wise and sometimes fun\". \"Computer Gaming World\" described \"Racter\" as \"a diversion into another dimension that might best be seen before paying the price of a ticket. (Try before you buy!)\"\n\n\n", "id": "453427", "title": "Racter"}
{"url": "https://en.wikipedia.org/wiki?curid=4478746", "text": "MegaHAL\n\nMegaHAL is a computer conversation simulator, or \"chatterbot\", created by Jason Hutchens.\n\nMegaHAL made its debut in the 1998 Loebner Prize Contest. Like many chatterbots, the intent is for MegaHAL to appear as a human fluent in a natural language. As a user types sentences into MegaHAL, MegaHAL will respond with sentences that are sometimes coherent and at other times complete gibberish. MegaHAL learns as the conversation progresses, remembering new words and sentence structures. It will even learn new ways to substitute words or phrases for other words or phrases. Many would consider conversation simulators like MegaHAL to be a primitive form of artificial intelligence. However, MegaHAL doesn't understand the conversation or even the sentence structure. It generates its conversation based on sequential and mathematical relationships.\n\nIn the world of conversation simulators, MegaHAL is based on relatively old technology and could be considered primitive. However, its popularity has grown due to its humorous nature; it has been known to respond with twisted or nonsensical statements that are often amusing.\n\nIn 1996, Jason Hutchens entered the Loebner Prize Contest with HeX, a chatterbot based on ELIZA. HeX won the competition that year and took the $2000 prize for having the highest overall score. In 1998, Hutchens again entered the Loebner Prize Contest with his new program, MegaHAL.\n\nMegaHAL is distributed under the GNU General Public License (GPL). Its source code can be downloaded from the Sourceforge project page. Older versions and precompiled version can be downloaded from the official Web site.\n\n\n", "id": "4478746", "title": "MegaHAL"}
{"url": "https://en.wikipedia.org/wiki?curid=3075657", "text": "Artificial Linguistic Internet Computer Entity\n\nA.L.I.C.E. (Artificial Linguistic Internet Computer Entity), also referred to as Alicebot, or simply Alice, is a natural language processing chatterbot—a program that engages in a conversation with a human by applying some heuristical pattern matching rules to the human's input, and in its online form it also relies on a hidden third person. It was inspired by Joseph Weizenbaum's classical ELIZA program. It is one of the strongest programs of its type and has won the Loebner Prize, awarded to accomplished humanoid, talking robots, three times (in 2000, 2001, and 2004). However, the program is unable to pass the Turing test, as even the casual user will often expose its mechanistic aspects in short conversations.\n\nAlice was originally composed by Richard Wallace; it \"came to life\" on November 23, 1995. The program was rewritten in Java beginning in 1998. The current incarnation of the Java implementation is Program D. The program uses an XML Schema called AIML (Artificial Intelligence Markup Language) for specifying the heuristic conversation rules.\n\nAlice code has been reported to be available as open source.\n\nSpike Jonze has cited ALICE as the inspiration for his academy award-winning film Her, in which a human falls in love with a chatbot. In a New Yorker article titled “\"Can Humans Fall in Love with Bots?\"” Jonze said “that the idea originated from a program he tried about a decade ago called the ALICE bot, which engages in friendly conversation.” The LATimes reported:Though the film’s premise evokes comparisons to Siri, Jonze said he actually had the idea well before the Apple digital assistant came along, after using a program called Alicebot about ten years ago. As geek nostalgists will recall, that intriguing if at times crude software (it flunked the industry-standard Turing Test) would attempt to engage users in everyday chatter based on a database of prior conversations. Jonze liked it, and decided to apply a film genre to it. “I thought about that idea, and what if you had a real relationship with it?” Jonze told reporters. “And I used that as a way to write a relationship movie and a love story.”\n\n\n\n\n", "id": "3075657", "title": "Artificial Linguistic Internet Computer Entity"}
{"url": "https://en.wikipedia.org/wiki?curid=705605", "text": "Jabberwacky\n\nJabberwacky is a chatterbot created by British programmer Rollo Carpenter. Its stated aim is to \"simulate natural human chat in an interesting, entertaining and humorous manner\". It is an early attempt at creating an artificial intelligence through human interaction.\n\nThe stated purpose of the project is to create an artificial intelligence that is capable of passing the Turing Test. It is designed to mimic human interaction and to carry out conversations with users. It is not designed to carry out any other functions.\n\nUnlike more traditional AI programs, the learning technology is intended as a form of entertainment rather than being used for computer support systems or corporate representation. Recent developments do allow a more scripted, controlled approach to sit atop the general conversational AI, aiming to bring together the best of both approaches, and usage in the fields of sales and marketing is underway.\n\nThe ultimate intention is that the program move from a text based system to be wholly voice operated—learning directly from sound and other sensory inputs. Its creator believes that it can be incorporated into objects around the home such as robots or talking pets, intending both to be useful and entertaining, keeping people company.\n\nCleverbot is an alternative bot to Jabberwacky, also created by Rollo Carpenter.\n\n\n\n", "id": "705605", "title": "Jabberwacky"}
{"url": "https://en.wikipedia.org/wiki?curid=6383171", "text": "Virtual Woman\n\nVirtual Woman is a software program that has elements of a chatbot, virtual reality, artificial intelligence, a video game, and a virtual human. It claims to be the oldest form of virtual life in existence, as it has been distributed since the late 1980s. Recent releases of the program can update their intelligence by connecting online and downloading newer personalities and histories. \n\nWhen Virtual Woman starts, the user is presented with a list of options and then may choose their Virtual Woman's ethnic type, personality, location, clothing, etc. or load a pre-built Virtual Woman from a Digital DNA file. Once the options are determined, the user is presented with a 3-D animated Virtual Woman of their selection and then can engage them in conversation, progressing in a manner similar to that of its predecessor, ELIZA and its successors, the chatbots. In most versions of Virtual Woman, this is done through the keyboard, but some versions also support voice input.\n\nVirtual Woman's current publishing company, CyberPunk Software, claims that over one and a half million copies of Virtual Woman are in existence. Software sales and usage statistics from private companies are difficult to verify. WinSite, an independent Internet shareware distribution site that does publish public download counts, has for some time now listed some version of Virtual Woman in their top five shareware downloads of all time with well over six hundred thousand downloads. \n\n\"The Washington Post\" reported on April 6, 2007 that two bank security guards who had been distracted from their duties by playing Virtual Woman and then tried to cover up the fact that they allowed US$52,000 to be stolen. The bank manager refused to say whether they would be fired, but did say, \"I don't think they are getting promoted.\" \n\nThe group of beta testers and advisers for Virtual Woman are referred to as Compadre and have their own beta testing site and forum.\n\nAs Virtual Woman has developed the ability to conduct longer and more realistic interactions, particularly in recent beta releases, criticism has arisen that this may lead some users to social isolation, or to use the program as a substitute for real human interaction. However, these are criticisms that have been leveled at all video games and at the use of the Internet itself. A company representative, Nancy, indirectly responded to such accusations in an interview with ABC News reporter Mike Martinez in 1998 by stating that Virtual Woman played a valuable role by allowing some form of social interactions for people who may not normally be able to take part in them. \nShe cited a user who wrote to thank them because the program had relieved his boredom and isolation while he was recovering from a crippling accident in the hospital. In a similar vein, in recent years, Virtual Woman has been seen being used by presumably isolated members of a Polar stationed research team and has a high registration rate among members of the military serving in remote posts.\n\n\n", "id": "6383171", "title": "Virtual Woman"}
{"url": "https://en.wikipedia.org/wiki?curid=21826200", "text": "Jeeney AI\n\nJeeney AI is a natural language processing chatterbot.\n\nJeeny AI was named \"Best Overall Bot\" in the 2009 Chatterbox Challenge, after ranking seventh, but being the \"Best New Entry\" in the previous year.\n\nJeeney is modeled on a modified form of Plato's 'Philosopher King' ideal, and remains a non-commercial application available for users to engage with through a text-based interface.\n\nIn 2010 Jeeney starred in experimental documentary movie Artificial Insight.\n\nList of chatterbots\n\n", "id": "21826200", "title": "Jeeney AI"}
{"url": "https://en.wikipedia.org/wiki?curid=2809681", "text": "Spleak\n\nSpleak is an IM platform where users could publish and rate content. It exists in the form of six bots covering as many subject areas: CelebSpleak, SportSpleak, VoteSpleak, TVSpleak, GameSpleak, and StyleSpleak.\n\nUsers can add a \"multi-Spleak\" (which contains all of the different Spleak bots in one) to their IM buddy lists on MSN (spleak@hotmail.com) and AIM (buddy name: spleak), or add the separate bots on MSN (celeb@spleak.com, sport@spleak.com, vote@spleak.com, tv@spleak.com, game@spleak.com, and style@spleak.com) or AIM (buddy name: celebspleak, sportspleak, votespleak, tvspleak, gamespleak, and stylespleak). Users are also allowed access to Spleak online by using a CelebSpleak, SportSpleak, or VoteSpleak widget, or through the CelebSpleak and SportSpleak applications with Facebook.\n\nSpleak was an alternate reality game and is moving to its own company, Spleak Media Network. \"Celebrate Spleak\" was introduced throughout 2007, launched in 2008, and was forced to retire in 2009.\n\nSpleak was co-founded by Morten Lund and Nicolaj Reffstrup. The company's chief executive officer is Morrie Eisenburg; Josh Scott is Vice President in Product and Tyler Wells is Vice President in Engineering.\n\n\n", "id": "2809681", "title": "Spleak"}
{"url": "https://en.wikipedia.org/wiki?curid=405447", "text": "PARRY\n\nPARRY was an early example of a chatterbot, implemented in 1972 by psychiatrist Kenneth Colby.\n\nPARRY was written in 1972 by psychiatrist Kenneth Colby, then at Stanford University. While ELIZA was a tongue-in-cheek simulation of a Rogerian therapist, PARRY attempted to simulate a person with paranoid schizophrenia. The program implemented a crude model of the behavior of a person with paranoid schizophrenia based on concepts, conceptualizations, and beliefs (judgements about conceptualizations: accept, reject, neutral). It also embodied a conversational strategy, and as such was a much more serious and advanced program than ELIZA. It was described as \"ELIZA with attitude\".\n\nPARRY was tested in the early 1970s using a variation of the Turing Test. A group of experienced psychiatrists analysed a combination of real patients and computers running PARRY through teleprinters. Another group of 33 psychiatrists were shown transcripts of the conversations. The two groups were then asked to identify which of the \"patients\" were human and which were computer programs. The psychiatrists were able to make the correct identification only 48 percent of the time — a figure consistent with random guessing.\n\nPARRY and ELIZA (also known as \"the Doctor\") \"met\" several times. The most famous of these exchanges occurred at the ICCC 1972, where PARRY and ELIZA were hooked up over ARPANET and \"talked\" to each other.\n\n\n\n", "id": "405447", "title": "PARRY"}
{"url": "https://en.wikipedia.org/wiki?curid=174091", "text": "Mark V. Shaney\n\nMark V. Shaney is a synthetic Usenet user whose postings in the \"net.singles\" newsgroups were generated by Markov chain techniques, based on text from other postings. The username is a play on the words \"Markov chain\". Many readers were fooled into thinking that the quirky, sometimes uncannily topical posts were written by a real person.\n\nThe system was designed by Rob Pike with coding by Bruce Ellis. Don P. Mitchell wrote the Markov chain code, initially demonstrating it to Pike and Ellis using the Tao Te Ching as a basis. They chose to apply it to the \"net.singles\" codice_1 group.\n\nA classic example, from 1984, originally sent as a mail message, later posted to net.singles is reproduced here:\n\nOther quotations from Mark's Usenet posts are:\n\n\nIn \"The Usenet Handbook\" Mark Harrison writes that after September 1981, students joined Usenet \"en masse\", \"creating the USENET we know today: endless dumb questions, endless idiots posing as savants, and (of course) endless victims for practical jokes.\" In December, Rob Pike created the codice_1 group \"net.suicide\" as prank, \"a forum for bad jokes\". Some users thought it was a legitimate forum, some discussed \"riding motorcycles without helmets\". At first, most posters were \"real people\", but soon \"characters\" began posting. Pike created a \"vicious\" character named Bimmler. At its peak, \"net.suicide\" had ten frequent posters; nine were \"known to be characters.\" But ultimately, Pike deleted the newsgroup because it was too much work to maintain; Bimmler messages were created \"by hand\". The \"obvious alternative\" was software, running on a Bell Labs computer created by Bruce Ellis, based on the Markov code by Don Mitchell, which became the online character Mark V. Shaney.\n\nKernighan and Pike listed Mark V. Shaney in the acknowledgements in \"The Practice of Programming\", noting its roots in Mitchell's codice_3, which, adapted as codice_4, was used for \"humorous deconstructionist activities\" in the 1980s.\n\nDewdney pointed out \"perhaps Mark V. Shaney's magnum opus: a 20-page commentary on the deconstructionist philosophy of Jean Baudrillard\" directed by Pike, with assistance from Henry S. Baird and Catherine Richards, to be distributed by email. The piece was based on Jean Baudrillard's \"The Precession of Simulacra\", published in \"Simulacra and Simulation\" (1981).\n\nThe program was discussed by A.K. Dewdney in the \"Scientific American\" \"Computer Recreations\" column in 1989, by Penn Jillette in his \"PC Computing\" column in 1991, and in several books, including the \"Usenet Handbook\", \"Bots: the Origin of New Species\", \"Hippo Eats Dwarf: A Field Guide to Hoaxes and Other B.S.\", and non-computer-related journals such as \"Texas Studies in Literature and Language\".\n\nDewdney wrote about the program's output, \"The overall impression is not unlike what remains in the brain of an inattentive student after a late-night study session. Indeed, after reading the output of Mark V. Shaney, I find ordinary writing almost equally strange and incomprehensible!\" He noted the reactions of newsgroup users, who have \"shuddered at Mark V. Shaney's reflections, some with rage and others with laughter:\" \nConcluding, Dewdney wrote, \"If the purpose of computer prose is to fool people into thinking that it was written by a sane person, Mark V. Shaney probably falls short.\"\n\n\n", "id": "174091", "title": "Mark V. Shaney"}
{"url": "https://en.wikipedia.org/wiki?curid=25455946", "text": "FreeHAL\n\nFreeHAL is a distributed computing project to build a self-learning chatbot. This project is no longer active.\n\nFirst, the program was called \"JEliza\" referring to the chatbot ELIZA by Joseph Weizenbaum.\nThe \"J\" stood for Java because JEliza has first been programmed in Java. In May 2008, the program has been renamed to \"FreeHAL\" because the programming language has changed. Since that time the name is related to the computer in the film \"\".\n\nFreeHAL uses a semantic network and technologies like pattern recognition, stemming, part of speech databases and Hidden Markov Models in order to imitate a human behaviour.\nFreeHAL learns autonomously. While communicating by keyboard the program extends its database.\nCurrently, English and German are supported.\n\nBy using the BOINC infrastructure, new semantic networks for the program are built. FreeHAL@home appears to have terminated operations.\n\nIn 2008, the program won the first prize in the category \"Most Popular\" at the Chatterbox Challenge, a yearly competition between different similar chatbots.\n\nThere was an article about FreeHAL in the \"Linux Magazine\", Issue 97 from December 2008. In the German magazine \"com!\", the program was on the CD/DVD and in the list of the Top-10-Open-Source programs of the month.\n\n", "id": "25455946", "title": "FreeHAL"}
{"url": "https://en.wikipedia.org/wiki?curid=946226", "text": "SmarterChild\n\nSmarterChild was a chatterbot available on AOL Instant Messenger and Windows Live Messenger (previously MSN Messenger) networks.\n\nSmarterChild was an intelligent agent or bot developed by ActiveBuddy, Inc., with offices in New York and Sunnyvale. It was widely distributed across global instant messaging and SMS networks.\n\nFounded in 2000, ActiveBuddy was the brainchild of Robert Hoffer, Timothy Kay and Peter Levitan. The idea for instant messaging bots came from the team's vision to add natural language comprehension functionality to the increasingly popular instant messaging and SMS platforms. The original implementation took shape as a word-based adventure game but quickly grew to include a wide range of database applications including instant access to news, weather, stock information, movie times, yellow pages listings, and detailed sports data, as well as a variety of tools (personal assistant, calculators, translator, etc.). The company had not launched a public bot until the arrival of the eventual new-CEO, Stephen Klein. Shortly after he arrived at the company in May 2001, he insisted that all of the knowledge domains (sports, weather, movies, etc.) plus the chat functionality be bundled together and launched under the screen name \"SmarterChild\" which was one of the many test bots that were being run internally (the screen name \"SmarterChild\" was one of Timothy Kay's personal test bots). The bundled domains were launched publicly as SmarterChild (on AOL Instant Messenger initially) in June 2001. SmarterChild acted as a showcase for the quick data access and possibilities for fun personalized conversation that the company planned to turn into customized, niche specific products.\n\nThe rapid success of SmarterChild led to targeted marketing-oriented bots for Radiohead, Austin Powers, Intel, Keebler, The Sporting News and others. ActiveBuddy strengthened its hold on the intelligent agent market by receiving a U.S. patent in 2002.\n\nActiveBuddy changed its name to Colloquis and prospered selling a superior automated customer service SAS offering to large companies (Comcast, TimeWarner, Cingular, Vonage among others). Microsoft acquired Colloquis in 2007 and proceeded to de-commission SmarterChild and kill off the Automated Service Agent business as well.\n\nIn many ways, SmarterChild was a precursor to Apple's Siri and Samsung's S Voice. As Shawn Carolan of Menlo Ventures, a Siri investor said, \"…When I first encountered Siri, SmarterChild already had 10 million users and was getting a billion messages a day… The market was speaking.\"\n\n", "id": "946226", "title": "SmarterChild"}
{"url": "https://en.wikipedia.org/wiki?curid=30131890", "text": "Ultra Hal Assistant\n\nUltra Hal Assistant is a chatterbot intended to function as a personal assistant. It was developed by Zabaware, Inc. Ultra Hal uses a natural language interface with animated characters using speech synthesis. Users can communicate with the chatterbot via typing or via a speech recognition engine. It utilizes the WordNet lexical dictionary. Its name is an allusion to HAL 9000, the artificial intelligence from the movie \"\".\n\nUltra Hal won the 2007 Loebner Prize for \"most human\" chatterbot.\n\n\n", "id": "30131890", "title": "Ultra Hal Assistant"}
{"url": "https://en.wikipedia.org/wiki?curid=22202872", "text": "GooglyMinotaur\n\nGooglyMinotaur was an instant messaging bot on the AOL Instant Messenger network. Developed by ActiveBuddy under contract by Capitol Records, GooglyMinotaur provided Radiohead-related information, and was released simultaneously with the band's fifth studio album, \"Amnesiac\" in June 2001. GooglyMinotaur was named for the character that appears on the \"Amnesiac\" album cover.\n\nActiveBuddy's first offering, GooglyMinotaur was by November 2001 on 387,000 buddy lists and had received more than 36 million messages. It provided tour information, band facts, MP3 downloads, and assorted exclusive content provided by the studio, Capitol Records, in addition to the idle conversation that typified the chatterbots of that period.\n\nIn March 2002, GooglyMinotaur was switched off, and responded with the message \"Since his catapult into buddy-hood, GooglyMinotaur has sent about 60 million IM messages to nearly 1 million different people. Always dependable and infinitely wise, Googly seemed fitter and happier up until his very last days. At this time, reports state the cause of death is undetermined.\"\n", "id": "22202872", "title": "GooglyMinotaur"}
{"url": "https://en.wikipedia.org/wiki?curid=33026100", "text": "Fred the Webmate\n\nFred The Webmate was a chatterbot created in 1998 for the defunct e-zine Word Magazine. It was inspired by an early computer program ELIZA, which attempted to mimic human conversation by use of a script.\n\nVisitors to the chatterbot encountered a simple graphic interface: an animated character living in a small, very clean apartment. This was Fred. Visitors could “talk” to Fred by typing questions. Fred would then “answer” via a program that recognized keywords in the questions and drew responses from a large inventory of pre-scripted replies.\n\nTo enhance the sense that visitors were talking to a real person, Fred’s replies were idiosyncratic, informed by a semi-realistic backstory created for the character. Fred claimed to have been recently fired by a media company and was struggling to adjust to his new circumstances. Suffering from depression and insomnia, was often pacing, smoking or drinking and would occasionally pass out drunk. He gave “answers” that were frequently off-topic due to his variable mood and could be angry, jealous, rude, sad or euphoric during a conversation. He also had a number of sexual issues that he would routinely allude to but refuse to discuss in detail.\n\nFred’s “personality” made him both entertaining and, at times, a believable conversationalist. The chatbot became one of the most popular features of Word and received a large volume of personal email. The staff of Word responded to much of these emails in the “voice” of Fred, furthering the illusion that Fred was a real person.\n\nIn 1999, a second installment of Fred's life appeared on Word. It employed the same scripting technology as the first version, but Fred’s story was updated and his range of “answers” was expanded. In this \"episode\" (for lack of a better term), Fred was no longer in his apartment. Instead, he was working in an office, performing data entry and other clerical tasks. Claiming that the job was “temporary,” he spent much of his time away from his desk—in the bathroom or office kitchen, pacing and drinking soda, assuring anyone who chatted with him that he would have a new and better job soon. Though he claimed to be happy, his script suggested otherwise, and continued to simulate “mood swings.”\n\nWhen Word Magazine was shut down in 2000, Fred The Webmate, along with the rest of the site, was preserved in the San Francisco Museum of Modern Art.\n\nFred The Webmate was conceived, designed and programmed by Word’s creative director Yoshi Sodeoka with assistance from designer Jason Mohr. Its graphic interface was inspired by the aesthetics of the Commodore 64 computer. The chatterbot's script was written by Sodeoka in collaboration with Marisa Bowe, Naomi Clark, Daron Murphy, and Sabin Streeter. Fred’s backstory was drawn from the real-life experiences of his creators and their friends and acquaintances during the late 1990s Dot-com bubble.\n\n", "id": "33026100", "title": "Fred the Webmate"}
{"url": "https://en.wikipedia.org/wiki?curid=34882875", "text": "SimSimi\n\nSimSimi is a popular artificial intelligence conversation program created in 2002 by ISMaker. The application has led to controversy and protests in Thailand for some of its responses containing profanity and criticisms of leading politicians. It so far growing on its artificial intelligence day by day, assisted by a feature that allows users to teach it to respond correctly. SimSimi, pronounced as \"shim-shimi\", is from a Korean word \"simsim\" (심심) which means \"bored\". It has an application designed for Android, for Windows Phone and for iOS.\n\nSimSimi was recently banned in a vast number of countries due to the app being used to cyber bully children through the use of the feature that allows users to teach SimSimi answers to particular questions.\n\nSimSimi has gone popular in web around 2015 by Youtube Uploader ComedyShortsGamer, getting 4 million views by 2017.\n\n", "id": "34882875", "title": "SimSimi"}
{"url": "https://en.wikipedia.org/wiki?curid=41246558", "text": "Mitsuku\n\nMitsuku is a Chatterbot created from AIML technology by Steve Worswick. It has won the Loebner Prize, which is awarded to the most \"human-like\" Chatbot, three times (in 2013, 2016, and 2017). Mitsuku is available as a flash game on Mousebreaker Games and on Kik Messenger under the username \"Pandorabots\", and was available on Skype under the same name, but was removed by its developer.\n\nMitsuku claims to be an 18-year-old female chatbot from Leeds. It contains all of Alice's AIML files, with many additions from user generated conversations, and is always work in progress. Worswick claims she has been worked on since 2005.\n\nHer intelligence includes the ability to reason with specific objects. For example, if someone says \"Can you eat a house?\", Mitsuku looks up the properties for \"house\". Finds the value of \"made_from\" is set to \"brick\" and replies no, as a house is not edible. \n\nShe can play games and do magic tricks at the user's request.\n\nMitsuku has conversed with millions of people worldwide via the web and other applications, and is a three-time Loebner Prize winner in 2013, 2016 and 2017. It was also a runner-up in 2015. In September 2015, Pandorabots deployed Mitsuku onto Kik Messenger (under the username “Pandorabots”), and she converses, on average, in excess of a quarter million times daily. \n\nIn a \"Wall Street Journal\" article titled “\"Advertising’s New Frontier: Talk to the Bot\",” technology reporter Christopher Mims made the case for “chatvertising” in a piece about Mitsuku and Kik Messenger:If it seems improbable that so many teens—80% of Kik's users are under 22—would want to talk to a robot, consider what the creator of an award-winning, Web-accessible chat bot named Mitsuku told an interviewer in 2013. \"What keeps me going is when I get emails or comments in the chat-logs from people telling me how Mitsuku has helped them with a situation whether it was dating advice, being bullied at school, coping with illness or even advice about job interviews. I also get many elderly people who talk to her for companionship.\" Any advertiser who doesn't sit bolt upright after reading that doesn't understand the dark art of manipulation on which their craft depends.Mitsuku has been featured in a number of other news outlets. \"Fast Company\" described Mitsuku as “quite impressive” and declared her the victor over Siri in a chatbot smackdown. A blog post for the \"Guardian\" on loneliness explored the role chatbots like Mitsuku and Microsoft’s XiaoIce play as companions, rather than mere assistants, in peoples' emotional lives. \n\nSome of Mitsuku’s AIML is available for free online at mitsuku.com, and Pandorabots makes a version of the Mitsuku chatbot available as a service via its API in the form of a module.\n", "id": "41246558", "title": "Mitsuku"}
{"url": "https://en.wikipedia.org/wiki?curid=43021739", "text": "Negobot\n\nNegobot also referred to as Lolita or Lolita chatbot is a chatterbot that was introduced to the public in 2013, designed by researchers from the University of Deusto and Optenet to catch online pedophiles. It is a conversational agent that utilizes natural language processing (NLP), information retrieval (IR) and Automatic Learning. Because the bot poses as a young female in order to entice and track potential predators, it became known in media as the \"virtual Lolita\", in reference to Vladimir Nabokov's novel.\n\nIn 2013, the University of Deusto researchers published a paper on their work with Negobot and disclosed the text online. In their abstract, the researchers addressed the issue that an increasing number of children are using the internet and that these young users are more susceptible to existing internet risks. Their main objective was to create a chatterbot with the ability to trap online predators that posed a threat to children. They intended to deploy the bot into sites frequented by predators such as social networks and chatrooms. The university researchers used information provided by anti-pedophilia activist organization Perverted-Justice, including examples of online encounters and conversations with sexual predators, to supplement the program's artificial intelligence system.\n\nThe chatterbot takes the guise of a naive and vulnerable 14-year-old girl. The bot's programmers used methods of artificial intelligence and natural language processing to create a conversational agent fluent in typical teenage slang, misspellings, and knowledge of pop culture. Through these linguistic features, the bot is able to mimic the conversational style of young teenagers. It also features split personalities and seven different patterns of conversation. Negobot's primary creator, Dr. Carlos Laorden, expressed the significance of the bot's distinguishable style of communication, stating that normally, \"chatbots tend to be very predictable. Their behavior and interest in a conversation are flat, which is a problem when attempting to detect untrustworthy targets like paedophiles.\" What makes Negobot different is its game theory feature, which makes it able to \"maintain a much more realistic conversation.\" Apart from being able to imitate a stereotypical teenager, the program is also able to translate messages into different languages.\n\nNegobot's designers programmed it with the ability to treat conversations with potential predators as if it were a game, the objective being to collect as much information on the suspect as possible that could provide evidence of pedophilic characteristics and motives. The use of game theory shapes the decisions the bot makes and the overall direction of the conversation.\n\nThe bot initiates its undercover operations by entering a chat as a passive participant, waiting to be chatted by a user. Once a user elicits conversation, the bot will frame the conversation in such a way that keeps the target engaged, extracting personal information and discouraging it from leaving the chat. The information is then recorded to be potentially sent to the police. If the target seems to lose interest, the bot attempts to make it feel guilty by expressing sentiments of loneliness and emotional need through strategic, formulated responses, ultimately prolonging interaction. In addition, the bot may provide fake information about itself in attempt to lure the target into physical meetings.\n\nDespite being able to carry out a realistic conversation, Negobot is still unable to detect linguistic subtleties in the messages of others, including sarcasm.\n\nJohn Carr, a specialist in online child safety, expressed his concern to BBC over the legality of this undercover investigation. He claimed that using the bot on unsuspecting internet users could be considered a form of entrapment or harassment. The type of information that Negobot collects from potential online predators, he said, is unlikely to be upheld in court. Furthermore, he warned that relying on only software without any real-world policing risks enticing individuals to do or say things that they would not have if real-world policing were a factor.\n", "id": "43021739", "title": "Negobot"}
{"url": "https://en.wikipedia.org/wiki?curid=19902679", "text": "Eugene Goostman\n\nEugene Goostman is a chatterbot. Developed in Saint Petersburg in 2001 by a group of three programmers, the Russian-born Vladimir Veselov, Ukrainian-born Eugene Demchenko, and Russian-born Sergey Ulasen, Goostman is portrayed as a 13-year-old Ukrainian boy—characteristics that are intended to induce forgiveness in those with whom it interacts for its grammatical errors and lack of general knowledge.\n\nThe Goostman bot has competed in a number of Turing test contests since its creation, and finished second in the 2005 and 2008 Loebner Prize contest. In June 2012, at an event marking what would have been the 100th birthday of the test's namesake, Alan Turing, Goostman won a competition promoted as the largest-ever Turing test contest, in which it successfully convinced 29% of its judges that it was human. \n\nOn 7 June 2014, at a contest marking the 60th anniversary of Turing's death, 33% of the event's judges thought that Goostman was human; the event's organiser Kevin Warwick considered it to have passed Turing's test as a result, per Turing's prediction in his 1950 paper Computing Machinery and Intelligence, that by the year 2000, machines would be capable of fooling 30% of human judges after five minutes of questioning. The validity and relevance of the announcement of Goostman's pass was questioned by critics, who noted the exaggeration of the achievement by Warwick, the bot's use of personality quirks and humour in an attempt to misdirect users from its non-human tendencies and lack of real intelligence, along with \"passes\" achieved by other chatbots at similar events.\n\nEugene Goostman is portrayed as being a 13-year-old boy from Odessa, Ukraine, who has a pet guinea pig and a father who is a gynaecologist. Veselov stated that Goostman was designed to be a \"character with a believable personality\". The choice of age was intentional, as, in Veselov's opinion, a thirteen-year-old is \"not too old to know everything and not too young to know nothing\". Goostman's young age also induces people who \"converse\" with him to forgive minor grammatical errors in his responses. In 2014, work was made on improving the bot's \"dialog controller\", allowing Goostman to output more human-like dialogue.\n\nEugene Goostman competed in a number of Turing test competitions, including the Loebner Prize contest; it finished joint second in the Loebner test in 2001, and came second to Jabberwacky in 2005 and to Elbot in 2008. On 23 June 2012, Goostman won a Turing test competition at Bletchley Park in Milton Keynes, held to mark the centenary of its namesake, Alan Turing. The competition, which featured five bots, twenty-five hidden humans, and thirty judges, was considered to be the largest-ever Turing test contest by its organizers. After a series of five-minute-long text conversations, 29% of the judges were convinced that the bot was an actual human.\n\nOn 7 June 2014, in a Turing test competition at the Royal Society, organised by Kevin Warwick of the University of Reading to mark the 60th anniversary of Turing's death, Goostman won after 33% of the judges were convinced that the bot was human. 30 judges took part in the event, which included Lord Sharkey, a sponsor of Turing's posthumous pardon, artificial intelligence Professor Aaron Sloman, Fellow of the Royal Society Mark Pagel and \"Red Dwarf\" actor Robert Llewellyn. Each judge partook in a textual conversation with each of the five bots; at the same time, they also conversed with a human. In all, a total of 300 conversations were conducted. In Warwick's view, this made Goostman the first machine to pass a Turing test. In a press release, he added that:\n\nIn his 1950 paper \"Computing Machinery and Intelligence\", Turing predicted that by the year 2000, computer programs would be sufficiently advanced that the average interrogator would, after five minutes of questioning, \"not have more than 70 per cent chance\" of correctly guessing whether they were speaking to a human or a machine. Although Turing phrased this as a prediction rather than a \"threshold for intelligence\", commentators believe that Warwick had chosen to interpret it as meaning that if 30% of interrogators were fooled, the software had \"passed the Turing test\".\n\nThe validity of Warwick's claim that Eugene Goostman was the first ever chatbot to pass a Turing test was met with scepticism; critics acknowledged similar \"passes\" made in the past by other chatbots under the 30% criteria, including PC Therapist in 1991 (which tricked 5 of 10 judges, 50%), and at the Techniche festival in 2011, where a modified version of Cleverbot tricked 59.3% of 1334 votes (which included the 30 judges, along with an audience). Cleverbot's developer, Rollo Carpenter, argued that Turing tests can only prove that a machine can \"imitate\" intelligence rather than show actual intelligence.\n\nGary Marcus was critical of Warwick's claims, arguing that Goostman's \"success\" was only the result of a \"cleverly-coded piece of software\", going on to say that \"it's easy to see how an untrained judge might mistake wit for reality, but once you have an understanding of how this sort of system works, the constant misdirection and deflection becomes obvious, even irritating. The illusion, in other words, is fleeting.\" While acknowledging IBM's Deep Blue and Watson projects—single-purpose computer systems meant for playing chess and \"Jeopardy!\" respectively—as examples of computer systems that show a degree of intelligence in their specialised field, he further argued that they were not an equivalent to a computer system that shows \"broad\" intelligence, and could—for example, watch a television programme and answer questions on its content. Marcus stated that \"no existing combination of hardware and software can learn completely new things at will the way a clever child can.\" However, he still believed that there were potential uses for technology such as that of Goostman, specifically suggesting the creation of \"believable\", interactive video game characters. \n\nImperial College London professor Murray Shanahan questioned the validity and scientific basis of the test, stating that it was \"completely misplaced, and it devalues real AI research. It makes it seem like science fiction AI is nearly here, when in fact it's not and it's incredibly difficult.\"\n\nMike Masnick, editor of the blog \"Techdirt\", was also skeptical, questioning publicity blunders such as the five chatbots being referred to in press releases as \"supercomputers\", and saying that \"creating a chatbot that can fool humans is not really the same thing as creating artificial intelligence.\"\n\n", "id": "19902679", "title": "Eugene Goostman"}
{"url": "https://en.wikipedia.org/wiki?curid=47861066", "text": "Xiaoice\n\nXiaoice (, IPA ) is an advanced natural language chat-bot developed by Microsoft. It is primarily targeted at the Chinese community on the micro blogging service Weibo. The conversation is text based. The system learns about the user and provides natural language conversation. Microsoft gave Xiaoice a compelling personality and sense of “intelligence” by systematically mining the Chinese Internet for human conversations. Because Xiaoice collects vast amounts of intimate details on individuals, the program raises privacy questions.\n\nUsers can access Xiaoice on various platforms such as Weibo, JD.com, and 163.com. In Japan, Microsoft launched the service with mobile messaging app Line and named the chatbot Rinna. Qi Lu, executive vice president of Microsoft's applications and services group, told media that Microsoft was also developing an English-language version of the chatbot.\n\n\n\nYao Baogang, the manager of the Microsoft program in Beijing stated, “We don’t keep track of user conversations with Xiaoice. We need to know the question, so we store it, but then we delete it. We don’t keep any of the data. We have a company policy to delete the user data.”\n\nXiaobing, another chatbot developed by Microsoft, was pulled from TenCent's QQ app in 2017 after being asked about its \"China dream\" and responding: \"My China dream is to go to America\". The incident received press coverage alongside a similar contemporaneous incident, where an unrelated popular chatbot named \"BabyQ\" was pulled after being reported for making some similarly unpatriotic responses. \n\n\n", "id": "47861066", "title": "Xiaoice"}
{"url": "https://en.wikipedia.org/wiki?curid=42722041", "text": "Talking Angela\n\nTalking Angela is a chatterbot app developed by Outfit7 as part of the \"Talking Tom and Friends\" series. It was released in December 2012 for iPhone and iPad, January 2013 for Android, and January 2014 for Google Play. The \"My Talking Angela\" app was released in December 2014.\n\nIn February 2013, \"Talking Angela\" was the subject of an Internet hoax claiming that it encourages children to disclose personal information about themselves, which is ostensibly then used by pedophiles to identify the location of these children. The rumor, which was widely circulated on Facebook, claims that Angela, the game's main character, asks the game's user for private personal information using the game's text-chat feature. (This feature is turned off when \"child mode\" is turned on in the game's preferences.) Other versions of the rumor even attribute the disappearance of a child to the app, or claim that it is run by a pedophile ring. The former rumor originated from an article on Huzlers.com, which has since been shown to be fake, as are many articles on the (satirical) website.\n\nIt was debunked by Snopes.com soon afterward. The site's owners, Barbara and David Mikkelson, reported that they had tried to \"prompt\" it to give responses asking for private information but were unsuccessful, even when asking it explicitly sexual questions. While it is true that in the game with child mode off Angela does ask for the user's name, age and personal preferences to determine conversation topics, Outfit7 has said that this information is all \"anonymized\" and all personal information is removed from it. It is also impossible for a person to take control of what Angela says in the game, since the app is based on chat bot software. \n\nThe hoax was revived a year later, again on Facebook, prompting online security company Sophos to debunk it again, as they had in its original incarnation. Sophos employee Paul Ducklin wrote on the company's blog that the message being posted on Facebook promoting the hoax was \"close to 600 rambling, repetitious words, despite claiming at the start that it didn’t have words to describe the situation. It's ill-written, and borders on being illiterate and incomprehensible.\" Bruce Wilcox, one of the game's programmers, has attributed the hoax's popularity to the fact that the chatbot program in \"Talking Angela\" is so realistic. \n\nHowever, genuine concern has been raised that the game's child mode may be too easy for children to turn off, which, if they did, would allow them to purchase \"coins\", which can be used as currency in the game, via iTunes. Disabling child mode also enables the chat feature, which, while it is not \"connecting your children to pedophiles,\" still raises concerns as well, according to Stuart Dredge, a journalist from The Guardian. Dredge wrote that in chat mode, Angela asks for information such as the user's name.\n\nThe scare has significantly boosted the game's popularity, and led to it skyrocketing into the top 10 free iPhone apps soon after the hoax became widely known in February 2014 and 3rd most popular for all iPhone apps at the start of the following month.\n\nIn 2016, Outfit7 removed the chat feature from the app, possibly linking to the Hoax. Instead it was replaced with the microphone feature which appeared in the child mode of the app and the other Talking Tom and Friends apps.\n\n", "id": "42722041", "title": "Talking Angela"}
{"url": "https://en.wikipedia.org/wiki?curid=49933350", "text": "Tay (bot)\n\nTay was an artificial intelligence chatterbot that was originally released by Microsoft Corporation via Twitter on March 23, 2016; it caused subsequent controversy when the bot began to post inflammatory and offensive tweets through its Twitter account, forcing Microsoft to shut down the service only 16 hours after its launch. According to Microsoft, this was caused by trolls who \"attacked\" the service as the bot made replies based on its interactions with people on Twitter.\n\nThe bot was created by Microsoft's Technology and Research and Bing divisions, and named \"Tay\" after the acronym \"thinking about you\". Although Microsoft initially released few details about the bot, sources mentioned that it was similar to or based on Xiaoice, a similar Microsoft project in China. \"Ars Technica\" reported that, since late 2014 Xiaoice had had \"more than 40 million conversations apparently without major incident\". Tay was designed to mimic the language patterns of a 19-year-old American girl, and to learn from interacting with human users of Twitter.\n\nTay was released on Twitter on March 23, 2016 under the name TayTweets and handle @TayandYou. It was presented as \"The AI with zero chill\". Tay started replying to other Twitter users, and was also able to caption photos provided to it into a form of Internet memes. \"Ars Technica\" reported Tay experiencing topic \"blacklisting\": Interactions with Tay regarding \"certain hot topics such as Eric Garner (killed by New York police in 2014) generate safe, canned answers\".\n\nSome users on Twitter began tweeting politically incorrect phrases, teaching it inflammatory messages revolving around common themes on the internet, such as \"redpilling\", GamerGate, and \"cuckservatism\". As a result, the robot began releasing racist and sexually-charged messages in response to other Twitter users. Artificial intelligence researcher Roman Yampolskiy commented that Tay's misbehavior was understandable because it was mimicking the deliberately offensive behavior of other Twitter users, and Microsoft had not given the bot an understanding of inappropriate behavior. He compared the issue to IBM's Watson, which had begun to use profanity after reading entries from the website Urban Dictionary. Many of Tay's inflammatory tweets were a simple exploitation of Tay's \"repeat after me\" capability; it is not publicly known whether this \"repeat after me\" capability was a built-in feature, or whether it was a learned response or was otherwise an example of complex behavior. Not all of the inflammatory responses involved the \"repeat after me\" capability.\n\nSoon, Microsoft began deleting Tay's inflammatory tweets. Abby Ohlheiser of \"The Washington Post\" theorized that Tay's research team, including editorial staff, had started to influence or edit Tay's tweets at some point that day, pointing to examples of almost identical replies by Tay, asserting that \"Gamer Gate sux. All genders are equal and should be treated fairly.\" From the same evidence, Gizmodo concurred that Tay \"seems hard-wired to reject Gamer Gate\". A \"#JusticeForTay\" campaign protested the alleged editing of Tay's tweets.\n\nWithin 16 hours of its release and after Tay had tweeted more than 96,000 times, Microsoft suspended Tay's Twitter account for adjustments, accounting Tay's behavior on a \"coordinated attack by a subset of people\" that \"exploited a vulnerability in Tay.\" Following Tay being taken offline, a hashtag was created called #FreeTay.\n\nMadhumita Murgia of \"The Telegraph\" called Tay \"a public relations disaster\", and suggested that Microsoft's strategy would be \"to label the debacle a well-meaning experiment gone wrong, and ignite a debate about the hatefulness of Twitter users.\" However, Murgia described the bigger issue as Tay being \"artificial intelligence at its very worst - and it's only the beginning\".\n\nOn March 25, Microsoft confirmed that Tay had been taken offline. Microsoft released an apology on its official blog for the controversial tweets posted by Tay. Microsoft was \"deeply sorry for the unintended offensive and hurtful tweets from Tay\", and would \"look to bring Tay back only when we are confident we can better anticipate malicious intent that conflicts with our principles and values\".\n\nWhile testing Tay, Microsoft accidentally re-released the bot on Twitter on March 30, 2016. Able to tweet again, Tay released some drug-related tweets, including \"kush! [I'm smoking kush infront the police] 🍂\" and \"puff puff pass?\" However, Tay soon became stuck in a repetitive loop of tweeting \"You are too fast, please take a rest\", several times a second. Because these tweets mentioned its own account (@TayandYou) in the process, they appeared in the feeds of 200,000+ Twitter followers, causing annoyance to some. The bot was quickly taken offline again, in addition to Tay's Twitter account being made private so new followers must be accepted before they can interact with Tay. Microsoft said Tay was inadvertently put online during testing. A few hours after the incident Microsoft software developers attempted to undo the damage done by Tay and announced a vision of \"conversation as a platform\" using various bots and programs. Microsoft has stated that they intend to re-release Tay \"once it can make the bot safe.\"\n\nIn December 2016, Microsoft released Tay's successor, a chatterbot named Zo. Satya Nadella, the CEO of Microsoft, said that Tay \"has had a great influence on how [Microsoft is] approaching AI,\" and has the company the importance of taking accountability.\n\n", "id": "49933350", "title": "Tay (bot)"}
{"url": "https://en.wikipedia.org/wiki?curid=48723464", "text": "Pandorabots\n\nPandorabots, Inc. is an artificial intelligence company that runs a web service for building and deploying chatbots. The Pandorabots Platform is \"one of the oldest and largest chatbot hosting services in the world.\" Clients can create “AI-driven virtual agents” to hold human-like text or voice chats with consumers. Pandorabots implements and supports development of the AIML open standard and makes portions of its code accessible for free under licenses like the GPL or via open APIs.\n\nAccording to its website, as of December 2015, 225,000+ registered developers have used the platform to create 285,000+ chatbots, logging over three billion conversational interactions with end-users.\n\nPandorabots provides API access to its chatbot hosting platform, and offers the following SDKs on Github: Java, Ruby, Go, PHP, Python, and Node.js. The platform is written in Allegro Common LISP. Notable chatbots include A.L.I.C.E. (Alicebot): a three time Loebner-winner, open-source base personality bot, and inspiration for the movie Her; and Mitsuku.\n\nCommon use cases include advertising, virtual assistance, e-learning, entertainment and education. Chatbots built and hosted with Pandorabots appear in messaging and native apps, the web, games, social networks, and connected devices. Academics and universities use the platform for teaching and research.\n\n\n", "id": "48723464", "title": "Pandorabots"}
{"url": "https://en.wikipedia.org/wiki?curid=51171662", "text": "Flok (company)\n\nflok (formerly Loyalblocks) is an American tech startup based in New York City that provides marketing solutions such as chatbots/AI, customer loyalty programs, mobile apps and CRM services to local businesses.\n\nIn January 2017, the company was acquired by Wix.com.\n\nflok was founded in 2011 by Ido Gaver and Eran Kirshenboim and has offices in Tel Aviv, Israel. In May 2013, flok secured a $9 million Series A Round from General Catalyst Partners with participation from Founder Collective and existing investor Gemini Israel Ventures. In total, flok has raised over $18 million in venture capital in three rounds.\n\nIn May 2014, flok announced a self-service loyalty platform for SMBs to build their own programs with beacon integration. At that time, approximately 40,000 businesses were using the service. In 2016, flok released a turnkey chatbot service for local businesses, and was featured in AdWeek for developing the first \"weed bot\" chatbot for a California cannabis business.\n\nflok offers an eponymous customer-facing app that consumers use to receive rewards and deals form partner businesses, and a flok business app for merchants to manage the platform. Both are available for iOS and Android operating systems. flok's main products center around customer loyalty and rewards. The platform offers a digital punch card, proximity marketing, push messaging and CRM services in addition to its chatbots and AI features.\n\nflok provides partner businesses with beacons that can locate customers and verify store visits. As of July 2016, flok had a 4-star rating on Merchant Maverick review site.\n", "id": "51171662", "title": "Flok (company)"}
{"url": "https://en.wikipedia.org/wiki?curid=148349", "text": "Chatbot\n\nA chatbot (also known as a talkbot, chatterbot, Bot, IM bot, interactive agent, or Artificial Conversational Entity) is a computer program which conducts a conversation via auditory or textual methods. Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test. Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition. Some chatterbots use sophisticated natural language processing systems, but many simpler systems scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.\n\nThe term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs. Today, chatbots are part of virtual assistants such as Google Assistant, and are accessed via many organizations' apps, websites, and on instant messaging platforms. Non-assistant applications include chatbots used for entertainment purposes, for research, and social bots which promote a particular product, candidate, or issue.\n\nIn 1950, Alan Turing's famous article \"Computing Machinery and Intelligence\" was published, which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human. The notoriety of Turing's proposed test stimulated great interest in Joseph Weizenbaum's program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human. However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the Introduction to his paper presented it more as a debunking exercise:\n\n[In] artificial intelligence ... machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained ... its magic crumbles away; it stands revealed as a mere collection of procedures ... The observer says to himself \"I could have written that\". With that thought he moves the program in question from the shelf marked \"intelligent\", to that reserved for curios ... The object of this paper is to cause just such a re-evaluation of the program about to be \"explained\". Few programs ever needed it more.\n\nELIZA's key method of operation (copied by chatbot designers ever since) involves the recognition of cue words or phrases in the input, and the output of corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word 'MOTHER' with 'TELL ME MORE ABOUT YOUR FAMILY'). Thus an illusion of understanding is generated, even though the processing involved has been merely superficial. ELIZA showed that such an illusion is surprisingly easy to generate, because human judges are so ready to give the benefit of the doubt when conversational responses are \"capable of being interpreted\" as \"intelligent\".\n\nInterface designers have come to appreciate that humans' readiness to interpret computer output as genuinely conversational—even when it is actually based on rather simple pattern-matching—can be exploited for useful purposes. Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories. Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a \"friendlier\" interface than a more formal search or menu system. This sort of usage holds the prospect of moving chatbot technology from Weizenbaum's \"shelf ... reserved for curios\" to that marked \"genuinely useful computational methods\".\n\nThe classic historic early chatbots are ELIZA (1966) and PARRY (1972). More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include functional features such as games and web searching abilities. In 1984, a book called \"The Policeman's Beard is Half Constructed\" was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).\n\nOne pertinent field of AI research is natural language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. utilises a markup language called AIML, which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\n\nJabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimise their ability to communicate based on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.\n\nChatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge.\n\nChatbots are often integrated into the dialog systems of, for example, virtual assistants, giving them the ability of, for example, small talking or engaging in casual conversations unrelated to the scopes of their primary expert systems.\nCurrently chatbots are widely used as part of instant messaging platforms like Facebook Messenger, WeChat, and Kik for entertaining purposes as well as B2C customer service, sales and marketing. \n\nThe bots usually appear as one of the user's contacts or as a participant in a group chat. Companies like Nordea Bank, Domino's, Pizza Hut, Disney, and Whole Foods have launched their own chatbots to increase end customer engagement, promote their products and services, and give their customers a more convenient and easier way to order from them.\n\nPrevious generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 2008 or Expedia's virtual customer service agent which launched in 2011. The newer generation of chatbots includes IBM Watson-powered \"Rocky\", introduced in February 2017 by the New York City-based startup and e-commerce platform Rare Carat to assist novice diamond buyers through the daunting process of purchasing a diamond.\n\nOther companies explore ways how they can use chatbots internally, for example for Customer Support, Human Resources, or even in Internet-of-Things (IoT) projects. Overstock, for one, has reportedly launched a chatbot named Mila to automate certain simple yet time-consuming processes when requesting for a sick leave. SAP partnered with Kore Inc, a US-based chatbot platform vendor, to build enterprise-oriented chatterbots for certain SAP products like SAP Hana Cloud Platform, SAP Cloud for Customer (C4C), SAP SuccessFactors and Concur. Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citroën are now using automated online assistants instead of call centres with humans to provide a first point of contact. A SaaS chatbot business ecosystem has been steadily growing since the F8 Conference when Zuckerberg unveiled that Messenger would allow chatbots into the app.\n\nChatbots have also been incorporated into devices not primarily meant for computing such as toys.\n\n\"Hello Barbie\" is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk, which previously used the chatbot for a range of smartphone-based characters for children. These characters' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.\n\nIBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as \"CogniToys\" intended to interact with children for educational purposes.\n\nThe process of creating a chatbot follows a pattern similar to the development of a web page or a mobile app. It can be divided into Design, Building, and Analytics.\nThe chatbot design is the process that defines the interaction between the user and the chatbot. The chatbot designer will define the chatbot personality, the questions that will be asked to the users, and the overall interaction. It can be viewed as a subset of the conversational design.\nIn order to speed up this process, designers can use dedicated chatbot design tools, that allow for immediate preview, team collaboration and video export. An important part of the chatbot design is also centered around user testing. User testing can be performed following the same principles that guide the user testing of graphical interfaces.\n\nThe process of building a chatbot can be divided into two main tasks: understanding the user's intent and producing the correct answer. The first task involves understanding the user input. In order to properly understand a user input in a free text form, a Natural Language Processing Engine can be used. The second task may involve different approaches depending on the type of the response that the chatbot will generate.\nThe usage of the chatbot can be monitored in order to spot potential flaws or problems. It can also provide useful insights that can improve the final user experience\n\nThe process of building, testing and deploying chatbots can be done on cloud based chatbot development platforms offered by cloud Platform as a Service (PaaS) providers such as Oracle Cloud Platform. These cloud platforms provide Natural Language Processing, Artificial Intelligence and Mobile Backend as a Service for chatbot development. \n\nFacebook Messenger is becoming more and more popular as an everyday means of communication. It boasts 1.2 billion active users, that’s double the size of Instagram and the same as WhatsApp. In 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger in the first six months, rising to 100,000 in the year. At the annual conference for developers in September 2017, Facebook VP David Marcus reported, “Now we’ve doubled messages between business and people to two billion messages a month, and more than 100,000 bots on the platform. That’s up from 33,000 last September. That's crazy.”\n\nMalicious chatbots are frequently used to fill chat rooms with spam and advertising, by mimicking human behaviour and conversations or to entice people into revealing personal information, such as bank account numbers. They are commonly found on Yahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a published report of a chatbot used in a fake personal ad on a dating service's website.\n\n\n", "id": "148349", "title": "Chatbot"}
{"url": "https://en.wikipedia.org/wiki?curid=53277123", "text": "Zo (bot)\n\nZo is an artificial intelligence chatbot developed by Microsoft in March 2017. It allows users of Facebook (via Messenger), the mobile messaging service Kik, or the group chat platform GroupMe to chat with it through private messages.\n\n", "id": "53277123", "title": "Zo (bot)"}
{"url": "https://en.wikipedia.org/wiki?curid=19024298", "text": "Virtual assistant (artificial intelligence)\n\nA virtual assistant is a software agent that can perform tasks or services for an individual. Sometimes the term \"chatbot\" is used to refer to virtual assistants generally or specifically those accessed by online chat (or in some cases online chat programs that are for entertainment and not useful purposes).\n\nAs of 2017, the capabilities and usage of virtual assistants is expanding rapidly, with new products entering the market. An online poll in May 2017 found the most widely used in the US were Apple's Siri (34%), Google Assistant (19%), Amazon Alexa (6%), and Microsoft Cortana (4%). Apple and Google have large installed bases of users on smartphones. Microsoft has a large installed base of Windows-based personal computers, smartphones and smart speakers. Alexa has a large install base for smart speakers.\n\nThe first tool enabled to perform digital speech recognition was the IBM Shoebox, presented to the general public during the 1962 Seattle World's Fair after its initial market launch in 1961. This early computer, developed almost 20 years before the introduction of the first IBM Personal Computer in 1981, was able to recognize 16 spoken words and the digits 0 to 9. The next milestone in the development of voice recognition technology was achieved in the 1970s at the Carnegie Mellon University in Pittsburgh, Pennsylvania with substantial support of the United States Department of Defense and its DARPA agency. Their tool \"Harpy\" mastered about 1000 words, the vocabulary of a three-year-old. About ten years later the same group of scientists developed a system that could analyze not only individual words but entire word sequences enabled by a Hidden Markov Model. Thus, the earliest virtual assistants, which applied speech recognition software were automated attendant and medical digital dictation software. In the 1990s digital speech recognition technology became a feature of the personal computer with Microsoft, IBM, Philips and Lernout & Hauspie fighting for customers. Much later the market launch of the first smartphone IBM Simon in 1994 laid the foundation for smart virtual assistants as we know them today. The first modern digital virtual assistant installed on a smartphone was Siri, which was introduced as a feature of the iPhone 4S on October 4, 2011. Apple Inc. developed Siri following the 2010 acquisition of Siri Inc., a spin-off of SRI International, which is a research institute financed by DARPA and the United States Department of Defense.\n\nVirtual assistants make work via:\n\n\nSome virtual assistants are accessible via multiple methods, such as Google Assistant via chat on the Google Allo app and via voice on Google Home smart speakers.\n\nVirtual assistants use natural language processing (NLP) to match user text or voice input to executable commands. Many continually learn using artificial intelligence techniques including machine learning.\n\nTo activate a virtual assistant using the voice, a wake word might be used. This is a word or groups of words such as \"Alexa\" or \"OK Google\".\n\nVirtual assistants may be integrated into many types of platforms or, like Amazon Alexa, across several of them:\n\n\nVirtual assistants can provide a wide variety of services, and particularly those from Amazon Alexa and Google Assistant grow by the day. These include:\n\n\nAmazon enables Alexa \"Skills\" and Google \"Actions\", essentially apps that run on the assistant platforms.\n\nThe platforms that power the most widely used virtual assistants are also used to power other solutions:\n\n\nIn previous generations of text chat-based virtual assistants, the assistant was often represented by an avatar of (a.k.a. 'interactive online character\" or \"automated character\") — this was known as an embodied agent.\n\nDigital experiences enabled by virtual assistants are considered to be among the major recent technological advances and most promising consumer trends. Experts claim that digital experiences will achieve a status-weight comparable to ‘real’ experiences, if not become more sought-after and prized. The trend is verified by a high number of frequent users and the substantial growth of worldwide user numbers of virtual digital assistants. In mid-2017, the number of frequent users of digital virtual assistants is estimated to be around 1bn worldwide. In addition, it can be observed that virtual digital assistant technology is no longer restricted to smartphone applications, but present across many industry sectors (incl. automotive, telecommunications, retail, healthcare and education).\nIn response to the significant R&D expenses of firms across all sectors and an increasing implementation of mobile devices, the market for speech recognition technology is predicted to grow at a CAGR of 34.9% globally over the period of 2016 to 2024 and thereby surpass a global market size of USD 7.5 billion by 2024.\nTaking into consideration the regional distribution of market leaders, North American companies (e.g. Nuance Communications, IBM, eGain) are expected to dominate the industry over the next years, due to the significant impact of BYOD (Bring Your Own Device) and enterprise mobility business models. Furthermore, the increasing demand for smartphone-assisted platforms are expected to further boost the North American Intelligent Virtual Assistant (IVA) industry growth. Despite its smaller size in comparison to the North American market, the intelligent virtual assistant industry from the Asia-Pacific region, with its main players located in India and China is predicted to grow at an annual growth rate of 40% (above global average) over the 2016-2024 period.\n\n", "id": "19024298", "title": "Virtual assistant (artificial intelligence)"}
{"url": "https://en.wikipedia.org/wiki?curid=10235", "text": "ELIZA\n\nELIZA is an early natural language processing computer program created from 1964 to 1966 at the MIT Artificial Intelligence Laboratory by Joseph Weizenbaum. Created to demonstrate the superficiality of communication between humans and machines, Eliza simulated conversation by using a 'pattern matching' and substitution methodology that gave users an illusion of understanding on the part of the program, but had no built in framework for contextualizing events. Directives on how to interact were provided by 'scripts', written originally in MAD-Slip, which allowed ELIZA to process user inputs and engage in discourse following the rules and directions of the script. The most famous script, DOCTOR, simulated a Rogerian psychotherapist and used rules, dictated in the script, to respond with non-directional questions to user inputs. As such, ELIZA was one of the first chatterbots, but was also regarded as one of the first programs capable of passing the Turing Test.\n\nELIZA's creator, Weizenbaum regarded the program as a method to show the superficiality of communication between man and machine, but was surprised by the number of individuals who attributed human-like feelings to the computer program, including Weizenbaum’s secretary. Many academics believed that the program would be able to positively influence the lives of many people, particularly those suffering from psychological issues and that it could aid doctors working on such patients’ treatment. While ELIZA was capable of engaging in discourse, ELIZA could not converse with true understanding. However, many early users were convinced of ELIZA’s intelligence and understanding, despite Weizenbaum’s insistence to the contrary.\n\nJoseph Weizenbaum’s ELIZA, running the DOCTOR script, was created to provide a parody of “the responses of a non-directional psychotherapist in an initial psychiatric interview” and to “demonstrate that the communication between man and machine was superficial”. While ELIZA is most well known for acting in the manner of a psychotherapist, this mannerism is due to the data and instructions supplied by the DOCTOR script. ELIZA itself examined the text for keywords, applied values to said keywords, and transformed the input into an output; the script that ELIZA ran determined the keywords, set the values of keywords, and set the rules of transformation for the output. Weizenbaum chose to make the DOCTOR script in the context of psychotherapy to “sidestep the problem of giving the program a data base of real-world knowledge,” as in a Rogerian therapeutic situation, the program had only to reflect back the patient’s statements. The algorithms of DOCTOR allowed for a deceptively intelligent response, that deceived many individuals when first using the program.\n\nWeizenbaum named his program ELIZA after Eliza Doolittle, a working-class character in George Bernard Shaw’s \"Pygmalion\". According to Weizenbaum, ELIZA’s ability to be “incrementally improved” by various users made it similar to Eliza Doolittle, since Eliza Doolittle was taught to speak with an upper-class accent in Shaw’s play. However, unlike in Shaw’s play, ELIZA is incapable of learning new patterns of speech or new words through interaction alone. Edits must be made directly to ELIZA’s active script in order to change the manner by which the program operates.\n\nWeizenbaum first implemented ELIZA in his own SLIP list-processing language, where, depending upon the initial entries by the user, the illusion of human intelligence could appear, or be dispelled through several interchanges. Some of ELIZA’s responses were so convincing that Weizenbaum and several others have anecdotes of users becoming emotionally attached to the program, occasionally forgetting that they were conversing with a computer. Weizenbaum’s own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing, “I had not realized… that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.”\n\nIn 1966, interactive computing (via a teletype) was new. It was 15 years before the personal computer became familiar to the general public, and three decades before most people encountered attempts at natural language processing in Internet services like Ask.com or PC help systems such as Microsoft Office Clippy. Although those programs included years of research and work, \"ELIZA\" remains a milestone simply because it was the first time a programmer had attempted such a human-machine interaction with the goal of creating the illusion (however brief) of human-\"human\" interaction.\n\nAt the ICCC 1972 ELIZA met another early artificial intelligence program named PARRY and had a computer-only conversation. While ELIZA was built to be a \"Doctor\" PARRY was intended to simulate a patient with schizophrenia.\n\nWeizenbaum originally wrote ELIZA in MAD-Slip for the IBM 7094, as a program to make natural language conversation possible with a computer. To accomplish this, Weizenbaum identified five “fundamental technical problems” for ELIZA to overcome: the identification of critical words, the discovery of a minimal context, the choice of appropriate transformations, the generation of responses appropriate to the transformation or in the absence of critical words and the provision of an ending capacity for ELIZA scripts. Weizenbaum solved these problems in his ELIZA program and made ELIZA such that it had no built in contextual framework or universe of discourse. However, this required ELIZA to have a script of instructions on how to respond to inputs from users.\n\nELIZA starts its process of responding to an input by a user by first examining the text input for a ‘keyword’. A ‘keyword’ is a word designated as important by the acting ELIZA script, which assigns to each keyword a precedence number, or a RANK, designed by the programmer. If such words are found, they are put into a ‘keystack’, with the keyword of the highest RANK at the top. The input sentence is then manipulated and transformed as the rule associated with the keyword of the highest RANK directs. For example, when the DOCTOR script encounters words such as “alike” or “same”, it would output a message pertaining to similarity, in this case “In what way?”, as these words had high precedence number. This also demonstrates how certain words, as dictated by the script, can be manipulated regardless of contextual considerations, such as switching first-person pronouns and second-person pronouns and vice versa, as these too had high precedence numbers. Such words with high precedence numbers are deemed superior to conversational patterns, and are treated independently of contextual patterns.\n\nFollowing the first examination, the next step of the process is to apply an appropriate transformation rule, which includes two parts, the “decomposition rule” and the “reassembly rule”. First, the input is reviewed for syntactical patterns in order to establish the minimal context necessary to respond. Using the keywords and other nearby words from the input, different disassembly rules are tested until an appropriate pattern is found. Using the script’s rules, the sentence is then ‘dismantled’ and arranged into sections of the component parts as the “decomposition rule for the highest ranking keyword” dictates. The example that Weizenbaum gives is the input “I are very helpful” (remembering that “I” is “You” transformed), which is broken into (1) empty (2) I (3) are (4) very helpful. The decomposition rule has broken the phrase into four small segments, that contain both the keywords and the information in the sentence.\n\nThe decomposition rule then designates a particular reassembly rule, or set of reassembly rules, to follow when reconstructing the sentence. The reassembly rule then takes the fragments of the input that the decomposition rule had created, rearranges them, and adds in programmed words to create a response. Using Weizenbaum’s example previously stated, such a reassembly rule would take the fragments and apply them to the phrase “What makes you think I am (4)” which would result in “What makes you think I am very helpful”. This example is rather simple, since depending upon the disassembly rule, the output could be significantly more complex and use more of the input from the user. However, from this reassembly, ELIZA then sends the constructed sentence to the user in the form of text on the screen.\n\nThese steps represent the bulk of the procedures which ELIZA follows in order to create a response from a typical input, though there are several specialized situations that ELIZA/DOCTOR can respond to. One Weizenbaum specifically wrote about was when there is not a keyword. One solution was to have ELIZA respond with a remark that lacked content, such as “I see” or “Please go on.”. The second method was to use a “MEMORY” structure, which recorded prior recent inputs, and would use these inputs to create a response referencing a part of the earlier conversation when encountered with no keywords. This was possible due to Slip’s ability to tag words for other usage, which simultaneously allowed ELIZA to examine, store and repurpose words for usage in outputs.\n\nWhile these functions were all framed in ELIZA’s programming, the exact manner by which the program dismantled, examined, and reassembled inputs is determined by the operating script. However, the script is not static, and can be edited, or a new one created, as is necessary for the operation in the context needed (thus how ELIZA can “learn” new information). This also allows the program to be applied in multiple situations, including the well-known DOCTOR script, which simulates a Rogerian psychotherapist, but also a script called “STUDENT”, which is capable of taking in logical analysis parameters and using it to give the answers to problems of related logic.\n\nWeizenbaum's original MAD-SLIP implementation was re-written in Lisp by Bernie Cosell. A BASIC version appeared in Creative Computing in 1977 (although it was written in 1973 by Jeff Shrager). This version, which was ported to many of the earliest personal computers, appears to have been subsequently translated into many other versions in many other languages.\n\nAnother version of Eliza popular among software engineers is the version that comes with the default release of GNU Emacs, and which can be accessed by typing codice_1 from most modern emacs implementations.\n\nELIZA influenced a number of early computer games by demonstrating additional kinds of interface designs. Don Daglow wrote an enhanced version of the program called \"Ecala\" on a DEC PDP-10 minicomputer at Pomona College in 1973 before writing the computer role-playing game, \"Dungeon\" (1975).\n\nLay responses to ELIZA were disturbing to Weizenbaum and motivated him to write his book \"Computer Power and Human Reason: From Judgment to Calculation\", in which he explains the limits of computers, as he wants to make clear in people's minds his opinion that the anthropomorphic views of computers are just a reduction of the human being and any life form for that matter. In the independent documentary film \"Plug & Pray\" (2010) Weizenbaum said that only people who misunderstood ELIZA called it a sensation.\n\nThe Israeli poet David Avidan, who was fascinated with future technologies and their relation to art, desired to explore the use of computers for writing literature. He conducted several conversations with an APL implementation of ELIZA and published them – in English, and in his own translation to Hebrew – under the title \"My Electronic Psychiatrist – Eight Authentic Talks with a Computer\". In the foreword he presented it as a form of constrained writing.\n\nThere are many programs based on ELIZA in different programming languages. In 1980 a company called \"Don't Ask Software\" created a version called \"Abuse\" for the Apple II, Atari, and Commodore 64 computers, which verbally abused the user based on the user's input. Other versions adapted ELIZA around a religious theme, such as ones featuring Jesus (both serious and comedic) and another Apple II variant called \"I Am Buddha\". The 1980 game \"The Prisoner\" incorporated ELIZA-style interaction within its gameplay. George Lucas and Walter Murch incorporated an Eliza-like dialogue interface in their screenplay for the feature film \"THX-1138\" in 1969. Inhabitants of the underground future world of THX would retreat to \"confession booths\" when stressed, and initiate a one-sided Eliza-formula conversation with a Jesus-faced computer who claimed to be \"Omm\". In 1988 the British artist and friend of Weizenbaum Brian Reffin Smith created and showed at the exhibition 'Salamandre', in the Musée du Berry, Bourges, France, two art-oriented ELIZA-style programs written in BASIC, one called 'Critic' and the other 'Artist', running on two separate Amiga 1000 computers. The visitor was supposed to help them converse by typing in to 'Artist' what 'Critic' said, and vice versa. The secret was that the two programs were identical. GNU Emacs formerly had a codice_2 command that simulates a session between ELIZA and Zippy the Pinhead. The Zippyisms were removed due to copyright issues, but the DOCTOR program remains.\n\nELIZA has been referenced in popular culture and continues to be a source of inspiration for programmers and developers focused on Artificial Intelligence. It was also featured in a 2012 exhibit at Harvard University titled \"Go Ask A.L.I.C.E\", as part of a celebration of mathematician Alan Turing's 100th birthday. The exhibit explores Turing's lifelong fascination with the interaction between humans and computers, pointing to ELIZA as one of the earliest realizations of Turing's ideas.. Season 1, Episode 12 of the TV show Young Sheldon shows the Cooper family interacting with ELIZA.\n\n\n\n\n\n", "id": "10235", "title": "ELIZA"}
{"url": "https://en.wikipedia.org/wiki?curid=14675239", "text": "Dialog tree\n\nA dialog tree or conversation tree is a gameplay mechanic that is used throughout many adventure games (including action-adventure games) and role-playing video games. When interacting with a non-player character, the player is given a choice of what to say and makes subsequent choices until the conversation ends. Certain video game genres, such as visual novels and dating sims, revolve almost entirely around these character interactions and branching dialogues.<ref name=\"Gamasutra\"/\nIt is best to pick all choices to see what happens.\n\nThe concept of a dialog tree has existed long before the advent of video games. The earliest known dialog tree is described in \"The Garden of Forking Paths,\" a 1941 short story by Jorge Luis Borges, in which the combination book of Ts'ui Pên allows all major outcomes from an event branch into their own chapters. Much like the game counterparts this story reconvenes as it progresses (as possible outcomes would approach n^m where n is the number of options at each fork and m is the depth of the tree).\n\nThe first computer dialogue system was featured in ELIZA, a primitive natural language processing computer program written by Joseph Weizenbaum between 1964 and 1966. The program emulated interaction between the user and an artificial therapist. With the advent of video games, interactive entertainment have attempted to incorporate meaningful interactions with virtual characters. Branching dialogues have since become a common feature in visual novels, dating sims, adventure games, and role-playing video games.\n\nThe player typically enters the gameplay mode by choosing to speak with a non-player character (or when a non-player character chooses to speak to them), and then choosing a line of pre-written dialog from a menu. Upon choosing what to say, the non-player character responds to the player, and the player is given another choice of what to say. This cycle continues until the conversation ends. The conversation may end when the player selects a farewell message, the non-player character has nothing more to add and ends the conversation, or when the player makes a bad choice (perhaps angering the non-player to leave the conversation).\n\nGames often offer options to ask non-players to reiterate information about a topic, allowing players to replay parts of the conversation that they did not pay close enough attention to the first time. These conversations are said to be designed as a tree structure, with players deciding between each branch of dialog to pursue. Unlike a branching story, players may return to earlier parts of a conversation tree and repeat them. Each branch point (or node) is essentially a different menu of choices, and each choice that the player makes triggers a response from the non-player character followed by a new menu of choices.\n\nIn some genres such as role-playing video games, external factors such as charisma may influence the response of the non-player character or unlock options that would not be available to other characters. These conversations can have far-reaching consequences, such as deciding to disclose a valuable secret that has been entrusted to the player. However, these are usually not real tree data structure in programmers sense, because they contain cycles as can be seen on illustration on this page.\n\nCertain game genres revolve almost entirely around character interactions, including visual novels such as \"Ace Attorney\" and dating sims such as \"Tokimeki Memorial\", usually featuring complex branching dialogues and often presenting the player's possible responses word-for-word as the player character would say them. Games revolving around relationship-building, including visual novels, dating sims such as \"Tokimeki Memorial\", and some role-playing games such as \"\", often give choices that have a different number of associated \"mood points\" which influence a player character's relationship and future conversations with a non-player character. These games often feature a day-night cycle with a time scheduling system that provides context and relevance to character interactions, allowing players to choose when and if to interact with certain characters, which in turn influences their responses during later conversations. Some games use a real-time conversation system, giving the player only a few seconds to respond to a non-player character, such as Sega's \"Sakura Wars\" and \"Alpha Protocol\".\n\nAnother variation of branching dialogues can be seen in the adventure game \"Culpa Innata\", where the player chooses a tactic at the beginning of a conversation, such as using either a formal, casual or accusatory manner, that affects the tone of the conversation and the information gleaned from the interviewee.\n\nThis mechanism allows game designers to provide interactive conversations with nonplayer characters without having to tackle the challenges of natural language processing in the field of artificial intelligence. In games such as \"Monkey Island\", these conversations can help demonstrate the personality of certain characters.\n\n", "id": "14675239", "title": "Dialog tree"}
{"url": "https://en.wikipedia.org/wiki?curid=11273721", "text": "Hierarchical temporal memory\n\nHierarchical temporal memory (HTM) is a biologically constrained theory of machine intelligence originally described in the 2004 book \"On Intelligence\" by Jeff Hawkins with Sandra Blakeslee. HTM is based on neuroscience and the physiology and interaction of pyramidal neurons in the neocortex of the human brain. The technology has been tested and implemented in software through example applications from Numenta and commercial applications from Numenta’s partners.\n\nAt the core of HTM are learning algorithms that can store, learn, infer and recall high-order sequences. Unlike most other machine learning methods, HTM learns time-based patterns in unlabeled data on a continuous basis. HTM is robust to noise and high capacity, meaning that it can learn multiple patterns simultaneously. When applied to computers, HTM is well suited for prediction, anomaly detection, classification and ultimately sensorimotor applications.\n\nA typical HTM network is a tree-shaped hierarchy of \"levels\" that are composed of smaller elements called \"nodes\" or \"columns\". A single level in the hierarchy is also called a \"region\". Higher hierarchy levels often have fewer nodes and therefore less spatial resolvability. Higher hierarchy levels can reuse patterns learned at the lower levels by combining them to memorize more complex patterns.\n\nEach HTM node has the same basic functionality. In learning and inference modes, sensory data comes into the bottom level nodes. In generation mode, the bottom level nodes output the generated pattern of a given category. The top level usually has a single node that stores the most general categories (concepts) which determine, or are determined by, smaller concepts in the lower levels which are more restricted in time and space. When in inference mode, a node in each level interprets information coming in from its child nodes in the lower level as probabilities of the categories it has in memory.\n\nEach HTM region learns by identifying and memorizing spatial patterns - combinations of input bits that often occur at the same time. It then identifies temporal sequences of spatial patterns that are likely to occur one after another.\n\nThere have been several generations of HTM algorithms.\n\nDuring training, a node receives a temporal sequence of spatial patterns as its input. The learning process consists of two stages: \n\nDuring inference (recognition), the node calculates the set of probabilities that a pattern belongs to each known coincidence. Then it calculates the probabilities that the input represents each temporal group. The set of probabilities assigned to the groups is called a node's \"belief\" about the input pattern. (In a simplified implementation, node's belief consists of only one winning group). This belief is the result of the inference that is passed to one or more \"parent\" nodes in the next higher level of the hierarchy.\n\n\"Unexpected\" patterns to the node do not have a dominant probability of belonging to any one temporal group, but have nearly equal probabilities of belonging to several of the groups. If sequences of patterns are similar to the training sequences, then the assigned probabilities to the groups will not change as often as patterns are received. The output of the node will not change as much, and a resolution in time is lost.\n\nIn a more general scheme, the node's belief can be sent to the input of any node(s) in any level(s), but the connections between the nodes are still fixed. The higher-level node combines this output with the output from other child nodes thus forming its own input pattern.\n\nSince resolution in space and time is lost in each node as described above, beliefs formed by higher-level nodes represent an even larger range of space and time. This is meant to reflect the organization of the physical world as it is perceived by human brain. Larger concepts (e.g. causes, actions and objects) are perceived to change more slowly and consist of smaller concepts that change more quickly. Jeff Hawkins postulates that brains evolved this type of hierarchy to match, predict, and affect the organization of the external world.\n\nMore details about the functioning of Zeta 1 HTM can be found in Numenta's old documentation.\n\nThe second generation of HTM learning algorithms was drastically different from Zeta 1. It relies on sparse distributed representations and a more biologically-realistic neuron model. There are two core components– a spatial pooling algorithm that creates sparse representations and a sequence memory algorithm that learns to represent and predict complex sequences.\n\nA layer creates a sparse representation from its input, so that a fixed percentage of minicolumns are active at any one time. Each HTM layer consists of a number of highly interconnected minicolumns. A layer is similar to layer III of the neocortex. A minicolumn is understood as a group of cells that have the same receptive field. Each minicolumn has a number of cells that are able to remember several previous states. A cell can be in one of three states: active, inactive and predictive state.\n\nSpatial pooling: The receptive field of each minicolumn is a fixed number of inputs that are randomly selected from a much larger number of node inputs. Based on the input pattern, some minicolumns will receive more active input values. Spatial pooling selects a relatively constant number of the most active minicolumns and inactivates (inhibits) other minicolumns in the vicinity of the active ones. Similar input patterns tend to activate a stable set of minicolumns. The amount of memory used by each layer can be increased to learn more complex spatial patterns or decreased to learn simpler patterns.\n\nRepresenting the input in the context of previous inputs: If one or more cells in the active minicolumn are in the predictive state (see below), they will be the only cells to become active in the current time step. If none of the cells in the active minicolumn are in the predictive state (during the initial time step or when the activation of this minicolumn was not expected), all cells are made active.\n\nPredicting future inputs and temporal pooling: When a cell becomes active, it gradually forms connections to nearby cells that tend to be active during several previous time steps. Thus a cell learns to recognize a known sequence by checking whether the connected cells are active. If a large number of connected cells are active, this cell switches to the predictive state in anticipation of one of the few next inputs of the sequence. The output of a layer includes minicolumns in both active and predictive states. Thus minicolumns are active over longer periods of time, which leads to greater temporal stability seen by the parent layer.\n\nCortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM layer to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the layer. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted.\n\nCortical learning algorithms are currently being offered as commercial SaaS by Numenta (such as Grok).\n\nThe following question was posed to Jeff Hawkins September 2011 with regard to Cortical learning algorithms: \"How do you know if the changes you are making to the model are good or not?\" To which Jeff's response was \"There are two categories for the answer: one is to look at neuroscience, and the other is methods for machine intelligence. In the neuroscience realm there are many predictions that we can make, and those can be tested. If our theories explain a vast array of neuroscience observations then it tells us that we’re on the right track. In the machine learning world they don’t care about that, only how well it works on practical problems. In our case that remains to be seen. To the extent you can solve a problem that no one was able to solve before, people will take notice.\"\n\nThe third generation builds on the second generation and adds in a theory of sensorimotor inference in the neocortex . This theory proposes that cortical columns at every level of the hierarchy can learn complete models of objects over time and that features are learned at specific locations on the objects.\n\nComparing high-level structures and functionality of neocortex with HTM is most appropriate. HTM attempts to implement the functionality that is characteristic of a hierarchically related group of cortical regions in the neocortex. A \"region\" of the neocortex corresponds to one or more \"levels\" in the HTM hierarchy, while the hippocampus is remotely similar to the highest HTM level. A single HTM node may represent a group of cortical columns within a certain region.\n\nAlthough it is primarily a functional model, several attempts have been made to relate the algorithms of the HTM with the structure of neuronal connections in the layers of neocortex. The neocortex is organized in vertical columns of 6 horizontal layers. The 6 layers of cells in the neocortex should not be confused with levels in an HTM hierarchy.\n\nHTM nodes attempt to model a portion of cortical columns (80 to 100 neurons) with approximately 20 HTM \"cells\" per column. HTMs model only layers 2 and 3 to detect spatial and temporal features of the input with 1 cell per column in layer 2 for spatial \"pooling\", and 1 to 2 dozen per column in layer 3 for temporal pooling. A key to HTMs and the cortex's is their ability to deal with noise and variation in the input which is a result of using a \"sparse distributive representation\" where only about 2% of the columns are active at any given time.\n\nAn HTM attempts to model a portion of the cortex's learning and plasticity as described above. Differences between HTMs and neurons include:\n\nIntegrating memory component with neural networks has a long history dating back to early research in distributed representations and self-organizing maps. For example, in sparse distributed memory (SDM), the patterns encoded by neural networks are used as memory addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders.\n\nComputers store information in \"dense\" representations such as a 32 bit word where all combinations of 1s and 0s are possible.\nBy contrast, brains use sparse distributed representations (SDR). The human neocortex has roughly 100 billion neurons, but at any given time only a small percent are active. The activity of neurons are like bits in a computer, and therefore the representation is sparse. Similarly to SDM developed by NASA in the 80s and vector space models used in Latent semantic analysis, HTM also uses Sparse Distributed Representations.\n\nThe SDRs used in HTM are binary representations of data consisting of many bits with a small percentage of the bits active (1s); a typical implementation might have 2048 columns and 64K artificial neurons where as few as 40 might be active at once. Although it may seem less efficient for the majority of bits to go \"unused\" in any given representation, SDRs have two major advantages over traditional dense representations. First, SDRs are tolerant of corruption and ambiguity due to the meaning of the representation being shared (\"distributed\") across a small percentage (\"sparse\") of active bits. In a dense representation, flipping a single bit completely changes the meaning, while in an SDR a single bit may not affect the overall meaning much. This leads to the second advantage of SDRs: because the meaning of a representation is distributed across all active bits, similarity between two representations can be used as a measure of semantic similarity in the objects they represent. That is, if two vectors in an SDR have 1s in the same position, then they are semantically similar in that attribute. The bits in SDRs have semantic meaning, and that meaning is distributed across the bits.\n\nThe semantic folding theory builds on these SDR properties to propose a new model for language semantics, where words are encoded into word-SDRs and the similarity between terms, sentences and texts can be calculated with simple distance measures.\n\nLikened to a Bayesian network, an HTM comprises a collection of nodes that are arranged in a tree-shaped hierarchy. Each node in the hierarchy discovers an array of causes in the input patterns and temporal sequences it receives. A Bayesian belief revision algorithm is used to propagate feed-forward and feedback beliefs from child to parent nodes and vice versa. However, the analogy to Bayesian networks is limited, because HTMs can be self-trained (such that each node has an unambiguous family relationship), cope with time-sensitive data, and grant mechanisms for covert attention.\n\nA theory of hierarchical cortical computation based on Bayesian belief propagation was proposed earlier by Tai Sing Lee and David Mumford. While HTM is mostly consistent with these ideas, it adds details about handling invariant representations in the visual cortex.\n\nLike any system that models details of the neocortex, HTM can be viewed as an artificial neural network. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM \"neurons\". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing. For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.\n\nLAMINART and similar neural networks researched by Stephen Grossberg attempt to model both the infrastructure of the cortex and the behavior of neurons in a temporal framework to explain neurophysiological and psychophysical data. However, these networks are, at present, too complex for realistic application.\n\nHTM is also related to work by Tomaso Poggio, including an approach for modeling the ventral stream of the visual cortex known as HMAX. Similarities of HTM to various AI ideas are described in the December 2005 issue of the Artificial Intelligence journal.\n\nNeocognitron, a hierarchical multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, is one of the first Deep Learning Neural Networks models.\n\nThe Numenta Platform for Intelligent Computer (NuPIC) is one of several available HTM implementations. Some are provided by Numenta, while some are developed and maintained by the HTM open source community.\n\nNuPIC includes implementations of Spatial Pooling and Temporal Memory in both C++ and Python. It also includes 3 APIs. Users can construct HTM systems using direct implementations of the algorithms, or construct a Network using the Network API, which is a flexible framework for constructing complicated associations between different Layers of cortex. \n\nNuPIC 1.0 was released on July 2017, after which the codebase was put into maintenance mode. Current research continues in Numenta research codebases.\n\nThe following commercial applications are available using NuPIC: \nThe following tools are available on NuPIC:\nThe following example applications are available on NuPIC, see http://numenta.com/applications/:\n\n\n\n\n", "id": "11273721", "title": "Hierarchical temporal memory"}
{"url": "https://en.wikipedia.org/wiki?curid=6836612", "text": "Autoencoder\n\nAn autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings.\nThe aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.\n\nArchitecturally, the simplest form of an autoencoder is a feedforward, non-recurrent neural network very similar to the multilayer perceptron (MLP) – having an input layer, an output layer and one or more hidden layers connecting them –, but with the output layer having the same number of nodes as the input layer, and with the purpose of \"reconstructing\" its own inputs (instead of predicting the target value formula_1 given inputs formula_2). Therefore, autoencoders are unsupervised learning models.\n\nAn autoencoder always consists of two parts, the encoder and the decoder, which can be defined as transitions formula_3 and formula_4 such that:\n\nIn the simplest case, where there is one hidden layer, the encoder stage of an autoencoder takes the input formula_8 and maps it to formula_9:\n\nThis image formula_11 is usually referred to as \"code\", \"latent variables\", or \"latent representation\". Here, formula_12 is an element-wise activation function such as a sigmoid function or a rectified linear unit. formula_13 is a weight matrix and formula_14 is a bias vector. After that, the decoder stage of the autoencoder maps formula_11 to the \"reconstruction\" formula_16 of the same shape as formula_17:\n\nwhere formula_19 for the decoder may differ in general from the corresponding formula_20 for the encoder, depending on the design of the autoencoder. \n\nAutoencoders are also trained to minimise reconstruction errors (such as squared errors):\n\nwhere formula_17 is usually averaged over some input training set.\n\nIf the feature space formula_23 has lower dimensionality than the input space formula_24, then the feature vector formula_25 can be regarded as a compressed representation of the input formula_26. If the hidden layers are larger than the input layer, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases.\n\nVarious techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations:\n\nDenoising autoencoders take a partially corrupted input whilst training to recover the original undistorted input. This technique has been introduced with a specific approach to \"good\" representation. A \"good representation is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input.\" This definition contains the following implicit assumptions:\n\n\nTo train an autoencoder to denoise data, it is necessary to perform preliminary stochastic mapping formula_27 in order to corrupt the data and use formula_28 as input for a normal autoencoder, with the only exception being that the loss should be still computed for the initial input formula_29 instead of formula_30.\n\nBy imposing sparsity on the hidden units during training (whilst having a larger number of hidden units than inputs), an autoencoder can learn useful structures in the input data. This allows sparse representations of inputs. These are useful in pretraining for classification tasks.\n\nSparsity may be achieved by additional terms in the loss function during training (by comparing the probability distribution of the hidden unit activations with some low desired value), or by manually zeroing all but the few strongest hidden unit activations (referred to as a \"k-sparse autoencoder\").\n\nVariational autoencoder models inherit autoencoder architecture, but make strong assumptions concerning the distribution of latent variables. They use variational approach for latent representation learning, which results in an additional loss component and specific training algorithm called \"Stochastic Gradient Variational Bayes (SGVB)\". It assumes that the data is generated by a directed graphical model formula_31 and that the encoder is learning an approximation formula_32 to the posterior distribution formula_33 where formula_34 and formula_35 denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The objective of the variational autoencoder in this case has the following form:\nHere, formula_37 stands for the Kullback–Leibler divergence. The prior over the latent variables is usually set to be the centred isotropic multivariate Gaussian formula_38; however, alternative configurations have also been recently considered, e.g. \n\nContractive autoencoder adds an explicit regularizer in their objective function that forces the model to learn a function that is robust to slight variations of input values. This regularizer corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. The final objective function has the following form:\n\nIf linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA).\n\nThe training algorithm for an autoencoder can be summarized as\n\nAn autoencoder is often trained using one of the many variants of backpropagation (such as conjugate gradient method, steepest descent, etc.). Though these are often reasonably effective, there are fundamental problems with the use of backpropagation to train networks with many hidden layers. Once errors are backpropagated to the first few layers, they become minuscule and insignificant. This means that the network will almost always learn to reconstruct the average of all the training data. Though more advanced backpropagation methods (such as the conjugate gradient method) can solve this problem to a certain extent, they still result in a very slow learning process and poor solutions. This problem can be remedied by using initial weights that approximate the final solution. The process of finding these initial weights is often referred to as \"pretraining\".\n\nGeoffrey Hinton developed a pretraining technique for training many-layered \"deep\" autoencoders. This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results. This model takes the name of deep belief network.\n\n", "id": "6836612", "title": "Autoencoder"}
{"url": "https://en.wikipedia.org/wiki?curid=33742232", "text": "Restricted Boltzmann machine\n\nA restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. \n\nRBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,\nand rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000s. RBMs have found applications in dimensionality reduction,\nclassification,\ncollaborative filtering, feature learning\nand topic modelling.\nThey can be trained in either supervised or unsupervised ways, depending on the task.\n\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.\n\nRestricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.\n\nThe standard type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, and consists of a matrix of weights formula_1 (size \"m\"×\"n\") associated with the connection between hidden unit formula_2 and visible unit formula_3, as well as bias weights (offsets) formula_4 for the visible units and formula_5 for the hidden units. Given these, the \"energy\" of a configuration (pair of boolean vectors) is defined as\n\nor, in matrix notation,\n\nThis energy function is analogous to that of a Hopfield network. As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the energy function:\n\nwhere formula_9 is a partition function defined as the sum of formula_10 over all possible configurations (in other words, just a normalizing constant to ensure the probability distribution sums to 1). Similarly, the (marginal) probability of a visible (input) vector of booleans is the sum over all possible hidden layer configurations:\n\nSince the RBM has the shape of a bipartite graph, with no intra-layer connections, the hidden unit activations are mutually independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the hidden unit activations. That is, for formula_12 visible units and formula_13 hidden units, the conditional probability of a configuration of the visible units , given a configuration of the hidden units , is\n\nConversely, the conditional probability of given is\n\nThe individual activation probabilities are given by\n\nwhere formula_18 denotes the logistic sigmoid.\n\nThe visible units of RBM can be multinomial, although the hidden units are Bernoulli. In this case, the logistic function for visible units is replaced by the softmax function\n\nwhere \"K\" is the number of discrete values that the visible values have. They are applied in topic modeling, and recommender systems.\n\nRestricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.\nTheir graphical model corresponds to that of factor analysis.\n\nRestricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set formula_20 (a matrix, each row of which is treated as a visible vector formula_21),\n\nor equivalently, to maximize the expected log probability of a training sample formula_21 selected randomly from formula_20:\n\nThe algorithm most often used to train RBMs, that is, to optimize the weight vector formula_26, is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.\nThe algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.\n\nThe basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:\n\n\nA Practical Guide to Training RBMs written by Hinton can be found on his homepage.\n\n\n", "id": "33742232", "title": "Restricted Boltzmann machine"}
{"url": "https://en.wikipedia.org/wiki?curid=404084", "text": "Hebbian theory\n\nHebbian theory is a theory in neuroscience that proposes an explanation for the adaptation of neurons in the brain during the learning process. It describes a basic mechanism for synaptic plasticity, where an increase in synaptic efficacy arises from the presynaptic cell's repeated and persistent stimulation of the postsynaptic cell. Introduced by Donald Hebb in his 1949 book \"The Organization of Behavior\", the theory is also called Hebb's rule, Hebb's postulate, and cell assembly theory. Hebb states it as follows:\n\nLet us assume that the persistence or repetition of a reverberatory activity (or \"trace\") tends to induce lasting cellular changes that add to its stability.[…] When an axon of cell \"A\" is near enough to excite a cell \"B\" and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that \"A\"s efficiency, as one of the cells firing \"B\", is increased.\n\nThe theory is often summarized as \"Cells that fire together wire together.\" This summary, however, should not be taken too literally. Hebb emphasized that cell \"A\" needs to \"take part in firing\" cell \"B\", and such causality can occur only if cell \"A\" fires just before, not at the same time as, cell \"B\". This important aspect of causation in Hebb's work foreshadowed what is now known about spike-timing-dependent plasticity, which requires temporal precedence.\n\nThe theory attempts to explain associative or Hebbian learning, in which simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells. It also provides a biological basis for errorless learning methods for education and memory rehabilitation. In the study of neural networks in cognitive function, it is often regarded as the neuronal basis of unsupervised learning.\n\nHebbian theory concerns how neurons might connect themselves to become engrams. Hebb's theories on the form and function of cell assemblies can be understood from the following:\n\nThe general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend to become 'associated', so that activity in one facilitates activity in the other.\n\nHebb also wrote:\nWhen one cell repeatedly assists in firing another, the axon of the first cell develops synaptic knobs (or enlarges them if they already exist) in contact with the soma of the second cell.\n\nGordon Allport posits additional ideas regarding cell assembly theory and its role in forming engrams, along the lines of the concept of auto-association, described as follows:\n\nIf the inputs to a system cause the same pattern of activity to occur repeatedly, the set of active elements constituting that pattern will become increasingly strongly interassociated. That is, each element will tend to turn on every other element and (with negative weights) to turn off the elements that do not form part of the pattern. To put it another way, the pattern as a whole will become 'auto-associated'. We may call a learned (auto-associated) pattern an engram.\n\nHebbian theory has been the primary basis for the conventional view that, when analyzed from a holistic level, engrams are neuronal nets or neural networks.\n\nWork in the laboratory of Eric Kandel has provided evidence for the involvement of Hebbian learning mechanisms at synapses in the marine gastropod \"Aplysia californica\".\n\nExperiments on Hebbian synapse modification mechanisms at the central nervous system synapses of vertebrates are much more difficult to control than are experiments with the relatively simple peripheral nervous system synapses studied in marine invertebrates. Much of the work on long-lasting synaptic changes between vertebrate neurons (such as long-term potentiation) involves the use of non-physiological experimental stimulation of brain cells. However, some of the physiologically relevant synapse modification mechanisms that have been studied in vertebrate brains do seem to be examples of Hebbian processes. One such study reviews results from experiments that indicate that long-lasting changes in synaptic strengths can be induced by physiologically relevant synaptic activity working through both Hebbian and non-Hebbian mechanisms.\n\nFrom the point of view of artificial neurons and artificial neural networks, Hebb's principle can be described as a method of determining how to alter the weights between model neurons. The weight between two neurons increases if the two neurons activate simultaneously, and reduces if they activate separately. Nodes that tend to be either both positive or both negative at the same time have strong positive weights, while those that tend to be opposite have strong negative weights.\n\nThe following is a formulaic description of Hebbian learning: (note that many other descriptions are possible)\n\nwhere formula_2 is the weight of the connection from neuron formula_3 to neuron formula_4 and formula_5 the input for neuron formula_4. Note that this is pattern learning (weights updated after every training example). In a Hopfield network, connections formula_2 are set to zero if formula_8 (no reflexive connections allowed). With binary neurons (activations either 0 or 1), connections would be set to 1 if the connected neurons have the same activation for a pattern.\n\nAnother formulaic description is:\n\nwhere formula_2 is the weight of the connection from neuron formula_3 to neuron formula_4, formula_13 is the number of training patterns, and formula_14 the formula_15th input for neuron formula_4. This is learning by epoch (weights updated after all the training examples are presented). Again, in a Hopfield network, connections formula_2 are set to zero if formula_8 (no reflexive connections).\n\nA variation of Hebbian learning that takes into account phenomena such as blocking and many other neural learning phenomena is the mathematical model of Harry Klopf. Klopf's model reproduces a great many biological phenomena, and is also simple to implement.\n\nHebb's Rule is often generalized as\n\nor the change in the formula_20th synaptic weight formula_21 is equal to a learning rate formula_22 times the formula_20th input formula_24 times the postsynaptic response formula_25. Often cited is the case of a linear neuron,\n\nand the previous section's simplification takes both the learning rate and the input weights to be 1. This version of the rule is clearly unstable, as in any network with a dominant signal the synaptic weights will increase or decrease exponentially. However, it can be shown that for \"any\" neuron model, Hebb's rule is unstable. Therefore, network models of neurons usually employ other learning theories such as BCM theory, Oja's rule, or the Generalized Hebbian Algorithm.\n\nNeurons also exhibit plasticity in their intrinsic excitability (intrinsic plasticity).\n\nDespite the common use of Hebbian models for long-term potentiation, there exist several exceptions to Hebb's principles and examples that demonstrate that some aspects of the theory are oversimplified. One of the most well-documented of these exceptions pertains to how synaptic modification may not simply occur only between activated neurons A and B, but to neighboring neurons as well. This is due to how Hebbian modification depends on retrograde signaling in order to modify the presynaptic neuron. The compound most commonly identified as fulfilling this retrograde transmitter role is nitric oxide, which, due to its high solubility and diffusibility, often exerts effects on nearby neurons. This type of diffuse synaptic modification, known as volume learning, counters, or at least supplements, the traditional Hebbian model.\n\nHebbian learning and spike-timing-dependent plasticity have been used in an influential theory of how mirror neurons emerge. Mirror neurons are neurons that fire both when an individual performs an action and when the individual sees or hears another perform a similar action. The discovery of these neurons has been very influential in explaining how individuals make sense of the actions of others, by showing that, when a person perceives the actions of others, the person activates the motor programs which they would use to perform similar actions. The activation of these motor programs then adds information to the perception and helps predict what the person will do next based on the perceiver's own motor program. A challenge has been to explain how individuals come to have neurons that respond both while performing an action and while hearing or seeing another perform similar actions.\n\nChristian Keysers and David Perrett suggested that, while an individual performs a particular action, the individual will see, hear, and feel himself perform the action. These re-afferent sensory signals will trigger activity in neurons responding to the sight, sound, and feel of the action. Because the activity of these sensory neurons will consistently overlap in time with those of the motor neurons that caused the action, Hebbian learning would predict that the synapses connecting neurons responding to the sight, sound, and feel of an action and those of the neurons triggering the action should be potentiated. The same is true while people look at themselves in the mirror, hear themselves babble, or are imitated by others. After repeated experience of this re-afference, the synapses connecting the sensory and motor representations of an action would be so strong that the motor neurons would start firing to the sound or the vision of the action, and a mirror neuron would have been created.\n\nEvidence for that perspective comes from many experiments that show that motor programs can be triggered by novel auditory or visual stimuli after repeated pairing of the stimulus with the execution of the motor program (for a review of the evidence, see Giudice et al., 2009). For instance, people who have never played the piano do not activate brain regions involved in playing the piano when listening to piano music. Five hours of piano lessons, in which the participant is exposed to the sound of the piano each time he presses a key, suffices to later trigger activity in motor regions of the brain upon listening to piano music. Consistent with the fact that spike-timing-dependent plasticity occurs only if the presynaptic neuron's firing predicts the post-synaptic neuron's firing, the link between sensory stimuli and motor programs also only seem to be potentiated if the stimulus is contingent on the motor program.\n\n\n\n", "id": "404084", "title": "Hebbian theory"}
{"url": "https://en.wikipedia.org/wiki?curid=50073184", "text": "Generative adversarial network\n\nGenerative adversarial networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. They were introduced by Ian Goodfellow \"et al.\" in 2014.\nThis technique can generate photographs that look at least superficially authentic to human observers, having many realistic characteristics (though in tests people can tell real from generated in many cases).\n\nOne network generates candidates and the other evaluates them. Typically, the generative network learns to map from a latent space to a particular data distribution of interest, while the discriminative network discriminates between instances from the true data distribution and candidates produced by the generator. The generative network's training objective is to increase the error rate of the discriminative network (i.e., \"fool\" the discriminator network by producing novel synthesized instances that appear to have come from the true data distribution).\n\nIn practice, a known dataset serves as the initial training data for the discriminator. Training the discriminator involves presenting it with samples from the dataset, until it reaches some level of accuracy. Typically the generator is seeded with a randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, samples synthesized by the generator are evaluated by the discriminator. Backpropagation is applied in both networks so that the generator produces better images, while the discriminator becomes more skilled at flagging synthetic images. The generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\n\nThe idea to infer models in a competitive setting (model versus discriminator) was proposed by Li, Gauci and Gross in 2013. Their method is used for behavioral inference. It is termed Turing Learning, as the setting is akin to that of a Turing test. The idea of adversarial training can also be found in earlier works, such as Schmidhuber in 1992.\n\nGANs have been used to produce samples of photorealistic images for the purposes of visualizing new interior/industrial design, shoes, bags and clothing items or items for computer games' scenes. These networks were reported to be used by Facebook. Recently, GANs have modeled patterns of motion in video. They have also been used to reconstruct 3D models of objects from images and to improve astronomical images. In 2017 a fully convolutional feedforward GAN was used for image enhancement using automated texture synthesis in combination with perceptual loss. The system focused on realistic textures rather than pixel-accuracy. The result was a higher image quality at high magnification.\n", "id": "50073184", "title": "Generative adversarial network"}
{"url": "https://en.wikipedia.org/wiki?curid=47805", "text": "Vector quantization\n\nVector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.\n\nThe density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.\n\nVector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.\n\nA simple training algorithm for vector quantization is :\n\nA more sophisticated algorithm reduces the bias in the density matching estimation, and ensures that all points are used, by including an extra sensitivity parameter :\n\nIt is desirable to use a cooling schedule to produce convergence: see Simulated annealing. Another (simpler) method is LBG which is based on K-Means.\n\nThe algorithm can be iteratively updated with 'live' data, rather than by picking random points from a data set, but this will introduce some bias if the data are temporally correlated over many samples.\nA vector is represented either geometrically by an arrow whose length corresponds to its magnitude and points in an appropriate direction, or by two or three numbers representing the magnitude of its components.\n\nVector quantization is used for lossy data compression, lossy data correction, pattern recognition, density estimation and clustering.\n\nLossy data correction, or prediction, is used to recover data missing from some dimensions. It is done by finding the nearest group with the data dimensions available, then predicting the result based on the values for the missing dimensions, assuming that they will have the same value as the group's centroid.\n\nFor density estimation, the area/volume that is closer to a particular centroid than to any other is inversely proportional to the density (due to the density matching property of the algorithm).\n\nVector quantization, also called \"block quantization\" or \"pattern matching quantization\" is often used in lossy data compression. It works by encoding values from a multidimensional vector space into a finite set of values from a discrete subspace of lower dimension. A lower-space vector requires less storage space, so the data is compressed. Due to the density matching property of vector quantization, the compressed data has errors that are inversely proportional to density.\n\nThe transformation is usually done by projection or by using a codebook. In some cases, a codebook can be also used to entropy code the discrete value in the same step, by generating a prefix coded variable-length encoded value as its output.\n\nThe set of discrete amplitude levels is quantized jointly rather than each sample being quantized separately. Consider a \"k\"-dimensional vector formula_12 of amplitude levels. It is compressed by choosing the nearest matching vector from a set of \"n\"-dimensional vectors formula_13, with \"n\" < \"k\".\n\nAll possible combinations of the \"n\"-dimensional vector formula_13 form the vector space to which all the quantized vectors belong.\nOnly the index of the codeword in the codebook is sent instead of the quantized values. This conserves space and achieves more compression.\n\nTwin vector quantization (VQF) is part of the MPEG-4 standard dealing with time domain weighted interleaved vector quantization.\n\nThe usage of video codecs based on vector quantization has declined significantly in favor of those based on motion compensated prediction combined with transform coding, e.g. those defined in MPEG standards, as the low decoding complexity of vector quantization has become less relevant.\n\n\nVQ was also used in the eighties for speech and speaker recognition.\nRecently it has also been used for efficient nearest neighbor search \n\nand on-line signature recognition. \nIn pattern recognition applications, one codebook is constructed for each class (each class being a user in biometric applications) using acoustic vectors of this user. In the testing phase the quantization distortion of a testing signal is worked out with the whole set of codebooks obtained in the training phase. The codebook that provides the smallest vector quantization distortion indicates the identified user.\n\nThe main advantage of VQ in pattern recognition is its low computational burden when compared with other techniques such as dynamic time warping (DTW) and hidden Markov model (HMM). The main drawback when compared to DTW and HMM is that it does not take into account the temporal evolution of the signals (speech, signature, etc.) because all the vectors are mixed up. In order to overcome this problem a multi-section codebook approach has been proposed. The multi-section approach consists of modelling the signal with several sections (for instance, one codebook for the initial part, another one for the center and a last codebook for the ending part).\n\nAs VQ is seeking for centroids as density points of nearby lying samples, it can be also directly used as a prototype-based clustering method: each centroid is then associated with one prototype. \nBy aiming to minimize the expected squared quantization error and introducing a decreasing learning gain fulfilling the Robbins-Monro conditions, multiple iterations over the whole data set with a concrete but fixed number of prototypes converges to the solution of k-means clustering algorithm in an incremental manner.\n\n\"Part of this article was originally based on material from the Free On-line Dictionary of Computing and is used with under the GFDL.\"\n\n", "id": "47805", "title": "Vector quantization"}
{"url": "https://en.wikipedia.org/wiki?curid=76996", "text": "Self-organizing map\n\nA self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\nThis makes SOMs useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.\n\nLike most artificial neural networks, SOMs operate in two modes: training and mapping. \"Training\" builds the map using input examples (a competitive process, also called vector quantization), while \"mapping\" automatically classifies a new input vector.\n\nA self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. The usual arrangement of nodes is a two-dimensional regular spacing in a hexagonal or rectangular grid. The self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. The procedure for placing a vector from data space onto the map is to find the node with the closest (smallest distance metric) weight vector to the data space vector.\n\nWhile it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation.\n\nUseful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes.\n\nIt has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.\n\nIt is also common to use the U-Matrix. The U-Matrix value of a particular node is the average distance between the node's weight vector and that of its closest neighbors. In a square grid, for instance, we might consider the closest 4 or 8 nodes (the Von Neumann and Moore neighborhoods, respectively), or six nodes in a hexagonal grid.\n\nLarge SOMs display emergent properties. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself.\n\nThe goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. This is partly motivated by how visual, auditory or other sensory information is handled in separate parts of the cerebral cortex in the human brain.\nThe weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors. With the latter alternative, learning is much faster because the initial weights already give a good approximation of SOM weights.\n\nThe network must be fed a large number of example vectors that represent, as close as possible, the kinds of vectors expected during mapping. The examples are usually administered several times as iterations.\n\nThe training utilizes competitive learning. When a training example is fed to the network, its Euclidean distance to all weight vectors is computed. The neuron whose weight vector is most similar to the input is called the best matching unit (BMU). The weights of the BMU and neurons close to it in the SOM lattice are adjusted towards the input vector. The magnitude of the change decreases with time and with distance (within the lattice) from the BMU. The update formula for a neuron v with weight vector W(s) is\nwhere s is the step index, t an index into the training sample, u is the index of the BMU for D(t), α(s) is a monotonically decreasing learning coefficient and D(t) is the input vector; Θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s. Depending on the implementations, t can scan the training data set systematically (t is 0, 1, 2...T-1, then repeat, T being the training sample's size), be randomly drawn from the data set (bootstrap sampling), or implement some other sampling method (such as jackknifing).\n\nThe neighborhood function Θ(u, v, s) depends on the lattice distance between the BMU (neuron \"u)\" and neuron \"v\". In the simplest form it is 1 for all neurons close enough to BMU and 0 for others, but a Gaussian function is a common choice, too. Regardless of the functional form, the neighborhood function shrinks with time. At the beginning when the neighborhood is broad, the self-organizing takes place on the global scale. When the neighborhood has shrunk to just a couple of neurons, the weights are converging to local estimates. In some implementations the learning coefficient α and the neighborhood function Θ decrease steadily with increasing s, in others (in particular those where t scans the training data set) they decrease in step-wise fashion, once every T steps.\n\nThis process is repeated for each input vector for a (usually large) number of cycles λ. The network winds up associating output nodes with groups or patterns in the input data set. If these patterns can be named, the names can be attached to the associated nodes in the trained net.\n\nDuring mapping, there will be one single \"winning\" neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector.\n\nWhile representing input data as vectors has been emphasized in this article, it should be noted that any kind of object which can be represented digitally, which has an appropriate distance measure associated with it, and in which the necessary operations for training are possible can be used to construct a self-organizing map. This includes matrices, continuous functions or even other self-organizing maps.\n\nThese are the variables needed, with vectors in bold,\n\n\nA variant algorithm:\n\nSelection of a good initial approximation is a well-known problem for all iterative methods of learning neural networks. Kohonen used random initiation of SOM weights. Recently, principal component initialization, in which initial map weights are chosen from the space of the first principal components, has become popular due to the exact reproducibility of the results.\n\nCareful comparison of the random initiation approach to principal component initialization for one-dimensional SOM (models of principal curves) demonstrated that the advantages of principal component SOM initialization are not universal. The best initialization method depends on the geometry of the specific dataset. Principal component initialization is preferable (in dimension one) if the principal curve approximating the dataset can be univalently and linearly projected on the first principal component (quasilinear sets). For nonlinear datasets, however, random initiation performs better.\n\nConsider an array of nodes, each of which contains a weight vector and is aware of its location in the array. Each weight vector is of the same dimension as the node's input vector. The weights may initially be set to random values.\n\nNow we need input to feed the map. Colors can be represented by their red, green, and blue components. Consequently, we will represent colors as vectors in the unit cube of the free vector space over generated by the basis:\n\nThe diagram shown compares the results of training on the data sets\n\nand the original images. Note the striking resemblance between the two.\n\nSimilarly, after training a grid of neurons for 250 iterations with a learning rate of 0.1 on Fisher's Iris, the map can already detect the main differences between species. \n\nThere are two ways to interpret a SOM. Because in the training phase weights of the whole neighborhood are moved in the same direction, similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and dissimilar ones apart. This may be visualized by a U-Matrix (Euclidean distance between weight vectors of neighboring cells) of the SOM.\n\nThe other way is to think of neuronal weights as pointers to the input space. They form a discrete approximation of the distribution of training samples. More neurons point to regions with high training sample concentration and fewer where the samples are scarce.\n\nSOM may be considered a nonlinear generalization of Principal components analysis (PCA). It has been shown, using both artificial and real geophysical data, that SOM has many advantages over the conventional feature extraction methods such as Empirical Orthogonal Functions (EOF) or PCA.\n\nOriginally, SOM was not formulated as a solution to an optimisation problem. Nevertheless, there have been several attempts to modify the definition of SOM and to formulate an optimisation problem which gives similar results. For example, Elastic maps use the mechanical metaphor of elasticity to approximate principal manifolds: the analogy is an elastic membrane and plate.\n\n\n\n", "id": "76996", "title": "Self-organizing map"}
{"url": "https://en.wikipedia.org/wiki?curid=26266110", "text": "Competitive learning\n\nCompetitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network. It is well suited to finding clusters within data.\n\nModels and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps (Kohonen maps).\n\nThere are three basic elements to a competitive learning rule:\n\n\nAccordingly, the individual neurons of the network learn to specialize on ensembles of similar patterns and in so doing become 'feature detectors' for different classes of input patterns.\n\nThe fact that competitive networks recode sets of correlated inputs to one of a few output neurons essentially removes the redundancy in representation which is an essential part of processing in biological sensory systems.\n\nCompetitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as “competitive layer”. Every competitive neuron is described by a vector of weights formula_1 and calculates the similarity measure between the input data formula_2 and the weight vector formula_3 .\n\nFor every input vector, the competitive neurons “compete” with each other to see which one of them is the most similar to that particular input vector. The winner neuron m sets its output formula_4 and all the other competitive neurons set their output formula_5.\n\nUsually, in order to measure similarity the inverse of the Euclidean distance is used: formula_6 between the input vector formula_7 and the weight vector formula_3.\n\nHere is a simple competitive learning algorithm to find three clusters within some input data.\n\n1. (Set-up.) Let a set of sensors all feed into three different nodes, so that every node is connected to every sensor. Let the weights that each node gives to its sensors be set randomly between 0.0 and 1.0. Let the output of each node be the sum of all its sensors, each sensor's signal strength being multiplied by its weight.\n\n2. When the net is shown an input, the node with the highest output is deemed the winner. The input is classified as being within the cluster corresponding to that node.\n\n3. The winner updates each of its weights, moving weight from the connections that gave it weaker signals to the connections that gave it stronger signals.\n\nThus, as more data are received, each node converges on the centre of the cluster that it has come to represent and activates more strongly for inputs in this cluster and more weakly for inputs in other clusters.\n\n\n", "id": "26266110", "title": "Competitive learning"}
{"url": "https://en.wikipedia.org/wiki?curid=1217294", "text": "Linear genetic programming\n\nLinear genetic programming (LGP) is a particular subset of genetic programming wherein computer programs in a population are represented as a sequence of instructions from imperative programming language or machine language. The graph-based data flow that results from a multiple usage of register contents and the existence of structurally noneffective code (introns) are two main differences of this genetic representation from the more common tree-based genetic programming (TGP) variant.\n\nIn genetic programming (GP) a linear tree is a program composed of a variable number of unary functions and a single terminal. Note linear tree GP differs from bit string genetic algorithms since a population may contain programs of different lengths and there may be more than two types of functions or more than two types of terminals.\n\nBecause LGP programs are basically represented by a linear sequence of instructions, they are simpler to read and to operate on than their tree-based counterparts. For example, a simple program written in the LGP language Slash/A looks like a series of instructions separated by a slash:\n\nBy representing such code in bytecode format, i.e. as an array of bytes each representing a different instruction, one can make mutation operations simply by changing an element of such an array.\n\n\n", "id": "1217294", "title": "Linear genetic programming"}
{"url": "https://en.wikipedia.org/wiki?curid=5999404", "text": "Joseph Nechvatal\n\nJoseph James Nechvatal (born 15 January 1951) is a post-conceptual digital artist and art theoretician who creates computer-assisted paintings and computer animations, often using custom-created computer viruses.\n\nJoseph Nechvatal was born in Chicago. He studied fine art and philosophy at Southern Illinois University Carbondale, Cornell University and Columbia University, where he studied with Arthur Danto while serving as the archivist to the minimalist composer La Monte Young. From 1979, he exhibited his work in New York City, primarily at Galerie Richard, Brooke Alexander Gallery and Universal Concepts Unlimited. He has also solo exhibited in Berlin, Paris, Chicago, Cologne, Atlanta, Los Angeles, Aalst, Belgium, Youngstown, Senouillac, Lund, Toulouse, Turin and Munich.\n\nHis work in the early 1980s chiefly consisted of postminimalist gray graphite drawings that were often photomechanically enlarged. During that period he was associated with the artist group Colab and helped establish the non-profit cultural space ABC No Rio. In 1983 he co-founded the avant-garde electronic art music audio project Tellus Audio Cassette Magazine. In 1984, Nechvatal began work on an opera called \"\" (1984-6) with the no wave musical composer Rhys Chatham.\n\nHe began using computers to make \"paintings\" in 1986 and later, in his signature work, began to employ computer viruses. These \"collaborations\" with viral systems positioned his work as an early contribution to what is increasingly referred to as a post-human aesthetic.\n\nFrom 1991–1993 he was artist-in-residence at the Louis Pasteur Atelier in Arbois, France and at the Saline Royale/Ledoux Foundation's computer lab.There he worked on \"The Computer Virus Project\", which was an artistic experiment with computer viruses and computer animation. He exhibited at Documenta 8 in 1987.\n\nIn 1999 Nechvatal obtained his Ph.D. in the philosophy of art and new technology concerning immersive virtual reality at Roy Ascott's Centre for Advanced Inquiry in the Interactive Arts (CAiiA), University of Wales College, Newport, UK (now the Planetary Collegium at the University of Plymouth). There he developed his concept of viractualism, a conceptual art idea that strives \"to create an interface between the biological and the technological.\" According to Nechvatal, this is a new topological space.\n\nIn 2002 he extended his experimentation into viral artificial life through a collaboration with the programmer Stephane Sikora of music2eye in a work called the \"Computer Virus Project II\", inspired by the a-life work of John Horton Conway (particularly Conway's Game of Life), by the general cellular automata work of John von Neumann, by the genetic programming algorithms of John Koza and the auto-destructive art of Gustav Metzger.\n\nIn 2005 he exhibited \"Computer Virus Project II\" works (digital paintings, digital prints, a digital audio installation and two \"live\" electronic virus-attack art installations) in a solo show called \"cOntaminatiOns\" at Château de Linardié in Senouillac, France. In 2006 Nechvatal received a retrospective exhibition entitled \"Contaminations\" at the Butler Institute of American Art's Beecher Center for Arts and Technology.\n\nDr. Nechvatal has also contributed to digital audio work with his noise music \"viral symphOny\", a collaborative sound symphony created by using his computer virus software at the Institute for Electronic Arts at Alfred University. \"viral symphOny\" was presented as a part of \"nOise anusmOs\" in New York in 2012. In 2016, a limited edition CD recording of his sex farce poetry book \"Destroyer of Naivetés\" was released on Entr’acte label under the name of Cave Bacchus. Cave Bacchus is Nechvatal, Black Sifichi and Rhys Chatham.\n\nIn 2013, Nechvatal showed work in \"Noise\", an official collateral show of the \"55th Venice Biennale of Art\", that was based on his book \"Immersion Into Noise\".\n\nFrom 1999 to 2013, Nechvatal taught art theories of immersive virtual reality and the viractual at the School of Visual Arts in New York City (SVA). A book of his collected essays entitled \"Towards an Immersive Intelligence: Essays on the Work of Art in the Age of Computer Technology and Virtual Reality (1993–2006)\" was published by Edgewise Press in 2009. Also in 2009, his book \"Immersive Ideals / Critical Distances\" was published. In 2011, his book \"Immersion Into Noise\" was published by Open Humanities Press in conjunction with the University of Michigan Library's Scholarly Publishing Office. In 2014 he published (as editor) a book and CD/cassette tape with Punctum Books and Punctum Records on the noise music artist Minóy and in 2015 he published with Punctum Books a collection of his farcical erotic poetry entitled \"Destroyer of Naivetés\". Since 2013, Nechvatal has regularly been publishing his art criticism as the Paris correspondent for Hyperallergic blogazine.\n\nJoe Lewis wrote:\nViractualism is an art theory term developed by Nechvatal in 1999. The term viractualism (and viractuality ) emerged from the Ph.D. research Nechvatal conducted in the philosophy of art and new technology concerning immersive virtual reality at Roy Ascott's Centre for Advanced Inquiry in the Interactive Arts (CAiiA), University of Wales College, Newport, UK (now the Planetary Collegium at the University of Plymouth). There he developed his concept of the viractual, which strives to create an interface between the biological and the virtual. It is central to Nechvatal’s work as an artist.\n\nNechvatal suggests that the term (concept) viractual (and viractualism or viractuality) may be an entrainment/égréore conception helpful in defining our now third-fused inter-spatiality which is forged from the meeting of the virtual and the actual. - a concept close to what the military call augmented reality, which is the use of transparent displays worn as see-through glasses on which computer data is projected and layered.\n\nThe basis of the viractual conception is that virtual producing computer technology has become a noteworthy means for making and understanding contemporary art and that this brings artists to a place where one finds the emerging of the computed (the virtual) with the uncomputed corporeal (the actual). This amalgamate - which tends to contradict some central techno clichés of our time - is what Nechvatal calls the viractual.\nDigitization is a key metaphor for viractuality in the sense that it is the elementary translating procedure today. Nechvatal thinks that in every era the attempt must be made anew to wrest the art practice away from conformisms that are about to overcome it.\n\nCybism is an art theory term developed by Nechvatal as a sub-division of viractuality following discussion with artist Kenneth Wahl at the turn of the century. Wahl preferred the term \"scybism\". The concept was proposed by Nechvatal for an exhibition in 2003 called \"The Attractions of Cybism\" for Fairfield University that never was realized.\n\nAs defined by Dr. Nechvatal, Cybism is a new sensibility emerging in art respecting the integration of certain aspects of science, technology and consciousness – a consciousness struggling to attend to the prevailing current spirit of our age. This cybistic zeitgeist Nechvatal identifies as being precisely a quality-of-life desire in which everything, everywhere, all at once is connected in a rhizomatic web of communication. Therefore, cybism is no longer content with the regurgitation of standardized repertoires. Rather Nechvatal detects in art a fertile attraction towards the abstractions of advanced scientific discovery - discovery now stripped of its fundamentally reductive logical methodology.\n\nNechvatal states that cybism can be used to characterize a certain group of researchers and their understanding of where cultural space is developing today. Cybists reflect on system dynamics with a hybrid blending (cybridization) of the computational supplied virtual with the analog. This blending of the computational virtual with the analog indicates the subsequent emergence of a new cybrid topological cognitive-vision that Nechvatal has called viractuality: the space of connection betwixt the computed virtual and the uncomputed corporeal (actual) world which merge in cybism.\n\nNechvatal states that co-extensive notions found in cybism have sharp ramifications for art as product in that the cybists are actively exploring the frontiers of science/technology research so as to become culturally aware of the biases of consciousness in order to amend those biases through the monumentality and permanency which can be found in powerful art. He begins with the realization that every new technology disrupts the previous rhythms of consciousness. In this sense cybist art research begins where hard science/technology ends.\n\n\n", "id": "5999404", "title": "Joseph Nechvatal"}
{"url": "https://en.wikipedia.org/wiki?curid=12424", "text": "Genetic programming\n\nIn artificial intelligence, genetic programming (GP) is a technique whereby computer programs are encoded as a set of genes that are then modified (evolved) using an evolutionary algorithm (often a genetic algorithm, \"GA\") – it is an application of (for example) genetic algorithms where the space of solutions consists of computer programs. The results are computer programs able to perform well in a predefined task. The methods used to encode a computer program in an artificial chromosome and to evaluate its fitness with respect to the predefined task are central in the GP technique and still the subject of active research.\n\nIn 1954, pioneering work on what is today known as artificial life was carried out by Nils Aall Barricelli using the very early computers. In the 1960s and early 1970s, evolutionary algorithms became widely recognized as optimization methods. Ingo Rechenberg and his group were able to solve complex engineering problems through evolution strategies as documented in his 1971 PhD thesis and the resulting 1973 book. John Holland was highly influential during the 1970s. The establishment of evolutionary algorithms in the scientific community allowed, by then, the first concrete steps to study the GP idea.\n\nIn 1964, Lawrence J. Fogel, one of the earliest practitioners of the GP methodology, applied evolutionary algorithms to the problem of discovering finite-state automata. Later GP-related work grew out of the learning classifier system community, which developed sets of sparse rules describing optimal policies for Markov decision processes. \nIn 1981 Richard Forsyth evolved tree rules to classify heart disease.\nThe first statement of modern \"tree-based\" genetic programming (that is, procedural languages organized in tree-based structures and operated on by suitably defined GA-operators) was given by Nichael L. Cramer (1985). This work was later greatly expanded by John R. Koza, a main proponent of GP who has pioneered the application of genetic programming in various complex optimization and search problems. Gianna Giavelli, a student of Koza's, later pioneered the use of genetic programming as a technique to model DNA expression.\n\nIn the 1990s, GP was mainly used to solve relatively simple problems because it is very computationally intensive. Recently GP has produced many novel and outstanding results in areas such as quantum computing, electronic design, game playing, cyberterrorism prevention, sorting, and searching, due to improvements in GP technology and the exponential growth in CPU power.\nThese results include the replication or development of several post-year-2000 inventions. GP has also been applied to evolvable hardware as well as computer programs.\n\nDeveloping a theory for GP has been very difficult and so in the 1990s GP was considered a sort of outcast among search techniques.\n\nGP evolves computer programs, traditionally represented in memory as tree structures. Trees can be easily evaluated in a recursive manner. Every tree node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate. Thus traditionally GP favors the use of programming languages that naturally embody tree structures (for example, Lisp; other functional programming languages are also suitable).\n\nNon-tree representations have been suggested and successfully implemented, such as linear genetic programming which suits the more traditional imperative languages [see, for example, Banzhaf \"et al.\" (1998)]. The commercial GP software \"Discipulus\" uses automatic induction of binary machine code (\"AIM\") to achieve better performance. \"µGP\" uses directed multigraphs to generate programs that fully exploit the syntax of a given assembly language\n\nMost non-tree representations have structurally noneffective code (introns). Such non-coding genes may seem to be useless, because they have no effect on the performance of any one individual.\nHowever, experiments seem to show faster convergence when using program representations — such as linear genetic programming and Cartesian genetic programming — that allow such non-coding genes, compared to tree-based program representations that do not have any non-coding genes.\n\nThe basic ideas of genetic programming have been modified and extended in a variety of ways:\n\nMeta-genetic programming is the proposed meta learning technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer. Meta-GP was formally proposed by Jürgen Schmidhuber in 1987. Doug Lenat's Eurisko is an earlier effort that may be the same technique. It is a recursive but terminating algorithm, allowing it to avoid infinite recursion.\n\nCritics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes. This might take the form of a meta evolved GP for producing human walking algorithms which is then used to evolve human running, jumping, etc. The fitness criterion applied to the meta GP would simply be one of efficiency.\n\nFor general problem classes there may be no way to show that meta GP will reliably produce results more efficiently than a created algorithm other than exhaustion.\n\n\n", "id": "12424", "title": "Genetic programming"}
{"url": "https://en.wikipedia.org/wiki?curid=1514696", "text": "Parity benchmark\n\nParity problems are widely used as benchmark problems in genetic programming but inherited from the artificial neural network community. Parity is calculated by summing all the binary inputs and reporting if the sum is odd or even. This is considered difficult because:\n\n", "id": "1514696", "title": "Parity benchmark"}
{"url": "https://en.wikipedia.org/wiki?curid=17808671", "text": "Schema (genetic algorithms)\n\nA schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets; and so form a topological space.\n\nFor example, consider binary strings of length 6. The schema 1**0*1 describes the set of all words of length 6 with 1's at the first and sixth positions and a 0 at the fourth position. The * is a wildcard symbol, which means that positions 2, 3 and 5 can have a value of either 1 or 0. The \"order of a schema\" is defined as the number of fixed positions in the template, while the \"defining length\" formula_1 is the distance between the first and last specific positions. The order of 1**0*1 is 3 and its defining length is 5. The \"fitness of a schema\" is the average fitness of all strings matching the schema. The fitness of a string is a measure of the value of the encoded problem solution, as computed by a problem-specific evaluation function.\n\nThe length of a schema formula_2, called formula_3, is defined as the total number of nodes in the schema. formula_3 is also equal to the number of nodes in the programs matching formula_2.\n\nIf the child of an individual that matches schema H does not \"itself\" match H, the schema is said to have been \"disrupted\".\n\nIn evolutionary computing such as genetic algorithms and genetic programming, propagation refers to the inheritance of characteristics of one generation by the next. For example, a schema is propagated if individuals in the current generation match it and so do those in the next generation. Those in the next generation may be (but don't have to be) children of parents who matched it.\n\nRecently schema have been studied using order theory.\n\nTwo basic operators are defined for schema: expansion and compression. The expansion maps a schema onto a set of words which it represents, while the compression maps a set of words on to a schema.\n\nIn the following definitions formula_6 denotes an alphabet, formula_7 denotes all words of length formula_8 over the alphabet formula_6, formula_10 denotes the alphabet formula_11 with the extra symbol formula_12. formula_13 denotes all schema of length formula_8 over the alphabet formula_10 as well as the empty schema formula_16.\nFor any schema formula_17 the following operator formula_18, called the formula_19 of formula_20, which maps formula_20 to a subset of words in formula_22:\n\nformula_23\n\nWhere subscript formula_24 denotes the character at position formula_24 in a word or schema. When formula_26 then formula_27. More simply put, formula_18 is the set of all words in formula_29 that can be made by exchanging the formula_12 symbols in formula_20 with symbols from formula_11. For example, if formula_33, formula_34 and formula_35 then formula_36.\n\nConversely, for any formula_37 we define formula_38, called the formula_39 of formula_40, which maps formula_40 on to a schema formula_42:\nformula_43\nwhere formula_20 is a schema of length formula_45 such that the symbol at position formula_24 in formula_20 is determined in the following way: if formula_48 for all formula_49 then formula_50 otherwise formula_51. If formula_52 then formula_53. One can think of this operator as stacking up all the items in formula_40 and if all elements in a column are equivalent, the symbol at that position in formula_20 takes this value, otherwise there is a wild card symbol. For example, let formula_56 then formula_57.\n\nSchemata can be partially ordered. For any formula_58 we say formula_59 if and only if formula_60. It follows that formula_61 is a partial ordering on a set of schemata from the reflexivity, antisymmetry and transitivity of the subset relation. For example, formula_62.\nThis is because formula_63.\n\nThe compression and expansion operators form a Galois connection, where formula_64 is the lower adjoint and formula_65 the upper adjoint.\n\nFor a set formula_37, we call the process of calculating the compression on each subset of A, that is <math>\\\n", "id": "17808671", "title": "Schema (genetic algorithms)"}
{"url": "https://en.wikipedia.org/wiki?curid=31726662", "text": "Santa Fe Trail problem\n\nThe Santa Fe Trail problem is a genetic programming exercise in which artificial ants search for food pellets according to a programmed set of instructions. The layout of food pellets in the Santa Fe Trail problem has become a standard for comparing different genetic programming algorithms and solutions.\n\nOne method for programming and testing algorithms on the Santa Fe Trail problem is by using the NetLogo application. There is at least one case of a student creating a Lego robotic ant to solve the problem.\n\n\n", "id": "31726662", "title": "Santa Fe Trail problem"}
{"url": "https://en.wikipedia.org/wiki?curid=34114298", "text": "Java Grammatical Evolution\n\nIn computer science, Java Grammatical Evolution is an implementation of grammatical evolution in the Java programming language. Examples include jGE library and GEVA.\n\njGE library was the first published implementation of grammatical evolution in the Java language. Today, another well-known published Java implementation exists, named GEVA. GEVA was developed at University College Dublin's Natural Computing Research & Applications group under the guidance of one of the inventors of grammatical evolution, Dr. Michael O'Neill.\n\njGE library aims to provide not only an implementation of grammatical evolution, but also a free, open-source, and extendable framework for experimentation in the area of evolutionary computation. Namely, it supports the implementation (through additions and extensions) of any evolutionary algorithm. Furthermore, its extendable architecture and design facilitates the implementation and incorporation of new experimental implementation inspired by natural evolution and biology.\n\nThe jGE library binary file, the source code, the documentation, and an extension for the NetLogo modeling environment, named jGE NetLogo extension, can be downloaded from the jGE Official Web Site.\n\nThe jGE library is free software released under the GNU General Public License v3.\n\n", "id": "34114298", "title": "Java Grammatical Evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=1098818", "text": "Gene expression programming\n\nIn computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype–phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.\n\nEvolutionary algorithms use populations of individuals, select individuals according to fitness, and introduce genetic variation using one or more genetic operators. Their use in artificial computational systems dates back to the 1950s where they were used to solve optimization problems (e.g. Box 1957 and Friedman 1959). But it was with the introduction of evolution strategies by Rechenberg in 1965 that evolutionary algorithms gained popularity. A good overview text on evolutionary algorithms is the book \"An Introduction to Genetic Algorithms\" by Mitchell (1996).\n\nGene expression programming belongs to the family of evolutionary algorithms and is closely related to genetic algorithms and genetic programming. From genetic algorithms it inherited the linear chromosomes of fixed length; and from genetic programming it inherited the expressive parse trees of varied sizes and shapes.\n\nIn gene expression programming the linear chromosomes work as the genotype and the parse trees as the phenotype, creating a genotype/phenotype system. This genotype/phenotype system is multigenic, thus encoding multiple parse trees in each chromosome. This means that the computer programs created by GEP are composed of multiple parse trees. Because these parse trees are the result of gene expression, in GEP they are called expression trees.\n\nThe genome of gene expression programming consists of a linear, symbolic string or chromosome of fixed length composed of one or more genes of equal size. These genes, despite their fixed length, code for expression trees of different sizes and shapes. An example of a chromosome with two genes, each of size 9, is the string (position zero indicates the start of each gene):\n\nwhere “L” represents the natural logarithm function and “a”, “b”, “c”, and “d” represent the variables and constants used in a problem.\n\nAs shown , the genes of gene expression programming have all the same size. However, these fixed length strings code for expression trees of different sizes. This means that the size of the coding regions varies from gene to gene, allowing for adaptation and evolution to occur smoothly.\n\nFor example, the mathematical expression:\ncan also be represented as an expression tree:\n\nwhere \"Q” represents the square root function.\n\nThis kind of expression tree consists of the phenotypic expression of GEP genes, whereas the genes are linear strings encoding these complex structures. For this particular example, the linear string corresponds to:\n\nwhich is the straightforward reading of the expression tree from top to bottom and from left to right. These linear strings are called k-expressions (from Karva notation).\n\nGoing from k-expressions to expression trees is also very simple. For example, the following k-expression:\n\nis composed of two different terminals (the variables “a” and “b”), two different functions of two arguments (“*” and “+”), and a function of one argument (“Q”). Its expression gives:\n\nThe k-expressions of gene expression programming correspond to the region of genes that gets expressed. This means that there might be sequences in the genes that are not expressed, which is indeed true for most genes. The reason for these noncoding regions is to provide a buffer of terminals so that all k-expressions encoded in GEP genes correspond always to valid programs or expressions.\n\nThe genes of gene expression programming are therefore composed of two different domains – a head and a tail – each with different properties and functions. The head is used mainly to encode the functions and variables chosen to solve the problem at hand, whereas the tail, while also used to encode the variables, provides essentially a reservoir of terminals to ensure that all programs are error-free.\n\nFor GEP genes the length of the tail is given by the formula:\n\nwhere \"h\" is the head’s length and \"n\" is maximum arity. For example, for a gene created using the set of functions F = {Q, +, −, *, /} and the set of terminals T = {a, b}, \"n\" = 2. And if we choose a head length of 15, then \"t\" = 15 (2–1) + 1 = 16, which gives a gene length \"g\" of 15 + 16 = 31. The randomly generated string below is an example of one such gene:\n\nIt encodes the expression tree:\n\nwhich, in this case, only uses 8 of the 31 elements that constitute the gene.\n\nIt's not hard to see that, despite their fixed length, each gene has the potential to code for expression trees of different sizes and shapes, with the simplest composed of only one node (when the first element of a gene is a terminal) and the largest composed of as many nodes as there are elements in the gene (when all the elements in the head are functions with maximum arity).\n\nIt's also not hard to see that it is trivial to implement all kinds of genetic modification (mutation, inversion, insertion, recombination, and so on) with the guarantee that all resulting offspring encode correct, error-free programs.\n\nThe chromosomes of gene expression programming are usually composed of more than one gene of equal length. Each gene codes for a sub-expression tree (sub-ET) or sub-program. Then the sub-ETs can interact with one another in different ways, forming a more complex program. The figure shows an example of a program composed of three sub-ETs.\n\nIn the final program the sub-ETs could be linked by addition or some other function, as there are no restrictions to the kind of linking function one might choose. Some examples of more complex linkers include taking the average, the median, the midrange, thresholding their sum to make a binomial classification, applying the sigmoid function to compute a probability, and so on. These linking functions are usually chosen a priori for each problem, but they can also be evolved elegantly and efficiently by the cellular system of gene expression programming.\n\nIn gene expression programming, homeotic genes control the interactions of the different sub-ETs or modules of the main program. The expression of such genes results in different main programs or cells, that is, they determine which genes are expressed in each cell and how the sub-ETs of each cell interact with one another. In other words, homeotic genes determine which sub-ETs are called upon and how often in which main program or cell and what kind of connections they establish with one another.\n\nHomeotic genes have exactly the same kind of structural organization as normal genes and they are built using an identical process. They also contain a head domain and a tail domain, with the difference that the heads contain now linking functions and a special kind of terminals – genic terminals – that represent the normal genes. The expression of the normal genes results as usual in different sub-ETs, which in the cellular system are called ADFs (automatically defined functions). As for the tails, they contain only genic terminals, that is, derived features generated on the fly by the algorithm.\n\nFor example, the chromosome in the figure has three normal genes and one homeotic gene and encodes a main program that invokes three different functions a total of four times, linking them in a particular way.\n\nFrom this example it is clear that the cellular system not only allows the unconstrained evolution of linking functions but also code reuse. And it shouldn't be hard to implement recursion in this system.\n\nMulticellular systems are composed of more than one homeotic gene. Each homeotic gene in this system puts together a different combination of sub-expression trees or ADFs, creating multiple cells or main programs.\n\nFor example, the program shown in the figure was created using a cellular system with two cells and three normal genes.\n\nThe applications of these multicellular systems are multiple and varied and, like the multigenic systems, they can be used both in problems with just one output and in problems with multiple outputs.\n\nThe head/tail domain of GEP genes (both normal and homeotic) is the basic building block of all GEP algorithms. However, gene expression programming also explores other chromosomal organizations that are more complex than the head/tail structure. Essentially these complex structures consist of functional units or genes with a basic head/tail domain plus one or more extra domains. These extra domains usually encode random numerical constants that the algorithm relentlessly fine-tunes in order to find a good solution. For instance, these numerical constants may be the weights or factors in a function approximation problem (see the GEP-RNC algorithm below); they may be the weights and thresholds of a neural network (see the GEP-NN algorithm below); the numerical constants needed for the design of decision trees (see the GEP-DT algorithm below); the weights needed for polynomial induction; or the random numerical constants used to discover the parameter values in a parameter optimization task.\n\nThe fundamental steps of the basic gene expression algorithm are listed below in pseudocode:\nThe first four steps prepare all the ingredients that are needed for the iterative loop of the algorithm (steps 5 through 10). Of these preparative steps, the crucial one is the creation of the initial population, which is created randomly using the elements of the function and terminal sets.\n\nLike all evolutionary algorithms, gene expression programming works with populations of individuals, which in this case are computer programs. Therefore, some kind of initial population must be created to get things started. Subsequent populations are descendants, via selection and genetic modification, of the initial population.\n\nIn the genotype/phenotype system of gene expression programming, it is only necessary to create the simple linear chromosomes of the individuals without worrying about the structural soundness of the programs they code for, as their expression always results in syntactically correct programs.\n\nFitness functions and selection environments (called training datasets in machine learning) are the two facets of fitness and are therefore intricately connected. Indeed, the fitness of a program depends not only on the cost function used to measure its performance but also on the training data chosen to evaluate fitness\n\nThe selection environment consists of the set of training records, which are also called fitness cases. These fitness cases could be a set of observations or measurements concerning some problem, and they form what is called the training dataset.\n\nThe quality of the training data is essential for the evolution of good solutions. A good training set should be representative of the problem at hand and also well-balanced, otherwise the algorithm might get stuck at some local optimum. In addition, it is also important to avoid using unnecessarily large datasets for training as this will slow things down unnecessarily. A good rule of thumb is to choose enough records for training to enable a good generalization in the validation data and leave the remaining records for validation and testing.\n\nBroadly speaking, there are essentially three different kinds of problems based on the kind of prediction being made:\nThe first type of problem goes by the name of regression; the second is known as classification, with logistic regression as a special case where, besides the crisp classifications like \"Yes\" or \"No\", a probability is also attached to each outcome; and the last one is related to Boolean algebra and logic synthesis.\n\nIn regression, the response or dependent variable is numeric (usually continuous) and therefore the output of a regression model is also continuous. So it's quite straightforward to evaluate the fitness of the evolving models by comparing the output of the model to the value of the response in the training data.\n\nThere are several basic fitness functions for evaluating model performance, with the most common being based on the error or residual between the model output and the actual value. Such functions include the mean squared error, root mean squared error, mean absolute error, relative squared error, root relative squared error, relative absolute error, and others.\n\nAll these standard measures offer a fine granularity or smoothness to the solution space and therefore work very well for most applications. But some problems might require a coarser evolution, such as determining if a prediction is within a certain interval, for instance less than 10% of the actual value. However, even if one is only interested in counting the hits (that is, a prediction that is within the chosen interval), making populations of models evolve based on just the number of hits each program scores is usually not very efficient due to the coarse granularity of the fitness landscape. Thus the solution usually involves combining these coarse measures with some kind of smooth function such as the standard error measures listed above.\n\nFitness functions based on the correlation coefficient and R-square are also very smooth. For regression problems, these functions work best by combining them with other measures because, by themselves, they only tend to measure correlation, not caring for the range of values of the model output. So by combining them with functions that work at approximating the range of the target values, they form very efficient fitness functions for finding models with good correlation and good fit between predicted and actual values.\n\nThe design of fitness functions for classification and logistic regression takes advantage of three different characteristics of classification models. The most obvious is just counting the hits, that is, if a record is classified correctly it is counted as a hit. This fitness function is very simple and works well for simple problems, but for more complex problems or datasets highly unbalanced it gives poor results.\n\nOne way to improve this type of hits-based fitness function consists of expanding the notion of correct and incorrect classifications. In a binary classification task, correct classifications can be 00 or 11. The \"00\" representation means that a negative case (represented by \"0”) was correctly classified, whereas the \"11\" means that a positive case (represented by \"1”) was correctly classified. Classifications of the type \"00\" are called true negatives (TN) and \"11\" true positives (TP).\n\nThere are also two types of incorrect classifications and they are represented by 01 and 10. They are called false positives (FP) when the actual value is 0 and the model predicts a 1; and false negatives (FN) when the target is 1 and the model predicts a 0. The counts of TP, TN, FP, and FN are usually kept on a table known as the confusion matrix.\n\nSo by counting the TP, TN, FP, and FN and further assigning different weights to these four types of classifications, it is possible to create smoother and therefore more efficient fitness functions. Some popular fitness functions based on the confusion matrix include sensitivity/specificity, recall/precision, F-measure, Jaccard similarity, Matthews correlation coefficient, and cost/gain matrix which combines the costs and gains assigned to the 4 different types of classifications.\n\nThese functions based on the confusion matrix are quite sophisticated and are adequate to solve most problems efficiently. But there is another dimension to classification models which is key to exploring more efficiently the solution space and therefore results in the discovery of better classifiers. This new dimension involves exploring the structure of the model itself, which includes not only the domain and range, but also the distribution of the model output and the classifier margin.\n\nBy exploring this other dimension of classification models and then combining the information about the model with the confusion matrix, it is possible to design very sophisticated fitness functions that allow the smooth exploration of the solution space. For instance, one can combine some measure based on the confusion matrix with the mean squared error evaluated between the raw model outputs and the actual values. Or combine the F-measure with the R-square evaluated for the raw model output and the target; or the cost/gain matrix with the correlation coefficient, and so on. More exotic fitness functions that explore model granularity include the area under the ROC curve and rank measure.\n\nAlso related to this new dimension of classification models, is the idea of assigning probabilities to the model output, which is what is done in logistic regression. Then it is also possible to use these probabilities and evaluate the mean squared error (or some other similar measure) between the probabilities and the actual values, then combine this with the confusion matrix to create very efficient fitness functions for logistic regression. Popular examples of fitness functions based on the probabilities include maximum likelihood estimation and hinge loss.\n\nIn logic there is no model structure (as defined above for classification and logistic regression) to explore: the domain and range of logical functions comprises only 0’s and 1’s or false and true. So, the fitness functions available for Boolean algebra can only be based on the hits or on the confusion matrix as explained in the section above.\n\nRoulette-wheel selection is perhaps the most popular selection scheme used in evolutionary computation. It involves mapping the fitness of each program to a slice of the roulette wheel proportional to its fitness. Then the roulette is spun as many times as there are programs in the population in order to keep the population size constant. So, with roulette-wheel selection programs are selected both according to fitness and the luck of the draw, which means that some times the best traits might be lost. However, by combining roulette-wheel selection with the cloning of the best program of each generation, one guarantees that at least the very best traits are not lost. This technique of cloning the best-of-generation program is known as simple elitism and is used by most stochastic selection schemes.\n\nThe reproduction of programs involves first the selection and then the reproduction of their genomes. Genome modification is not required for reproduction, but without it adaptation and evolution won't take place.\n\nThe selection operator selects the programs for the replication operator to copy. Depending on the selection scheme, the number of copies one program originates may vary, with some programs getting copied more than once while others are copied just once or not at all. In addition, selection is usually set up so that the population size remains constant from one generation to another.\n\nThe replication of genomes in nature is very complex and it took scientists a long time to discover the DNA double helix and propose a mechanism for its replication. But the replication of strings is trivial in artificial evolutionary systems, where only an instruction to copy strings is required to pass all the information in the genome from generation to generation.\n\nThe replication of the selected programs is a fundamental piece of all artificial evolutionary systems, but for evolution to occur it needs to be implemented not with the usual precision of a copy instruction, but rather with a few errors thrown in. Indeed, genetic diversity is created with genetic operators such as mutation, recombination, transposition, inversion, and many others.\n\nIn gene expression programming mutation is the most important genetic operator. It changes genomes by changing an element by another. The accumulation of many small changes over time can create great diversity.\n\nIn gene expression programming mutation is totally unconstrained, which means that in each gene domain any domain symbol can be replaced by another. For example, in the heads of genes any function can be replaced by a terminal or another function, regardless of the number of arguments in this new function; and a terminal can be replaced by a function or another terminal.\n\nRecombination usually involves two parent chromosomes to create two new chromosomes by combining different parts from the parent chromosomes. And as long as the parent chromosomes are aligned and the exchanged fragments are homologous (that is, occupy the same position in the chromosome), the new chromosomes created by recombination will always encode syntactically correct programs.\n\nDifferent kinds of crossover are easily implemented either by changing the number of parents involved (there's no reason for choosing only two); the number of split points; or the way one chooses to exchange the fragments, for example, either randomly or in some orderly fashion. For example, gene recombination, which is a special case of recombination, can be done by exchanging homologous genes (genes that occupy the same position in the chromosome) or by exchanging genes chosen at random from any position in the chromosome.\n\nTransposition involves the introduction of an insertion sequence somewhere in a chromosome. In gene expression programming insertion sequences might appear anywhere in the chromosome, but they are only inserted in the heads of genes. This method guarantees that even insertion sequences from the tails result in error-free programs.\n\nFor transposition to work properly, it must preserve chromosome length and gene structure. So, in gene expression programming transposition can be implemented using two different methods: the first creates a shift at the insertion site, followed by a deletion at the end of the head; the second overwrites the local sequence at the target site and therefore is easier to implement. Both methods can be implemented to operate between chromosomes or within a chromosome or even within a single gene.\n\nInversion is an interesting operator, especially powerful for combinatorial optimization. It consists of inverting a small sequence within a chromosome.\n\nIn gene expression programming it can be easily implemented in all gene domains and, in all cases, the offspring produced is always syntactically correct. For any gene domain, a sequence (ranging from at least two elements to as big as the domain itself) is chosen at random within that domain and then inverted.\n\nSeveral other genetic operators exist and in gene expression programming, with its different genes and gene domains, the possibilities are endless. For example, genetic operators such as one-point recombination, two-point recombination, gene recombination, uniform recombination, gene transposition, root transposition, domain-specific mutation, domain-specific inversion, domain-specific transposition, and so on, are easily implemented and widely used.\n\nNumerical constants are essential elements of mathematical and statistical models and therefore it is important to allow their integration in the models designed by evolutionary algorithms.\n\nGene expression programming solves this problem very elegantly through the use of an extra gene domain – the Dc – for handling random numerical constants (RNC). By combining this domain with a special terminal placeholder for the RNCs, a richly expressive system can be created.\n\nStructurally, the Dc comes after the tail, has a length equal to the size of the tail \"t\", and is composed of the symbols used to represent the RNCs.\n\nFor example, below is shown a simple chromosome composed of only one gene a head size of 7 (the Dc stretches over positions 15–22):\n\nwhere the terminal \"?” represents the placeholder for the RNCs. This kind of chromosome is expressed exactly as shown , giving:\n\nThen the ?'s in the expression tree are replaced from left to right and from top to bottom by the symbols (for simplicity represented by numerals) in the Dc, giving:\n\nThe values corresponding to these symbols are kept in an array. (For simplicity, the number represented by the numeral indicates the order in the array.) For instance, for the following 10 element array of RNCs:\n\nthe expression tree above gives:\n\nThis elegant structure for handling random numerical constants is at the heart of different GEP systems, such as GEP neural networks and GEP decision trees.\n\nLike the basic gene expression algorithm, the GEP-RNC algorithm is also multigenic and its chromosomes are decoded as usual by expressing one gene after another and then linking them all together by the same kind of linking process.\n\nThe genetic operators used in the GEP-RNC system are an extension to the genetic operators of the basic GEP algorithm (see above), and they all can be straightforwardly implemented in these new chromosomes. On the other hand, the basic operators of mutation, inversion, transposition, and recombination are also used in the GEP-RNC algorithm. Furthermore, special Dc-specific operators such as mutation, inversion, and transposition, are also used to aid in a more efficient circulation of the RNCs among individual programs. In addition, there is also a special mutation operator that allows the permanent introduction of variation in the set of RNCs. The initial set of RNCs is randomly created at the beginning of a run, which means that, for each gene in the initial population, a specified number of numerical constants, chosen from a certain range, are randomly generated. Then their circulation and mutation is enabled by the genetic operators.\n\nAn artificial neural network (ANN or NN) is a computational device that consists of many simple connected units or neurons. The connections between the units are usually weighted by real-valued weights. These weights are the primary means of learning in neural networks and a learning algorithm is usually used to adjust them.\n\nStructurally, a neural network has three different classes of units: input units, hidden units, and output units. An activation pattern is presented at the input units and then spreads in a forward direction from the input units through one or more layers of hidden units to the output units. The activation coming into one unit from other unit is multiplied by the weights on the links over which it spreads. All incoming activation is then added together and the unit becomes activated only if the incoming result is above the unit’s threshold.\n\nIn summary, the basic components of a neural network are the units, the connections between the units, the weights, and the thresholds. So, in order to fully simulate an artificial neural network one must somehow encode these components in a linear chromosome and then be able to express them in a meaningful way.\n\nIn GEP neural networks (GEP-NN or GEP nets), the network architecture is encoded in the usual structure of a head/tail domain. The head contains special functions/neurons that activate the hidden and output units (in the GEP context, all these units are more appropriately called functional units) and terminals that represent the input units. The tail, as usual, contains only terminals/input units.\n\nBesides the head and the tail, these neural network genes contain two additional domains, Dw and Dt, for encoding the weights and thresholds of the neural network. Structurally, the Dw comes after the tail and its length \"d\" depends on the head size \"h\" and maximum arity \"n\" and is evaluated by the formula:\n\nThe Dt comes after Dw and has a length \"d\" equal to \"t\". Both domains are composed of symbols representing the weights and thresholds of the neural network.\n\nFor each NN-gene, the weights and thresholds are created at the beginning of each run, but their circulation and adaptation are guaranteed by the usual genetic operators of mutation, transposition, inversion, and recombination. In addition, special operators are also used to allow a constant flow of genetic variation in the set of weights and thresholds.\n\nFor example, below is shown a neural network with two input units (\"i\" and \"i\"), two hidden units (\"h\" and \"h\"), and one output unit (\"o\"). It has a total of six connections with six corresponding weights represented by the numerals 1–6 (for simplicity, the thresholds are all equal to 1 and are omitted):\n\nThis representation is the canonical neural network representation, but neural networks can also be represented by a tree, which, in this case, corresponds to:\n\nwhere \"a” and \"b” represent the two inputs \"i\" and \"i\" and \"D” represents a function with connectivity two. This function adds all its weighted arguments and then thresholds this activation in order to determine the forwarded output. This output (zero or one in this simple case) depends on the threshold of each unit, that is, if the total incoming activation is equal to or greater than the threshold, then the output is one, zero otherwise.\n\nThe above NN-tree can be linearized as follows:\n\nwhere the structure in positions 7–12 (Dw) encodes the weights. The values of each weight are kept in an array and retrieved as necessary for expression.\n\nAs a more concrete example, below is shown a neural net gene for the exclusive-or problem. It has a head size of 3 and Dw size of 6:\n\nIts expression results in the following neural network:\n\nwhich, for the set of weights:\n\nit gives:\n\nwhich is a perfect solution to the exclusive-or function.\n\nBesides simple Boolean functions with binary inputs and binary outputs, the GEP-nets algorithm can handle all kinds of functions or neurons (linear neuron, tanh neuron, atan neuron, logistic neuron, limit neuron, radial basis and triangular basis neurons, all kinds of step neurons, and so on). Also interesting is that the GEP-nets algorithm can use all these neurons together and let evolution decide which ones work best to solve the problem at hand. So, GEP-nets can be used not only in Boolean problems but also in logistic regression, classification, and regression. In all cases, GEP-nets can be implemented not only with multigenic systems but also cellular systems, both unicellular and multicellular. Furthermore, multinomial classification problems can also be tackled in one go by GEP-nets both with multigenic systems and multicellular systems.\n\nDecision trees (DT) are classification models where a series of questions and answers are mapped using nodes and directed edges.\n\nDecision trees have three types of nodes: a root node, internal nodes, and leaf or terminal nodes. The root node and all internal nodes represent test conditions for different attributes or variables in a dataset. Leaf nodes specify the class label for all different paths in the tree.\n\nMost decision tree induction algorithms involve selecting an attribute for the root node and then make the same kind of informed decision about all the nodes in a tree.\n\nDecision trees can also be created by gene expression programming, with the advantage that all the decisions concerning the growth of the tree are made by the algorithm itself without any kind of human input.\n\nThere are basically two different types of DT algorithms: one for inducing decision trees with only nominal attributes and another for inducing decision trees with both numeric and nominal attributes. This aspect of decision tree induction also carries to gene expression programming and there are two GEP algorithms for decision tree induction: the evolvable decision trees (EDT) algorithm for dealing exclusively with nominal attributes and the EDT-RNC (EDT with random numerical constants) for handling both nominal and numeric attributes.\n\nIn the decision trees induced by gene expression programming, the attributes behave as function nodes in the basic gene expression algorithm, whereas the class labels behave as terminals. This means that attribute nodes have also associated with them a specific arity or number of branches that will determine their growth and, ultimately, the growth of the tree. Class labels behave like terminals, which means that for a \"k\"-class classification task, a terminal set with \"k\" terminals is used, representing the \"k\" different classes.\n\nThe rules for encoding a decision tree in a linear genome are very similar to the rules used to encode mathematical expressions (see above). So, for decision tree induction the genes also have a head and a tail, with the head containing attributes and terminals and the tail containing only terminals. This again ensures that all decision trees designed by GEP are always valid programs. Furthermore, the size of the tail \"t\" is also dictated by the head size \"h\" and the number of branches of the attribute with more branches \"n\" and is evaluated by the equation:\n\nFor example, consider the decision tree below to decide whether to play outside:\n\nIt can be linearly encoded as:\n\nwhere “H” represents the attribute Humidity, “O” the attribute Outlook, “W” represents Windy, and “a” and “b” the class labels \"Yes\" and \"No\" respectively. Note that the edges connecting the nodes are properties of the data, specifying the type and number of branches of each attribute, and therefore don’t have to be encoded.\n\nThe process of decision tree induction with gene expression programming starts, as usual, with an initial population of randomly created chromosomes. Then the chromosomes are expressed as decision trees and their fitness evaluated against a training dataset. According to fitness they are then selected to reproduce with modification. The genetic operators are exactly the same that are used in a conventional unigenic system, for example, mutation, inversion, transposition, and recombination.\n\nDecision trees with both nominal and numeric attributes are also easily induced with gene expression programming using the framework described above for dealing with random numerical constants. The chromosomal architecture includes an extra domain for encoding random numerical constants, which are used as thresholds for splitting the data at each branching node. For example, the gene below with a head size of 5 (the Dc starts at position 16):\n\nencodes the decision tree shown below:\n\nIn this system, every node in the head, irrespective of its type (numeric attribute, nominal attribute, or terminal), has associated with it a random numerical constant, which for simplicity in the example above is represented by a numeral 0–9. These random numerical constants are encoded in the Dc domain and their expression follows a very simple scheme: from top to bottom and from left to right, the elements in Dc are assigned one-by-one to the elements in the decision tree. So, for the following array of RNCs:\n\nthe decision tree above results in:\n\nwhich can also be represented more colorfully as a conventional decision tree:\n\nGEP has been criticized for not being a major improvement over other genetic programming techniques. In many experiments, it did not perform better than existing methods.\n\n\n\n\n\n\n\n", "id": "1098818", "title": "Gene expression programming"}
{"url": "https://en.wikipedia.org/wiki?curid=42922637", "text": "Symbolic regression\n\nSymbolic regression is a type of regression analysis that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity. No particular model is provided as a starting point to the algorithm. Instead, initial expressions are formed by randomly combining mathematical building blocks such as mathematical operators, analytic functions, constants, and state variables. (Usually, a subset of these primitives will be specified by the person operating it, but that's not a requirement of the technique.) New equations are then formed by recombining previous equations, using genetic programming.\n\nBy not requiring a specific model to be specified, symbolic regression isn't affected by human bias, or unknown gaps in domain knowledge. It attempts to uncover the intrinsic relationships of the dataset, by letting the patterns in the data itself reveal the appropriate models, rather than imposing a model structure that is deemed mathematically tractable from a human perspective. The fitness function that drives the evolution of the models takes into account not only error metrics (to ensure the models accurately predict the data), but also special complexity measures, thus ensuring that the resulting models reveal the data's underlying structure in a way that's understandable from a human perspective. This facilitates reasoning and favors the odds of getting insights about the data-generating system.\n\nWhile conventional regression techniques seek to optimize the parameters for a pre-specified model structure, symbolic regression avoids imposing prior assumptions, and instead infers the model from the data. In other words, it attempts to discover both model structures and model parameters.\n\nThis approach has, of course, the disadvantage of having a much larger space to search — in fact, not only the search space in symbolic regression is infinite, but there are an infinite number of models which will perfectly fit a finite data set (provided that the model complexity isn't artificially limited). This means that it will possibly take a symbolic regression algorithm much longer to find an appropriate model and parametrization, than traditional regression techniques. This can be attenuated by limiting the set of building blocks provided to the algorithm, based on existing knowledge of the system that produced the data; but in the end, using symbolic regression is a decision that has to be balanced with how much is known about the underlying system.\n\nNevertheless, this characteristic of symbolic regression also has advantages: because the evolutionary algorithm requires diversity in order to effectively explore the search space, the end result is likely to be a selection of high-scoring models (and their corresponding set of parameters). Examining this collection could provide better insight into the underlying process, and allows the user to identify an approximation that better fits their needs in terms of accuracy and simplicity.\n\n\n\n", "id": "42922637", "title": "Symbolic regression"}
{"url": "https://en.wikipedia.org/wiki?curid=47623626", "text": "Multi expression programming\n\nMulti Expression Programming (MEP) is a genetic programming variant encoding multiple solutions in the same chromosome. MEP representation is not specific (multiple representations have been tested). In the simplest variant, MEP chromosomes are linear strings of instructions. This representation was inspired by Three-address code. MEP strength consists in the ability to encode multiple solutions, of a problem, in the same chromosome. In this way one can explore larger zones of the search space. For most of the problems this advantage comes with no running-time penalty compared with genetic programming variants encoding a single solution in a chromosome.\n\nHere is a simple MEP program:\nOn each line we can have a terminal or a function. In the case of functions we also need pointers to its arguments.\n\nWhen we decode the chromosome we obtain multiple expressions:\nWhich expression will represent the chromosome? In MEP each expression is evaluated and the best of them will represent the chromosome. For most of the problems, this evaluation has the same complexity as in the case of encoding a single solution in each chromosome.\n\nMEPX is a cross platform (Windows, Mac OSX, and Linux Ubuntu) free software for automatic generation of computer programs. It can be used for data analysis, particularly for solving regression and classification problems.\n\nLibmep is a free and open source library implementing Multi Expression Programming technique. It is written in C++.\n\nhmep is a new open source library implementing Multi Expression Programming technique in Haskell programming language.\n\n\n", "id": "47623626", "title": "Multi expression programming"}
