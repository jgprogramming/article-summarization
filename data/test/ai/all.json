[
  {
    "url": "https://en.wikipedia.org/wiki?curid=41939036",
    "text": "Termite-inspired robots\n\nTermite-inspired robots or TERMES robots are biomimetic autonomous robots capable of building complex structures without a central controller. A prototype team of termite-inspired robots was demonstrated by Harvard University researchers in 2014, following four years of development. Their engineering was inspired by the complex mounds that termites build, and was accomplished by developing simple rules to allow the robots to navigate and move building blocks in their environment. By following these simple rules, the robots could construct complex structures through a process called \"stigmergy\", without requiring constant human instruction or supervision.\n\nSocial insects such as termites are capable of constructing elaborate structures such as mounds with complex tunnel systems. They are capable of doing so without an overall plan for the design of a structure, without directly communicating information with each other about how to construct a structure, and without a leader to guide them in constructing a specific structure. Roboticists have been interested in the problem of whether robots can be designed with the limited sensory and motor capabilities of insects such as termites and yet, by following simple rules, construct elaborate and complex structures.\n\nAs part of the TERMES project, the Harvard team designed each robot to perform a few simple behaviors: to move forward, move backward, turn, move up or down a step the size of a specially designed brick, and to move while carrying a brick on top of it. To perform these locomotor behaviors, each robot was equipped with whegs. To detect features of its environment and other robots, each robot was equipped with the following sensors: seven infrared sensors to detect black and white patterns for navigation; an accelerometer to detect tilt angle for climbing; and five ultrasound sonar detectors for determining distance from the perimeter and to nearby robots. To lift, lower, and place the specially designed bricks, each robot had an arm with a spring-loaded gripper to hold a brick while carried. The project's specially designed construction bricks were 21.5 cm × 21.5 cm × 4.5 cm, which was bigger than the footprint (17.5 cm × 11.0 cm) of the robots.\n\nDuring the Harvard trials, groups of termite-inspired robots constructed structures by stigmergy. That is, instead of directly communicating with each other, the robots detected features of their environment and followed simple rules for moving and placing bricks in response to the configuration of bricks that existed at a given time. The rules for constructing a given structure were designed to guarantee that different structures emerged depending on the number of robots involved and on the initial placement of bricks.\n\nThe TERMES robots constitute a proof of concept for the development of relatively simple teams of robots that are capable of constructing complex structures in, for example, remote or hostile locations. The Harvard researchers who built the initial termite-inspired robots have speculated that future robots could be designed to build a base for human habitation on Mars in preparation for the arrival of human astronauts. More immediate possibilities include teams of termite-inspired robots that are capable of moving sandbags and building levees in flood zones.\n\n",
    "id": "41939036",
    "title": "Termite-inspired robots"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25781",
    "text": "Robot\n\nA robot is a machine—especially one programmable by a computer— capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed to take on human form but most robots are machines designed to perform a task with no regard to how they look.\n\nRobots can be autonomous or semi-autonomous and range from humanoids such as Honda's \"Advanced Step in Innovative Mobility\" (ASIMO) and TOSY's \"TOSY Ping Pong Playing Robot\" (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed \"swarm\" robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous Things are expected to proliferate in the coming decade, with home robotics and the autonomous car as some of the main drivers.\n\nThe branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.\n\nFrom the time of ancient civilization there have been many accounts of user-configurable automated devices and even automata resembling animals and humans, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.\n\nThe term comes from a Czech word, \"robota\", meaning \"forced labor\"; the word 'robot' was first used to denote a fictional humanoid in a 1920 play \"R.U.R.\" by the Czech writer, Karel Čapek but it was Karel's brother Josef Čapek who was the word's true inventor. Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen. The first commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.\n\nRobots have replaced humans in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions. The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.\n\nThe word \"robot\" can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots. There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior, especially behavior which mimics humans or other animals. Closely related to the concept of a \"robot\" is the field of Synthetic Biology, which studies entities whose nature is more comparable to beings than to machines.\n\nThe idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China, Ancient Greece, and Ptolemaic Egypt, attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas, the artificial birds of Mozi and Lu Ban, a \"speaking\" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the \"Lie Zi\".\n\nMany ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the Cretan island of Europa from pirates.\n\nIn ancient Greece, the Greek engineer Ctesibius (c. 270 BC) \"applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures.\" In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called \"The Pigeon\". Hero of Alexandria , a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.\nThe 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka. \n\nIn ancient China, the 3rd century text of the \"Lie Zi\" describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs. There are also accounts of flying automata in the \"Han Fei Zi\" and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (\"ma yuan\") that could successfully fly. In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.\nThe beginning of automata is associated with the invention of early Su Song's astronomical clock tower featured mechanical figurines that chimed the hours. His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.\n\n\"Samarangana Sutradhara\", a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.\n\nIn Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw. The design was probably based on anatomical research recorded in his \"Vitruvian Man\". It is not known whether he attempted to build it.\n\nIn Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century \"Karakuri zui\" (\"Illustrated Machinery\", 1796). One such automaton was the karakuri ningyō, a mechanized puppet. Different variations of the karakuri existed: the \"Butai karakuri\", which were used in theatre, the \"Zashiki karakuri\", which were small and used in homes, and the \"Dashi karakuri\" which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.\n\nIn France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.\n\nRemotely operated vehicles were demonstrated in the late 19th century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).\n\nThe Brennan torpedo, invented by Louis Brennan in 1877 was powered by two contra-rotating propellors that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it \"the world's first \"practical\" guided missile\". In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by \"Hertzian\" (radio) waves and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.\n\nArchibald Low, known as the \"father of radio guidance systems\" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.\n\n'Robot' was first applied as a term for artificial automata in a 1920 play \"R.U.R.\" by the Czech writer, Karel Čapek. However, Josef Čapek was named by his brother Karel as the true inventor of the term robot. The word 'robot' itself was not new, having been in Slavic language as \"robota\" (forced laborer), a term which classified those peasants obligated to compulsory service under the feudal system widespread in 19th century Europe (see: Robot Patent).\nČapek's fictional story postulated the technological creation of artificial human bodies without souls, and the old theme of the feudal \"robota\" class eloquently fit the imagination of a new class of manufactured, artificial workers.\n\nIn 1928, one of the first humanoid robots, Eric, was exhibited at the annual exhibition of the Model Engineers Society in London, where it delivered a speech. Invented by W. H. Richards, the robot's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control. Both Eric and his \"brother\" George toured the world.\n\nWestinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair. Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.\n\nThe first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors – essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named \"Elmer\" and \"Elsie\", were constructed between 1948 and 1949 and were often described as \"tortoises\" due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\n\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's \"turtles\" may be found in the form of BEAM robotics.\nThe first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry. Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them. Devol's patent for the first digitally operated programmable robotic arm represents the foundation of the modern robotics industry.\n\nThe first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. In 1973, a robot with six electromechanically driven axes was patented by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.\n\nCommercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.\n\nVarious techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent \"generation\" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.\n\nAs robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System is an open-source set of programs being developed at Stanford University, the Massachusetts Institute of Technology and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a \"Windows for robots\" system with its Robotics Developer Studio, which has been available since 2007.\n\nJapan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.\n\nMany future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.\nAs early as 1982 people were confident that someday robots would:\n1. clean parts by removing molding flash\n2. spray paint automobiles with absolutely no human presence\n3. pack things in boxes—for example, orient and nest chocolate candies in candy boxes\n4. make electrical cable harness\n5. load trucks with boxes—a packing problem\n6. handle soft goods, such as garments and shoes\n7. shear sheep\n8. prosthesis\n9. cook fast food and work in other service industries\n10. household robot.\n\nGenerally such predictions are overly optimistic in timescale.\n\nIn 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator. Many analysts believe that self-driving trucks may eventually revolutionize logistics. By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia. Some analysts believe that within the next few decades, most trucks will be self-driving.\n\nA literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.\n\nBaxter is a new robot introduced in 2012 which learns by guidance. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding in order to be used. This means Baxter needs no programming in order to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. Sawyer was added in 2015 for smaller, more precise tasks.\n\nThe word \"robot\" was introduced to the public by the Czech interwar writer Karel Čapek in his play \"R.U.R. (Rossum's Universal Robots)\", published in 1920. The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called \"robots.\" The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).\n\nKarel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the \"Oxford English Dictionary\" in which he named his brother, the painter and writer Josef Čapek, as its actual originator.\n\nIn an article in the Czech journal \"Lidové noviny\" in 1933, he explained that he had originally wanted to call the creatures \"laboři\" (\"workers\", from Latin \"labor\"). However, he did not like the word, and sought advice from his brother Josef, who suggested \"roboti\". The word \"robota\" means literally \"corvée\", \"serf labor\", and figuratively \"drudgery\" or \"hard work\" in Czech and also (more general) \"work\", \"labor\" in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as \"robot\" in Hungarian). Traditionally the \"robota\" (Hungarian \"robot\") was the work period a serf (corvée) had to give for his lord, typically 6 months of the year. The origin of the word is the Old Church Slavonic (Old Bulgarian) \"rabota\" \"servitude\" (\"work\" in contemporary Bulgarian and Russian), which in turn comes from the Proto-Indo-European root \"*orbh-\". \"Robot\" is cognate with the German root \"Arbeit\" (work).\n\nThe word robotics, used to describe this field of study, was coined by the science fiction writer Isaac Asimov. Asimov created the \"\"Three Laws of Robotics\"\" which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. \"People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical,\" said Dr. Joanna Bryson of the University of Bath.)\n\nMobile robots have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the \"automated guided vehicle\" or \"automatic guided vehicle\" (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers. AGVs are discussed later in this article.\n\nMobile robots are also found in industry, military and security environments. They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.\n\nMobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.\n\nIndustrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.\n\nThe International Organization for Standardization gives a definition of a manipulating industrial robot in ISO 8373:\n\n\"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications.\"\n\nThis definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.\n\nMost commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term \"service robot\" is less well-defined. The International Federation of Robotics has proposed a tentative definition, \"A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations.\"\n\nRobots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.\n\nThere are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST LEGO League, Junior FIRST LEGO League, and FIRST Tech Challenge competitions.\n\nThere have also been devices shaped like robots such as the teaching computer, Leachim (1974), and 2-XL (1976), a robot shaped game / teaching toy based on an 8-track tape player, both invented Michael J. Freeman.\n\nModular robots are a new breed of robots that are designed to increase the utilization of robots by modularizing their architecture. The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U and H shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These \"ANAT robots\" can be designed with \"n\" DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.\n\nModular robotic technology is currently being applied in hybrid transportation, industrial automation, duct cleaning and handling. Many research centres and universities have also studied this technology, and have developed prototypes.\n\nA \"collaborative robot\" or \"cobot\" is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.\n\nThe collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.\n\nRethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks. Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer. , 190 companies in the US have bought Baxters and they are being used commercially in the UK.\n\nRoughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa. 40% of all the robots in the world are in Japan, making Japan the country with the highest number of robots.\n\nAs robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior, and whether robots might be able to claim any kind of social, cultural, ethical or legal rights. One scientific team has said that it is possible that a robot brain will exist by 2019. Others predict robot intelligence breakthroughs by 2050. Recent advances have made robotic behavior more sophisticated. The social impact of intelligent robots is subject of a 2010 documentary film called \"Plug & Pray\".\n\nVernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this \"the Singularity\". He suggests that it may be somewhat or possibly very dangerous for humans. This is discussed by a philosophy called Singularitarianism.\n\nIn 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls. Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns. In 2015, the Nao alderen robots were shown to have a capability for a degree of self-awareness. Researchers at the Rensselaer Polytechnic Institute AI and Reasoning Lab in New York conducted an experiment where a robot became aware of itself, and corrected its answer to a question once it had realised this.\n\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots. The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.\n\nOne robot in particular, the EATR, has generated public concerns over its fuel source, as it can continually refuel itself using organic substances. Although the engine for the EATR is designed to run on biomass and vegetation specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.\n\nManuel De Landa has noted that \"smart missiles\" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.\n\nFor centuries, people have predicted that machines would make workers obsolete and increase unemployment, although the causes of unemployment are usually thought to be due to social policy.\n\nA recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.\n\nLawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to improve redundancy laws.\n\nKevin J. Delaney said \"Robots are taking human jobs. But Bill Gates believes that governments should tax companies’ use of them, as a way to at least temporarily slow the spread of automation and to fund other types of employment.\" The robot tax would also help pay a guaranteed living wage to the displaced workers.\n\nAt present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.\n\nRobots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. All robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.\n\nGeneral-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in. Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.\n\nOver the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.\n\nIndustrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.\n\nMass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy. Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.\n\nMobile robots, following markers or wires in the floor, or using vision or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.\n\nLimited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.\n\nDeveloped to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.\n\nSuch as SmartLoader, SpeciMinder, ADAM, Tug Eskorta, and MT 400 with Motivity are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.\n\nThere are many jobs which humans would rather leave to robots. The job may be boring, such as domestic cleaning, or dangerous, such as exploring inside a volcano. Other jobs are physically inaccessible, such as exploring another planet, cleaning the inside of a long pipe, or performing laparoscopic surgery.\n\nAlmost every unmanned space probe ever launched was a robot. Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.\n\nTeleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time. They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely. Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets. Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).\n\nRobots are used to automate picking fruit on orchards at a cost lower than that of human pickers.\n\nDomestic robots are simple robots dedicated to a single task work in home use. They are used in simple but unwanted jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.\n\nMilitary robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.\n\nUnmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own. The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection. Flight trials are expected to begin in 2011.\n\nThe AAAI has studied this topic in depth and its president has commissioned a study to look at this issue.\n\nSome have suggested a need to build \"Friendly AI\", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane. Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics. An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee. Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as \"Robot Legal Studies.\" Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.\n\nMining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia. Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.\n\nDrilling, longwall and rockbreaking machines are now also available as autonomous robots. The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths. Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination. These systems greatly enhance the safety and efficiency of mining operations.\n\nRobots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.\n\nRobots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1, through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.\n\nThe population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them. Humans make the best carers, but where they are unavailable, robots are gradually being introduced.\n\nFRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.\n\nScript Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form. The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient's medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it's the correct drug for the correct patient and then seals the vials and sends it out front to be picked up. The robot is a very time efficient device that the pharmacy depends on to fill prescriptions.\n\nMcKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors. The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to its either stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify its pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.\n\nWhile most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.\n\nOne approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.\n\nNanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10 meters). Also known as \"nanobots\" or \"nanites\", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest. Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog, manufacturing, weaponry and cleaning. Some people have suggested that if there were nanobots which could reproduce, the earth would turn into \"grey goo\", while others argue that this hypothetical outcome is nonsense.\n\nA few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task, like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.\n\nRobots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.\n\nInspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors. Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.\n\nRobotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called \"haptic interfaces\", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of \"virtual\" objects, which users can experience through their sense of touch.\n\nRobotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also \"bionic men/women\", or humans with significant mechanical enhancements) have become a staple of science fiction.\n\nThe first reference in Western literature to mechanical servants appears in Homer's \"Iliad\". In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots. According to the Rieu translation, \"Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods.\" The words \"robot\" or \"android\" are not used to describe them, but they are nevertheless mechanical devices human in appearance. \"The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)\". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).\n\nPossibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992) who published over five-hundred books. Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works. Asimov carefully considered the problem of the ideal set of instructions robots might be given in order to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law. These were introduced in his 1942 short story \"Runaround\", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: \"A robot may not harm humanity, or, by inaction, allow humanity to come to harm\"; the rest of the laws are modified sequentially to acknowledge this.\n\nAccording to the \"Oxford English Dictionary,\" the first passage in Asimov's short story \"Liar!\" (1941) that mentions the First Law is the earliest recorded use of the word \"robotics\". Asimov was not initially aware of this; he assumed the word already existed by analogy with \"mechanics,\" \"hydraulics,\" and other similar terms denoting branches of applied knowledge.\n\nRobots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the \"Star Wars\" franchise.\n\nThe concept of humanoid sex robots has elicited both public attention and concern. Opponents of the concept have stated that the development of sex robots would be morally wrong. They argue that the introduction of such devices would be socially harmful, and demeaning to women and children.\n\nFears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race.\n\"Frankenstein\" (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.\n\nOther works with similar themes include \"The Mechanical Man\", \"The Terminator, Runaway, RoboCop\", the Replicators in \"Stargate\", the Cylons in \"Battlestar Galactica\", the Cybermen and Daleks in \"Doctor Who\", \"The Matrix\", \"Enthiran\" and \"I, Robot\". Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are \"\", \"Red Planet\" and \"Enthiran\". \n\nThe 2017 game Horizon Zero Dawn explores themes of robotics in warfare, robot ethics, and the AI control problem, as well as the positive or negative impact such technologies could have on the environment.\n\nAnother common theme is the reaction, sometimes called the \"uncanny valley\", of unease and even revulsion at the sight of robots that mimic humans too closely.\n\nMore recently, fictional representations of artificially intelligent robots in films such as \"A.I. Artificial Intelligence\" and \"Ex Machina\" and the 2016 TV adaptation of \"Westworld\" have engaged audience sympathy for the robots themselves.\n\n\n\n\n\n\n",
    "id": "25781",
    "title": "Robot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27547393",
    "text": "For Inspiration and Recognition of Science and Technology\n\nFor Inspiration and Recognition of Science and Technology (FIRST) is an international youth organization that operates the FIRST Robotics Competition, FIRST LEGO League, FIRST LEGO League Jr., and FIRST Tech Challenge competitions.\nFounded by Dean Kamen and Woodie Flowers in 1989, its expressed goal is to develop ways to inspire students in engineering and technology fields. Its philosophy is expressed by the organization as \"coopertition\" and \"gracious professionalism\".\nFIRST also operates FIRST Place, a research facility at FIRST headquarters in Manchester, New Hampshire, where it holds educational programs and day camps for students and teachers.\n\nFIRST operates as a non-profit public charity corporation. It licenses qualified teams, usually affiliated with schools or other youth organizations, to participate in its competitions. The teams in turn pay a fee to \"FIRST\"; these fees, the majority of which are redistributed to pay for teams' kit of parts and other services, consist of the majority of \"FIRST\"'s revenue.\n\nThe supreme body of FIRST is its board of directors, which includes corporate executives and former government officials. FIRST also has an executive advisory board and several senior advisors; these advisors include engineers, involved volunteers, and other senior organizers. Day-to-day operations are run by a senior management team, consisting of a president and five vice presidents.\n\nThe first and highest-scale program developed through \"FIRST\" is the \"FIRST\" Robotics Competition (FRC), which is designed to inspire high school students to become engineers by giving them real world experience working with engineers to develop a robot. The inaugural FIRST Robotics Competition was held in 1992 in the Manchester Memorial High School gymnasium. , over 3,000 high school teams totaling over 46,000 students from Australia, Brazil, Canada, Turkey, Israel, Mexico, the Netherlands, the United States, the United Kingdom, and more compete in the annual competition.\n\nThe competition challenge changes each year, and the teams can only reuse certain components from previous years. The robots weigh at most , without batteries and bumpers. The kit issued to each team contains a base set of parts. Registration and the kit of parts together cost about US$6,000. In addition to that, teams are allowed to spend another $3,500 on their robot. The purpose of this rule is to lessen the influence of money on teams' competitiveness. Details of the game have been released on the first Saturday in January (except when that Saturday falls on January 1 or 2), and the teams have been given six weeks to construct a robot that can accomplish the game's tasks.\n\nIn 2011, teams participated in 48 regional and district competitions throughout March in an effort to qualify for the \"FIRST\" Championship in St. Louis in April. Previous years' Championships have been held in Atlanta, Georgia, Houston, Texas and at Walt Disney World's Epcot. On October 7, 2009, FIRST announced that the Championship Event will be held in St. Louis, Missouri for 2011 through 2013.\nEach year the \"FIRST\" Robotics Competition has scholarships for the participants in the program. In 2011, there were over $14 million worth of scholarships from more than 128 colleges and universities, associations, and corporations.\n\nThe district competition system was introduced in Michigan and as of 2017 has expanded to include districts in the Pacific Northwest, the Mid-Atlantic, the Washington DC area, New England, Georgia, North Carolina, Ontario, and Israel. When they were created in 2017, the Ontario and Israel districts became the first districts outside of the United States. The district competition system changed the traditional \"regional\" events by allowing teams to compete in multiple smaller events and using an associated ranking algorithm to determine which teams would advance to the next level of the competition. In general, there have been pushes to move more regions to the districts system; California, Texas, and New York have especially been pushed to move to the district system.\n\nThe FIRST Tech Challenge (FTC), formerly \"FIRST\" Vex Challenge (FVC), is a mid-level robotics competition announced by \"FIRST\" on March 22, 2005. According to \"FIRST\", this competition was designed to be a more accessible and affordable option for schools. FIRST has also said that the FTC program was created for those of an intermediate skill level. FIRST Tech Challenge robots are approximately one-third the scale of their FRC counterparts. The FTC competition is meant to provide a transition for students from the FLL competition to the FRC competition. FTC was developed for the Vex Robotics Design System, which is available commercially.\n\nThe 2005 FVC pilot season featured a demonstration of the \"FIRST\" Vex Challenge using a 1/3 linear scale mock-up of the 2004 FRC Competition, . For their 2005-2006 Pilot Season, FVC teams played the Half-Pipe Hustle game using racquet balls and ramps.\n\nFor the 2006-2007 FTC Season, the \"FIRST\" Tech Challenge teams competed in the Hangin'-A-Round challenge using softballs, rotating platforms, a hanging bar, and a larger 'Atlas' ball which is significantly larger than most Vex robots and harder to manipulate. Competitions were held around the United States, Canada, and Mexico.\n\nFor the 2008-2009 FTC season, a new kit was introduced, as \"FIRST\" moved away from the VEX platform and worked with several different vendors to create a custom kit and control system for FTC known as Tetrix. Based around the LEGO Mindstorms NXT \"brain\" and including secondary specialized controllers to overcome the limitations of the NXT, teams use a Bluetooth link between the NXT and a laptop running FTC driver station software. A team's drivers then use either one or two USB gamepads to control their robots.\n\nFor the 2015-2016 FTC season, in a partnership with Qualcomm, the LEGO Mindstorms NXT was replaced as the \"brain\" of the robot by an android device which communicates to a separate \"driver station\" android device via Wifi Direct. In addition, students were allowed to use either MIT App Inventor or Android Studio (Java language) to program their robots.\n\nIn 1998, the \"FIRST\" LEGO League (FLL), a program similar to the \"FIRST\" Robotics Competition, was formed. It is aimed at 9 to 14-year-old students and utilizes LEGO Mindstorms sets (EV3, NXT, RCX) to build palm-sized LEGO robots, which are then programmed using either the ROBOLAB software (RCX-based systems) or Mindstorms NXT or EV3 software (for NXT or EV3-based systems respectively) to autonomously compete against other teams. The ROBOLAB software is based on National Instruments' LabVIEW industrial control engineering software. The combination of interchangeable LEGO parts, computer 'bricks', sensors, and the aforementioned software, provide preteens and teenagers with the capability to build simple models of real-life robotic systems. This competition also utilizes a research element that is themed with each year's game, and deals with a real-world situation for students to learn about through the season.\n\nThe simplistic nature of its games, its relatively low team startup costs, and its association with the Lego Group mean that it is the most extensive of all \"FIRST\" competitions, despite a lower profile and fewer sponsors than \"FIRST\" Tech Challenge or \"FIRST\" Robotics Competition. In 2009, 14,725 teams from 56 countries participated in local, regional, national, and international competitions, compared with around 1,600 teams in roughly 10 countries for FRC.\n\n\"FIRST\" LEGO League Jr. is a variation of the \"FIRST\" LEGO League, aimed towards elementary school children, in which kids ages 5 to 8 build LEGO models dealing with that year's FLL challenge. At least one part of a model has a moving component. The teams participate in exhibitions around the country, where they demonstrate and explain their models and research for award opportunities.\n\nThe \"FIRST\" Championship is the annual event which celebrates the finale of all of their programs by bringing them all together for their final rounds in the same event. The \"FIRST\" Championship is scheduled to be in St. Louis, Missouri through 2017. At the 2014 Championship, \"FIRST\" announced changes to the 2015 structure that will bring a more \"Olympic Village\" feeling, and involves a rearrangement of the programs around the city.\n\n\"FIRST\" itself is a self-supporting organization; however, individual teams typically rely on outside funding sources. It also takes significant outside funds to run regional events and the \"FIRST\" Championship. In 2010, \"FIRST\" was a recipient of a Google Project 10^100 grant.\n\nTeams may request that team members, whether mentors or students, contribute to the costs of running a team. For example, members may pay a fee or donate tools and facilities.\n\nTeams frequently give other teams support. This may mean providing funds, tools, or facilities. Gracious professionalism and Coopertition are core tenets of the \"FIRST\" philosophy.\n\nGracious Professionalism is a major belief in the \"FIRST\" community. At every regional and national competition, the judges look for teams to be graciously professional. What gracious professionalism is all about is \"competing on an even playing field\". That means that each team wants their competition at the best. The way the team system is set up is that every team is matched up with two other teams per match at random. Therefore, a team's ally in one match may become an opponent in the next match. Traditionally, outside of \"FIRST\", when one shares resources in a competition, one only does so with their allies.\n\nHowever, with the element of gracious professionalism, one would share resources with their opponent as well. For example, if a team needs a part or tool to fix their robot, it is expected that any team, even an opposing team would give that team a hand in order to compete. \nThis helps student learn that success is in learning and helping others no matter the circumstances. With this in mind, the judges give a Gracious Professionalism award at every \"FIRST\" Robotics Competition tournament, to a team that shows outstanding gracious professionalism.\n\nThe term \"Gracious Professionalism\" was created by Dr. Woodie Flowers, \"FIRST\" National Advisor and Pappalardo Professor Emeritus of Mechanical Engineering, Massachusetts Institute of Technology.\n\nThe most common method of monetary and resource sponsorship teams comes through the community surrounding the team. Since the majority of teams are based around a school or a school district, schools often provide the infrastructure needed to run a team. Local governments and individual citizens may provide funds and other support to teams. Local universities and colleges often give significant funds to teams.\n\nCorporate donations and grants usually provide the majority of a mature team's funds. Major donors include BAE Systems, Google, Raytheon, and National Instruments.\n\nEach year during his speech at the kickoff event, founder Dean Kamen gives the student participants a homework assignment. It often involves spreading the word about \"FIRST\" in various ways, such as increasing attendance at regionals (2005), mentoring rookie teams, making sure that \"FIRST\"-specific scholarships are applied for (2004), and researching the capabilities of motors and disseminating that information to other teams (2006). In 2007, Dean's homework was for each team to contact their government officials (e.g. mayors, legislators, governors, federal officials) and invite them to a \"FIRST\" regional or the championship to expose them to the competition and increase the level of political awareness of FIRST. In 2008, it was to inform the media more about \"FIRST\". In 2009, the homework was for each team to have all students, mentors, and other persons involved with their team (past or present) register with \"FIRST\". One goal of this registration process was to provide \"FIRST\" with data to demonstrate that many people had benefited from their experiences in \"FIRST\" robotics and to encourage more funding of robotics-related events.\n\nAt the World Championship in Atlanta, speakers have included former President of the United States George Herbert Walker Bush in 2008, and United States Secretary of Education Arne Duncan in 2010. In 2010, former U.S. Undersecretary of Commerce and Director of the U.S. Patent and Trademark Office Jon Dudas was selected to be the President of FIRST.\n\nAt the Championship in St. Louis, President of the United States Barack Obama has spoken via a pre-recorded message every year from 2011-2014.\n\n\"FIRST\" has received the attention of politicians in Canada as well. Ontario MPP Bob Delaney and Ontario MPP Victor Fedeli have made remarks in the Legislative Assembly of Ontario regarding their FRC experiences and showing their support.\n\nNASA, through its Robotics Alliance Project, is a major supporter of \"FIRST\".\n\n\"FIRST\" seeks to promote a philosophy of teamwork and collaboration among engineers and encourages competing teams to remain friendly, helping each other out when necessary. Terms frequently applied to this ethos are \"Gracious Professionalism\" and \"Coopertition\"; terms coined by Woodie Flowers and Kamen that support respect towards one's competitors and integrity in one's actions. The concept of Gracious Professionalism grew from a robotics class that Flowers taught at Massachusetts Institute of Technology. Coopertition is patented under US Patent 7,507,169 by Dean Kamen.\n\nNote: All years indicate the year that the championship for that game was held.\n\n",
    "id": "27547393",
    "title": "For Inspiration and Recognition of Science and Technology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43065875",
    "text": "Terrainability\n\nThe terrainability of a machine or robot is defined as its ability to negotiate terrain irregularities.\n\nTerrainability is a term coined in the research community and related to locomotion in the field of mobile robotics. Its various definitions generically describe the ability of the robot to handle various terrains in terms of their ground support, obstacle sizes and spacing, passive/dynamic stability, etc.\n",
    "id": "43065875",
    "title": "Terrainability"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34846522",
    "text": "Ivar Mendez\n\nIvar Mendez, M.D., PhD, is the Fred H. Wigmore Professor and Chairman of Surgery at the University of Saskatchewan and the Unified Head of Surgery for the Province of Saskatchewan. He is internationally known for his work in cell transplantation for Parkinson's disease and the use of remote presence robotics in neurosurgery and primary health care.\n\nHe also holds an appointment at the Department of Psychology and Neuroscience at Dalhousie University and he is one of the founders of the Brain Repair Centre. He is also the president and founder of the Ivar Mendez International Foundation that is dedicated to providing health and educational assistance to children in the Bolivian Andes. Mendez is a photographer and sculptor and has published 4 books of photography.\n\nMendez was born in La Paz, Bolivia and immigrated with his family to Canada as a teenager. He obtained a BSc degree from the University of Toronto and then an M.D. from the University of Western Ontario (UWO). He did a neurosurgical residency training in London, and was certified in Neurosurgery from the Royal College of Physician and Surgeons of Canada in 1994 and from the American Board of Neurological Surgery in 1996. He became a fellow of the American College of Surgeons in 1998 and became a member of the College Board of Governors in 2015. His interest in regenerative medicine led him to obtain a PhD in Anatomy and Neurobiology from the UWO his PhD thesis was on “Neurotransmitter Interactions in Nigral Grafts”. He did a postdoctoral fellowship at the University of Lund in Sweden under the supervision of Anders Björklund, considered the “father” of cell transplantation in Parkinson's disease. In 2014, Saint Mary's University (Halifax) in Nova Scotia awarded Mendez a Doctor of Science (honoris causa) degree for his contribution to Neuroscience and he was inducted a Fellow to the Canadian Academy of Health Sciences. In 2016, Dr. Mendez received the Government of Canada Public Service Award of Excellence for the use of remote presence robotic technology to improve healthcare in the Canadian North.\n\nHe pioneered the technique of multiple grafts to restore dopamine input to the parkinsonian mammalian brain. This technique was translated into clinical trials in patients with Parkinson's disease and showed long-term survival of those grafts. He also pioneered the use of Glial Derived Neurotrophic Factor (GDNF) in combination with fetal cells in humans. Mendez invented a transplantation delivery system to inject cells into the human brain. With his team, he performed the first long-distance brain surgery robotic telementoring in the world by using a robotic arm to mentor neurosurgeons located 400 km away.\nIn 2015, Mendez and his team printed the first 3D brain for planning deep brain stimulation surgery. Research in 3D brain printing led in 2016 to the development of a virtual reality (VR) brain for medical education and surgical planning applications.\n\n\nMendez has established a Canadian charitable organization, the Ivar Mendez International Foundation, to provide nutrition, dental care and art program to children in remote locations of the Bolivian Andes.\n\n\nHe has published 4 books of photography and has had several exhibitions of his photography and sculpture \n\n\n",
    "id": "34846522",
    "title": "Ivar Mendez"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43921696",
    "text": "Salvius\n\nSalvius () is the first open source humanoid robot to be built in the United States. Introduced in 2008, Salvius, whose name is derived from the word 'salvaged', has been constructed with an emphasis on using recycled components and materials to reduce the costs of designing and construction. The robot is designed to be able to perform a wide range of tasks by having a body structure that is similar to that of a human. The primary goal for Salvius is to create a robot that can function dynamically in a domestic environment.\n\nSalvius is a part of the open source movement which means that all of the robot's source code is freely available for others to use. Unlike other humanoid robots, Salvius benefits from the advantages of open source software such as allowing any problems to be quickly addressed by a community of developers. The open source nature of the robot's code also makes it possible for anyone to learn about how it works. Salvius has been used as a resource by STEM educators to enable students to learn about many subjects in science and technology.\n\nUnlike many robots, salvius does not use an acronym for a name. The name \"Salvius\" dates back to the time of the Roman Empire, however, it was chosen for this robot because of its similarity to the word \"salvage\". Names haven been a significant part of this robot's development. Salvius is tattooed with the names of the individuals and businesses that have contributed to the project's progress.\n\nSalvius is intended to be a resource for developers to experiment with machine learning and kinematic applications for humanoid robots. The robot is designed to allow new hardware features to be added or removed as needed using plug and play USB connections. Recent changes to the robots design have improved the robot's ability to connect to other devices so that developers can also investigate new ways that robots can interact with the Internet of Things (IoT).\n\nThe robots construction has been documented since 2010. Along with emphasis on recycling, any commercially available parts used on the robot were chosen with availability and economic affordability in mind. Much of the robot's hardware is also open source. Hardware items such as the Raspberry Pi and Arduino microcontrollers were selected because of their open source design and the support communities that exist for these components. The robot uses multiple Arduino microcontrollers which were chosen based on the versatility and popularity of the platform across communities.\n\nThe robot's computer runs Raspbian Linux and primarily uses open source software. Salvius is able to operate autonomously as well as being controlled remotely using an online interface. The reason behind making the robot open source was to provide a platform for students and enthusiasts to build their own robots. The robot's programming languages include: Python, Arduino, and JavaScript. Python was chosen because of its status as the supported language of the Raspberry Pi. C is used for programming the Arduino micro-controllers that the robot's main computer, a Raspberry Pi, communicates with. By sending tasks off to several other boards it allows the robot to do parallel processing and to distribute the work load. The [star network] topography of the robot's network also prevents a failure in one of the Arduino procession nodes from crippling the robot entirely.\n\nSalvius has an API which allows users to send and retrieve data from the robot. When the robot's wireless connection is turned on, the robot can be controlled web interface to see exactly what the robot is seeing and to direct its actions accordingly. Since all the software is installed on the robot the user only needs a device with a working internet connection and a browser.\n\nThe robot is controlled by a network of Raspberry Pi and Arduino microcontrollers. The Raspberry Pi acts as a server which allows [high level programming languages] to be used to control the robot. A combination of several computers allows various tasks to be processed in parallel so that they can be completed quicker. The robot uses Grove motor controllers to control a variety of motors. Most of the robots motors have been salvaged from a variety of sources and reused to construct the robot.\n\nThe robots design incorporates a variety of sensors which allow the robot to successfully interact with its environment. Sensors that have been used on the robot include: touch, sound, light, ultrasonic, and a PIR (Passive infrared sensor). The robot also has an ethernet-connected IP camera which serves as its primary optical input device.\n",
    "id": "43921696",
    "title": "Salvius"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44777146",
    "text": "Frubber\n\nFrubber (from \"flesh rubber\") is a patented elastic form of rubber used in robotics. The spongy elastomer has been used by Hanson Robotics for the face of its android robots, including Einstein 3 and Sophia.\n",
    "id": "44777146",
    "title": "Frubber"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=45185419",
    "text": "Robotic sensors\n\nRobotic sensors are used to estimate a robot's condition and environment. These signals are passed to a controller to enable appropriate behavior.\n\nSensors in robots are based on the functions of human sensory organs. Robots require extensive information about their environment in order to function effectively.\n\nSensors provide analogs to human senses and can monitor other phenomena for which humans lack explicit sensors.\n\nSensors can measure physical properties, such as the distance between objects, the presence of light and the frequency of sound.\nThey can measure:\n\nMotion controllers, potentiometers, tacho-generators and encoder are used as joint sensors, whereas strain-gauge based sensing is used at the end-effector location for contact force control.\n\nIt is the part of the robot.Internal sensors measure the robot's internal state. They are used to measure position, velocity and acceleration of the robot joint or end effectors.\n\nPosition sensors measure the position of a joint (the degree to which the joint is extended). They include:\n\nA velocity or speed sensor measures consecutive position measurements at known intervals and computes the time rate of change in the position values.\n\nIn a parts feeder, a vision sensor can eliminate the need for an alignment pallet. Vision-enabled insertion robots can precisely perform fitting and insertion operations of machine parts.\n\n",
    "id": "45185419",
    "title": "Robotic sensors"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44484413",
    "text": "Autonomous spaceport drone ship\n\nAn autonomous spaceport drone ship (ASDS) is an ocean-going vessel derived from a deck barge, outfitted with station-keeping engines and a large landing platform. Construction of such ships was commissioned by aerospace company SpaceX to allow for recovery of rocket first-stages at sea for high-velocity missions which do not carry enough fuel to return to the launch site after lofting spacecraft onto an orbital trajectory.\n\nSpaceX has two operational drone ships: Just Read the Instructions in the Pacific for launches from Vandenberg, and Of Course I Still Love You in the Atlantic for launches from Cape Canaveral. , 17 Falcon 9 flights have attempted to land on a drone ship, with 12 of them succeeding, the first vertical landing being the CRS-8 mission in April 2016.\n\nThe ASDS ships are a key component of the SpaceX reusable launch system development program which aims to significantly lower the price of space launch services through \"full and rapid reusability.\" Any flights going to geostationary orbit or exceeding escape velocity will require landing at sea, encompassing about half of SpaceX missions.\n\nIn 2009, SpaceX CEO Elon Musk articulated ambitions for \"creating a paradigm shift in the traditional approach for reusing rocket hardware.\"\n\nIn October 2014, SpaceX publicly announced that they had contracted with a Louisiana shipyard to build a floating landing platform for reusable orbital launch vehicles. Early information indicated that the platform would carry an approximately landing pad and would be capable of precision positioning so that the platform could hold its position for launch vehicle landing. On 22 November 2014 Musk released a photograph of the \"autonomous spaceport drone ship\" along with additional details of its construction and size.\n\nAs of December 2014, the first drone ship used, the McDonough Marine Service's \"Marmac 300\" barge, was based in Jacksonville, Florida, at the northern tip of the JAXPORT Cruise Terminal () where SpaceX built a stand to secure the Falcon stage during post-landing operations. The stand consists of four , tall and wide pedestal structures bolted to a concrete base. A mobile crane will lift the stage from the ship and place it on the stand. Tasks such as removing or folding back the landing legs prior to placing the stage in a horizontal position for trucking will occur here.\n\nThe ASDS landing location for the first landing test was in the Atlantic approximately northeast of the launch location at Cape Canaveral, and southeast of Charleston, South Carolina.\nOn 23 January 2015, during repairs to the ship following the unsuccessful first test, Musk announced that the ship was to be named \"Just Read the Instructions\", with a sister ship planned for west coast launches to be named \"Of Course I Still Love You\". On 29 January, SpaceX released a manipulated photo of the ship with the name illustrating how it would look once painted. Both ships are named after two GCUs (General Contact Units), spaceships commanded by autonomous artificial intelligences, that appear in \"The Player of Games\", a Culture novel by Iain M. Banks.\n\nThe first \"Just Read the Instructions\" was retired in May 2015 after approximately six months of service in the Atlantic, and its duties were assumed by \"Of Course I Still Love You\". The former ASDS was modified by removing the wings that had extended the barge surface and the equipment (thrusters, communications gear, etc.) that had been added to refit it as an ASDS; these items were saved for future reuse.\n\nSpaceX subsequently leased two additional deck barges—\"Marmac 303\" and \"Marmac 304\"—and initiated refit to construct two additional autonomous-operation-capable ASDS ships, built on the hulls of these Marmac barges.\n\nThe second ASDS barge, \"Of Course I Still Love You\" (\"OCISLY\"), had been under construction in a Louisiana shipyard since early 2015 using a different hull—\"Marmac 304\"—in order to service launches on the east coast. It was built as a replacement for the first \"Just Read the Instructions\" and entered operational service for Falcon 9 Flight 19 in late June 2015. As of June 2015, its home port was Jacksonville, Florida, but after December 2015, it was transferred further south, at Port Canaveral.\n\nWhile the dimensions of the ship are nearly identical to the original ASDS, several enhancements were made including a steel blast wall erected between the aft containers and the landing deck. The ship was in place for a first-stage landing test on the CRS-7 mission, which failed on launch on 28 June 2015.\n\nOn 8 April 2016 the first stage, which launched the Dragon CRS-8 spacecraft, successfully landed for the first time ever on \"OCISLY\", which is also the first ever drone ship landing.\n\nA third ASDS barge, using the \"Marmac 303\" barge, was built during 2015 in a Louisiana shipyard, and the barge transited the Panama Canal in June 2015 carrying its wing extensions as cargo on the deck because the ASDS, when complete, would be too wide to pass through the canal.\n\nThe home port for the \"Marmac 303\" is the Port of Los Angeles, at the AltaSea marine research and business campus in San Pedro's outer harbor. The landing platform and tender vessels began docking there in July 2015 in advance of the main construction of AltaSea which is scheduled for 2017.\n\nSpaceX announced that the \"Marmac 303\" would be the second ASDS to be named \"Just Read the Instructions\" (\"JRtI\") in January 2016, shortly before its first use as a landing platform for Falcon 9 Flight 21.\n\nOn 17 January 2016, \"JRtI\" was put to first use in an attempt to recover a Falcon 9 first-stage booster from the Jason-3 mission from Vandenberg. The booster successfully landed on the deck, however, a lockout collet failed to engage on one of the legs causing the rocket to tip over, exploding on impact with the deck. On January 14, 2017, SpaceX launched Falcon 9 Flight 29 from Vandenberg and landed the first stage on the \"JRtI\" that was located about downrange in the Pacific Ocean, making it the first successful landing in the Pacific.\n\nASDSes are autonomous vessels capable of precision positioning, originally stated to be within even under storm conditions, using GPS position information and four diesel-powered azimuth thrusters. In addition to the autonomous operating mode, the ships may also be telerobotically controlled.\n\nThe azimuth thrusters are hydraulic propulsion outdrive units with modular diesel-hydraulic-drive power units and a modular controller all manufactured by Thrustmaster, a marine equipment manufacturer. The returning rocket must not only land within the confines of the deck surface but must also deal with ocean swells and GPS errors.\n\nSpaceX equips the ships with a variety of sensor and measurement technology to gather data on the booster returns and landing attempts, including commercial off the shelf GoPro cameras.\n\nAt the center of the ASDS landing pads is a circle that encloses the SpaceX stylized \"X\" in an X-marks-the-spot landing point.\n\nThe two ASDS names used so far, Just Read the Instructions (\"JRtI\"), and Of Course I Still Love You (\"OCISLY\"), pay homage to the works of the late science fiction author Iain M. Banks, and are drawn from his \"Culture\" fictional universe. Both \"JRtI\" and \"OCISLY\" are names of enormous, sentient starships, which appeared in the novel \"The Player of Games\".\n\nThe landing platform of the upper deck of the first vessel named \"Just Read the Instructions\" was while the span of the Falcon 9 v1.1 landing legs was .\n\n\"Of Course I Still Love You\" was built as a refit of the \"Marmac 304\" barge. It is home ported in Florida at Port Canaveral since December 2015, after being ported for a year at the Port of Jacksonville during most of 2015.\n\n\"Just Read the Instructions\" was built as a refit of the deck barge \"Marmac 303\" in 2015 for service in the Pacific Ocean. It is home ported at the Port of Los Angeles in southern California.\n\nA tug, \"Elsbeth III\" on the east coast, is used to bring the ASDS to its oceanic position. During rocket landing operations a support ship, \"Go Quest\" on the east coast, is typically standing by some distance away from the crewless ASDS. Following landing, technicians and engineers will board the landing platform, and secure the rocket's landing legs to lock the vehicle in place for transport back to port. The rocket stage is secured to the deck of the drone ship with steel hold downs welded on to the feet of the landing legs. In June 2017, \"OCISLY\" started being deployed with a robot that drives under the rocket and grabs onto the hold-down clamps located on the outside of the Falcon 9's structure after landing. Fans call the robot ‘Optimus Prime’ or ‘Roomba,' the latter of which has been retroactively transformed into an acronym for 'Remotely Operated Orientation & Mass Balance Adjustment.'\n\nOne of the series of autonomous ships has been used for four flight tests, three in early 2015 (although only two actually attempted landings on an ASDS) plus one more in January 2016.\n\nThe first test was 10 January 2015\nwhen SpaceX conducted a controlled-descent flight test to land the first-stage of Falcon 9 Flight 14 on a solid surface after it was used to loft a contracted payload toward Earth orbit. SpaceX projected prior to the first landing attempt that the likelihood of successfully landing on the platform would be 50 percent or less.\n\nSpaceX attempted a landing on \"Just Read the Instructions\" on 10 January 2015. Many of the test objectives were achieved, including precision control of the rocket's descent to land on the platform at a specific point in the south Atlantic Ocean and a large amount of test data was obtained from the first use of grid fin control surfaces used for more precise reentry positioning. However the landing was a hard landing.\n\nThe SpaceX webcast indicated that the boostback burn and reentry burns for the descending first-stage occurred, and that the descending rocket then went \"below the horizon,\" as expected, which eliminated the live telemetry signal. Shortly thereafter, SpaceX released information that the rocket did get to the drone spaceport ship as planned, but \"landed hard ... Ship itself is fine. Some of the support equipment on the deck will need to be replaced.\"\n\n\"Just Read the Instructions\" was towed to sea once again in early February for the DSCOVR satellite launch on 11 February 2015 but, in the event, was not used for a landing attempt. Ocean conditions of -high waves interfered with the ASDS recovery duties for the landing, so the ship returned to port and no landing test occurred. SpaceX executed a soft landing in the sea to continue data gathering for future landing attempts. The soft landing was successful, Elon Musk tweeted that it landed with a lateral accuracy of away from the target and in a vertical position.\n\nOn 14 April 2015, SpaceX made a second and final attempt to land a Falcon first-stage on the \"Marmac 300\" drone ship. Early news from Elon Musk suggested that it made a hard landing. He later clarified that it appeared to have made a vertical landing on the ship, but then toppled over due to excessive remaining lateral momentum.\n\nIn order to prepare for Falcon 9 Flight 19 on 28 June 2015, the new ASDS, \"Of Course I Still Love You\", was towed out to sea to prepare for a third landing test. This was its first operational assignment. However, the Falcon launch rocket disintegrated before first-stage shutdown so the mission never progressed to the point where the controlled-descent test was to be initiated.\n\nSpaceX indicated on 7 January that there would be an attempt to land on the new west coast ASDS following the launch of Falcon 9 Flight 21 scheduled for 17 January 2016.\n\n\"Just Read the Instructions\" was located about downrange from the launch site in the Pacific Ocean. Elon Musk later reported that the first-stage did successfully soft-land on the ship, but a lockout latch on one of the landing legs failed to latch and the first-stage fell over, causing a breach of the propellant tanks and a deflagration on impact with the drone ship.\n\nDuring a launch of a heavy communications satellite on 4 March 2016, SpaceX did an experimental descent and landing attempt with very low propellant margins. For the first time, and in order to reduce the propellant required, SpaceX attempted the landing burn with three engines. SpaceX had indicated that the test was unlikely to result in a successful landing and recovery. In the event, one engine flamed out early, and the rocket hit the deck surface with considerable velocity, destroying the rocket and causing damage to the drone ship's deck. By March 21, 2016, the deck of the drone ship was nearly repaired.\n\nThe Falcon 9 first-stage performed a successful landing on \"Of Course I Still Love You\" in the Atlantic Ocean off the coast of Florida at T+9 minutes and 10 seconds after liftoff, the first-ever successful landing of the first-stage on an Autonomous Spaceport Drone Ship. The rocket was successfully affixed to the barge for the maritime transport portion of the journey back to port, and successfully completed its journey, entering Port Canaveral early in the morning on 12 April 2016.\n\nOn May 6, 2016, SpaceX landed the first stage of the Falcon 9 on \"OCISLY\" during the JCSat-14 mission, its second time successfully landing on a drone ship at sea, and its first time recovering a booster from a high-velocity (GTO) mission.\n\nOn May 27, 2016, SpaceX landed the first stage of a Falcon 9 on \"OCISLY\" during the Thaicom 8 mission, its third time successfully landing on a drone ship at sea.\n\nOn June 15, 2016, SpaceX failed to land the first stage of the Falcon 9 on \"OCISLY\" during the Asia Broadcast Satellite/Eutelsat mission. Elon Musk tweeted that one of the three engines had low thrust, and when the rocket was just off the deck, the engines ran out of oxidizer.\n\nSpaceX's Falcon 9 Flight 28 propelled the Japanese JCSAT-16 telecommunications satellite to a geosynchronous transfer orbit on August 14, 2016. The first stage re-entered the atmosphere and during the night landed vertically on \"Of Course I Still Love You\", positioned in the Atlantic Ocean nearly 400 miles from the Florida coastline; unlike previous successful landings, this landing-burn only used one engine, not three.\n\nOn January 14, 2017, the first-stage of a Falcon 9 landed on the Pacific ASDS \"JRtI\" during the Iridium NEXT-1 mission. This marked the first successful landing on \"JRtI\" and the first landing in the Pacific Ocean.\n\nOn March 30, 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You,\" during the SES-10 launch. This was the first successful launch and landing of a previously flown orbital booster.\n\nOn 23 June 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You,\" during the BulgariaSat-1 launch. This was the second successful launch and landing of a previously flown orbital booster. This was also the first booster to have landed on both drone ships. While the landing was considered a success, the first-stage was \"slammed sideways\" and suffered a 'hard landing' which resulted in 'most of the emergency crush core being used'.\n\nOn 25 June 2017, the first stage of a Falcon 9 landed on \"Just Read the Instructions,\" during the Iridium launch.\n\nOn 24 August 2017, the first-stage of a Falcon 9 landed on \"Just Read the Instructions\" during the FORMOSAT-5 launch.\n\nOn 9 October 2017, the first stage of a Falcon 9 landed on \"Just Read the Instructions,\" during the Iridium launch.\n\nOn 11 October 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You\" during the SES-11 launch.\n\nOn 30 October 2017, the first-stage of a Falcon 9 landed on \"Of Course I Still Love You\" during the Koreasat 5A mission.\n\n\n",
    "id": "44484413",
    "title": "Autonomous spaceport drone ship"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=45370555",
    "text": "LUTZ Pathfinder\n\nThe two-seater prototype pod has been built by Coventry-based RDM Group, and was first shown to the public in February 2015.\n\nThe LUTZ (Low-carbon Urban Transport Zone) Pathfinder pod is part of the UK Government's Transport Systems Catapult Autodrive project, a 20 million project.\n\nThree pods were tested initially in Milton Keynes during 2015 to ensure that they can comply with the Highway Code. A further trial, with a four seat vehicle developed from the LUTZ, commenced along a guided busway near Cambridge for possible use in an after hours service.\n\nThe pod is a two-seater electric car with space for luggage. It has a limited top speed of and has a range of or can run for eight hours. The self-driving equipment includes 19 sensors, cameras, radar and Lidar. Users can hail them by using a smartphone app.\n\nThe autonomous control software is developed by Mobile Robotics Group from University of Oxford.\n\nThe Lutz Pathfinder pod has been developed by the UK Automotive Council, Department for Business, Innovation and Skills and RDM Group.\n\nThe first trial of autonomous operation on a public road, with pedestrians, cycles and other vehicles, was conducted in Milton Keynes on 11 October 2016. The vehicles \"operated as expected.\"\n\nIn October 2017 a trial service, using a four-seat vehicle, between Trumpington Park and Ride and Cambridge railway station along the guided busway, for possible use in an after hours service. The £250,000 project was part funded by Innovate UK and delivered in partnership with Connecting Cambridgeshire and the Smart Cambridge Programme. If the trial is successful a ten seat vehicle will be used in routine service.\n\n",
    "id": "45370555",
    "title": "LUTZ Pathfinder"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=45533018",
    "text": "CougarTech\n\nCougarTech, FRC team 2228, is a FIRST Robotics Competition team that was founded in 2007, and is a school-based team from the Honeoye Falls-Lima Central School District in Honeoye Falls, New York. The team also represents the Rush–Henrietta Central School District in competition. CougarTech is a veteran team that prides itself on its coopertition with other teams as well as its teaching of younger teams. During the all important six weeks of build season, the team builds a robot to play the year's new game that FIRST designs each year. During the off season, the team focuses work on public outreach, recruitment, fundraising and sponsorship to support their team and FIRST. \n\n\n",
    "id": "45533018",
    "title": "CougarTech"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=189749",
    "text": "Automaton\n\nAn automaton (; plural: automata or automatons) is a self-operating machine, or a machine or control mechanism designed to automatically follow a predetermined sequence of operations, or respond to predetermined instructions. Some automata, such as bellstrikers in mechanical clocks, are designed to give the illusion to the casual observer that they are operating under their own power.\n\nThe word \"automaton\" is the latinization of the Greek , \"automaton\", (neuter) \"acting of one's own will\". This word was first used by Homer to describe automatic door opening, or automatic movement of wheeled tripods. It is more often used to describe non-electronic moving machines, especially those that have been made to resemble human or animal actions, such as the \"jacks\" on old public striking clocks, or the cuckoo and any other animated figures on a cuckoo clock.\n\nThere are many examples of automata in Greek mythology: Hephaestus created automata for his workshop; Talos was an artificial man of bronze; Daedalus used quicksilver to install voice in his moving statues; King Alkinous of the Phaiakians employed gold and silver watchdogs.\n\nThe automata in the Hellenistic world were intended as tools, toys, religious idols, or prototypes for demonstrating basic scientific principles. Numerous water powered automata were built by Ktesibios, a Greek inventor and the first head of the Great Library of Alexandria, for example he \"\"used water to sound a whistle and make a model owl move. He had invented the world's first \"cuckoo\" clock\"\". This tradition continued in Alexandria with inventors such as the Greek mathematician Hero of Alexandria (sometimes known as Heron), whose writings on hydraulics, pneumatics, and mechanics described siphons, a fire engine, a water organ, the aeolipile, and a programmable cart.\n\nComplex mechanical devices are known to have existed in Hellenistic Greece, though the only surviving example is the Antikythera mechanism, the earliest known analog computer. It is thought to have come originally from Rhodes, where there was apparently a tradition of mechanical engineering; the island was renowned for its automata; to quote Pindar's seventh Olympic Ode:\n\nHowever, the information gleaned from recent scans of the fragments indicate that it may have come from the colonies of Corinth in Sicily and implies a connection with Archimedes.\n\nAccording to Jewish legend, Solomon used his wisdom to design a throne with mechanical animals which hailed him as king when he ascended it; upon sitting down an eagle would place a crown upon his head, and a dove would bring him a Torah scroll. It's also said that when King Solomon stepped upon the throne, a mechanism was set in motion. As soon as he stepped upon the first step, a golden ox and a golden lion each stretched out one foot to support him and help him rise to the next step. On each side, the animals helped the King up until he was comfortably seated upon the throne.\n\nIn ancient China, a curious account of automata is found in the Lie Zi text, written in the 3rd century BC. Within it there is a description of a much earlier encounter between King Mu of Zhou (1023-957 BC) and a mechanical engineer known as Yan Shi, an 'artificer'. The latter proudly presented the king with a life-size, human-shaped figure of his mechanical handiwork:\nThe king stared at the figure in astonishment. It walked with rapid strides, moving its head up and down, so that anyone would have taken it for a live human being. The artificer touched its chin, and it began singing, perfectly in tune. He touched its hand, and it began posturing, keeping perfect time...As the performance was drawing to an end, the robot winked its eye and made advances to the ladies in attendance, whereupon the king became incensed and would have had Yen Shih [Yan Shi] executed on the spot had not the latter, in mortal fear, instantly taken the robot to pieces to let him see what it really was. And, indeed, it turned out to be only a construction of leather, wood, glue and lacquer, variously coloured white, black, red and blue. Examining it closely, the king found all the internal organs complete—liver, gall, heart, lungs, spleen, kidneys, stomach and intestines; and over these again, muscles, bones and limbs with their joints, skin, teeth and hair, all of them artificial...The king tried the effect of taking away the heart, and found that the mouth could no longer speak; he took away the liver and the eyes could no longer see; he took away the kidneys and the legs lost their power of locomotion. The king was delighted.\nOther notable examples of automata include Archytas's dove, mentioned by Aulus Gellius. Similar Chinese accounts of flying automata are written of the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban, who made artificial wooden birds (\"ma yuan\") that could successfully fly according to the \"Han Fei Zi\" and other texts.\n\nThe manufacturing tradition of automata continued in the Greek world well into the Middle Ages. On his visit to Constantinople in 949 ambassador Liutprand of Cremona described automata in the emperor Theophilos' palace, including \n\"lions, made either of bronze or wood covered with gold, which struck the ground with their tails and roared with open mouth and quivering tongue,\" \"a tree of gilded bronze, its branches filled with birds, likewise made of bronze gilded over, and these emitted cries appropriate to their species\" and \"the emperor’s throne\" itself, which \"was made in such a cunning manner that at one moment it was down on the ground, while at another it rose higher and was to be seen up in the air.\" \nSimilar automata in the throne room (singing birds, roaring and moving lions) were described by Luitprand's contemporary Constantine Porphyrogenitus, who later became emperor, in his book \"Περὶ τῆς Βασιλείου Τάξεως\".\n\nIn the mid-8th century, the first wind powered automata were built: \"statues that turned with the wind over the domes of the four gates and the palace complex of the Round City of Baghdad\". The \"public spectacle of wind-powered statues had its private counterpart in the 'Abbasid palaces where automata of various types were predominantly displayed.\" Also in the 8th century, the Muslim alchemist, Jābir ibn Hayyān (Geber), included recipes for constructing artificial snakes, scorpions, and humans that would be subject to their creator's control in his coded \"Book of Stones\". In 827, Caliph Al-Ma'mun had a silver and golden tree in his palace in Baghdad, which had the features of an automatic machine. There were metal birds that sang automatically on the swinging branches of this tree built by Muslim inventors and engineers. The Abbasid Caliph Al-Muqtadir also had a golden tree in his palace in Baghdad in 915, with birds on it flapping their wings and singing. In the 9th century, the Banū Mūsā brothers invented a programmable automatic flute player and which they described in their \"Book of Ingenious Devices\".\n\nAl-Jazari described complex programmable humanoid automata amongst other machines he designed and constructed in the \"Book of Knowledge of Ingenious Mechanical Devices\" in 1206. His automaton was a boat with four automatic musicians that floated on a lake to entertain guests at royal drinking parties. His mechanism had a programmable drum machine with pegs (cams) that bump into little levers that operate the percussion. The drummer could be made to play different rhythms and drum patterns if the pegs were moved around. According to Charles B. Fowler, the automata were a \"robot band\" which performed \"more than fifty facial and body actions during each musical selection.\"\n\nAl-Jazari constructed a hand washing automaton first employing the flush mechanism now used in modern toilets. It features a female automaton standing by a basin filled with water. When the user pulls the lever, the water drains and the automaton refills the basin. His \"peacock fountain\" was another more sophisticated hand washing device featuring humanoid automata as servants who offer soap and towels. Mark E. Rosheim describes it as follows: \"Pulling a plug on the peacock's tail releases water out of the beak; as the dirty water from the basin fills the hollow base a float rises and actuates a linkage which makes a servant figure appear from behind a door under the peacock and offer soap. When more water is used, a second float at a higher level trips and causes the appearance of a second servant figure — with a towel!\" Al-Jazari thus appears to have been the first inventor to display an interest in creating human-like machines for practical purposes such as manipulating the environment for human comfort.\n\n\"Samarangana Sutradhara\", a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.\n\nVillard de Honnecourt, in his 1230s sketchbook, show plans for animal automata and an angel that perpetually turns to face the sun. At the end of the thirteenth century, Robert II, Count of Artois built a pleasure garden at his castle at Hesdin that incorporated several automata as entertainment in the walled park. The work was conducted by local workmen and overseen by the Italian knight Renaud Coignet. It included monkey marionettes, a sundial supported by lions and \"wild men\", mechanized birds, mechanized fountains and a bellows-operated organ. The park was famed for its automata well into the fifteenth century before it was destroyed by English soldiers in the sixteenth.\n\nThe Chinese author Xiao Xun wrote that when the Ming Dynasty founder Hongwu (r. 1368–1398) was destroying the palaces of Khanbaliq belonging to the previous Yuan Dynasty, there were—among many other mechanical devices—automata found that were in the shape of tigers.\n\nThe Renaissance witnessed a considerable revival of interest in automata. Hero's treatises were edited and translated into Latin and Italian. Giovanni Fontana created mechanical devils and rocket-propelled animal automata. Numerous clockwork automata were manufactured in the 16th century, principally by the goldsmiths of the Free Imperial Cities of central Europe. These wondrous devices found a home in the cabinet of curiosities or \"Wunderkammern\" of the princely courts of Europe. Hydraulic and pneumatic automata, similar to those described by Hero, were created for garden grottoes.\n\nLeonardo da Vinci sketched a more complex automaton around the year 1495. The design of Leonardo's robot was not rediscovered until the 1950s. The robot could, if built successfully, move its arms, twist its head, and sit up.\n\nThe Smithsonian Institution has in its collection a clockwork monk, about high, possibly dating as early as 1560. The monk is driven by a key-wound spring and walks the path of a square, striking his chest with his right arm, while raising and lowering a small wooden cross and rosary in his left hand, turning and nodding his head, rolling his eyes, and mouthing silent obsequies. From time to time, he brings the cross to his lips and kisses it. It is believed that the monk was manufactured by Juanelo Turriano, mechanician to the Holy Roman Emperor Charles V.\n\nA new attitude towards automata is to be found in Descartes when he suggested that the bodies of animals are nothing more than complex machines - the bones, muscles and organs could be replaced with cogs, pistons and cams. Thus mechanism became the standard to which Nature and the organism was compared. France in the 17th century was the birthplace of those ingenious mechanical toys that were to become prototypes for the engines of the Industrial Revolution. Thus, in 1649, when Louis XIV was still a child, an artisan named Camus designed for him a miniature coach, and horses complete with footmen, page and a lady within the coach; all these figures exhibited a perfect movement. According to P. Labat, General de Gennes constructed, in 1688, in addition to machines for gunnery and navigation, a peacock that walked and ate. Athanasius Kircher produced many automata to create Jesuit shows, including a statue which spoke and listened via a speaking tube.\nThe world's first successfully-built biomechanical automaton is considered to be \"The Flute Player\", invented by the French engineer Jacques de Vaucanson in 1737. He also constructed the Digesting Duck, a mechanical duck that gave the false illusion of eating and defecating, seeming to endorse Cartesian ideas that animals are no more than machines of flesh.\n\nIn 1769, a chess-playing machine called the Turk, created by Wolfgang von Kempelen, made the rounds of the courts of Europe purporting to be an automaton. The Turk was operated from inside by a hidden human director, and was not a true automaton.\nOther 18th century automaton makers include the prolific Swiss Pierre Jaquet-Droz (see Jaquet-Droz automata) and his contemporary Henri Maillardet. Maillardet, a Swiss mechanic, created an automaton capable of drawing four pictures and writing three poems. Maillardet's Automaton is now part of the collections at the Franklin Institute Science Museum in Philadelphia. Belgian-born John Joseph Merlin created the mechanism of the Silver Swan automaton, now at Bowes Museum. A musical elephant made by the French clockmaker Hubert Martinet in 1774 is one of the highlights of Waddesdon Manor. Tipu's Tiger is another late-18th century example of automata, made for Tipu Sultan, featuring a European soldier being mauled by a tiger.\n\nAccording to philosopher Michel Foucault, Frederick the Great, king of Prussia from 1740 to 1786, was \"obsessed\" with automata. According to Manuel de Landa, \"he put together his armies as a well-oiled clockwork mechanism whose components were robot-like warriors\".\n\nJapan adopted automata during the Edo period (1603–1867); they were known as \"karakuri ningyō\".\n\nAutomata, particularly watches and clocks, were popular in China during the 18th and 19th centuries, and items were produced for the Chinese market. Strong interest by Chinese collectors in the 21st century brought many interesting items to market where they have had dramatic realizations.\n\nThe famous magician Jean Eugène Robert-Houdin (1805–1871) was known for creating automata for his stage shows.\nIn 1840, Italian inventor Innocenzo Manzetti constructed a flute-playing automaton, in the shape of a man, life-size, seated on a chair. Hidden inside the chair were levers, connecting rods and compressed air tubes, which made the automaton's lips and fingers move on the flute according to a program recorded on a cylinder similar to those used in player pianos. The automaton was powered by clockwork and could perform 12 different arias. As part of the performance it would rise from the chair, bow its head, and roll its eyes.\n\nThe period 1860 to 1910 is known as \"The Golden Age of Automata\". During this period many small family based companies of Automata makers thrived in Paris. From their workshops they exported thousands of clockwork automata and mechanical singing birds around the world. It is these French automata that are collected today, although now rare and expensive they attract collectors worldwide. The main French makers were Bontems, Lambert, Phalibois, Renou, Roullet & Decamps, Theroude and Vichy.\n\nContemporary automata continue this tradition with an emphasis on art, rather than technological sophistication. Contemporary automata are represented by the works of Cabaret Mechanical Theatre in the United Kingdom, Dug North and Chomick+Meder, Thomas Kuntz, Arthur Ganson, Joe Jones in the United States, Le Défenseur du Temps by French artist Jacques Monestier, and François Junod in Switzerland.\n\nSome mechanized toys developed during the 18th and 19th centuries were automata made with paper. Despite the relative simplicity of the material, paper automata require a high degree of technical ingenuity.\n\nThe potential educational value of mechanical toys in teaching transversal skills has been recognised by the European Union education project \"Clockwork objects, enhanced learning: Automata Toys Construction\" (CLOHE).\n\nExamples of automaton clocks include Chariot clock and Cuckoo Clocks. The Cuckooland Museum exhibits autonomous clocks.\n\n\n\n",
    "id": "189749",
    "title": "Automaton"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2083415",
    "text": "Navigation mesh\n\nA navigation mesh, or navmesh, is an abstract data structure used in artificial intelligence applications to aid agents in pathfinding through complicated spaces. This approach has been known since at least the mid-1980s in robotics, where it has been called a meadow map, and was popularized in video game AI in 2000.\n\nA navigation mesh is a collection of two-dimensional convex polygons (a polygon mesh) that define which areas of an environment are traversable by agents. In other words, a character in a game could freely walk around within these areas unobstructed by trees, lava, or other barriers that are part of the environment. Adjacent polygons are connected to each other in a graph.\n\nPathfinding within one of these polygons can be done trivially in a straight line because the polygon is convex and traversable. Pathfinding between polygons in the mesh can be done with one of the large number of graph search algorithms, such as A*. Agents on a navmesh can thus avoid computationally expensive collision detection checks with obstacles that are part of the environment.\n\nRepresenting traversable areas in a 2D-like form simplifies calculations that would otherwise need to be done in the \"true\" 3D environment, yet unlike a 2D grid it allows traversable areas that overlap above and below at different heights. The polygons of various sizes and shapes in navigation meshes can represent arbitrary environments with greater accuracy than regular grids can.\n\nNavigation meshes can be created manually, automatically, or by some combination of the two. In video games, a level designer might manually define the polygons of the navmesh in a level editor. This approach can be quite labor intensive. Alternatively, an application could be created that takes the level geometry as input and automatically outputs a navmesh.\n\nIt is commonly assumed that the environment represented by a navmesh is static – it does not change over time – and thus the navmesh can be created offline and be immutable. However, there has been some investigation of online updating of navmeshes for dynamic environments.\n\nIn robotics, using linked convex polygons in this manner has been called \"meadow mapping\", coined in a 1986 technical report by Ronald C. Arkin.\n\nNavigation meshes in video game artificial intelligence are usually credited to Greg Snook's 2000 article \"Simplified 3D Movement and Pathfinding Using Navigation Meshes\" in \"Game Programming Gems\". In 2001, J.M.P. van Waveren described a similar structure with convex and connected 3D polygons, dubbed the \"Area Awareness System\", used for bots in \"Quake III Arena\".\n\n\n",
    "id": "2083415",
    "title": "Navigation mesh"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28928489",
    "text": "Common normal (robotics)\n\nIn robotics the common normal of two non-intersecting joint axes is a line perpendicular to both axes.\n\nThe common normal can be used to characterize robot arm links, by using the \"common normal distance\" and the angle between the link axes in a plane perpendicular to the common normal. When two consecutive joint axes are parallel, the common normal is not unique and an arbitrary common normal may be used, usually one that passes through the center of a coordinate system.\n\nThe common normal is widely used in the representation of the frames of reference for robot joints and links, and the selection of minimal representations with the Denavit–Hartenberg parameters.\n\n",
    "id": "28928489",
    "title": "Common normal (robotics)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47133704",
    "text": "Care-O-bot\n\nCare-O-bot is the product vision of a mobile robot assistant to actively support humans e.g. in their daily life, in hotels, health care institutions or hospitals, developed by the Fraunhofer Institute for Manufacturing Engineering and Automation.\n\nThe fourth generation, Care-O-bot 4, was completed in January 2015. While its predecessors from 1998 onwards were used primarily in the development of technological fundamentals, Care-O-bot 4 is a modular product family providing the basis for commercial service robot solutions. In 2015, the product design of Care-O-bot 4 was awarded with the Red Dot Award: Product Design. As only 1,6 percent of all applications, Care-O-bot 4 received the recognition \"Best of the Best”.\n\nNumerous research institutions and universities around the world work with Care-O-bot to advance the areas of applications.\n\n",
    "id": "47133704",
    "title": "Care-O-bot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15046825",
    "text": "IOIO\n\nIOIO (pronounced \"yo-yo\") is a series of open source PIC microcontroller-based boards that allow Android mobile applications to interact with external electronics. The device was invented by Ytai Ben-Tsvi in 2011, and was first manufactured by SparkFun Electronics. The name \"IOIO\" is inspired by the function of the device, which enables applications to receive external input (\"I\") and produce external output (\"O\").\n\nThe IOIO board contains a single PIC MCU that acts as a USB host/USB slave and communicates with an Android app running on a connected Android device. The board provides connectivity via USB, USB-OTG or Bluetooth, and is controllable from within an Android application using the Java API.\n\nIn addition to basic digital input/output and analog input, the IOIO library also handles PWM, I2C, SPI, UART, Input capture, Capacitive sensing and advanced motor control. To connect to older Android devices that use USB 2.0 in slave mode, newer IOIO models use USB On-The-Go to act as a host for such devices. Some models also support the Google Open Accessory USB protocol.\n\nThe IOIO motor control API can drive up to 9 motors and any number of binary actuators in synchronization and cycle-accurate precision. Developers may send a sequence of high-level commands to the IOIO, which performs the low-level waveform generation on-chip. The IOIO firmware supports 3 different kinds of motors; stepper motors, DC motors and servo motors.\n\nDevice firmware may be updated on-site by the user. For first-generation devices updating is performed using an Android device and the \"IOIO Manager\" application available on Google Play. Second-generation IOIO-OTG devices must be updated using a desktop computer running the \"IOIODude\" application.\n\nThe IOIO supports both computers and Android devices as first-class hosts, and provides the exact API on both types of devices. First-generation devices can only communicate with PCs over Bluetooth, while IOIO-OTG devices can use either Bluetooth or USB. PC applications may use APIs for Java or C# to communicate with the board; Java being the official API.\n\nThe IOIO hardware and software is entirely open source, and enabled the creation of hundreds of DIY robotic projects around the world.\n\nThe board has been featured in various learning kits, which aim to help students write Android applications that can interact with the external world.\n\nThe Qualcomm Snapdragon Micro Rover is a 3D printed robot that leverages an Android smartphone and the IOIO to control the robot's motors and sensors. A team led by Israeli inventor Dr. Guy Hoffman created an emotionally-sensitive robot, that relies on the IOIO to control the robot's hardware.\n\nThe IOIO has been variously described as a \"geek's paradise\", \"an easy way to get I/O from an Android device’s USB connection\" and \"a USB I/O breakout board for Android smartphones which turns your handset into a super-Arduino of sorts\". It featured as a recommended \"gift for geeks\" in a Scientific Computing article.\n\nAccording to SlashGear, an online electronics magazine:\nAccording to SparkFun, the first manufacturer of the device:\nAccording to Ytai Ben-Tsvi, the inventor of the device:\n\nThe first-generation IOIO boards (known as \"IOIO V1\") contain the following on-board features: This generation only supports USB slave mode, and requires a USB master as the host (PC or newer Android phones).\n\nThe IOIO V1 is a 3.3 V logic level device, and features a 5 V DC/DC switching regulator and a 3.3V linear regulator. The 5 V regulator supports a 5–15 V input range and up to 1.5 A load. This facilitates charging a connected Android device as well as driving several small motors or similar loads.\n\nThe second-generation IOIO boards (known as \"IOIO-OTG\") contain the following on-board features: As the name suggests, a key feature of this generation is the introduction of USB-OTG, supporting USB master or slave mode. This enables the IOIO to connect to older Android phones that only support USB slave mode, in addition.\n\nThe IOIO-OTG is a 3.3 V logic level device, with some of the pins being 5 V tolerant. It features a 5 V DC/DC switching regulator and a 3.3 V linear regulator. The 5 V regulator supports a 5–15 V input range and up to 3 A load. This facilitates charging a connected Android device as well as driving several small motors or similar loads.\n\n",
    "id": "15046825",
    "title": "IOIO"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47796548",
    "text": "Amaryllo\n\nAmaryllo International B.V., founded in Amsterdam, the Netherlands, pioneers in AI as a Service market: Amaryllo offers real-time data mining, patented camera robot, fast object recognition, secure 256-bit encrypted P2P network, and flexible cloud storage to B2G and B2B market. \n\nAmaryllo develops a new type of patented robotic cameras that can talk, hear, sense, recognize human faces, and auto-track intruders 360 degrees and even from behind. Company debuted world's first security robot based on WebRTC protocol, iCamPRO FHD, and won the 2015 CES Best of Innovation Award under Embedded Technology category. Amaryllo's home security robots employ 256-bit encryption technologies and run on WebRTC protocol. Amaryllo is ranked as the number-one smart home camera robot company. \n\nAmaryllo revealed its first smart home security products at Internationale Funkausstellung Berlin (IFA) 2013 with a Skype-enabled IP camera called iCam HD. iCam HD was the first smart home IP camera certified by Plugged-into-Skype program with 256-bit encryption. Amaryllo announced its second Skype-certified smart home product, iBabi HD, at CES 2014. Company was chosen as a Cool Vendor by Gartner in Connected Home 2014 for being the first company offering the highest possible security protection in consumers security market. Amaryllo introduced WebRTC-based smart home products after Microsoft terminated embedded Skype services in mid 2014.Company has been developing a series of camera robots with focus on its auto-tracking and facial recognition technologies since. The professional outdoor IP65 grade camera robots, ATOM AR3 and ATOM AR3S, were introduced in late 2016.\n\nAmaryllo debuted its facial recognition technologies on the new auto-tracking model, ATOM at IFA 2016. ATOM is designed to recognize human faces from learning faces. It takes 0.5 seconds to detect a human face and another 0.5 second to identify a person, totaling only 1 second to recognize a human face. It can recognize over 100 people simultaneously. Amaryllo is offering facial recognition to its B2B partners. ASUS SmartHome platform has integrated ATOM to its offering to take advantage of this new technology.\n\nUnlike conventional tracking technologies which employ remote computers to perform object tracking algorithm, resulting in higher implementation costs and bulkier systems, Amaryllo uses multi-core processor embedded in compact cameras to realize complete tracking systems in single units. Each camera must be embedded with sufficient computation power and memory to possess artificial intelligence to analyze real-time image comparison to track moving objects, to stream real-time full HD video, to encode 256-bit encryption, to upload recorded videos, etc. Amaryllo security drones act as individual security robots to track moving objects on their own without commands from remote computers. With a 1920 x 1080 full HD resolution, these drones are able to track intruders over 30 feet away. Infrared lights are aesthetically hidden by a mask and are activated when the environment is dark. This enables Amaryllo drones to track objects in a complete darkness.\n\nAmaryllo added speech functions to its advanced camera robots. Multiple languages are introduced including English, Spanish, Arabic, French, Japanese, and Chinese. When faces or sensors are detected, camera robots will talk in pre-selected languages and say \"Hello\", \"Good Morning\", \"Good Afternoon\", etc. Camera robots can report hourly time such as \"It's 4 PM\" and even check your email arrival. If there is new email, camera robots will say \"You've got mail\" to remind owners. \n\nCompany's patent-pending technologies further enable security robots to track objects 360 degrees by introducing multiple motion sensors around the drones, so once a sensor is triggered, embedded CPUs will guide the drones to turn to the spotted direction to follow objects even from behind. This innovative design eliminates the need to implement multiple cameras in a single unit to reduce cost. Amaryllo security robots can talk to intruders if they are spotted and track intruders. Amaryllo iCamPRO FHD won 2015 WebRTC World Product of the Year Award.\n\nAmaryllo develops cloud-based artificial intelligent with its camera robots to recognize any objects. By taking advantage of large cloud computation power, faces, human body, vehicle, animals, birds, air planes, etc. can be detected and recognized in seconds. Over 100 human faces can be detected and recognized in seconds. Amaryllo uses real-time picture frame analysis to identify faces. Once detected, security drones will send out face alerts to registered users instantly. Amaryllo uses real-time picture frame analysis to identify faces. Once a humanoid face is recognized, robots will deliver face snapshots to smart devices within seconds. This patent-pending solution can eliminate possible false alerts found in popular network cameras with alerts generated by Passive infrared sensor.\n\nAmaryllo robots are linked to Google Services. If consumers receive an email, Amaryllo robots will say \"You've got mail.\" If consumers have appointments at 3 PM, Amaryllo robots will say \"You have an appointment at 3 pm,\" minutes before the meeting to remind consumers. These robots can also say \"Hello\", \"Good Morning\", \"Good Afternoon\", etc. when they detect events. These events could be motion, audio, or face detection pre-determined by users. These robots are wirelessly connected to networks, so they are aware of local time and can report time on an hourly fashion. For example, it will say \"It's 5 PM,\" acting as a regular clock. More interactive voice communications are reported.\n\nTo eliminate common false alarm triggered by popular audio or PIR sensors, Amaryllo devises an option on app to allow consumers to define areas where they are ignored or they are monitored by their robots. If robots are led to a certain area where movement later stops, robots will rotate themselves to resume to their pre-defined \"home\" positions to safeguard the most important asset. The combination of the above, enables Amaryllo robots to reduce false alarm raised by conventional pixel-based sensors.\n\nAmaryllo was the first to establish global Peer-to-Peer (P2P) server based on WebRTC protocol in smart home service. Amaryllo Live is a plug-in-free H.264-based browser service allowing consumers to access their cameras anywhere anytime. It runs on WebRTC protocol and Firefox is the first browser company to support Amaryllo Live. Other browser companies have vowed to support WebRTC H.264-based codec. Amaryllo server measures each communications channel and dynamically adjusts the video format based on available bandwidth to effectively alleviate video latency and results in a better video communications experience. It also permits a true two-way audio talk.\n\nAmaryllo offers free and paid unlimited cloud storage plans available to its products. Unconventionally, Amaryllo cloud service includes video alerts allowing consumers to review urgent video messages from smart devices remotely. Video alerts are handy as brief recorded videos provide additional information to consumers in comparison against typical single frame picture alerts. Amaryllo launched Urgent Home Care Service with an introduction of iCare FHD. With a touch of a remote control, the patent-pending technologies deliver video alerts to registered members in seconds. iCare FHD can detect faces and sends out real-time face alert video to family members. This is the first urgent home care products employing WebRTC technologies.\n\n",
    "id": "47796548",
    "title": "Amaryllo"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48081904",
    "text": "Translational drift\n\nTranslational drift also known as melty brain or tornado drive is a form of locomotion, notably found in certain combat robots.\n\nThe principle is applied to spinning robots, where the driving wheels are normally on for the whole revolution, resulting in an increased rotational energy, which is stored for destructive effect, but, given perfect symmetry, no net translational acceleration. The drive works by modulating the power to the wheel or wheels that spin the robot. The net application of force in one direction results in acceleration in the plane - it can't really be characterised as \"forward\", \"backward and so forth, as the whole robot is spinning. However, in a standard configuration an accelerometer is used to determine the speed of rotation, and a light emitting diode is turned on once per revolution, to give a nominal forward direction indicator to the operator. The internal controls implement the commands received from the remote control to modulate the drive to the wheels, typically by turning it off for part of a revolution.\n\nOpen Melt is an open source implementation of melty brain, the code being licensed under Creative Commons Attribution-Noncommercial-Share Alike licence.\n\n",
    "id": "48081904",
    "title": "Translational drift"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=80579",
    "text": "Droid (robot)\n\nA droid is a fictional robot possessing some degree of artificial intelligence in the \"Star Wars\" science fiction franchise. Coined by special effects artist John Stears, the term is a clipped form of \"android\", a word originally reserved for robots designed to look and act like a human. The word \"droid\" has been a registered trademark of Lucasfilm Ltd since 1977.\n\nThe franchise, which began with the 1977 film \"Star Wars\", features a variety of droids designed to perform specific functions.\n\nA protocol droid specializes in translation, etiquette and cultural customs, and is typically humanoid in appearance. The most notable example is C-3PO, introduced in \"Star Wars\" and featured in all sequels and prequels. 4-LOM is a protocol droid turned bounty hunter who responds to Darth Vader's call to capture the \"Millennium Falcon\" in \"The Empire Strikes Back\" (1980). TC-14 is a droid with feminine programming that appears in \"\" (1999), and ME-8D9 is an \"ancient protocol droid of unknown manufacture\" that resides and works as a translator at Maz Kanata’s castle on Takodana in the 2015 \"\".\n\nAn astromech droid is one of a series of \"versatile utility robots generally used for the maintenance and repair of starships and related technology\". These small droids usually possess \"a variety of tool-tipped appendages that are stowed in recessed compartments\". R2-D2 is an astromech droid introduced in 1977's \"Star Wars\" and featured in all subsequent films. The malfunctioning droid R5-D4 also makes a brief appearance in \"Star Wars\". U9-C4 is a timid droid sent on a mission with D-Squad, an all-droid special unit in \"\", C1-10P is an oft-repaired, \"outmoded\" astromech who is one of the \"Star Wars Rebels\" regular characters, and BB-8 is the astromech droid of X-wing fighter pilot Poe Dameron in \"The Force Awakens\".\n\nA battle droid is a class of military robot in the \"Star Wars\" prequel trilogy of films (1999–2005) and the \"\" TV series (2008–14), used as an easily controlled alternative to human soldiers. The tall, thin B1 model resembles the Geonosian race, who designed the droids, and are known to \"suffer programming glitches that manifest as personality quirks\". The droideka is a three-legged heavy infantry unit with twin blasters and the ability to generate a force shield and transform into a disk shape. Multiple other types of specialized battle droids have been featured in the films and the TV series.\n\nHK-47 is a humanoid soldier robot, designed as a violent killer, which first appeared in the 2003 video game \"\".\n\n\"Star Wars: The Clone Wars\" has featured WAC-47, a \"pit droid\" programmed as a pilot and sent on a mission with the all-droid special unit D-Squad, and AZI-3, a medical droid serving the cloners of Kamino who helps uncover the secret of Order 66. The 2015 young adult novel \"\" by Cecil Castellucci and Jason Fry introduces the droid PZ-4CO, to whom Leia Organa dictates her memoirs. PZ-4CO also appears in \"The Force Awakens\" (2015). In the 2016 film \"Rogue One\", K-2SO is an Imperial enforcer droid reprogrammed by the Rebel Alliance.\nThe term \"Droid\" was first used in the science fiction story \"Robots of the World! Arise!\" by Mari Wolf, published in \"If: Worlds of Science Fiction\" in July 1952.\nLucasfilm registered \"droid\" as a trademark in 1977.\n\nThe term \"Droid\" has been used by Verizon Wireless under licence from Lucasfilm, for their line of smartphones based on the Android operating system. Motorola's late-2009 Google Android-based cell phone is called the Droid. This line of phone has been expanded to include other Android-based phones released under Verizon, including the HTC Droid Eris, the HTC Droid Incredible, Motorola Droid X, Motorola Droid 2, and Motorola Droid Pro. The term was also used for the Lucasfilm projects EditDroid, a non-linear editing system, and SoundDroid, an early digital audio workstation.\n\n\n",
    "id": "80579",
    "title": "Droid (robot)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48508780",
    "text": "Robotic materials\n\nRobotic materials are composite materials that combine sensing, actuation, computation, and communication in a repeatable or amorphous pattern. Robotic materials can be considered \"computational metamaterials\" in that they extend the original definition of a metamaterial as \"macroscopic composites having a man-made, three-dimensional, periodic cellular architecture designed to produce an optimized combination, not available in nature, of two or more responses to specific excitation\" by being fully programmable. That is, unlike in a conventional metamaterial, the relationship between a specific excitation and response is governed by sensing, actuation, and a computer program that implements the desired logic.\n\nThe idea of creating materials that embed computation is closely related to the concept of programmable matter, a term coined in 1991 by Toffoli and Margolus, describing dense arrays of computing elements that could solve complex finite-element like simulations of material systems, and then later developed to describe a class of materials consisting of identical, mobile building blocks, also known as catoms that are fully reconfigurable, therefore allowing materials to arbitrarily change their physical properties.\n\nRobotic materials build up on the original concept of programmable matter, but focus on the structural properties of the embedding polymers without claim of universal property changes. Here the term \"robotic\" refers to the confluence of sensing, actuation, and computation.\n\nRobotic materials allow to off-load computation inside the material, most notably signal processing that arises during high-bandwidth sensing applications or feedback control that is required by fine-grained distributed actuation. Examples for such applications include camouflage, shape change, load balancing, and robotic skins as well as equipping robots with more autonomy by off-loading some of the signal processing and controls into the material.\n\nResearch in robotic materials ranges from the device-level and manufacturing to the distributed algorithms that equip robotic materials with intelligence. As such it intersects the fields of composite materials, sensor networks, distributed algorithms, and due to the scale of the involved computation, swarm intelligence. Unlike any individual field, the design of the structure, sensors, actuators, communication infrastructure, and distributed algorithms are tightly intertwined. For example, the material properties of the structural material will affect how signals to be sensed propagate through the material, at which distance computational elements need to be spaced, and what signal processing needs to be done. Similarly, structural properties are closely related to the actual embedding of computing and communication infrastructure. Capturing these effects therefore requires interdisciplinary collaboration between materials, computer science, and robotics.\n",
    "id": "48508780",
    "title": "Robotic materials"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48834013",
    "text": "DAVI\n\nThe Dutch Automated Vehicle Initiative (DAVI) is a research and demonstration initiative developing automated vehicles for use on public roads.\n\nThe project is special in that, besides simply making driverless cars, it also focuses on having the automated vehicles share information amongst each other. The aim is to have the cars helping to avoid traffic congestion, by reducing the safety distance between the cars (from 2 seconds drive distance now to just 0.5 seconds) and avoiding sudden traffic slow-downs (due to manoeuvres undertaken by drivers).\n",
    "id": "48834013",
    "title": "DAVI"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48805092",
    "text": "Alice Cares\n\nAlice Cares is a 2015 Dutch film directed by Sander Burger. It is a documentary film that explores how a new robot technology may be able to aid many senior citizens in the developing western world. The film premiered at the Vancouver International Film Festival in Vancouver, B.C. on September 27, 2015.\n",
    "id": "48805092",
    "title": "Alice Cares"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49312388",
    "text": "Open Bionics\n\nOpen Bionics, founded in 2014, is a UK based start-up company developing low-cost bionic hands. It is based inside the Bristol Robotics Laboratory, the largest robotics research centre in Europe.\n\nOpen Bionics creates advanced bionic hands for amputees using 3D scanning and 3D printing technology. Although the company is pre-sales they have released a number of user trial videos showing how their technology works.\n\nOpen Bionics have released designs for several of their prototypes with an open source license. The company encourages engineers and developers to contribute to the development of bionic hands by joining their developer community. These hands provide a research platform for robotics or a test platform for prosthetics research. Each hand has been designed to be easy to build and repair.\n\nA 3D printed robotic hand developed by The Open Hand Project using desktop 3D printers.\n\nSecond version of the 3D printed robotic hand developed by Open Bionics using desktop 3D printers.\n\nThe latest version of Open Bionics' robotic hand. The Ada hand can be assembled in under one hour, takes 20 hours to print on a desktop 3D printer, and can be controlled via EMG sensors. The Ada hand has fingers that are individually actuated and controlled via muscle tensing. The hand has multiple grip modes including open / close, tripod, pinch, point, and power grip.\n\nThe company has released their design files and code on their website, Instructables, Thingiverse, and GitHub.\n\nOpen Bionics announced a royalty-free licensing deal with Disney in 2015 to create official Iron Man, Star Wars, and Disney Frozen bionic hands for young amputees. The company is yet to set a release date for these superhero inspired bionic hands, although the company has successfully trialled an Iron Man and a Star Wars bionic hand with two children.\n\nOpen Bionics has won multiple awards for engineering and innovation including:\n",
    "id": "49312388",
    "title": "Open Bionics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49321811",
    "text": "UAV-related events\n\nUnmanned aerial vehicles (UAVs) or drones have frequently been involved in military operations. Non-military UAVs have often been reported as causing hazards to aircraft, or to people or property on the ground.\n\n\nThe pilot of British Airways flight BA727 reported a collision with a UAV to Metropolitan Police. The Airbus A320 was approaching Heathrow Airport when the collision happened. None of the 132 passengers or 5 crew were injured. After an inspection by engineers, the aircraft was cleared to take off for its next flight.\n\nMembers of the British government as well as the Labour Party and BALPA called for urgent action, including a register of drone users and geo-fencing of airports.\n\nIn late April 2016 Transport Secretary Patrick McLoughlin told MPs that experts believed that the incident was not related to UAVs. There was no damage to the aircraft and it wasn't clear if it was a plastic bag rather than a UAV. An investigation by the Air Accidents Investigation Branch had been closed due to lack of evidence. Police had searched a \"wide area\" of Richmond and found nothing.\n\nA UAV collided with a Black Hawk helicopter in the evening over the eastern shore of Staten Island. The helicopter was one of two with the 82nd Airborne Division flying out of Fort Bragg on duty for the United Nations General Assembly. The helicopters were able to continue flying and landed at Linden Airport. Nobody was hurt, but part of the UAV was found at the bottom of the main rotor system.\n\nIn December 2017 the National Transportation Safety Board issued an accident report into the collision, finding the pilot of the UAV at fault. The UAV operator deliberately flew the UAV 2.5 miles away from himsef and was unaware of the helicopters' presence. The operator did not know of the collision until contacted by the NTSB and when interviewed by them showed only a general cursory awareness of the regulations. There was also a Temporary flight restriction in place, which allowed the Black Hawk but not the UAV. UAVs are prohibited from flying beyond line of sight under FAA regulations. \n\nA Beech King Air A100 of Skyjet Aviation collided with a UAV as the former was approaching Jean Lesage Airport near Quebec City. The aircraft landed safely despite being hit on the wing. Thei aircraft had flown from Rouyn-Noranda Airport to Rouyn-Noranda Airport with six passengers and two crew. A spokeswoman for Quebec City Police said that neither the UAV or operator had been found. It had been flying at 1500 feet - five times the maximum altitude that UAVs are permitted to fly in Canada. Regulations banning flying of UAVs near airports had been introduced earlier in 2017. Minister of Transport Marc Garneau released a statement saying \"Although the vast majority of drone operators fly responsibly, it was our concern for incidents like this that prompted me to take action and issue interim safety measures restricting where recreational drones could be flown. I would like to remind drone operators that endangering the safety of an aircraft is extremely dangerous and a serious offence.\"\n\n\n\n\n\nIn January 2017 a 23 year old UAV operator from Xiaoshan was detained because of footage taken from a drone flying near airliners descending to land at Hangzhou Xiaoshan International Airport. The incident came to light when footage was uploaded to QQ. This was an eight-second clip from a ten-minute recording taken from an altitude of 450m. The operator had flown it to photograph a sunset, but had also recorded several airliners flying past. The model used was a DJI Mavic and the manufacturer strongly condemned the incident.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUAVs have historically had a much higher loss rate than manned military aircraft. In addition to anti-aircraft weapons, UAVs are vulnerable to power and communications link losses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "id": "49321811",
    "title": "UAV-related events"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49548813",
    "text": "Robot leg\n\nA robot leg (or robotic leg) is a mechanical leg that performs the same functions that a human leg can. The robotic leg is typically programmed to execute similar functions as a human leg. A robotic leg is similar to a prosthetic leg. However, a robotic leg can be controlled electrically or mechanically. To have the robotic leg emulate human leg behaviors, surgeons must redirect the nerves that previously controlled some of the person’s lower-leg muscles to cause the thigh muscles to contract. Sensors embedded in the robotic leg measure the electrical pulses created by both a re-innervated muscle contraction, and the existing thigh muscle.\n\nA robotic Leg attaches to an individual who has had a lower extremity amputation—of a portion of a leg or foot. Doctors and technicians measure the remaining limb structure and of the person’s prosthesis to ideally fit the robotic leg. After they attach the robotic leg, they embed the sensors in the robotic leg that measure the electrical activity created by re-innervated muscle contraction, and existing thigh muscle.\n\nRobotic legs have become a popular target of research and development in the past few years. Robotic legs can be applied to people who have had limb amputating surgery caused by a major trauma, or a disease like cancer. Robotic legs have also seen much applicable use in military combat injuries. Massachusetts Institute of Technology (MIT) has been working to create a “bionic” or robotic leg that can function as if it had organic muscles.\n\n",
    "id": "49548813",
    "title": "Robot leg"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49944357",
    "text": "Bricksmart\n\nBricksmart is a FIRST Lego League community team of students of ages 10–14 located in The Woodlands, Texas. Over the past five years Bricksmart has received numerous accolades in various Texas FLL tournaments.\n\n2011: The team was founded\n\n2011-2012 (Senior Solutions): Bricksmart placed third at the Woodlands Preparatory Qualifier and advanced to the South Texas Championship\n\n2013-2014 (Nature's Fury): Bricksmart placed second at the Woodlands Preparatory Qualifier and won second place for innovative design in the research project at the South Texas Championship. The team also placed fourth overall at the South Texas Championship.\n\n2014: Bricksmart placed 4th in the All-Earth Ecobot challenge.\n\n2014-2015 (World Class): The team placed second at the Huntsville Qualifier and placed first at the South Texas Championship. That advanced them to the FIRST World Festival in St. Louis, Missouri.\n\n2015-2016 (Trash Trek): Bricksmart placed first at the Texas Torque Qualifier and won Champion's Award for the second time at the South Texas Championship. From May 19–22, they competed at the Razorback FLL Invitational, held in Fayetteville, Arkansas. In 2016 Bricksmart founded a business, now defunct, called Ecosmartz. \n\n2016-2017 (Animal Allies): Bricksmart is competing in the Animal Allies challenge.\n\n2017-2018 The team switches from FLL competitions and now competes in FTC competitions. \n\nFor their 2016 project, the team started a business. To reduce trash in their community, Ecosmartz hauls compostable food waste from grocery stores and restaurants in The Woodlands, and delivers it to a local composting facility.\nAs of November 5, 2016 EcoSmartz has ceased operations and is defunct. There are no plans to open the business in the near future.\n",
    "id": "49944357",
    "title": "Bricksmart"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49959510",
    "text": "Georges Giralt PhD Award\n\n]\nThe Georges Giralt PhD Award is a European scientific prize for extraordinary contributions in robotics. It is yearly awarded at the \"European Robotics Forum\" by \"euRobotics\" AISBL, a non-profit organisation based in Brussels with the objective to turn robotics beneficial for Europe’s economy and society.\n\nThe high reputation of the \"Georges Giralt PhD Award\" is based on the prominent role of the awarding institution \"euRobotics\". With more than 250 member organisations, euRobotics represents the academic and industrial robotics community in Europe. Moreover, it provides the European robotics community with a legal entity to engage in a public/private partnership with the European Commission.\n\nEntitled for participation for the \"Georges Giralt PhD Award\" are all robotics-related dissertations which have been successfully defended at a European university.\n\nThe US-American counterpart is the Dick Volz Award.\n\n",
    "id": "49959510",
    "title": "Georges Giralt PhD Award"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49803819",
    "text": "Australian Research Council Centre of Excellence for Robotic Vision\n\nThe Australian Centre for Robotic Vision is an unincorporated collaborative venture with funding of $25.6m over seven years to pursue a research agenda tackling the critical and complex challenge of applying robotics in the real world.\n\nThe Center is made up of an interdisciplinary team from four leading Australian universities\nresearch organisations and international universities\n\nThe Centre aims to achieve breakthrough science and technology in robotic vision by addressing four key research objectives: robust vision, vision and action, semantic vision, and algorithms and architecture. Together the four research objectives form the Centre’s research themes, which serve as organisational groupings of the Centre’s research projects.\n\nWill develop new sensing technologies and robust algorithms that allow robots to use visual perception in all viewing conditions: night and day, rain or shine, summer or winter, fast moving or static.\n\nWill create new theory and methods for using image data for control of robotic systems that navigate through space, grasp objects, interact with humans and use motion to assist in seeing.\n\nWill produce novel learning algorithms that can both detect and recognise a large, and potentially ever increasing, number of object classes from robotically acquired images, with increasing reliability over time.\n\nWill create novel technologies and techniques to ensure that the algorithms developed across the themes can be run in real-time on robotic systems deployed in large-scale real-world applications.\n",
    "id": "49803819",
    "title": "Australian Research Council Centre of Excellence for Robotic Vision"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20756967",
    "text": "Cyborg\n\nA cyborg (short for \"cybernetic organism\") is a being with both organic and biomechatronic body parts. The term was coined in 1960 by Manfred Clynes and Nathan S. Kline.\n\nThe term cyborg is not the same thing as bionic, biorobot or android; it applies to an organism that has restored function or enhanced abilities due to the integration of some artificial component or technology that relies on some sort of feedback. While cyborgs are commonly thought of as mammals, including humans, they might also conceivably be any kind of organism.\n\nD. S. Halacy's \"Cyborg: Evolution of the Superman\" in 1965 featured an introduction which spoke of a \"new frontier\" that was \"not merely space, but more profoundly the relationship between 'inner space' to 'outer space' – a bridge...between mind and matter.\"\n\nIn popular culture, some cyborgs may be represented as visibly mechanical (e.g., Cyborg from DC Comics, the Cybermen in the \"Doctor Who\" franchise or The Borg from \"Star Trek\" or Darth Vader from \"Star Wars\") or as almost indistinguishable from humans (e.g., the \"Human\" Cylons from the re-imagining of \"Battlestar Galactica\", etc.). Cyborgs in fiction often play up a human contempt for over-dependence on technology, particularly when used for war, and when used in ways that seem to threaten free will. Cyborgs are also often portrayed with physical or mental abilities far exceeding a human counterpart (military forms may have inbuilt weapons, among other things).\n\nAccording to some definitions of the term, the physical attachments humanity has with even the most basic technologies have already made them cyborgs. In a typical example, a human with an artificial cardiac pacemaker or implantable cardioverter-defibrillator would be considered a cyborg, since these devices measure voltage potentials in the body, perform signal prpocessing, and can deliver electrical stimuli, using this synthetic feedback mechanism to keep that person alive. Implants, especially cochlear implants, that combine mechanical modification with any kind of feedback response are also cyborg enhancements. Some theorists cite such modifications as contact lenses, hearing aids, or intraocular lenses as examples of fitting humans with technology to enhance their biological capabilities. As cyborgs currently are on the rise some theorists argue there is a need to develop new definitions of aging and for instance a bio-techno-social definition of aging has been suggested.\n\nThe term is also used to address human-technology mixtures in the abstract. This includes not only commonly used pieces of technology such as phones, computers, the Internet, etc. but also artifacts that may not popularly be considered technology; for example, pen and paper, and speech and language. When augmented with these technologies and connected in communication with people in other times and places, a person becomes capable of much more than they were before. An example is a computer, which gains power by using Internet protocols to connect with other computers. Another example, which is becoming more and more relevant is a bot-assisted human or human-assisted-bot, used to target social media with likes and shares. Cybernetic technologies include highways, pipes, electrical wiring, buildings, electrical plants, libraries, and other infrastructure that we hardly notice, but which are critical parts of the cybernetics that we work within.\n\nBruce Sterling in his universe of Shaper/Mechanist suggested an idea of alternative cyborg called Lobster, which is made not by using internal implants, but by using an external shell (e.g. a Powered Exoskeleton). Unlike human cyborgs that appear human externally while being synthetic internally (e.g. the Bishop type in the Alien franchise), Lobster looks inhuman externally but contains a human internally (e.g. Elysium, RoboCop). The computer game \"\" prominently featured cyborgs called Omar, where \"Omar\" is a Russian translation of the word \"Lobster\" (since the Omar are of Russian origin in the game).\n\nThe concept of a man-machine mixture was widespread in science fiction before World War II. As early as 1843, Edgar Allan Poe described a man with extensive prostheses in the short story \"The Man That Was Used Up\". In 1911, Jean de la Hire introduced the Nyctalope, a science fiction hero who was perhaps the first literary cyborg, in \"Le Mystère des XV\" (later translated as \"The Nyctalope on Mars\"). Edmond Hamilton presented space explorers with a mixture of organic and machine parts in his novel \"The Comet Doom\" in 1928. He later featured the talking, living brain of an old scientist, Simon Wright, floating around in a transparent case, in all the adventures of his famous hero, Captain Future. He uses the term explicitly in the 1962 short story, \"After a Judgment Day,\" to describe the \"mechanical analogs\" called \"Charlies,\" explaining that \"[c]yborgs, they had been called from the first one in the 1960s...cybernetic organisms.\" In the short story \"No Woman Born\" in 1944, C. L. Moore wrote of Deirdre, a dancer, whose body was burned completely and whose brain was placed in a faceless but beautiful and supple mechanical body.\n\nThe term was coined by Manfred E. Clynes and Nathan S. Kline in 1960 to refer to their conception of an enhanced human being who could survive in extraterrestrial environments:\n\nTheir concept was the outcome of thinking about the need for an intimate relationship between human and machine as the new frontier of space exploration was beginning to open up. A designer of physiological instrumentation and electronic data-processing systems, Clynes was the chief research scientist in the Dynamic Simulation Laboratory at Rockland State Hospital in New York.\n\nThe term first appears in print five months earlier when \"The New York Times\" reported on the Psychophysiological Aspects of Space Flight Symposium where Clynes and Kline first presented their paper.\n\nA book titled \"Cyborg: Digital Destiny and Human Possibility in the Age of the Wearable Computer\" was published by Doubleday in 2001. Some of the ideas in the book were incorporated into the 35 mm motion picture film \"Cyberman\".\n\nCyborg tissues structured with carbon nanotubes and plant or fungal cells have been used in artificial tissue engineering to produce new materials for mechanical and electrical uses.\nThe work was presented by Di Giacomo and Maresca at MRS 2013 Spring conference on Apr, 3rd, talk number SS4.04. The cyborg obtained is inexpensive, light and has unique mechanical properties. It can also be shaped in desired forms. Cells combined with MWCNTs co-precipitated as a specific aggregate of cells and nanotubes that formed a viscous material. Likewise, dried cells still acted as a stable matrix for the MWCNT network. When observed by optical microscopy the material resembled an artificial \"tissue\" composed of highly packed cells. The effect of cell drying is manifested by their \"ghost cell\" appearance. A rather specific physical interaction between MWCNTs and cells was observed by electron microscopy suggesting that the cell wall (the most outer part of fungal and plant cells) may play a major active role in establishing a CNTs network and its stabilization. This novel material can be used in a wide range of electronic applications from heating to sensing and has the potential to open important new avenues to be exploited in electromagnetic shielding for radio frequency electronics and aerospace technology. In particular using Candida albicans cells cyborg tissue materials with temperature sensing properties have been reported.\n\nIn current prosthetic applications, the C-Leg system developed by Otto Bock HealthCare is used to replace a human leg that has been amputated because of injury or illness. The use of sensors in the artificial C-Leg aids in walking significantly by attempting to replicate the user's natural gait, as it would be prior to amputation. Prostheses like the C-Leg and the more advanced iLimb are considered by some to be the first real steps towards the next generation of real-world cyborg applications. Additionally cochlear implants and magnetic implants which provide people with a sense that they would not otherwise have had can additionally be thought of as creating cyborgs.\n\nIn vision science, direct brain implants have been used to treat non-congenital (acquired) blindness. One of the first scientists to come up with a working brain interface to restore sight was private researcher William Dobelle.\nDobelle's first prototype was implanted into \"Jerry\", a man blinded in adulthood, in 1978. A single-array BCI containing 68 electrodes was implanted onto Jerry's visual cortex and succeeded in producing phosphenes, the sensation of seeing light. The system included cameras mounted on glasses to send signals to the implant. Initially, the implant allowed Jerry to see shades of grey in a limited field of vision at a low frame-rate. This also required him to be hooked up to a two-ton mainframe, but shrinking electronics and faster computers made his artificial eye more portable and now enable him to perform simple tasks unassisted.\n\nIn 1997, Philip Kennedy, a scientist and physician, created the world's first human cyborg from Johnny Ray, a Vietnam veteran who suffered a stroke. Ray's body, as doctors called it, was \"locked in\". Ray wanted his old life back so he agreed to Kennedy's experiment. Kennedy embedded an implant he designed (and named \"neurotrophic electrode\") near the part of Ray's brain so that Ray would be able to have some movement back in his body. The surgery went successfully, but in 2002, Johnny Ray died.\n\nIn 2002, Canadian Jens Naumann, also blinded in adulthood, became the first in a series of 16 paying patients to receive Dobelle's second generation implant, marking one of the earliest commercial uses of BCIs. The second generation device used a more sophisticated implant enabling better mapping of phosphenes into coherent vision. Phosphenes are spread out across the visual field in what researchers call the starry-night effect. Immediately after his implant, Naumann was able to use his imperfectly restored vision to drive slowly around the parking area of the research institute.\n\nIn contrast to replacement technologies, in 2002, under the heading Project Cyborg, a British scientist, Kevin Warwick, had an array of 100 electrodes fired into his nervous system in order to link his nervous system into the internet to investigate enhancement possibilities. With this in place Warwick successfully carried out a series of experiments including extending his nervous system over the internet to control a robotic hand, also receiving feedback from the fingertips in order to control the hand's grip. This was a form of extended sensory input. Subsequently, he investigated ultrasonic input in order to remotely detect the distance to objects. Finally, with electrodes also implanted into his wife's nervous system, they conducted the first direct electronic communication experiment between the nervous systems of two humans.<ref name=\"doi10.1001/archneur.60.10.1369|noedit\"></ref>\n\nSince 2004, British artist Neil Harbisson, has had a cyborg antenna implanted in his head that allows him to extend his perception of colors beyond the human visual spectrum through vibrations in his skull. His antenna was included within his 2004 passport photograph which has been claimed to confirm his cyborg status. In 2012 at TEDGlobal, Harbisson explained that he started to feel cyborg when he noticed that the software and his brain had united and given him an extra sense. Neil Harbisson is the first person to be officially recognized as a cyborg by a government; he is a and co-founder of the Cyborg Foundation (2004)\n\nFurthermore many cyborgs with multifunctional microchips injected into their hand are known to exist. With the chips they are able swipe cards, open or unlock doors, operate devices such as printers or, with some using a cryptocurrency, buy products, such as drinks, with a wave of the hand.\n\nbodyNET is an application of human-electronic interaction currently in development by researchers from Stanford University. The technology is based on stretchable semiconductor materials (Elastronic). According to their article in Nature (journal), the technology is composed of smart devices, screens, and a network of sensors that can be implanted into the body, woven into the skin or worn as clothes. It has been suggested, that this platform can potentially replace the smartphone in the future.\n\nThe US-based company Backyard Brains released what they refer to as \"The world's first commercially available cyborg\" called the RoboRoach. The project started as a University of Michigan biomedical engineering student senior design project in 2010 and was launched as an available beta product on 25 February 2011. The RoboRoach was officially released into production via a TED talk at the TED Global conference, and via the crowdsourcing website Kickstarter in 2013, the kit allows students to use microstimulation to momentarily control the movements of a walking cockroach (left and right) using a bluetooth-enabled smartphone as the controller. Other groups have developed cyborg insects, including researchers at North Carolina State University, UC Berkeley, and Nanyang Technological University, Singapore, but the RoboRoach was the first kit available to the general public and was funded by the National Institute of Mental Health as a device to serve as a teaching aid to promote an interest in neuroscience. Several animal welfare organizations including the RSPCA and PETA have expressed concerns about the ethics and welfare of animals in this project.\n\nIn medicine, there are two important and different types of cyborgs: the restorative and the enhanced. Restorative technologies \"restore lost function, organs, and limbs\". The key aspect of restorative cyborgization is the repair of broken or missing processes to revert to a healthy or average level of function. There is no enhancement to the original faculties and processes that were lost.\n\nOn the contrary, the enhanced cyborg \"follows a principle, and it is the principle of optimal performance: maximising output (the information or modifications obtained) and minimising input (the energy expended in the process)\". Thus, the enhanced cyborg intends to exceed normal processes or even gain new functions that were not originally present.\n\nAlthough prostheses in general supplement lost or damaged body parts with the integration of a mechanical artifice, bionic implants in medicine allow model organs or body parts to mimic the original function more closely. Michael Chorost wrote a memoir of his experience with cochlear implants, or bionic ear, titled \"Rebuilt: How Becoming Part Computer Made Me More Human.\" Jesse Sullivan became one of the first people to operate a fully robotic limb through a nerve-muscle graft, enabling him a complex range of motions beyond that of previous prosthetics. By 2004, a fully functioning artificial heart was developed. The continued technological development of bionic and nanotechnologies begins to raise the question of enhancement, and of the future possibilities for cyborgs which surpass the original functionality of the biological model. The ethics and desirability of \"enhancement prosthetics\" have been debated; their proponents include the transhumanist movement, with its belief that new technologies can assist the human race in developing beyond its present, normative limitations such as aging and disease, as well as other, more general incapacities, such as limitations on speed, strength, endurance, and intelligence. Opponents of the concept describe what they believe to be biases which propel the development and acceptance of such technologies; namely, a bias towards functionality and efficiency that may compel assent to a view of human people which de-emphasizes as defining characteristics actual manifestations of humanity and personhood, in favor of definition in terms of upgrades, versions, and utility.\n\nA brain-computer interface, or BCI, provides a direct path of communication from the brain to an external device, effectively creating a cyborg. Research of Invasive BCIs, which utilize electrodes implanted directly into the grey matter of the brain, has focused on restoring damaged eyesight in the blind and providing functionality to paralyzed people, most notably those with severe cases, such as Locked-In syndrome. This technology could enable people who are missing a limb or are in a wheelchair the power to control the devices that aide them through neural signals sent from the brain implants directly to computers or the devices. It is possible that this technology will also eventually be used with healthy people.\n\nDeep brain stimulation is a neurological surgical procedure used for therapeutic purposes. This process has aided in treating patients diagnosed with Parkinson's disease, Alzheimer's disease, Tourette syndrome, epilepsy, chronic headaches, and mental disorders. After the patient is unconscious, through anesthesia, brain pacemakers or electrodes, are implanted into the region of the brain where the cause of the disease is present. The region of the brain is then stimulated by bursts of electric current to disrupt the oncoming surge of seizures. Like all invasive procedures, deep brain stimulation may put the patient at a higher risk. However, there have been more improvements in recent years with deep brain stimulation than any available drug treatment.\n\nRetinal implants are another form of cyborgization in medicine. The theory behind retinal stimulation to restore vision to people suffering from retinitis pigmentosa and vision loss due to aging (conditions in which people have an abnormally low number of ganglion cells) is that the retinal implant and electrical stimulation would act as a substitute for the missing ganglion cells (cells which connect the eye to the brain.)\n\nWhile work to perfect this technology is still being done, there have already been major advances in the use of electronic stimulation of the retina to allow the eye to sense patterns of light. A specialized camera is worn by the subject, such as on the frames of their glasses, which converts the image into a pattern of electrical stimulation. A chip located in the user's eye would then electrically stimulate the retina with this pattern by exciting certain nerve endings which transmit the image to the optic centers of the brain and the image would then appear to the user. If technological advances proceed as planned this technology may be used by thousands of blind people and restore vision to most of them.\n\nA similar process has been created to aide people who have lost their vocal cords. This experimental device would do away with previously used robotic sounding voice simulators. The transmission of sound would start with a surgery to redirect the nerve that controls the voice and sound production to a muscle in the neck, where a nearby sensor would be able to pick up its electrical signals. The signals would then move to a processor which would control the timing and pitch of a voice simulator. That simulator would then vibrate producing a multitonal sound which could be shaped into words by the mouth.\n\nAn article published in \"Nature Materials\" in 2012 reported a research on \"cyborg tissues\" (engineered human tissues with embedded three-dimensional mesh of nanoscale wires), with possible medical implications.\n\nIn 2014, researchers from the University of Illinois at Urbana-Champaign and Washington University in St. Louis had developed a device that could keep a heart beating endlessly. By using 3D printing and computer modeling these scientist developed an electronic membrane that could successfully replace pacemakers. The device utilizes a \"spider-web like network of sensors and electrodes\" to monitor and maintain a normal heart-rate with electrical stimuli. Unlike traditional pacemakers that are similar from patient to patient, the elastic heart glove is made custom by using high-resolution imaging technology. The first prototype was created to fit a rabbit's heart, operating the organ in an oxygen and nutrient-rich solution. The stretchable material and circuits of the apparatus were first constructed by Professor John A. Rogers in which the electrodes are arranged in a s-shape design to allow them to expand and bend without breaking. Although the device is only currently used as a research tool to study changes in heart rate, in the future the membrane may serve as a safeguard from heart attacks.\n\nMilitary organizations' research has recently focused on the utilization of cyborg animals for the purposes of a supposed tactical advantage. DARPA has announced its interest in developing \"cyborg insects\" to transmit data from sensors implanted into the insect during the pupal stage. The insect's motion would be controlled from a Micro-Electro-Mechanical System (MEMS) and could conceivably survey an environment or detect explosives and gas. Similarly, DARPA is developing a neural implant to remotely control the movement of sharks. The shark's unique senses would then be exploited to provide data feedback in relation to enemy ship movement or underwater explosives.\n\nIn 2006, researchers at Cornell University invented a new surgical procedure to implant artificial structures into insects during their metamorphic development. The first insect cyborgs, moths with integrated electronics in their thorax, were demonstrated by the same researchers. The initial success of the techniques has resulted in increased research and the creation of a program called Hybrid-Insect-MEMS, HI-MEMS. Its goal, according to DARPA's Microsystems Technology Office, is to develop \"tightly coupled machine-insect interfaces by placing micro-mechanical systems inside the insects during the early stages of metamorphosis\".\n\nThe use of neural implants has recently been attempted, with success, on cockroaches. Surgically applied electrodes were put on the insect, which were remotely controlled by a human. The results, although sometimes different, basically showed that the cockroach could be controlled by the impulses it received through the electrodes. DARPA is now funding this research because of its obvious beneficial applications to the military and other areas\n\nIn 2009 at the Institute of Electrical and Electronics Engineers (IEEE) Micro-electronic mechanical systems (MEMS) conference in Italy, researchers demonstrated the first \"wireless\" flying-beetle cyborg. Engineers at the University of California at Berkeley have pioneered the design of a \"remote controlled beetle\", funded by the DARPA HI-MEMS Program. Filmed evidence of this can be viewed here. This was followed later that year by the demonstration of wireless control of a \"lift-assisted\" moth-cyborg.\n\nEventually researchers plan to develop HI-MEMS for dragonflies, bees, rats and pigeons. For the HI-MEMS cybernetic bug to be considered a success, it must fly from a starting point, guided via computer into a controlled landing within of a specific end point. Once landed, the cybernetic bug must remain in place.\n\nIn 2016 the first cyborg Olympics were celebrated in Zurich Switzerland. Cybathlon 2016 were the first Olympics for cyborgs and the first worldwide and official celebration of cyborg sports. In this event, 16 teams of people with disabilities used technological developments to turn themselves into cyborg athletes. There were six different events and its competitors used and controlled advanced technologies such as powered prosthetic legs and arms, robotic exoskeletons, bikes and motorized wheelchairs.\n\nIf on one hand this was already a remarkable improvement, as it allowed disabled people to compete and showed the several technological enhancements that are already making a difference, on the other hand it showed that there is still a long way to go. For instance, the exoskeleton race still required its participants to stand up from a chair and sit down, navigate a slalom and other simple activities such as walk over stepping stones and climb up and down stairs. Despite the simplicity of these activities, 8 of the 16 teams that participated in the event drop of before the start.\n\nNonetheless, one of the main goals of this event and such simple activities is to show how technological enhancements and advanced prosthetic can make a difference in peoples' lives. The next Cybathlon is expected to occur in 2020\n\nThe concept of the cyborg is often associated with science fiction. However, many artists have tried to create public awareness of cybernetic organisms; these can range from paintings to installations. Some artists who create such works are Neil Harbisson, Moon Ribas, Patricia Piccinini, Steve Mann, Orlan, H. R. Giger, Lee Bul, Wafaa Bilal, Tim Hawkinson and Stelarc.\n\nStelarc is a performance artist who has visually probed and acoustically amplified his body. He uses medical instruments, prosthetics, robotics, virtual reality systems, the Internet and biotechnology to explore alternate, intimate and involuntary interfaces with the body. He has made three films of the inside of his body and has performed with a third hand and a virtual arm. Between 1976–1988 he completed 25 body suspension performances with hooks into the skin. For 'Third Ear' he surgically constructed an extra ear within his arm that was internet enabled, making it a publicly accessible acoustical organ for people in other places. He is presently performing as his avatar from his second life site.\n\nTim Hawkinson promotes the idea that bodies and machines are coming together as one, where human features are combined with technology to create the Cyborg. Hawkinson's piece \"Emoter\" presented how society is now dependent on technology.\n\nWafaa Bilal is an Iraqi-American performance artist who had a small 10 megapixel digital camera surgically implanted into the back of his head, part of a project entitled 3rd I. For one year, beginning 15 December 2010, an image is captured once per minute 24 hours a day and streamed live to and the Mathaf: Arab Museum of Modern Art. The site also displays Bilal's location via GPS. Bilal says that the reason why he put the camera in the back of the head was to make an \"allegorical statement about the things we don't see and leave behind.\" As a professor at NYU, this project has raised privacy issues, and so Bilal has been asked to ensure that his camera does not take photographs in NYU buildings.\n\nMachines are becoming more ubiquitous in the artistic process itself, with computerized drawing pads replacing pen and paper, and drum machines becoming nearly as popular as human drummers. This is perhaps most notable in generative art and music. Composers such as Brian Eno have developed and utilized software which can build entire musical scores from a few basic mathematical parameters.\n\nScott Draves is a generative artist whose work is explicitly described as a \"cyborg mind\". His Electric Sheep project generates abstract art by combining the work of many computers and people over the internet.\n\nArtists have explored the term cyborg from a perspective involving imagination. Some work to make an abstract idea of technological and human-bodily union apparent to reality in an art form utilizing varying mediums, from sculptures and drawings to digital renderings.\nArtists that seek to make cyborg-based fantasies a reality often call themselves cyborg artists, or may consider their artwork \"cyborg\". How an artist or their work may be considered cyborg will vary depending upon the interpreter's flexibility with the term.\nScholars that rely upon a strict, technical description of cyborg, often going by Norbert Wiener's cybernetic theory and Manfred E. Clynes and Nathan S. Kline's first use of the term, would likely argue that most cyborg artists do not qualify to be considered cyborgs. Scholars considering a more flexible description of cyborgs may argue it incorporates more than cybernetics. Others may speak of defining subcategories, or specialized cyborg types, that qualify different levels of cyborg at which technology influences an individual. This may range from technological instruments being external, temporary, and removable to being fully integrated and permanent. Nonetheless, cyborg artists are artists. Being so, it can be expected for them to incorporate the cyborg idea rather than a strict, technical representation of the term, seeing how their work will sometimes revolve around other purposes outside of cyborgism.\n\nAs medical technology becomes more advanced, some techniques and innovations are adopted by the body modification community. While not yet cyborgs in the strict definition of Manfred Clynes and Nathan Kline, technological developments like implantable silicon silk electronics, augmented reality and QR codes are bridging the disconnect between technology and the body. Hypothetical technologies such as digital tattoo interfaces would blend body modification aesthetics with interactivity and functionality, bringing a transhumanist way of life into present day reality.\n\nIn addition, it is quite plausible for anxiety expression to manifest. Individuals may experience pre-implantation feelings of fear and nervousness. To this end, individuals may also embody feelings of uneasiness, particularly in a socialized setting, due to their post-operative, technologically augmented bodies, and mutual unfamiliarity with the mechanical insertion. Anxieties may be linked to notions of otherness or a cyborged identity.\n\nCyborgs have become a well-known part of science fiction literature and other media. Although many of these characters may be technically androids, they are often referred to as cyborgs. Well-known examples from film and television include RoboCop, The Terminator, Evangelion, United States Air Force Colonel Steve Austin in both \"Cyborg\" and, as acted out by Lee Majors, \"The Six Million Dollar Man,\" Replicants from \"Blade Runner\", Daleks and Cybermen from \"Doctor Who,\" the Borg from \"Star Trek,\" Darth Vader and General Grievous from \"Star Wars\", Inspector Gadget, and Cylons from the 2004 \"Battlestar Galactica\" series. From comics, manga and anime are characters such as 8 Man (the inspiration for \"RoboCop\"), Kamen Rider, \"Ghost in the Shell's\" Motoko Kusanagi, as well as characters from western comic books like Tony Stark (after his Extremis and Bleeding Edge armor) and Victor \"Cyborg\" Stone. The \"Deus Ex\" videogame series deals extensively with the near-future rise of cyborgs and their corporate ownership, as does the \"Syndicate\" series. William Gibson's Neuromancer features one of the first female cyborgs, a \"Razorgirl\" named Molly Millions, who has extensive cybernetic modifications and is one of the most prolific cyberpunk characters in the science fiction canon.\n\nSending humans to space is a dangerous task in which the implementation of various cyborg technologies could be used in the future for risk mitigation. Stephen Hawking, a renowned physicist, stated \"Life on Earth is at the ever-increasing risk of being wiped out by a disaster such as sudden global warming, nuclear war... I think the human race has no future if it doesn't go into space.\" The difficulties associated with space travel could mean it might be centuries before humans ever become a multi-planet species. There are many effect of spaceflight on the human body. One major issue of space exploration is the biological need for oxygen. If this necessity was taken out of the equation, space exploration would be revolutionized. A theory proposed by Manfred E. Clynes and Nathan S. Kline is aimed at tackling this problem. The two scientists theorized that the use of an inverse fuel cell that is \"capable of reducing CO2 to its components with removal of the carbon and re-circulation of the oxygen...\" could make breathing unnecessary. Another prominent issue is radiation exposure. Yearly, the average human on earth is exposed to approximately 0.30 rem of radiation, while an astronaut aboard the International Space Station for 90 days is exposed to 9 rem. To tackle the issue, Clynes and Kline theorized a cyborg containing a sensor that would detect radiation levels and a Rose osmotic pump \"which would automatically inject protective pharmaceuticals in appropriate doses.\" Experiments injecting these protective pharmaceuticals into monkeys have shown positive results in increasing radiation resistance.\n\nAlthough the effects of spaceflight on our body is an important issue, the advancement of propulsion technology is just as important. With our current technology, it would take us about 260 days to get to Mars. A study backed by NASA proposes an interesting way to tackle this issue through deep sleep, or torpor. With this technique, it would \"reduce astronauts' metabolic functions with existing medical procedures\". So far experiments have only resulted in patients being in torpor state for one week. Advancements to allow for longer states of deep sleep would lower the cost of the trip to mars as a result of reduced astronaut resource consumption.\n\nTheorists such as Andy Clark suggest that interactions between humans and technology result in the creation of a cyborg system. In this model \"cyborg\" is defined as a part biological, part mechanical system which results in the augmentation of the biological component and the creation of a more complex whole. Clark argues that this broadened definition is necessary to an understanding of human cognition. He suggests that any tool which is used to offload part of a cognitive process may be considered the mechanical component of a cyborg system. Examples of this human and technology cyborg system can be very low tech and simplistic, such as using a calculator to perform basic mathematical operations or pen and paper to make notes, or as high tech as using a personal computer or phone. According to Clark, these interactions between a person and a form of technology integrate that technology into the cognitive process in a way which is analogous to the way that a technology which would fit the traditional concept a cyborg augmentation becomes integrated with its biological host. Because all humans in some way use technology to augment their cognitive processes, Clark comes to the conclusion that we are \"natural-born cyborgs\".\n\nIn 2010, the Cyborg Foundation became the world's first international organization dedicated to help humans become cyborgs. The foundation was created by cyborg Neil Harbisson and Moon Ribas as a response to the growing number of letters and emails received from people around the world interested in becoming a cyborg. The foundation's main aims are to extend human senses and abilities by creating and applying cybernetic extensions to the body, to promote the use of cybernetics in cultural events and to defend cyborg rights. In 2010, the foundation, based in Mataró (Barcelona), was the overall winner of the Cre@tic Awards, organized by Tecnocampus Mataró.\n\nIn 2012, Spanish film director Rafel Duran Torrent, created a short film about the Cyborg Foundation. In 2013, the film won the Grand Jury Prize at the Sundance Film Festival's Focus Forward Filmmakers Competition and was awarded with $100,000 USD.\n\nGiven the technical scope of current and future implantable sensory/telemetric devices, these devices will be greatly proliferated, and will have connections to commercial, medical, and governmental networks. For example, in the medical sector, patients will be able to login to their home computer, and thus visit virtual doctor’s offices, medical databases, and receive medical prognoses from the comfort of their own home from the data collected through their implanted telemetric devices. However, this online network presents huge security concerns because it has been proven by several U.S. universities that hackers could get onto these networks and shut down peoples’ electronic prosthetics. These sorts of technologies are already present in the U.S. workforce as a firm in River Falls, Wisconsin called Three Square Market partnered with a Swedish firm called Biohacks Technology to implant RFID microchips in the hands of its employees (which are about the size of a grain of rice) that allow employees to access offices, computers, and even vending machines. More than 50 of the firms 85 employees were chipped. It was confirmed that the U.S. Food and Drug Administration approved of these implantations . If these devices are to be proliferated within society, then the question that begs to be answered is what regulatory agency will oversee the operations, monitoring, and security of these devices? According to this case study of Three Square Market, it seems that the FDA is assuming the role in regulating and monitoring these devices.\n\n\n",
    "id": "20756967",
    "title": "Cyborg"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50485535",
    "text": "Mind-controlled wheelchair\n\nA mind-controlled wheelchair is a mind-machine interfacing device that uses thought (neural impulses) to command the motorised wheelchair's motion. The first such device to reach production was designed by Diwakar Vaish, Head of Robotics and Research at A-SET Training & Research Institutes. The wheelchair is of great importance to patients suffering from locked-in syndrome (LIS), in which a patient is aware but cannot move or communicate verbally due to complete paralysis of nearly all voluntary muscles in the body except the eyes. Such wheelchairs can also be used in case of muscular dystrophy, a disease that weakens the musculoskeletal system and hampers locomotion (walking or moving).\n\nA mind-controlled wheelchair functions using a brain–computer interface: an electroencephalogram (EEG) worn on the user's forehead detects neural impulses that reach the scalp allowing the micro-controller on board to detect the user's thought process, interpret it, and control the wheelchair's movement.\n\nThe A-SET wheelchair comes standard with many different types of sensors, like temperature sensors, sound sensors and an array of distance sensors which detect any unevenness in the surface. The chair automatically avoids stairs and steep inclines. It also has a \"safety switch\": in case of danger, the user can close his eyes quickly to trigger an emergency stop.\n\n",
    "id": "50485535",
    "title": "Mind-controlled wheelchair"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39129580",
    "text": "Vaimos\n\nVaimos (Voilier Autonome Instrumenté pour Mesures Océanographiques de Surface) is an autonomous sailing boat with embedded instrumentation for ocean surface measurements.\n\nIts goal is to collect measurements at the surface of the ocean. This robot is the result of a collaboration between ENSTA Bretagne and IFREMER. ENSTA-Bretagne (OSM Team) develops control algorithms and the software architecture, IFREMER (LPO+RDT) builds the mechanics, the embedded instrumentation.\n\nBrest-Douarnenez. One of the longest trips the robot has done is Brest-Douarnenez where Vaimos has done more than 100 km in an autonomous mode. Since Vaimos has made trips more than 350 km long.\n\nWRSC. In 2013, Vaimos participated to the World Robotic Sailing Championship WRSC 2013 in Brest, France \n\nThe robot follows a desired trajectory which is a sequence of lines. When following a line, the robot has two modes. \n\nTwo nested control loops are implemented in the computer of Vaimos. The first loop corresponds to a low level controller which tunes the rudder and the sail provided in order to have a desired heading. The second loop generates the desired heading to make the line to follow attractive. With this two loop control strategy, we have the guarantee that the robot will always stay inside the required corridor. This is illustrated the spiral experiment\nwhere the robot has to follow a square spiral\n\n. The proof that the robot will always stay inside its corridor can be performed using interval analysis and set inversion\n\n. The proof assumes that the robot obeys to some uncertain state equations which is not always the case in practice.\n",
    "id": "39129580",
    "title": "Vaimos"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50647426",
    "text": "Soft robotics\n\nSoft Robotics is the specific subfield of robotics dealing with constructing robots from highly compliant materials, similar to those found in living organisms.\n\nSoft robotics draws heavily from the way in which living organisms move and adapt to their surroundings. In contrast to robots built from rigid materials, soft robots allow for increased flexibility and adaptability for accomplishing tasks, as well as improved safety when working around humans. These characteristics allow for its potential use in the fields of medicine and manufacturing.\n\nThe bulk of the field of soft robotics is based upon the design and construction of robots made completely from compliant materials, with the end result being similar to invertebrates like worms and octopi. The motion of these robots is difficult to model, as continuum mechanics apply to them, and they are sometimes referred to as continuum robots. Soft Robotics is the specific sub-field of robotics dealing with constructing robots from highly compliant materials, similar to those found in living organisms. Similarly, soft robotics also draws heavily from the way in which these living organisms move and adapt to their surroundings. This allows scientists to use soft robots to understand biological phenomena using experiments that cannot be easily performed on the original biological counterparts. In contrast to robots built from rigid materials, soft robots allow for increased flexibility and adaptability for accomplishing tasks, as well as improved safety when working around humans. These characteristics allow for its potential use in the fields of medicine and manufacturing. However, there exist rigid robots that are also capable of continuum deformations, most notably the snake-arm robot.\n\nAlso, certain soft robotic mechanics may be used as a piece in a larger, potentially rigid robot. Soft robotic end effectors exist for grabbing and manipulating objects, and they have the advantage of producing a low force that is good for holding delicate objects without breaking them.\n\nIn addition, hybrid soft-rigid robots may be built using an internal rigid framework with soft exteriors for safety. The soft exterior may be multifunctional, as it can act as both the actuators for the robot, similar to muscles in vertebrates, and as padding in case of a collision with a person.\n\nPlant cells can inherently produce hydrostatic pressure due to a solute concentration gradient between the cytoplasm and external surroundings (osmotic potential). Further, plants can adjust this concentration through the movement of ions across the cell membrane. This then changes the shape and volume of the plant as it responds to this change in hydrostatic pressure. This pressure derived shape evolution is desirable for soft robotics and can be emulated to create pressure adaptive materials through the use of fluid flow. The following equation models the cell volume change rate:\n\nThis principle has been leveraged in the creation of pressure systems for soft robotics. These systems are composed of soft resins and contain multiple fluid sacs with semi-permeable membranes. The semi-permeability allows for fluid transport that then leads to pressure generation. This combination of fluid transport and pressure generation then leads to shape and volume change.\nAnother biologically inherent shape changing mechanism is that of hygroscopic shape change. In this mechanism, plant cells react to changes in humidity. When the surrounding atmosphere has a high humidity, the plant cells swell, but when the surrounding atmosphere has a low humidity, the plant cells shrink. This volume change has been observed in pollen grains and pine cone scales.\n\nConventional manufacturing techniques, such as subtractive techniques like drilling and milling, are unhelpful when it comes to constructing soft robots as these robots have complex shapes with deformable bodies. Therefore, more advanced manufacturing techniques have been developed. Those include Shape Deposition Manufacturing (SDM), the Smart Composite Microstructure (SCM) process, and 3D multimaterial printing.\n\nSDM is a type of rapid prototyping whereby deposition and machining occur cyclically. Essentially, one deposits a material, machines it, embeds a desired structure, deposits a support for said structure, and then further machines the product to a final shape that includes the deposited material and the embedded part. Embedded hardware includes circuits, sensors, and actuators, and scientists have successfully embedded controls inside of polymeric materials to create soft robots, such as the Stickybot and the iSprawl.\n\nSCM is a process whereby one combines rigid bodies of carbon fiber reinforced polymer (CFRP) with flexible polymer ligaments. The flexible polymer act as joints for the skeleton. With this process, an integrated structure of the CFRP and polymer ligaments is created through the use of laser machining followed by lamination. This SCM process is utilized in the production of mesoscale robots as the polymer connectors serve as low friction alternatives to pin joints.\n\n3D printing can now produce shape morphing materials whose shape is photosensitive, thermally activated, or water responsive. Essentially, these polymers can automatically change shape upon interaction with water, light, or heat. One such example of a shape morphing material was created through the use of light reactive ink-jet printing onto a polystyrene target. Additionally, shape memory polymers have been rapid prototyped that comprise two different components: a skeleton and a hinge material. Upon printing, the material is heated to a temperature higher than the glass transition temperature of the hinge material. This allows for deformation of the hinge material, while not affecting the skeleton material. Further, this polymer can be continually reformed through heating.\n\nAll soft robots require some system to generate reaction forces, to allow the robot to move in and interact with its environment. Due to the compliant nature of these robots, this system must be able to move the robot without the use of rigid materials to act as the bones in organisms, or the metal frame in rigid robots. However, several solutions to this engineering problem exist and have found use, each possessing advantages and disadvantages.\n\nOne of these systems uses Dielectric Elastomeric Actuators (DEAs), materials that change shape through the application of a high-voltage electric field. These materials can produce high forces, and have high specific power (W/kg). However, these materials are best suited for applications in rigids robots, as they become inefficient when they do not act upon a rigid skeleton. Additionally, the high-voltages required can become a limiting factor in the potential practical applications for these robots.\n\nAnother system uses springs made of shape-memory alloy. Although made of metal, a traditionally rigid material, the springs are made from very thin wires and are just as compliant as other soft materials. These springs have a very high force-to-mass ratio, but stretch through the application of heat, which is inefficient energy-wise.\n\nPneumatic artificial muscles are yet another method used for controlling soft robots. By changing the pressure inside a flexible tube, it will act as a muscle, contracting and extending, and applying force to what it’s attached to. Through the use of valves, the robot may maintain a given shape using these muscles with no additional energy input. However, this method generally requires an external source of compressed air to function.\n\nSoft robots can be implemented in the medical profession, specifically for invasive surgery. Soft robots can be made to assist surgeries due to their shape changing properties. Shape change is important as a soft robot could navigate around different structures in the human body by adjusting its form. This could be accomplished through the use of fluidic actuation.\nSoft robots may also be used for the creation of flexible exosuits, for rehabilitation of patients, assisting the elderly, or simply enhancing the user’s strength. A team from Harvard created an exosuit using these materials in order to give the advantages of the additional strength provided by an exosuit, without the disadvantages that come with how rigid materials restrict a person’s natural movement.\n\nTraditionally, manufacturing robots have been isolated from human workers due to safety concerns, as a rigid robot colliding with a human could easily lead to injury due to the fast-paced motion of the robot. However, soft robots could work alongside humans safely, as in a collision the compliant nature of the robot would prevent or minimize any potential injury.\n\nThere is a company, Soft Robotics Inc., www.softroboticsinc.com, in Cambridge, MA that has commercialized soft robotics systems for industrial and collaborative robotics applications. These systems are in use in food packaging, consumer goods manufacturing, and retail logistics applications.\n\n\n\nThe 2014 Disney film Big Hero 6 revolved around a soft robot, Baymax, originally designed for use in the healthcare industry. In the film, Baymax is portrayed as a large yet unintimidating robot with an inflated vinyl exterior surrounding a mechanical skeleton. The basis of the Baymax concept come from real life research on applications of soft robotics in the healthcare field, such as roboticist Chris Atkeson's work at Carnegie Mellon's Robotics Institute.\n\n",
    "id": "50647426",
    "title": "Soft robotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50393660",
    "text": "Robotic prosthesis control\n\nRobotic prosthesis control is a method for controlling a prosthesis in such a way that the controlled robotic prosthesis restores a biologically accurate gait to a person with a loss of limb. This is special branch of control that has an emphasis on the interaction between humans and robotics.\n\nBack in the 1970 several researchers developed a tethered electrohydraulic transfemoral prosthesis. It only included a hydraulically actuated knee joint controlled by off-board electronics using a type of control called echo control. Echo control tries to take the kinematics from the sound leg and control the prosthetic leg to match the intact leg when it reaches that part of the gait cycle. In 1988 a battery-powered active knee joint powered by DC motors and controlled by a robust position tracking control algorithm was created by Popovic and Schwirtlich. Tracking control is a common method of control used to force a particular state, such as position, velocity, or torque, to track a particular trajectory. These are just two examples of previous work that has been done in this field.\n\nThis form of control is an approach used to control the dynamic interactions between the environment and a manipulator. This works by treating the environment as an admittance and the manipulator as the impedance. The relationship this imposes for robotic prosthesis the relationship in between force production in response to the motion imposed by the environment. This translates into the torque required at each joint during a single stride, represented as a series of passive impedance functions piece wise connected over a gait cycle. Impedance control doesn't regulate force or position independently, instead it regulates the relationship between force and position and velocity. To Design an impedance controller, a regression analysis of gait data is used to parameterize an impedance function. For lower limb prosthesis the impedance function looks similar to the following equation.\n\nformula_1The terms k (spring stiffness), θ (equilibrium angle), and b (dampening coefficient) are all parameters found through regression and tuned for different parts of the gait cycle and for a specific speed. This relationship is then programmed into a micro controller to determine the required torque at different parts of the walking phase.\n\nElectromyography (EMG) is a technique used for evaluating and recording the electrical activity produced by skeletal muscles. Advanced pattern recognition algorithms can take these recordings and decode the unique EMG signal patterns generated by muscles during specific movements. The patterns can be used to determine the intent of the user and provide control for a prosthetic limb. For lower limb robotic prosthesis it is important to be able to determine if the user wants to walk on level ground, up a slope, or up stairs. Currently this is where myoelectric control comes intro play. During transitions between these different modes of operation EMG signal becomes highly variable and can be used to compliment information from mechanical sensors to determine the intended mode of operation. Each patient that uses a robotic prosthesis that is tuned for this type of control has to have there system trained for them specifically. This is done by have them go through the different modes of operation and using that data to train there pattern recognition algorithm.\n\nThe speed-adaption mechanism is a mechanism used to determine the required torque from the joints at different moving speeds. During the stance phase it has been seen that quasistiffness, which is the derivative of the torque angle relationship with respect the angle, changes constantly as a function of walking speed. This means that over the stance phase, depending on the speed the subject is moving, there is a derivable torque angle relationship that can be used to control a lower limb prosthesis. During the swing phase joint torque increases proportionally to walking speed and the duration of the swing phase decreases proportionally to the stride time. These properties allow for trajectories to be derived that can be controlled around that accurately describe the angle trajectory over the swing phase. Because these two mechanism remain constant from person to person this method removes the speed and patient specific tuning required by most lower limb prosthetic controllers.\n\nWalking gait is classified as hybrid system, meaning that it has split dynamics. With this unique problem, a set of solutions to hybrid systems that undergo impacts was developed called Rapid Exponentially Stabilizing Control Lyapunov Functions(RES-CLF). Control Lyapunov function are used to stabilize a nonlinear system to a desired set of states. RES-CLFs can be realized using quadratic programs that take in several inequality constraints and return an optimal output. One problem with these are that they require a model of the system to develop the RES-CLFs. To remove the need of tuning to specific individuals Model Independent Quadratic Programs (MIQP) were used to derive CLFs. These CLFs are only focused on reducing the error in the desired output without any knowledge of what the desired torque should be. To provide this information an impedance control is added to provide a feed forward term that allows the MIQP to gather information about the system it is controlling without having a full model of the system.\n\nThis section is currently being left blank with the intent that other users will fill in this information.\n",
    "id": "50393660",
    "title": "Robotic prosthesis control"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50493193",
    "text": "Robotic non-destructive testing\n\nRobotic non-destructive testing (NDT) is a method of inspection used to assess the structural integrity of petroleum, natural gas, and water installations. Crawler-based robotic tools are commonly used for in-line inspection (ILI) applications in pipelines that cannot be inspected using traditional intelligent pigging tools (or unpiggable pipelines).\n\nRobotic NDT tools can also be used for mandatory inspections in inhospitable areas (e.g., tank interiors, subsea petroleum installations) to minimize danger to human inspectors, as these tools are operated remotely by a trained technician or NDT analyst. These systems transmit data and commands via either a wire (typically called an umbilical cable or tether) or wirelessly (in the case of battery-powered tetherless crawlers).\n\nRobotic NDT tools help pipeline operators and utility companies complete required structural integrity data sets for maintenance purposes in the following applications:\n\n\nPipeline conditions that may prevent or hinder a flow-driven pig inspection include:\n\n\nRobotic NDT tools also offer safety advantages in inhospitable areas:\n\n\nUsing robotics for infrastructure inspections can save the Department of Transportation millions in lane closures and heavy equipment rentals. By updating the 50year old methods currently in place for infrastructure inspections, the Department of Fransportation will get better results, allowing them to better allocate resources. \n\nPost-tension tendons that hold up large concrete structures worldwide (e.g., bridges) are still mostly inspected manually. Robotic inspection devices can peer through concrete and steel and take the guesswork out of post-tension tendon inspections. \n\nLightweight, portable without lane closures or heavy equipment can save money and prevent traffic interruptions and deadly accidents.\n\nReplacing the manual subjective inspection with robotics within the same budget is the key to fixing and maintaining a strong infrastructure. The information provided by robotic inspections will help extend the service life of valuable infrastructure assets, keep the public safe and save billions in untimely replacements.\n\nTethered robotic inspection tools have an umbilical cable attached to them, which provides power and control commands to the tool while relaying sensor data back to the technician. Tethered crawlers have the following advantages over untethered crawlers:\n\n\nTethered crawlers have the following disadvantages against untethered crawlers:\n\n\nUntethered robotic ILI crawlers are powered by onboard batteries; these tools transmit sensor data wirelessly to the tool operator or store the data for downloading upon tool retrieval. Untethered crawlers have the following advantages over tethered crawlers:\n\n\nUntethered crawlers have the following disadvantages against tethered crawlers:\n\n\nRobotic NDT tools employ suites of inspection sensors. This section describes common sensor types; most tools combine several types of sensor depending on factors such as robot size, design, and application.\n\nMain article – Electromagnetic acoustic transducers\n\nElectromagnetic acoustic transducers (EMAT) induce ultrasonic waves into uniformly-milled metal inspection objects (e.g., pipe walls, tank floors). Technicians can assess metal condition and detect anomalies based on the reflections of these waves – when the transducer passes over an anomaly, a new reflection appears between the initial pulse and the normal reflection.\n\nDirect beam EMAT, where the tool induces ultrasonic waves into the metal at a 0° angle (or perpendicular to the metal surface), is the most common inspection method. Direct beam inspections determine metal thickness as well as detect and measure the following defects:\n\n\nAngle beam inspections, where the tool induces ultrasonic waves into the metal at an angle relative to the metal surface, can be performed concurrently with direct beam inspections to confirm anomaly detections. An angle beam transducer only registers echoes from anomalies or reflectors that fall into the beam path; unlike direct beam, it does not receive reflections from the opposite wall of normal steel.\n\nThe combination of angle beam and direct beam methods may find additional anomalies and increase inspection accuracy. However, the angle beam method has a lower tolerance for surface debris than the direct beam method. Angle beam inspections discover crack-like anomalies parallel to the pipe axis and metal loss defects that are too small to detect via direct beam, including the following:\n\n\nBesides its uses in unpiggable pipelines, the non-contact nature of EMAT tools makes this method ideal for dry applications where liquid couplant requirements may make traditional UT tools undesirable (e.g., natural gas lines).\n\nWeld integrity is a crucial component of pipeline safety, especially girth welds (or the circumferential welds that join each section of pipe together). However, unlike the consistent molecular structure of milled steel, welds and their heat-affected zones (HAZs) have an anisotropic grain structure that attenuates ultrasonic signals and creates wave velocity variances that are difficult for ILI tools to analyze.\n\nOne angle-beam EMAT method employs a set of nine frequency-time (FT) scans on each side of the girth weld, where each frequency corresponds to a different input wave angle. The following figure shows a diagram of the inspection area covered by this method, where the green area represents the propagation of shear waves in the weld and surrounding metal.\n\nThe tool merges each set of FT scans into a single frequency-time matrix scan to display weld conditions, with anomalies color-coded by severity. This method of girth weld scanning is designed to detect the following weld defects:\n\n\nMain article – Magnetic flux leakage\n\nMagnetic flux leakage (MFL) tools use a sensor sandwiched between multiple powerful magnets to create and measure the flow of magnetic flux in the pipe wall. Structurally-sound steel has a uniform structure that allows regular flow of the magnetic flux, while anomalies and features interrupt the flow of flux in identifiable patterns; the sensor registers these flow interruptions and records them for later analysis. The following figure illustrates the principle of a typical MFL inspection tool; the left side of the diagram shows how an MFL tool works in structurally sound pipe, while the right side shows how the tool detects and measures a metal loss defect.\n\nMFL tools are used primarily to detect pitting corrosion, and some tool configurations can detect weld defects. One advantage of MFL tools over ultrasonic tools is the ability to maintain reasonable sensitivity through relatively thick surface coatings (e.g., paint, pipe liners).\n\nMain article – video inspection\n\nRobotic NDT tools employ cameras to provide technicians an optimal view of the inspection area. Some cameras provide specific views of the pipeline (e.g., straight forward, sensor contact area on the metal) to assist in controlling the tool, while other cameras are used to take high-resolution photographs of inspection findings.\n\nSome tools exist solely to perform video inspection; many of these tools include a mechanism to aim the camera to completely optimize technicians’ field of vision, and the lack of other bulky ILI sensor packages makes these tools exceptionally maneuverable. Cameras on multipurpose ILI tools are usually placed in locations that maximize technicians’ ability to analyze findings as well as optimally control the tool.\n\nMain article – surface metrology\n\nLaser profilometers project a shape onto the object surface. Technicians configure the laser (both angle of incidence and distance from the object) to ensure the shape is uniform on normal metal. Superficial anomalies (e.g., pitting corrosion, dents) distort the shape, allowing the inspection technicians to measure the anomalies using proprietary software programs. Photographs of these laser distortions provide visual evidence that improves the data analysis process and contributes to structural integrity efforts.\n\nMain article – Pulsed-eddy current\n\nPulsed-eddy current (PEC) tools use a probe coil to send a pulsed magnetic field into a metal object. The varying magnetic field induces eddy currents on the metal surface. The tool processes the detected eddy current signal and compares it to a reference signal set before the tool run; the material properties are eliminated to give a reading for the average wall thickness within the area covered by the magnetic field. The tool logs the signal for later analysis. The following diagram illustrates the principle of a typical PEC inspection tool.\n\nPEC tools can inspect accurately with a larger gap between the transducer and the inspection object than other tools, making it ideal for inspecting metal through non-metal substances (e.g., pipe coatings, insulation, marine growth).\n\nUnited States federal law requires baseline inspections to establish pipeline as-built statistics and subsequent periodic inspections to monitor asset deterioration. Pipeline operators also are responsible to designate high-consequence areas (HCAs) in all pipelines, perform regular assessments to monitor pipeline conditions, and develop preventive actions and response plans.\n\nState regulations for inspecting pipelines vary based on the level of public safety concerns. For example, a 2010 natural gas pipeline explosion in a San Bruno residential neighborhood led the California Public Utilities Commission to require safety enhancement plans from California natural gas transmission operators. The safety plan included numerous pipeline replacements and in-line inspections.\n\nThe federal Pipeline and Hazardous Materials Safety Administration (PHMSA) does not permit use of tetherless crawlers in HCAs due to the risk of getting stuck. Excavating buried pipelines to retrieve stuck tools beneath freeway crossings, river crossings or dense urban areas would impact the community infrastructure too greatly. Natural gas and oil pipeline operators therefore rely on tethered robotic ILI crawlers to inspect unpiggable pipelines.\n\nWilliams used a tethered robotic ILI crawler to inspect an unpiggable section of the Transco Pipeline in New Jersey in 2015. The pipeline system ran beneath the Hudson River; construction of a new condominium development nearby created a new HCA, requiring Williams to create an integrity management program per PHMSA regulations.\n\nAlyeska Pipeline Service Company inspected Pump Station 3 on the Trans-Alaska Pipeline System after an oil leak was discovered in an underground oil pipeline at Pump Station 1 in 2011. The spill resulted in a consent agreement between Alyeska and PHMSA requiring Alyeska to remove all liquid-transport piping from its system that could not be assessed using ILI tools or a similar suitable inspection technique. Because other ILI tools could not navigate the pipeline geometry common to each of the eleven pump stations along the pipeline, Alyeska received approval to use a tethered robotic ILI crawler manufactured by Diakont to complete an inspection project at Pump Station 3. This tool allowed Alyeska to only remove a few small aboveground fittings to permit crawler entry into the piping, saving the time and expense necessary to excavate hundreds of feet of pipe (some of which was also encased in concrete vaults) to inspect by hand.\n\nNuclear power plants in the United States are subject to unique integrity management mandates per the Nuclear Energy Institute (NEI) NEI 09-14, Guideline for the Management of Buried Piping Integrity.\n\n\nNatural gas pipeline operators can use tetherless robotic ILI crawlers for smaller distribution pipelines that are not located beneath critical infrastructure elements (e.g., freeway crossings).\n\n\nRobotic NDT tools have the following advantages over other NDT methods:\n\n\n\nRobotic tools have the following disadvantages against other NDT methods:\n\n\n\n",
    "id": "50493193",
    "title": "Robotic non-destructive testing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50780831",
    "text": "Prismizer\n\nPrismizer () is an audio effect using multiple different audio codecs that sounds similar to a vocoder in that it combines pitch and frequency characteristics from multiple sources (most frequently used on the human voice) but to a different, choral effect. It utilizes the Antares Harmony Engine plugin and recalls the sound of Auto-Tune. The name combines the two words, \"prism\" and \"harmonizer,\" in reference to the polyharmonic pitch, which provides saturation akin to the dispersion of light through a prism. \n\nWhile the Antares Harmony Engine plugin has been used in many albums, the 'Prismizer,' which references a specific configuration, debuted on the Chance the Rapper mixtape \"Coloring Book.\" He has stated that it will appear on the new Kanye West album, \"Turbo Grafx 16.\" It can be heard on Frank Ocean's sophomore album, \"Blonde,\" the Bon Iver album \"22, A Million,\" and \"Farewell, Starlite!,\" the debut album of its inventor, Francis Starlite of Francis and the Lights.\n\nThe live instrument version of the Prismizer was developed by Chris Messina, studio manager for Justin Vernon, and is known as \"The Messina.\". The Messina isn't an instrument, but rather a laptop running the \"Prismizer\" connected to a MIDI controller with some other form of outboard gear.\n\nFrancis Starlite of Francis and the Lights perfected the method of reaching bright polyphony sounds without using a vocoder or Auto-Tune. It works like light that passes through a prism area and, naturally, splits into a color spectrum. Something similar happens with the vocals — it is not monotonous, but scattered and voluminous, giving a choir effect to the sound. Chance the Rapper spoke on Prismizer in an interview to Zane Lowe after its debut on \"Coloring Book\":\nThat’s the one perpetual thing. I’ve heard a lot of songs with harmonizer, right? One of my favorite things that I see on the Internet is people commenting on the album, whenever people talk about this vocal sound that he’s created, they call it Auto-Tune, or they say this sounds like a lot like Bon Iver - Justin, who Francis worked with and showed a lot of this musical styling to, uses a very similar harmonizer effect - but there’s this very special thing that Francis does that he calls Prismizer. I love it, and it’s going to be Kanye’s album, and it’s on Frank’s album, but, this Prismizer thing that he does, he sings or he’ll take a vocal and then, very similar to a vocoder, instead of it being singular keys though, he builds chordal sound around it, so it sounds like a choir, so, when you first hear Ye’s vocal come in, it sounds like 15 cyborgs, all singing in Auto-Tune, but really it’s one vocal with Francis saying, ‘OK, this is the 3rd and the 5th and the 7th,’ and he just builds a full choir around it. This whole sound is my favorite thing because it resembles the choir sound that I’ve been trying to get forever, the choral sound, but it also kind of makes it futurist at the same time. That’s like, I think, one of the all-encompassing ideas of the whole project. Francis is credited on “Summer Friends” but he also did all of the Kanye vocals on it. He also does it at the end of “Same Drugs,” and, even though it’s not him playing on “How Great,” My Cousin Nicole came in and sang this lead vocal for “How Great Is Our God,” and my homie Peter came in and did Francis’s Prismizer with his same harmonizer and preset that we learned from him and really makes it sound like a whole new thing.\n",
    "id": "50780831",
    "title": "Prismizer"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50952451",
    "text": "Czech Institute of Informatics, Robotics and Cybernetics\n\nThe Czech Institute of Informatics, Robotics and Cybernetics (Český institut informatiky, robotiky a kybernetiky, CIIRC)] was established as a part of CTU on July 1, 2013. Its mission is to become an internationally respected research institute, which participates in education of students and is a place responsible for a technology transfer into the field of industry. In the premises of the CTU in Dejvice there has been two new buildings for CIIRC built since November 2013. The construction works should be finished and the buildings prepared for its inhabitants - employees of CIIRC at the end of summer 2016. CIIRC will reside in a new building, which used to serve as a Technical University canteen and one of the biggest Billa hypermarkets.\n\nCIIRC consists of eight research departments, CYPHY: Cyber-physical systems, INTSYS: Intelligent systems, IIG: Industrial informatics, RMP: Robotics and machine perception, IPA: Industrial production and automation, COGSYS: Cognitive systems and neurosciences, BEAT: Biomedical engineering and Assistive technologies, PLAT: Research Management of Platforms. Research departments are further divided into groups. The head of CIIRC is Vladimír Mařík.\n\n",
    "id": "50952451",
    "title": "Czech Institute of Informatics, Robotics and Cybernetics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21488059",
    "text": "Sex robot\n\nSex robots or sexbots are anthropomorphic robot sex dolls. , no functioning sex robots exist. There is controversy as to whether developing them would be morally justifiable.\n\nIn 2014, David Levy, the chess champion and author of \"Love and Sex with Robots\" said in an interview with \"Newsweek\" that \"I believe that loving sex robots will be a great boon to society ... There are millions of people out there who, for one reason or another, cannot establish good relationships.\" He estimates that this will take place by the mid-21st century.\n\nThere are ongoing attempts to make sex dolls socially interactive. In 2010, a sex doll called Roxxxy that had the capacity to play back pre-recorded speech cues was demonstrated at a trade show. In 2015, Matt McMullen, the creator of the RealDoll stated that he intended to create sex dolls with the capacity to hold conversations. Sexbots with a male design may be referred to as \"malebots\" or \"manbots\".\n\nBarcelona based Dr. Sergi Santos developed sex robot Samantha; the robot can switch between a sex setting (which can include Samantha simulating an orgasm) and a family mode. It can also can tell jokes and discuss philosophy.\n\nIn September 2015, Kathleen Richardson of De Montfort University and Erik Billing of the University of Skövde created the \"\", calling for a ban on the creation of anthropomorphic sex robots. In a journal article Richardson is critical of Levy and argues that the introduction of such devices would be socially harmful, and demeaning to women and children.\n\nIn September 2015, the Japanese company SoftBank, the makers of the \"Pepper\" robot, included a ban on robot sex. The robots user agreement states: \"The policy owner must not perform any sexual act or other indecent behaviour\".\n\nNoel Sharkey and Aimee van Wynsberghe of the Foundation for Responsible Robotics released a consultation report presenting a summary of the issues and various opinions about what could be society's intimate association with robots. The report includes an examination of how such robots could be employed as a sexual therapy tool for rapists or paedophiles. Sharkey warns that this could be \"problematic\" in terms of sex dolls resembling children.\n\nThe \"First International Congress on Love and Sex with Robots\" was held in Funchal, Madeira in November 2014. In October 2015 a second conference scheduled for November 2015 in Malaysia was declared illegal by the Malaysian Inspector-General of Police. The second conference was eventually held in December 2016 chaired by Dr. Kate Devlin at Goldsmiths, University of London in the United Kingdom. Devlin also founded the UK's first ever sex tech hackathon, also held in 2016 at Goldsmiths.\n\nIn 2016, a discussion of these issues was held at the 12th IFIP TC9 Human Choice & Computers Conference, entitled \"Technology and Intimacy: Choice or Coercion?\".\n\n",
    "id": "21488059",
    "title": "Sex robot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51775967",
    "text": "Real-time path planning\n\nPath planning and navigation play a significant role in robot motion planning and simulated virtual environments. Computing collision-free paths, addressing clearance, and designing dynamic representations and re-planning strategies are examples of important problems with roots in computational geometry and discrete artificial intelligence search methods, and which are being re-visited with innovative new perspectives from researchers in computer graphics and animation.\n",
    "id": "51775967",
    "title": "Real-time path planning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51925440",
    "text": "Crazyflie 2.0\n\nThe Crazyflie 2.0 is a lightweight, open source flying development platform based on a nano quadcopter.\n\nCrazyflie 2.0 is the second iteration of the open source Crazyflie nano quadcopter released in 2013 by Marcus Eliasson, Arnaud Taffanel, and Tobias Antonsson. The Crazyflie platform specifications are open source and available to anyone through the Bitcraze wiki and the Bitcraze Github repo\n\nThe Crazyflie 2.0 is a palm sized quadcopter weighing 27 grams supporting wireless control over radio and Bluetooth Low Energy (LE). It has a flight time of 7 minutes and a charge time of 40. As an open source project, its code and design specifications are available to anyone and the design was created with modification in mind. It's extensible with additional boards called \"decks\" - similar to Arduino shields - that are connected through two rows of pin headers.\n\nThe following decks are currently available:\n\n\nThe Crazyflie firmware is written in C++ and based on FreeRTOS. On the client side several languages can be used including Python, C++, C#, Ruby, NodeJS, Scala, Java and Ada/SPARK. There are also mobile clients for iOS an Android available.\n\n\n",
    "id": "51925440",
    "title": "Crazyflie 2.0"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7909383",
    "text": "Programmable matter\n\nProgrammable matter is matter which has the ability to change its physical properties (shape, density, moduli, conductivity, optical properties, etc.) in a programmable fashion, based upon user input or autonomous sensing. Programmable matter is thus linked to the concept of a material which inherently has the ability to perform information processing.\n\nProgrammable matter is a term originally coined in 1991 by Toffoli and Margolus to refer to an ensemble of fine-grained computing elements arranged in space. Their paper describes a computing substrate that is composed of fine-grained compute nodes distributed throughout space which communicate using only nearest neighbor interactions. In this context, programmable matter refers to compute models similar to cellular automata and lattice gas automata. The CAM-8 architecture is an example hardware realization of this model. This function is also known as \"digital referenced areas\" (DRA) in some forms of self-replicating machine science.\n\nIn the early 1990s, there was a significant amount of work in reconfigurable modular robotics with a philosophy similar to programmable matter.\n\nAs semiconductor technology, nanotechnology, and self-replicating machine technology have advanced, the use of the term programmable matter has changed to reflect the fact that\nit is possible to build an ensemble of elements which can be \"programmed\" to change their physical properties in reality, not just in simulation. Thus, programmable matter has come to mean \"any bulk substance which can be programmed to change its physical properties.\"\n\nIn the summer of 1998, in a discussion on artificial atoms and programmable matter, Wil McCarthy and G. Snyder coined the term \"quantum wellstone\" (or simply \"wellstone\") to describe this hypothetical but plausible form of programmable matter. McCarthy has used the term in his fiction.\n\nIn 2002, Seth Goldstein and Todd Mowry started the claytronics project at Carnegie Mellon University to investigate the underlying hardware and software mechanisms necessary to realize programmable matter.\n\nIn 2004, the DARPA Information Science and Technology group (ISAT) examined the potential of programmable matter. This resulted in the 2005–2006 study \"Realizing Programmable Matter\", which laid out a multi-year program for the research and development of programmable matter.\n\nIn 2007, programmable matter was the subject of a DARPA research solicitation and subsequent program.\n\n In one school of thought the programming could be external to the material and might be achieved by the \"application of light, voltage, electric or magnetic fields, etc.\" . For example, a liquid crystal display is a form of programmable matter. A second school of thought is that the individual units of the ensemble can compute and the result of their computation is a change in the ensemble's physical properties. An example of this more ambitious form of programmable matter is claytronics.\nThere are many proposed implementations of programmable matter. Scale is one key differentiator between different forms of programmable matter. At one end of the spectrum reconfigurable modular robotics pursues a form of programmable matter where the individual units are in the centimeter size range.\nAt the nanoscale end of the spectrum there are a tremendous number of different bases for programmable matter, ranging from shape changing molecules to quantum dots. Quantum dots are in fact often referred to as artificial atoms. In the micrometer to sub-millimeter range examples include MEMS-based units, cells created using synthetic biology, and the utility fog concept.\n\nAn important sub-group of programmable matter are robotic materials, which combine the structural aspects of a composite with the affordances offered by tight integration of sensors, actuators, computation and communication, while foregoing reconfiguration by particle motion.\n\nThere are many conceptions of programmable matter, and thus many discrete avenues of research using the name. Below are some specific examples of programmable matter.\n\nThese include materials that can change their properties based on some input, but do not have the ability to do complex computation by themselves.\n\nThe physical properties of several complex fluids can be modified by applying a current or voltage, as is the case with liquid crystals.\n\nMetamaterials are artificial composites that can be controlled to react in ways that do not occur in nature. One example developed by David Smith and then by John Pendry and David Schuri is of a material that can have its index of refraction tuned so that it can have a different index of refraction at different points in the material. If tuned properly this could result in an \"invisibility cloak.\"\n\nA further example of programmable -mechanical- metamaterial is presented by Bergamini et al. Here, a pass band within the phononic bandgap is introduced, by exploiting variable stiffness of piezoelectric elements linking aluminum stubs to the aluminum plate to create a phononic crystal as in the work of Wu et al. The piezoelectric elements are shunted to ground over synthetic inductors. Around the resonance frequency of the LC circuit formed by the piezoelectric and the inductors, the piezoelectric elements exhibit near zero stiffness, thus effectively disconnecting the stubs from the plate. This is considered an example of programmable mechanical metamaterial.\n\nAn active area of research is in molecules that can change their shape, as well as other properties, in response to external stimuli. These molecules can be used individually or en masse to form new kinds of materials. For example, J Fraser Stoddart's group at UCLA has been developing molecules that can change their electrical properties.\n\nAn electropermanent magnet is a type of magnet which consists of both an electromagnet and a dual material permanent magnet, in which the magnetic field produced by the electromagnet is used to change the magnetization of the permanent magnet. The permanent magnet consists of magnetically hard and soft materials, of which only the soft material can have its magnetization changed. When the magnetically soft and hard materials have opposite magnetizations the magnet has no net field, and when they are aligned the magnet displays magnetic behaviour.\n\nThey allow creating controllable permanent magnets where the magnetic effect can be maintained without requiring a continuous supply of electrical energy. For these reasons, electropermanent magnets are essential components of the research studies aiming to build programmable magnets that can give rise to self-building structures.\n\nSelf-reconfiguring modular robotics is a field of robotics in which a group of basic robot modules work together to dynamically form shapes and create behaviours suitable for many tasks. Like Programmable matter SRCMR aims to offer significant improvement to any kind of objects or system by introducing many new possibilities for example: 1. Most important is the incredible flexibility that comes from the ability to change the physical structure and behavior of a solution by changing the software that controls modules. 2. The ability to self-repair by automatically replacing a broken module will make SRCMR solution incredibly resilient. 3. Reducing the environmental foot print by reusing the same modules in many different solutions. Self-Reconfiguring Modular Robotics enjoys a vibrant and active research community.\n\nClaytronics is an emerging field of engineering concerning reconfigurable nanoscale robots ('claytronic atoms', or \"catoms\") designed to form much larger scale machines or mechanisms. The catoms will be sub-millimeter computers that will eventually have the ability to move around, communicate with other computers, change color, and electrostatically connect to other catoms to form different shapes.\n\nCellular automata are a useful concept to abstract some of the concepts of discrete units interacting to give a desired overall behavior.\n\nQuantum wells can hold one or more electrons. Those electrons behave like artificial atoms which, like real atoms, can form covalent bonds, but these are extremely weak. Because of their larger sizes, other properties are also widely different.\n\nSynthetic biology is a field that aims to engineer cells with \"novel biological functions.\" Such cells are usually used to create larger systems (e.g., biofilms) which can be \"programmed\" utilizing synthetic gene networks such as genetic toggle switches, to change their color, shape, etc. Such bioinspired approaches to materials production has been demonstrated, using self-assembling bacterial biofilm materials that can be programmed for specific functions, such as substrate adhesion, nanoparticle templating, and protein immobilization.\n\n",
    "id": "7909383",
    "title": "Programmable matter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52914225",
    "text": "Sharp INTELLOS automated unmanned ground vehicle (A-UGV)\n\nSharp INTELLOS automated unmanned ground vehicle (A-UGV) is a type of robotics system in the category called Automated unmanned ground vehicle (A-UGV) known for using a navigation surveillance platform developed by Sharp. The system combines automation, mobility and a variety of monitoring and detection capabilities to extend the impact of a traditional security force.\nIn Greek mythology, a giant man of bronze, known as either Talos or Talon, circled Crete's shores three times daily, protecting Europa from pirates and invaders. \"INTELLOS\" combines the concept of intelligence with both technology and security.\n\nThe product launched at the ASIS International 62nd Annual Seminar and Exhibits (ASIS 2016) in Orlando, Florida on September 12, 2016. \nThe Sharp INTELLOS A-UGV provides data that can enhance outdoor surveillance, security, safety, and maintenance inspections. The system offers those services through a durable cost-effective multi-terrain, mobile sensor platform that can capture video, audio and environmental data. Robots generally work best at repetitive tasks (tedious to humans). This type of automated robot is ideal for dull, dirty, and/or dangerous outdoor patrols, which may be unsafe or unsuitable for existing field personnel.\nRobots can serve to extend the capacity and reach of traditional guarding, by integrating with existing security infrastructures and using technology to help lower security operating costs for routine tasks. They can also be tied into current networks and systems, extending their capabilities, while avoiding incremental implementation costs.\n\nOrganizations may have areas around their property that are not covered by cameras or foot patrols. These vehicles monitor, record, and provide visual surveillance in and between these uncovered areas. \n\nAutomated unmanned ground vehicles can be used for video investigation missions. Optional mobile sensors can gather information around a property to be analyzed and reviewed by personnel. These robots follow predetermined or changeable routes within fenced areas and can be used to inspect for industries, such as public utilities, refineries and storage yards. They can navigate patrol paths, along waypoints, recording and securely transmitting to security personnel allowing them to decide when human inspection or intervention is necessary.\n\nThe Sharp INTELLOS A-UGV can also provide a visible deterrent, while helping to protect people, infrastructure and assets. The vehicle has a ruggedized design intended to withstand variations in temperature and terrains.\n\nThe propulsion (drive) system is powered by two lithium ION battery packs. The vehicle can operate on either one or two battery packs. These batteries are housed in user-replaceable cartridges that can be charged in an off-line charger.\n\nThe Sharp INTELLOS A-UGV has standard cameras and an optional thermal camera. The FLIR Thermal camera, which is mounted atop the Sharp INTELLOS A-UGV's adjustable-height boom, illuminates objects in environments without any ambient light, enabling nighttime surveillance as well as showing differences in temperature. The integrated FLIR Thermal camera helps personnel detect human, vehicular, and animal intrusions, which can be viewed and recorded with the AXIS P56 PTZ dome network series camera.\n\nThe Intellos is designed to carry optional MultiRAE gas and radiation sensors as well as a siren, flashing lights and voice announcements (including two-way communications). Its video, audio and optional sensor signals are transmitted by wireless mesh network or cellular network. Other functions include a PC/Server with preinstalled command and control software, navigation system using GPS (Global Positioning System) and RTK (Real Time Kinematic) to provide accurate positioning.\n\nThe Sharp INTELLOS CCAP (Command and Control Application Software) PC/Server is located in the Security Command Center and is used to control and monitor the vehicles patrol path operations. The software allows an authorized administrator to create and schedule patrols as well as determine waypoint actions to undertake. These actions include the activation of sensors at the appropriate time and location.\n\nIn the case of cellular, the unit includes a communication router that can automatically switch between the wireless network to cellular as a fail over in the event a wireless signal cannot be acquired. Transition to cellular requires a data plan on a public carrier's cellular network.\n\nSharp INTELLOS A-UGV was named one of the Top 30 Technology Innovations of 2016 by Security Sales & Integration and in 2017 INTELLOS was named the winner of the Law Enforcement / Guarding Systems category of the Security Industry Associaton's New Product Showcase.\n\nA U.S. federal trademark registration was filed for \"INTELLOS\" by Sharp Corporation. The United States Patent and Trademark Office has given the \"INTELLOS\" trademark serial number 86886871.\n\n",
    "id": "52914225",
    "title": "Sharp INTELLOS automated unmanned ground vehicle (A-UGV)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42148003",
    "text": "Underwater robotics\n\nUnderwater robotics is a branch of robotics. Underwater robots can be autonomous, or they can be remotely operated. This is an emerging science, which has become more popular with evolving technology. There are many applications of underwater robotics such as scientific exploration, military use, and hobbies.\nBesides capability of swimming an underwater robot also has multi DOF manipulators and end effectors on these arms of various types to perform underwater tasks such as construction, salvage, rescue and repair. They can also help in collecting items that are deeply submerged inside the sea, used by the military and scientists mostly.\n",
    "id": "42148003",
    "title": "Underwater robotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49580171",
    "text": "Passenger drone\n\nA passenger drone (also known as a drone taxi, flying taxi, or pilotless helicopter) is a type of unmanned aerial vehicle (UAV) that carries passengers. The first passenger drone was introduced at the Consumer Electronics Show (CES) 2016 by Chinese entrepreneurs and is called the Ehang 184.\n\nThe use of UAVs, or drones, has been popular in recent years. Once used primarily for recreation by hobbyists, drones are now used in military operations and for conducting research. More recently, commercial companies have explored using drones to transport merchandise. Since 2011, several commercial developers and amateur builders have conducted short manned flights on experimental electric multi-rotor craft. In January 2016, the first commercially produced drone capable of carrying a human was introduced by Chinese entrepreneurs at CES 2016.\n\nRadio controlled model airplanes have been a popular hobby since the 1970s. Drones, especially electric powered multi-rotor craft, have only emerged among hobbyists in the past ten to fifteen years. Drones differ from model airplanes in that they implement a measure of autonomy in their operation. Aerial drones have been used by militaries since World War II. Military drone capability expanded rapidly at the end of the twentieth century. Military drones have seen extensive use during campaigns in Iraq and Afghanistan.\n\nThroughout the twentieth century and more recently, designers have proposed and developed many radical ideas for personal flight. Among these are the personal jetpack introduced in the 1960s, the Aeromobil flying car concept of the early 1990s, and the Terrafugia flying vehicle concept of 2006. While these are steered by the pilot, and thus are not technically considered drones, they nevertheless serve as inspirational precursors to the flying passenger drones being developed today.\n\nThe future of passenger drones remains uncertain since this technology is so new. Innovation in aerial drone technology, and in aerial traffic coordination, control, and collision-avoidance could result in rapid proliferation of passenger drones for civilian travel. Several companies are exploring the use of passenger drones as air-taxis and for air-ambulance services. Passenger drone developers are working to overcome many challenges, including noise, small useful load, short flight times, airspace regulations, and scarce data on both safety and general operations.\n\n",
    "id": "49580171",
    "title": "Passenger drone"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53382545",
    "text": "Robot tax\n\nLeaders of the industrial and scientific communities argue that, to offset the social costs created by automation’s displacement effects, either robots should pay income tax, or their owners should pay a tax for replacing a worker with a robot. And this \"robot tax\" should be used to finance a universal basic income or guaranteed living wage. Bill Gates said the solution to the problem is simple, the government should start taxing robots i.e. make the robot owners cough up the money needed to re-establish the defunct workforce. But EU Commissioner Andrus Ansip, tasked with bringing Europe to the digital age, says no. He explained that a robot tax would only mean someone else would take the leading position and leave Europe behind.\n\n\n",
    "id": "53382545",
    "title": "Robot tax"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53534260",
    "text": "Robotic governance\n\nRobotic governance provides a regulatory framework to deal with autonomous and intelligent machines. This includes research and development activities as well as handling of these machines. The idea is related to the concepts of corporate governance, technology governance and IT-governance, which provide a framework for the management of organizations or the focus of a global IT infrastructure.\n\nRobotic governance describes the impact of robotics, automation technology and artificial intelligence on society from a holistic, global perspective, considers implications and provides recommendations for actions in a Robot Manifesto. This is realized by the Robotic Governance Foundation, an international non-profit organization.\n\nThe robotic governance approach is based on the German research on discourse ethics. Therefore, the discussion should involve all Stakeholders, including scientists, society, religion, politics, industry as well as labor unions in order to reach a consensus on how to shape the future of robotics and artificial intelligence. The compiled framework, the so-called Robot Manifesto, will provide voluntary guidelines for a self-regulation in the fields of research, development as well as use and sale of autonomous and intelligent systems.\n\nThe concept does not only appeal on the responsibility of researchers and robot manufacturers, but like with child labor and sustainability, also means a raising of opportunity costs. The greater public awareness and pressure will become concerning this topic, the harder it will get for companies to conceal or justify violations. Therefore, from a certain point it will be cheaper for organizations to invest in sustainable technologies and accepted.\n\nThe idea to set ethical standards for intelligent machines is not a new one and undoubtedly has its roots in science fiction literature. Even older is the discussion about ethics of intelligent, man-made creatures in general. Some of the earliest recorded examples can be found in Ovid's \"Metamorphoses\", in Pygmalion, in the Jewish golem mysticism (12th century) as well as in the idea of Homunkulus (Latin: little man) arisen from the alchemy of the Late Middle Ages.\n\nThe fundamental and philosophical question of these literary works is what will happen, if humans presume to create autonomous, conscious or even godlike creatures, machines, robots or androids. While most of the older works broach the issue of the act of creation, if it is morally appropriate and which dangers could arise, Isaac Asimov was the first to realize the necessity to restrict and regulate the freedom of action of machines. He wrote the first Three Laws of Robotics.\n\nAt least since the use of drones equipped with air-to-ground missiles in 1995 that can be used against ground targets, like e.g. the General Atomics MQ-1, and the resulting collateral damage, the discussion on the international regulation of remote controlled, programmable and autonomous machines attracted public attention. Nowadays, this discussion covers the entire range of programmable, intelligent and/or autonomous machines, drones as well as automation technology combined with Big Data and artificial intelligence. Lately, well-known visionaries like Stephen Hawking, Elon Musk and Bill Gates brought the topic to the focus of public attention and awareness. Due to the increasing availability of small and cheap systems for public service as well as commercial and private use, the regulation of robotics in all social dimensions gained a new significance. \n\nRobotic governance was first mentioned in the scientific community within a dissertation project at the Technical University of Munich, supervised by Professor Dr. emeritus Klaus Mainzer. The topic has been the subject of several scientific workshops, symposia and conferences ever since, including the Sensor Technologies & the Human Experience 2015, the Robotic Governance Panel at the We Robots 2015 Conference, a keynote at the 10th IEEE International Conference on Self-Adaptive and Self-Organizing Systems (SASO), a full day workshop on Autonomous Technologies and their Societal Impact as part of the 2016 IEEE International Conference on Prognostics and Health Management (PHM’16), a keynote at the 2016 IEEE International Conference on Cloud and Autonomic Computing (ICCAC), the FAS*W 2016: IEEE 1st International Workshops on Foundations and Applications of Self* Systems, the 2016 IEEE International Conference on Emerging Technologies and Innovative Business Practices for the Transformation of Societies (IEEE EmergiTech 2016) and the IEEE Global Humanitarian Technology Conference (GHTC 2016).\n\nSince 2015 IEEE even holds an own forum on robotic governance at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IEEE IROS): the first and second \"Annual IEEE IROS Futurist Forum\", which brought together worldwide renowned experts from a wide range of specialities to discuss the future of robotics and the need for regulation in 2015 and 2016. In 2016 Robotic governance has also been the topic of a plenary keynote presentation on the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2016) in Daejeon, South Korea.\n\nSeveral video statements and interviews on robotic governance, responsible use of robotics, automation technology and artificial intelligence as well as self-regulation in a world of Robotic Natives, with internationally recognized experts from research, economy and politics are published on the website of the Robotic Governance Foundation. Max Levchin, co-founder and former CTO of PayPal emphasized the need for robotic governance in the course of his Q&A session on the South by Southwest Festival (SXSW) 2016 in Austin and referred to the comments of his friend and colleague Elon Musk on this subject. Gerd Hirzinger, former head of the Institute of Robotics and Mechatronics of the German Aerospace Center, showed during his keynote speech at the IROS Futurist Forum 2015 the possibility of machines being so intelligent that it would be inevitable, one day, to prevent certain behavior. At the same event, Oussama Khatib, American roboticist and director of the stanford robotics lab, advocated to emphasize the user acceptance when producing intelligent and autonomous machines. Bernd Liepert, president of the euRobotics aisbl – the most important robotics community in Europe – recommended to establish robotic governance worldwide and underlined his wish for Europe taking the lead in this discussion, during his plenary keynote at the IEEE IROS 2015 in Hamburg. Hiroshi Ishiguro, inventor of the Geminoid and head of the Intelligent Robotics Laboratory at the University of Osaka, showed during the RoboBusiness Conference 2016 in Odense that it is impossible to stop technical progress. Therefore, it is necessary to accept the responsibility and to think about regulation. In the course of the same conference, Henrik I. Christensen, author of the U.S. Robotic Roadmap, underlined the importance of ethical and moral values in robotics and the suitability of robotic governance to create a regulatory framework.\n\n",
    "id": "53534260",
    "title": "Robotic governance"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53810462",
    "text": "Autonomous things\n\nAutonomous things, abbreviated AuT, or the Internet of autonomous things, abbreviated as IoAT, is an emerging term for the technological developments that are expected to bring computers into the physical environment as autonomous entities without human direction, freely moving and interacting with humans and other objects. \n\nSelf-navigating drones are the first AuT technology in (limited) deployment. It is expected that the first mass-deployment of AuT technologies will be the autonomous car, generally expected to be available around 2020. Other currently expected AuT technologies include home robotics (e.g., machines that provide care for the elderly, infirm or young), and military robots (air, land or sea autonomous machines with information-collection or target-attack capabilities).\n\nAuT technologies share many common traits, which justify the common notation. They are all based on recent breakthroughs in the domains of (deep) machine learning and artificial intelligence. They all require extensive and prompt regulatory developments to specify the requirements from them and to license and manage their deployment (see the further reading below). And they all require unprecedented levels of safety (e.g., automobile safety) and security, to overcome concerns about the potential negative impact of the new technology.\n\nAs an example, the autonomous car both addresses the main existing safety issues and creates new issues. It is expected to be much safer than existing vehicles, by eliminating the single most dangerous elementthe driver. The US's National Highway Traffic Safety Administration estimates 94 percent of US accidents were the result of human error and poor decision-making, including speeding and impaired driving, and the Center for Internet and Society at Stanford Law School claims that \"Some ninety percent of motor vehicle crashes are caused at least in part by human error\". So while safety standards like the ISO 26262 specify the required safety, there is still a burden on the industry to demonstrate acceptable safety.\n\nWhile car accidents claim every year 35,000 lives in the US, and 1.25 million worldwide, some believe that even \"a car that's 10 times as safe, which means 3,500 people die on the roads each year [in the US alone]\" would not be accepted by the public. The acceptable level may be closer to the current figures on aviation accidents and incidents, with under a thousand worldwide deaths in most yearsthree orders of magnitude lower than cars. This underscores the unprecedented nature of the safety requirements that will need to be met for cars, with similar levels of safety expected for other Autonomous Things.\n\n",
    "id": "53810462",
    "title": "Autonomous things"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50835878",
    "text": "Sensing floor\n\nA sensing floor is a floor with embedded sensors. \nDepending on their construction, these floors are either monobloc (e.g. structures made of a single frame, carpets).\nor modular (e.g. tiled floors, floors made of stripes of sensors). \nThe first sensing floor prototypes were developed in the 1990s, mainly for human gait analysis. \nSuch floors are usually used as a source of sensing information for an ambient intelligence.\nDepending on the type of sensors employed, sensing floors can measure load (pressure), proximity (to detect, track, and recognize humans), as well as the magnetic field (for detecting metallic objects like robots using magnetometers).\n\nSensing floors have a variety of usages:\n\nMore than 30 distinct sensing floor prototypes have been developed between 1990 and 2015.\nNotable examples of sensing floors have been developed by Oracle, MIT, and Inria \nAs of 2015, few sensing floors are available as commercial products, mainly targeting healthcare facilities (e.g. the GAITRite surface pressure sensing floor, and the SensFloor ).\n",
    "id": "50835878",
    "title": "Sensing floor"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=395851",
    "text": "International Conference on Intelligent Robots and Systems\n\nThe International Conference on Intelligent Robots and Systems (or IROS) is a conference about robotics and automation that is held in various countries and is sponsored by the IEEE. It started as the IEEE International Workshop on Intelligent Robots and Systems in 1988 and had been held annually ever since\n",
    "id": "395851",
    "title": "International Conference on Intelligent Robots and Systems"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53609258",
    "text": "Taurob tracker\n\nTaurob tracker is a mobile robot, manufactured by taurob GmbH in Austria. It has been originally developed as a remote controlled reconnaissance platform for fire departments.\n\nSince 2013 a ATEX certified variant (Zone 1), called taurob tracker Ex is available, which is able to drive in explosive atmospheres safely.\n\nA taurob tracker version with sensors for environments with extreme smoke (e.g. fires in tunnels or subway stations) is currently being developed in the EU funded \"SmokeBot\" project\n\nCompared to most other mobile robots, the taurob tracker has a unique track geometry which allows it to climb over obstacles with just one pair of tracks. Due this geometry the tracks do not loose their tension when raising or lowering the front wheels. Further advantages include improved traction (thus the name of the robot) on uneven ground and a rapid track exchange mechanism.\n\nIn 2016 a taurob tracker platform was used in the RoboCup Rescue League by team Hector.\n\nIn 2017 a variant of taurob tracker called \"Argonaut\" has won the ARGOS Challenge organised by Total S.A.. It is the first fully autonomous, ATEX certified mobile inspection robot for Oil and Gas installations. According to Total it will be used on their industrial sites by 2020. \n",
    "id": "53609258",
    "title": "Taurob tracker"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53815584",
    "text": "Dash Robotics, Inc\n\nDash Robotics, Inc. is a toy robotics startup company located in Hayward, California. Their main focus is on prototyping and manufacturing smart toys (sometimes called \"connected toys\"). The company is often referred to simply as \"Dash,\" and was founded in 2013. Although not officially affiliated with UC Berkeley, many of the company's members met there.\n\nThey are most widely known for creating Kamigami Robots, a biomimetic, foldable robot that was funded via Kickstarter in 2015. Kamigami received attention for their unique motion, which mimics that of a cockroach. The original concepts behind the robot's motion are attributed to Dr. Robert Full, a biologist at UC Berkeley, and can be seen outlined in his Ted Talk. Mattel licensed the Kamigami brand in spring of 2017, and says it will oversee a new release in Fall of 2017.\n\nPrevious products include Dash Beta and Dash VR. Dash Beta was a cardboard, app-controlled robot that required glue to be assembled, and acted as a predecessor to Kamigami. Dash Beta is no longer in production.\n\nMr. Dad Seal of Approval\n\nMaker Faire Editor's Choice\n\n",
    "id": "53815584",
    "title": "Dash Robotics, Inc"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54305107",
    "text": "Kate Devlin\n\nKate Devlin, born Adela Katharine Devlin is a British computer scientist specialising in Artificial intelligence and Human–computer interaction (HCI). She is best known for her work on human sexuality and robotics and was co-chair of the annual Love and Sex With Robots convention in 2016 held in London and was founder of the UK's first ever sex tech hackathon held in 2016 at Goldsmiths, University of London.\nShe is a senior lecturer in the department of computing at Goldsmiths, part of the University of London and is the author of \"The Brightness of Things: An Adventure in Light and Time\" in addition to several academic papers.\n\nDevlin began her university career in the humanities and graduated from Queen's University Belfast in 1997 with a BA (Honours) degree in archaeology. After deciding that archaeology presented her with limited future prospects, she returned to Queen's University to study computer science, and in 1999 she was awarded an MSc in that subject. She then moved to The University of Bristol, where in 2004 she was awarded a PhD in computer science.\n\nIn 2003 Devlin began researching computer graphics in archaeology at Bristol University, rendering 3D computer models of archaeological sites such as at Pompeii with attention to realistically rendering lighting effects caused by the spectral composition of light sources available at the time period in history. This involved experimental archaeology, recreating light sources and analysing the spectral range for each type of candle or fuel lamp.\n\nSince 2007 Devlin has worked in the field of human-computer interaction and artificial intelligence at Goldsmiths, and is a senior lecturer in several areas of computer science, including programming, graphics and animation.\n\nIn 2015 Devlin spoke to news broadcasters in the UK about institutionalised sexism within science research and academia after comments made by Sir Tim Hunt regarding women scientists working in mixed laboratories. While Devlin, along with many other commentators, acknowledged the comments to be 'banter' she expressed the frustration that many women have with sexism in the fields of science, technology, engineering, and mathematics and jokingly tweeted that she couldn't chair a departmental meeting because she was \"too busy swooning and crying.\" Devlin also speaks publicly and writes to encourage more women to pursue technology careers.\n\nIn 2016 Devlin co-chaired the \"International Congress on Love and Sex With Robots\" held in London, UK, an annual conference held since 2014, co-founded by Adrian David Cheok and David Levy, writer of the book of the same name, Love and Sex with Robots.\n\nAlso, in 2016, Devlin founded the first UK sex technology (sex tech) hackathon, a conference where scientists, students, academics and other people in the sex tech industry meet to pool ideas and build projects in the field of sex and intimacy with artificial partners.\n\nIn 2016 Devlin appeared several times in the media debating ethical issues concerning sex robots with Kathleen Richardson, fellow of the ethics of robotics at De Montfort University, and founder of Campaign Against Sex Robots which seeks to ban sex robots on the grounds that they encourage isolation, perpetuate the idea of women as property and are dehumanising. Devlin has argued that not only would a ban be impractical, but as technology develops more women need to be involved in order to diversify a field which is dominated by men creating products for heterosexual men. She also points out that the technology can be used as therapy, citing the use of artificial intelligence to treat anxiety, and the possible application towards understanding the psychology of sex offenders.\n\nDevlin frequently speaks at conferences and her areas of scientific interest include: the social and ethical problems of integrating artificial intelligence into sexual experience with computer systems and robots, the human and social consequences of AI as it becomes more sophisticated, and improving human sexual relationships by moving away from a \"hetero-normative male view\" of sex and intimacy using sex toys, robots and computer software. She has raised issues which she believes need addressing as this technology develops. These concerns include: if robots gain self-awareness, will they be able to give informed consent and be entitled to make choices regarding their own desires, and should they be supplied to the elderly in residential care facilities for companionship and sex.\n\nDevlin was named one of London's most influential people 2017 by the Progress 1000, London Evening Standard.\n\n\n\nDevlin has written for the \"New Scientist\", \"The Conversation\" and has presented a TEDx talk entitled \"Sex Robots\".\n\nDevlin has spoken publicly about living with bipolar disorder and epilepsy and how stress can affect both her academic and professional life, as well as how important it is to bring mental health issues into public debate to reduce the stigma attached.\n\nDevlin is open about her consensually non-monogamous relationships and has written about her experiences of polyamory.\n\nShe is also interested in, and has researched, the life story of Adela Breton, the Victorian archaeologist and explorer, and contributed to the \"Raising Horizons\" exhibition of 'trowel-blazing' women throughout the history of archaeology and geology.\n\nShe is divorced and has a daughter.\n\n\n",
    "id": "54305107",
    "title": "Kate Devlin"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54310332",
    "text": "Maarten Steinbuch\n\nMaarten Steinbuch is a serial entrepreneur and professor of Systems & Control at Eindhoven University of Technology. His research interests span from design and control of motion systems to robotics, automotive powertrains and fusion plasmas. He is most known for his work in the field of robotics.\n\nMaarten started his career at Philips Research during his postgraduate degree at Delft University of Technology in 1987. In 1998-1999 he had a year long stint at Philips CFT, after which he became a full professor of the Control Systems Group at the Mechanical Engineering department of Eindhoven University of Technology. During his academic career he received many awards, such as the KIVI Academic Society Award and the Simon Stevin Meester 2016, the highest Dutch award for scientific technological research.\n\nAside from teaching, Maarten has been involved in several companies as a (co-)founder. These include:\n",
    "id": "54310332",
    "title": "Maarten Steinbuch"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54620481",
    "text": "FIRST Global\n\nFIRST Global (For Inspiration and Recognition of Science and Technology) is a trade name for a nonprofit organization, the International First Committee Association. It promotes STEM education and careers for youth through Olympics-style robotics competitions called the FIRST Global Challenge. It was founded by Dean Kamen in 2016 as an expansion of FIRST, an organization with similar objectives. \n\nFIRST Global is a trade name for the International First Committee Association, a nonprofit corporation based in Manchester, New Hampshire, with a 501(c)3 designation from the IRS.\n\nIt was founded by the co-founder of FIRST, Dean Kamen, with the objective of promoting STEM education and careers in the developing world through Olympics-style robotics competitions. \n\nThe 2017 FIRST Global Challenge was held in Washington, D.C., from July 17–19, and the challenge was the use of robots to separate different colored balls, representing clean water and impurities in water, symbolizing the Engineering Grand Challenge (based on the Millennium Development Goal) of improving access to clean water in the developing world. Around 160 teams composed of 15- to 18-year-olds from 157 countries participated, and around 60% of teams were created or led by young women. Six continental teams also participated.\n\nDuring the closing ceremony of the 2017 FIRST Global Challenge, entrepreneur Ricardo Salinas, a founding member of FIRST Global, announced that Mexico City would host the 2018 FIRST Global Challenge.\n\nThe Global STEM Corps is a FIRST Global initiative that connects qualified volunteer mentors with students in developing countries to prepare them for competitions.\n",
    "id": "54620481",
    "title": "FIRST Global"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46530282",
    "text": "Robo-Taxi\n\nA Robo-Taxi, also known as a Robo-Cab, a self-driving taxi or a driverless taxi is an autonomous car (SAE Level 4 or 5) operated in an Autonomous Mobility on Demand (AMoD) e-hailing service. \n\nThe fact of eliminating the need for a human chauffeur, which represents a significant part of the operating costs of that type of services, could make it a very affordable solution for the customers and accelerate the spreading of Transportation-as-a-Service (TaaS) solutions as opposed to individual car ownership. However, it raises the issue of job destruction.\n\nSeveral studies highlighted that robo-taxis operated in an AMoD service could be one of the most rapidly adopted applications of autonomous cars at scale and a major mobility solution in the near future, especially in urban areas, providing the majority of vehicle miles in the United States within a decade of their first introduction. Moreover, they could have a very positive impact on road safety, pollution (since these services will most probably use electric cars and reduce the number of vehicles on the road), traffic congestion and parking.\n\nSeveral companies are testing robo-taxi services, especially in Asia and in the United-States. In most tests, there are human chauffeurs or \"safety drivers\" in these test cars to take control back in case of emergency. \n\nIn August 2016, MIT spinoff NuTonomy was the first company to make robo-taxis available to the public, starting to offer rides with a fleet of 6 modified Renault Zoes and Mitsubishi i-MiEVs in a limited area in Singapore. NuTonomy later signed three significant partnerships to develop its robo-taxi service: with Grab, Uber’s rival in Southeast Asia, with Groupe PSA, which is supposed to provide the company with Peugeot 3008 SUVs and the last one with Lyft to launch a robo-taxi service in Boston.\n\nIn September 2016, Uber started allowing a select group of users in Pittsburgh to order robo-taxis from a fleet of 14 modified Ford Fusions. The test extended to San Francisco with modified Volvo XC90s before being relocated to Tempe, Arizona in February 2017. In March 2017, one of Uber’s robo-taxis crashed in self-driving mode in Arizona, which led the company to suspend its tests before resuming them a few days later.\n\nIn early 2017, Waymo, the Google self-driving car project which became an independent company in 2016, started a large public robo-taxi test in Phoenix using 100 and then 500 more Chrysler Pacifica Hybrid minivans provided by Fiat Chrysler Automobiles as part of a partnership between the two companies. Waymo also signed a deal with Lyft to collaborate on self-driving cars in May 2017. In November 2017, Waymo revealed it had begun to operate some of its automated vehicles in Arizona without a safety driver behind the wheel.\n\nIn August 2017, Cruise Automation, a self-driving startup acquired by General Motors in 2016, launched the beta version of a robo-taxi service for its employees in San Francisco using a fleet of 46 Chevrolet Bolt EVs.\n\nMany automakers have announced their plans to develop robo-taxis before 2025 and specific partnerships have been signed between automakers, technology providers and service operators. Most significant disclosed information include: \n",
    "id": "46530282",
    "title": "Robo-Taxi"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55013965",
    "text": "Robot Aviation\n\nRobot Aviation is an aerospace company headquartered in the Oslo area, Norway with development and production facilities in Warsaw, Poland and offices in Grand Forks in North Dakota and Phoenix in Arizona in the US.\n\nRobot Aviation designs, develops and manufactures complete unmanned aerial systems (UAS) for various industrial applications. The company's products include both unmanned helicopters and fixed-wing aircraft.\n\nRobot Aviation was founded in 2008 in Gjøvik in Oppland in Norway by Ole Vidar Homleid and Oddvar Kristiansen. Although the company's official address was at Gjøvik, development and production of the first SkyRobot prototype (FX450) started at Skotfoss in Skien, Telemark in cooperation with the company Aero-Design of Paweł Różański from Warsaw, Poland.\n\n",
    "id": "55013965",
    "title": "Robot Aviation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=245926",
    "text": "Autonomous car\n\nAn autonomous car (also known as a driverless car, self-driving car, robotic car) and unmanned ground vehicle is a vehicle that is capable of sensing its environment and navigating without human input.\n\nAutonomous cars use a variety of techniques to detect their surroundings, such as radar, laser light, GPS, odometry and computer vision. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage. Autonomous cars must have control systems that are capable of analyzing sensory data to distinguish between different cars on the road.\n\nThe potential benefits of autonomous cars include reduced mobility and infrastructure costs, increased safety, increased mobility, increased customer satisfaction and reduced crime. Specifically a significant reduction in traffic collisions; the resulting injuries; and related costs, including less need for insurance. Autonomous cars are predicted to increase traffic flow; provide enhanced mobility for children, the elderly, disabled and the poor; relieve travelers from driving and navigation chores; lower fuel consumption; significantly reduce needs for parking space; reduce crime; and facilitate business models for transportation as a service, especially via the sharing economy. This shows the vast disruptive potential of the emerging technology. A frequently cited paper by Michael Osborne and Carl Benedikt Frey found that autonomous cars would make many jobs redundant.\n\nAmong the main obstacles to widespread adoption are technological challenges, disputes concerning liability; the time period needed to replace the existing stock of vehicles; resistance by individuals to forfeit control; consumer safety concerns; implementation of a workable legal framework and establishment of government regulations; risk of loss of privacy and security concerns, such as hackers or terrorism; concerns about the resulting loss of driving-related jobs in the road transport industry; and risk of increased suburbanization as travel becomes less costly and time-consuming. Many of these issues are due to the fact that autonomous objects, for the first time, allow computers to roam freely, with many related safety and security concerns.\n\nExperiments have been conducted on automating driving since at least the 1920s; promising trials took place in the 1950s. The first truly autonomous prototype cars appeared in the 1980s, with Carnegie Mellon University's Navlab and ALV projects in 1984 and Mercedes-Benz and Bundeswehr University Munich's EUREKA Prometheus Project in 1987. Since then, numerous companies and research organizations have developed prototypes. In 2015, the US states of Nevada, Florida, California, Virginia, and Michigan, together with Washington, D.C. allowed the testing of autonomous cars on public roads.\n\nIn 2017, Audi stated that its latest A8 would be autonomous at up to speeds of 60 km/h using its \"Audi AI\". The driver would not have to do safety checks such as frequently gripping the steering wheel. The Audi A8 was claimed to be the first production car to reach level 3 autonomous driving and Audi would be the first manufacturer to use laser scanners in addition to cameras and ultrasonic sensors for their system.\n\nIn November 2017, Waymo announced that it had begun testing driverless cars without a safety driver at the driver position, however; there is still an employee in the car.\n\n\"Autonomous\" means self-governance. Many historical projects related to vehicle autonomy have been \"automated\" (made to be \"automatic\") due to a heavy reliance on artificial hints in their environment, such as magnetic strips. Autonomous control implies satisfactory performance under significant uncertainties in the environment and the ability to compensate for system failures without external intervention.\n\nOne approach is to implement communication networks both in the immediate vicinity (for collision avoidance) and further away (for congestion management). Such outside influences in the decision process reduce an individual vehicle's autonomy, while still not requiring human intervention.\n\nWood et al. (2012) write \"This Article generally uses the term 'autonomous,' instead of the term 'automated.'\" The term \"autonomous\" was chosen \"because it is the term that is currently in more widespread use (and thus is more familiar to the general public). However, the latter term is arguably more accurate. 'Automated' connotes control or operation by a machine, while 'autonomous' connotes acting alone or independently. Most of the vehicle concepts (that we are currently aware of) have a person in the driver’s seat, utilize a communication connection to the Cloud or other vehicles, and do not independently select either destinations or routes for reaching them. Thus, the term 'automated' would more accurately describe these vehicle concepts\". As of 2017, most commercial projects focused on autonomous vehicles that did not communicate with other vehicles or an enveloping management regime.\n\nA classification system based on six different levels (ranging from fully manual to fully automated systems) was published in 2014 by SAE International, an automotive standardization body, as J3016, \"Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems\". This classification system is based on the amount of driver intervention and attentiveness required, rather than the vehicle capabilities, although these are very loosely related. In the United States in 2013, the National Highway Traffic Safety Administration (NHTSA) released a formal classification system, but abandoned this system in favor of the SAE standard in 2016. Also in 2016, SAE updated its classification, called J3016_201609.\n\nIn SAE's autonomy level definitions, \"driving mode\" means \"a type of driving scenario with characteristic dynamic driving task requirements (e.g., expressway merging, high speed cruising, low speed traffic jam, closed-campus operations, etc.)\"\nIn the formal SAE definition below, note in particular what happens in the shift from SAE 2 to from SAE 3: the human driver no longer has to monitor the environment. This is the final aspect of the ”dynamic driving task” that is now passed over from the human to the automated system. At SAE 3, the human driver still has the responsibility to intervene when asked to do so by the automated system. At SAE 4 the human driver is relieved of that responsibility and at SAE 5 the automated system will never need to ask for an intervention.\n\nModern self-driving cars generally use Bayesian Simultaneous localization and mapping (SLAM) algorithms, which fuse data from multiple sensors and an off-line map into current location estimates and map updates. SLAM with detection and tracking of other moving objects (DATMO), which also handles things such as cars and pedestrians, is a variant being developed at Google. Simpler systems may use roadside real-time locating system (RTLS) beacon systems to aid localisation. Typical sensors include lidar, stereo vision, GPS and IMU. Visual object recognition uses machine vision including neural networks. Udacity is developing an open-source software stack.\n\nAutonomous cars are being developed with deep learning, or neural networks. Deep neural networks have many computational stages, or levels in which neurons are simulated from the environment that activate the network . The neural network depends on an extensive amount of data extracted from real life driving scenarios. The neural network is activated and “learns” to perform the best course of action. Deep learning has been applied to answer to real life situations, and is used in the programming for autonomous cars. In addition, sensors, such as the LIDAR sensors already used in self-driving cars; cameras to detect the environment, and precise GPS navigation will be used in autonomous cars.\n\nTesting vehicles with varying degrees of autonomy can be done physically, in closed environments, on public roads (where permitted, typically with a license or permit or adhering to a specific set of operating principles) or virtually, i.e. in computer simulations.\n\nWhen driven on public roads, autonomous vehicles require a person to monitor their proper operation and \"take over\" when needed.\n\nSeveral companies are said to be testing autonomous technology in semi trucks. Otto, a self-driving trucking company that was acquired by Uber in August 2016, demoed their trucks on the highway before being acquired. In May 2017, San Francisco-based startup Embark announced a partnership with truck manufacturer Peterbilt to test and deploy autonomous technology in Peterbilt's vehicles. Google's Waymo has also said to be testing autonomous technology in trucks, however no timeline has been given for the project.\n\nIn Europe, cities in Belgium, France, Italy and the UK are planning to operate transport systems for autonomous cars, and Germany, the Netherlands, and Spain have allowed public testing in traffic. In 2015, the UK launched public trials of the LUTZ Pathfinder autonomous pod in Milton Keynes. Beginning in summer 2015 the French government allowed PSA Peugeot-Citroen to make trials in real conditions in the Paris area. The experiments were planned to be extended to other cities such as Bordeaux and Strasbourg by 2016. The alliance between French companies THALES and Valeo (provider of the first self-parking car system that equips Audi and Mercedes premi) is testing its own system. New Zealand is planning to use autonomous vehicles for public transport in Tauranga and Christchurch.\n\nTraffic collisions (and resulting deaths and injuries and costs), caused by human errors, such as delayed reaction time, tailgating, rubbernecking, and other forms of distracted or aggressive driving should be substantially reduced. Consulting firm McKinsey & Company estimated that widespread use of autonomous vehicles could \"eliminate 90% of all auto accidents in the United States, prevent up to in damages and health-costs annually and save thousands of lives.\"\n\nAutonomous cars could reduce labor costs; relieve travelers from driving and navigation chores, thereby replacing behind-the-wheel commuting hours with more time for leisure or work; and also would lift constraints on occupant ability to drive, distracted and texting while driving, intoxicated, prone to seizures, or otherwise impaired. For the young, the elderly, people with disabilities, and low-income citizens, autonomous cars could provide enhanced mobility. The removal of the steering wheel—along with the remaining driver interface and the requirement for any occupant to assume a forward-facing position—would give the interior of the cabin greater ergonomic flexibility. Large vehicles, such as motorhomes, would attain appreciably enhanced ease of use.\n\nAdditional advantages could include higher speed limits; smoother rides; and increased roadway capacity; and minimized traffic congestion, due to decreased need for safety gaps and higher speeds. Currently, maximum controlled-access highway throughput or capacity according to the U.S. Highway Capacity Manual is about 2,200 passenger vehicles per hour per lane, with about 5% of the available road space is taken up by cars. One study estimated that autonomous cars could increase capacity by 273% (~8,200 cars per hour per lane). The study also estimated that with 100% connected vehicles using vehicle-to-vehicle communication, capacity could reach 12,000 passenger vehicles per hour (up 445% from 2,200 pc/h per lane) traveling safely at with a following gap of about of each other. Currently, at highway speeds drivers keep between away from the car in front. These increases in highway capacity could have a significant impact in traffic congestion, particularly in urban areas, and even effectively end highway congestion in some places. The ability for authorities to manage traffic flow would increase, given the extra data and driving behavior predictability. combined with less need for traffic police and even road signage.\n\nSafer driving is expected to reduce the costs of vehicle insurance. Reduced traffic congestion and the improvements in traffic flow due to widespread use of autonomous cars will also translate into better fuel efficiency.\n\nBy reducing the (labor and other) cost of mobility as a service, autonomous cars could reduce the number of cars that are individually owned, replaced by taxi/pooling and other car sharing services. This could dramatically reduce the need for parking space, freeing scarce land for other uses. This would also dramatically reduce the size of the automotive production industry, with corresponding environmental and economic effects. Assuming the increased efficiency is not fully offset by increases in demand, more efficient traffic flow could free roadway space for other uses such as better support for pedestrians and cyclists.\n\nThe vehicles' increased awareness could aid the police by reporting on illegal passenger behavior, while possibly enabling other crimes, such as deliberately crashing into another vehicle or a pedestrian.\n\nIn spite of the various benefits to increased vehicle automation, some foreseeable challenges persist, such as disputes concerning liability, the time needed to turn the existing stock of vehicles from nonautonomous to autonomous, resistance by individuals to forfeit control of their cars, customer concern about the safety of driverless cars, and the implementation of legal framework and establishment of government regulations for self-driving cars. Other obstacles could be missing driver experience in potentially dangerous situations, ethical problems in situations where an autonomous car's software is forced during an unavoidable crash to choose between multiple harmful courses of action, and possibly insufficient Adaptation to Gestures and non-verbal cues by police and pedestrians.\n\nPossible technological obstacles for autonomous cars are:\n\nA direct impact of widespread adoption of autonomous vehicles is the loss of driving-related jobs in the road transport industry. There could be resistance from professional drivers and unions who are threatened by job losses. In addition, there could be job losses in public transit services and crash repair shops. The automobile insurance industry might suffer as the technology makes certain aspects of these occupations obsolete.\n\nPrivacy could be an issue when having the vehicle's location and position integrated into an interface in which other people have access to. In addition, there is the risk of automotive hacking through the sharing of information through V2V (Vehicle to Vehicle) and V2I (Vehicle to Infrastructure) protocols. There is also the risk of terrorist attacks. Self-driving cars could potentially be loaded with explosives and used as bombs.\n\nThe lack of stressful driving, more productive time during the trip, and the potential savings in travel time and cost could become an incentive to live far away from cities, where land is cheaper, and work in the city's core, thus increasing travel distances and inducing more urban sprawl, more fuel consumption and an increase in the carbon footprint of urban travel. There is also the risk that traffic congestion might increase, rather than decrease. Appropriate public policies and regulations, such as zoning, pricing, and urban design are required to avoid the negative impacts of increased suburbanization and longer distance travel.\n\nSome believe that once automation in vehicles reaches higher levels and becomes reliable, drivers will pay less attention to the road. Research shows that drivers in autonomous cars react later when they have to intervene in a critical situation, compared to if they were driving manually.\n\nEthical and moral reasoning come into consideration when programming the software that decides what action the car takes in an unavoidable crash; whether the autonomous car will crash into a bus, potentially killing people inside; or swerve elsewhere, potentially killing its own passengers or nearby pedestrians. A question that comes into play that programmers find difficult to answer is “what decision should the car make that causes the ‘smallest’ damage when it comes to people’s lives?” The ethics of autonomous vehicles is still in the process of being solved and could possibly lead to controversiality.\n\nIn 1999, Mercedes introduced Distronic, the first radar-assisted ACC, on the Mercedes-Benz S-Class (W220) and the CL-Class. The Distronic system was able to adjust the vehicle speed automatically to the car in front in order to always maintain a safe distance to other cars on the road.\n\nIn 2005, Mercedes refined the system (from this point called \"Distronic Plus\") with the Mercedes-Benz S-Class (W221) being the first car to receive the upgraded Distronic Plus system. Distronic Plus could now completely halt the car if necessary on E-Class and most Mercedes sedans. In an episode of \"Top Gear\", Jeremy Clarkson demonstrated the effectiveness of the cruise control system in the S-class by coming to a complete halt from motorway speeds to a round-about and getting out, without touching the pedals.\n\nBy 2017, Mercedes has vastly expanded its autonomous driving features on production cars: In addition to the standard Distronic Plus features such as an active brake assist, Mercedes now includes a steering pilot, a parking pilot, a cross-traffic assist system, night-vision cameras with automated danger warnings and braking assist (in case animals or pedestrians are on the road for example), and various other autonomous-driving features. In 2016, Mercedes also introduced its Active Brake Assist 4, which was the first emergency braking assistant with pedestrian recognition on the market.\n\nDue to Mercedes' history of gradually implementing advancements of their autonomous driving features that have been extensively tested, not many crashes that have been caused by it are known. One of the known crashes dates back to 2005, when German news magazine \"\"Stern\"\" was testing Mercedes' old Distronic system. During the test, the system did not always manage to brake in time. Ulrich Mellinghoff, then Head of Safety, NVH, and Testing at the Mercedes-Benz Technology Centre, stated that some of the tests failed due to the vehicle being tested in a metallic hall, which caused problems with the system's radar. Later iterations of the Distronic system have an upgraded radar and numerous other sensors, which are not susceptible to a metallic environment anymore. In 2008, Mercedes conducted a study comparing the crash rates of their vehicles equipped with Distronic Plus and the vehicles without it, and concluded that those equipped with Distronic Plus have an around 20% lower crash rate. In 2013, German Formula One driver Michael Schumacher was invited by Mercedes to try to crash a Mercedes C-Class vehicle, which was equipped with all safety features that Mercedes offered for its production vehicles at the time, which included the Active Blind Spot Assist, Active Lane Keeping Assist, Brake Assist Plus, Collision Prevention Assist, Distronic Plus with Steering Assist, Pre-Safe Brake, and Stop&Go Pilot. Due to the safety features, Schumacher was unable to crash the vehicle in realistic scenarios.\n\nIn midOctober 2015 Tesla Motors rolled out version 7 of their software in the U.S. that included Tesla Autopilot capability. On 9 January 2016, Tesla rolled out version 7.1 as an over-the-air update, adding a new \"summon\" feature that allows cars to self-park at parking locations without the driver in the car. Tesla's autonomous driving features can be classified as somewhere between level 2 and level 3 under the U.S. Department of Transportation’s National Highway Traffic Safety Administration (NHTSA) five levels of vehicle automation. At this level the car can act autonomously but requires the full attention of the driver, who must be prepared to take control at a moment's notice. Autopilot should be used only on limited-access highways, and sometimes it will fail to detect lane markings and disengage itself. In urban driving the system will not read traffic signals or obey stop signs. The system also does not detect pedestrians or cyclists.\n\nThe first fatal accident involving a vehicle being driven by itself took place in Williston, Florida on 7 May 2016 while a Tesla Model S electric car was engaged in Autopilot mode. The occupant was killed in a crash with an 18-wheel tractor-trailer. On 28 June 2016 the National Highway Traffic Safety Administration (NHTSA) opened a formal investigation into the accident working with the Florida Highway Patrol. According to the NHTSA, preliminary reports indicate the crash occurred when the tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway, and the car failed to apply the brakes. The car continued to travel after passing under the truck’s trailer. The NHTSA's preliminary evaluation was opened to examine the design and performance of any automated driving systems in use at the time of the crash, which involved a population of an estimated 25,000 Model S cars. On 8 July 2016, the NHTSA requested Tesla Motors provide the agency detailed information about the design, operation and testing of its Autopilot technology. The agency also requested details of all design changes and updates to Autopilot since its introduction, and Tesla's planned updates schedule for the next four months.\n\nAccording to Tesla, \"neither autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.\" The car attempted to drive full speed under the trailer, \"with the bottom of the trailer impacting the windshield of the Model S.\" Tesla also stated that this was Tesla’s first known autopilot death in over 130 million miles (208 million km) driven by its customers with Autopilot engaged. According to Tesla there is a fatality every 94 million miles (150 million km) among all type of vehicles in the U.S. However, this number also includes fatalities of the crashes, for instance, of motorcycle drivers with pedestrians.\n\nIn July 2016 the U.S. National Transportation Safety Board (NTSB) opened a formal investigation into the fatal accident while the Autopilot was engaged. The NTSB is an investigative body that has the power to make only policy recommendations. An agency spokesman said \"It's worth taking a look and seeing what we can learn from that event, so that as that automation is more widely introduced we can do it in the safest way possible.\". In January 2017, the NTSB released the report that concluded Tesla was not at fault; the investigation revealed that for Tesla cars, the crash rate dropped by 40 percent after Autopilot was installed.\n\nAccording to Tesla, starting 19 October 2016, all Tesla cars are built with hardware to allow full self-driving capability at the highest safety level (SAE Level 5). The hardware includes eight surround cameras and twelve ultrasonic sensors, in addition to the forward-facing radar with enhanced processing capabilities. The system will operate in \"shadow mode\" (processing without taking action) and send data back to Tesla to improve its abilities until the software is ready for deployment via over-the-air upgrades. After the required testing, Tesla hopes to enable full self-driving by the end of 2019 under certain conditions.\n\nIn August 2012, Alphabet (then Google) announced that their vehicles had completed over 300,000 autonomous-driving miles (500,000 km) accident-free, typically involving about a dozen cars on the road at any given time, and that they were starting to test with single drivers instead of in pairs. In late-May 2014, Alphabet revealed a new prototype that had no steering wheel, gas pedal, or brake pedal, and was fully autonomous. , Alphabet had test-driven their fleet in autonomous mode a total of . In December 2016, Alphabet Corporation announced that its technology would be spun-off to a new subsidiary called Waymo.\n\nBased on Alphabet's accident reports, their test cars have been involved in 14 collisions, of which other drivers were at fault 13 times, although in 2016 the car's software caused a crash.\n\nIn June 2015, Brin confirmed that 12 vehicles had suffered collisions as of that date. Eight involved rear-end collisions at a stop sign or traffic light, two in which the vehicle was side-swiped by another driver, one in which another driver rolled through a stop sign, and one where a Google employee was controlling the car manually. In July 2015, three Google employees suffered minor injuries when their vehicle was rear-ended by a car whose driver failed to brake at a traffic light. This was the first time that a collision resulted in injuries. On 14 February 2016 a Waymo vehicle attempted to avoid sandbags blocking its path. During the maneuver it struck a bus. Alphabet stated, \"In this case, we clearly bear some responsibility, because if our car hadn’t moved there wouldn’t have been a collision.\" Google characterized the crash as a misunderstanding and a learning experience.\n\nIn March 2017, an Uber test vehicle was involved in an accident in Arizona when another car failed to yield, flipping the Uber vehicle.\n\nIf fully autonomous cars become commercially available, they have the potential to be a disruptive innovation with major implications for society. The likelihood of widespread adoption is still unclear, but if they are used on a wide scale, policy makers face a number of unresolved questions about their effects.\n\nOne fundamental question is about their effect on travel behavior. Some people believe that they will increase car ownership and car use because it will become easier to use them and they will ultimately be more useful. This may in turn encourage urban sprawl and ultimately total private vehicle use. Others argue that it will be easier to share cars and that this will thus discourage outright ownership and decrease total usage, and make cars more efficient forms of transportation in relation to the present situation.\n\nPolicy-makers will have to take a new look at how infrastructure is to be built and how money will be allotted to build for autonomous vehicles. The need for traffic signals could potentially be reduced with the adoption of smart highways. Due to smart highways and with the assistance of smart technological advances implemented by policy change, the dependence on oil imports may be reduced because of less time being spent on the road by individual cars which could have an effect on policy regarding energy. On the other hand, autonomous vehicles could increase the overall number of cars on the road which could lead to a greater dependence on oil imports if smart systems are not enough to curtail the impact of more vehicles. However, due to the uncertainty of the future of autonomous vehicles, policy makers may want to plan effectively by implementing infrastructure improvements that can be beneficial to both human drivers and autonomous vehicles. Caution needs to be taken in acknowledgment to public transportation and that the use may be greatly reduced if autonomous vehicles are catered to through policy reform of infrastructure with this resulting in job loss and increased unemployment.\n\nOther disruptive effects will come from the use of autonomous vehicles to carry goods. s have the potential to make home deliveries significantly cheaper, transforming retail commerce and possibly making hypermarkets and supermarkets redundant. As of right now the U.S. Government defines automation into six levels, starting at level zero which means the human driver does everything and ending with level five, the automated system performs all the driving tasks. Also under the current law, manufacturers bear all the responsibility to self-certify vehicles for use on public roads. This means that currently as long as the vehicle is compliant within the regulatory framework, there are no specific federal legal barriers to a highly automated vehicle being offered for sale. Iyad Rahwan, an associate professor in the MIT Media lab said, \"Most people want to live in a world where cars will minimize casualties, but everyone wants their own car to protect them at all costs.\" Furthermore, industry standards and best practice are still needed in systems before they can be considered reasonably safe under real-world conditions.\n\nThe 1968 Vienna Convention on Road Traffic, subscribed to by over 70 countries worldwide, establishes principles to govern traffic laws. One of the fundamental prinicples of the Convention has been the concept that a driver is always fully in control and responsible for the behavior of a vehicle in traffic. The progress of technology that assists and takes over the functions of the driver is undermining this principle, implying that much of the groundwork must be rewritten.\nIn the United States, a non-signatory country to the Vienna Convention, state vehicle codes generally do not envisage — but do not necessarily prohibit — highly automated vehicles. To clarify the legal status of and otherwise regulate such vehicles, several states have enacted or are considering specific laws. In 2016, 7 states (Nevada, California, Florida, Michigan, Hawaii, Washington, and Tennessee), along with the District of Columbia, have enacted laws for autonomous vehicles. Incidents such as the first fatal accident by Tesla's Autopilot system have led to discussion about revising laws and standards for autonomous cars.\n\nIn September 2016, the US National Economic Council and Department of Transportation released federal standards that describe how automated vehicles should react if their technology fails, how to protect passenger privacy, and how riders should be protected in the event of an accident. The new federal guidelines are meant to avoid a patchwork of state laws, while avoiding being so overbearing as to stifle innovation.\n\nIn June 2011, the Nevada Legislature passed a law to authorize the use of autonomous cars. Nevada thus became the first jurisdiction in the world where autonomous vehicles might be legally operated on public roads. According to the law, the Nevada Department of Motor Vehicles (NDMV) is responsible for setting safety and performance standards and the agency is responsible for designating areas where autonomous cars may be tested. This legislation was supported by Google in an effort to legally conduct further testing of its Google driverless car. The Nevada law defines an autonomous vehicle to be \"a motor vehicle that uses artificial intelligence, sensors and global positioning system coordinates to drive itself without the active intervention of a human operator.\" The law also acknowledges that the operator will not need to pay attention while the car is operating itself. Google had further lobbied for an exemption from a ban on distracted driving to permit occupants to send text messages while sitting behind the wheel, but this did not become law. Furthermore, Nevada's regulations require a person behind the wheel and one in the passenger’s seat during tests.\n\nIn 2013, the government of the United Kingdom permitted the testing of autonomous cars on public roads. Before this, all testing of robotic vehicles in the UK had been conducted on private property.\n\nIn 2014 the Government of France announced that testing of autonomous cars on public roads would be allowed in 2015. 2000 km of road would be opened through the national territory, especially in Bordeaux, in Isère, Île-de-France and Strasbourg. At the 2015 ITS World Congress, a conference dedicated to intelligent transport systems, the very first demonstration of autonomous vehicles on open road in France was carried out in Bordeaux in early October 2015.\n\nIn 2015, a preemptive lawsuit against various automobile companies such as GM, Ford, and Toyota accused them of \"Hawking vehicles that are vulnerable to hackers who could hypothetically wrest control of essential functions such as brakes and steering.\"\n\nIn spring of 2015, the Federal Department of Environment, Transport, Energy and Communications in Switzerland (UVEK) allowed Swisscom to test a driverless Volkswagen Passat on the streets of Zurich.\n\nOn 19 February 2016, Assembly Bill No. 2866 was introduced in California that would allow completely autonomous vehicles to operate on the road, including those without a driver, steering wheel, accelerator pedal, or brake pedal. The Bill states the Department of Motor Vehicles would need to comply with these regulations by 1 July 2018 for these rules to take effect. This bill has yet to pass the house of origin.\n\nIn 2016, the Singapore Land Transit Authority in partnership with UK automotive supplier Delphi Automotive Plc will launch preparations for a test run of a fleet of automated taxis for an on-demand autonomous cab service to take effect in 2017.\n\nIn September 2016, the U.S. Department of Transportation released its Federal Automated Vehicles Policy, and California published discussions on the subject in October 2016.\n\nIn December 2016, the California Department of Motor Vehicles ordered Uber to remove its self-driving vehicles from the road in response to two red-light violations. Uber immediately blamed the violations on \"human-error\", and has suspended the drivers.\n\nIndividual vehicles may benefit from information obtained from other vehicles in the vicinity, especially information relating to traffic congestion and safety hazards. Vehicular communication systems use vehicles and roadside units as the communicating nodes in a peer-to-peer network, providing each other with information. As a cooperative approach, vehicular communication systems can allow all cooperating vehicles to be more effective. According to a 2010 study by the National Highway Traffic Safety Administration, vehicular communication systems could help avoid up to 79 percent of all traffic accidents.\n\nIn 2012, computer scientists at the University of Texas in Austin began developing smart intersections designed for autonomous cars. The intersections will have no traffic lights and no stop signs, instead using computer programs that will communicate directly with each car on the road.\n\nAmong connected cars, an unconnected one is the weakest link and will be increasingly banned from busy high-speed roads, predicted a Helsinki think tank in January 2016.\n\nIn a 2011 online survey of 2,006 US and UK consumers by Accenture, 49% said they would be comfortable using a \"driverless car\".\n\nA 2012 survey of 17,400 vehicle owners by J.D. Power and Associates found 37% initially said they would be interested in purchasing a fully autonomous car. However, that figure dropped to 20% if told the technology would cost $3,000 more.\n\nIn a 2012 survey of about 1,000 German drivers by automotive researcher Puls, 22% of the respondents had a positive attitude towards these cars, 10% were undecided, 44% were skeptical and 24% were hostile.\n\nA 2013 survey of 1,500 consumers across 10 countries by Cisco Systems found 57% \"stated they would be likely to ride in a car controlled entirely by technology that does not require a human driver\", with Brazil, India and China the most willing to trust autonomous technology.\n\nIn a 2014 US telephone survey by Insurance.com, over three-quarters of licensed drivers said they would at least consider buying a self-driving car, rising to 86% if car insurance were cheaper. 31.7% said they would not continue to drive once an autonomous car was available instead.\n\nIn a February 2015 survey of top auto journalists, 46% predict that either Tesla or Daimler will be the first to the market with a fully autonomous vehicle, while (at 38%) Daimler is predicted to be the most functional, safe, and in-demand autonomous vehicle.\n\nIn 2015 a questionnaire survey by Delft University of Technology explored the opinion of 5,000 people from 109 countries on automated driving. Results showed that respondents, on average, found manual driving the most enjoyable mode of driving. 22% of the respondents did not want to spend any money for a fully automated driving system. Respondents were found to be most concerned about software hacking/misuse, and were also concerned about legal issues and safety. Finally, respondents from more developed countries (in terms of lower accident statistics, higher education, and higher income) were less comfortable with their vehicle transmitting data. The survey also gave results on potential consumer opinion on interest of purchasing an automated car, stating that 37% of surveyed current owners were either \"definitely\" or \"probably\" interested in purchasing an automated car.\n\nIn 2016, a survey in Germany examined the opinion of 1,603 people, who were representative in terms of age, gender, and education for the German population, towards partially, highly, and fully automated cars. Results showed that men and women differ in their willingness to use them. Men felt less anxiety and more joy towards automated cars, whereas women showed the exact opposite. The gender difference towards anxiety was especially pronounced between young men and women but decreased with participants’ age.\n\nIn 2016, a PwC survey, in the United States, showing the opinion of 1,584 people, highlights that \"66 percent of respondents said they think autonomous cars are probably smarter than the average human driver\". People are still worried about safety and mostly the fact of having the car hacked. Nevertheless, only 13% of the interviewees see no advantages in this new kind of cars.\n\nWith the emergence of autonomous cars, there are various ethical issues arising. While morally, the introduction of autonomous vehicles to the mass market seems inevitable due to a reduction of crashes by up to 90% and their accessibility to disabled, elderly, and young passengers, there still remain some ethical issues that have not yet been fully solved. Those include, but are not limited to: the moral, financial, and criminal responsibility for crashes, the decisions a car is to make right before a (fatal) crash, privacy issues, and potential job loss.\n\nThere are different opinions on who should be held liable in case of a crash, in particular with people being hurt. Many experts see the car manufacturers themselves responsible for those crashes that occur due to a technical malfunction or misconstruction. Besides the fact that the car manufacturer would be the source of the problem in a situation where a car crashes due to a technical issue, there is another important reason why car manufacturers could be held responsible: it would encourage them to innovate and heavily invest into fixing those issues, not only due to protection of the brand image, but also due to financial and criminal consequences. However, there are also voices that argue those using or owning the vehicle should be held responsible since they know the risks involved in using such a vehicle. Experts suggest introducing a tax or insurances that would protect owners and users of autonomous vehicles of claims made by victims of an accident. Other possible parties that can be held responsible in case of a technical failure include software engineers that programmed the code for the autonomous operation of the vehicles, and suppliers of components of the AV.\n\nTaking aside the question of legal liability and moral responsibility, the question arises how autonomous vehicles should be programmed to behave in an emergency situation where either passengers or other traffic participants are endangered. A very visual example of the moral dilemma that a software engineer or car manufacturer might face in programming the operating software is described in an ethical thought experiment, the trolley problem: a conductor of a trolley has the choice of staying on the planned track and running over 5 people, or turn the trolley onto a track where it would kill only one person, assuming there is no traffic on it. There are two main considerations that need to be addressed. First, what moral basis would be used by an autonomous vehicle to make decisions? Second, how could those be translated into software code? Researchers have suggested, in particular, two ethical theories to be applicable to the behavior of autonomous vehicles in cases of emergency: deontology and utilitarianism. Asimov’s three laws of robotics are a typical example of deontological ethics. The theory suggests that an autonomous car needs to follow strict written-out rules that it needs to follow in any situation. Utilitarianism suggests the idea that any decision must be made based on the goal to maximize utility. This needs a definition of utility which could be maximizing the number of people surviving in a crash. Critics suggest that autonomous vehicles should adapt a mix of multiple theories to be able to respond morally right in the instance of a crash.\n\nPrivacy-related issues arise mainly from the interconnectivity of autonomous cars, making it just another mobile device that can gather any information about an individual. This information gathering ranges from tracking of the routes taken, voice recording, video recording, preferences in media that is consumed in the car, behavioral patterns, to many more streams of information.\n\nThe implementation of autonomous vehicles to the mass market might cost up to 5 million jobs in the US alone, making up almost 3% of the workforce. Those jobs include drivers of taxis, buses, vans, trucks, and e-hailing vehicles. Many industries, such as the auto insurance industry are indirectly affected. This industry alone generates an annual revenue of about $220 billions, supporting 277,000 jobs. To put this into perspective – this is about the number of mechanical engineering jobs. The potential loss of a majority of those jobs due to an estimated decline of accidents by up to 90% will have a tremendous impact on those individuals involved. Both India and China have placed bans on automated cars with the former citing protection of jobs.\n\n\n\nIntelligent or self-driving cars are a common theme in science fiction literature. Examples include:\n\n\n\n\n",
    "id": "245926",
    "title": "Autonomous car"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1006293",
    "text": "Biorobotics\n\nBiorobotics is a term that loosely covers the fields of cybernetics, bionics and even genetic engineering as a collective study.\n\nBiorobotics is often used to refer to a real subfield of robotics: studying how to make robots that emulate or simulate living biological organisms mechanically or even chemically. \n\nThe term is also used in a reverse definition: making biological organisms as manipulatable and functional as robots, or making biological organisms as components of robots. In the latter sense, biorobotics can be referred to as a theoretical discipline of comprehensive genetic engineering in which organisms are created and designed by artificial means. The creation of life from non-living matter for example, would be biorobotics. The field is in its infancy and is sometimes known as synthetic biology or bionanotechnology.\n\nBio-inspired robotics is the practice of making robots that are inspired by real biological systems, while being simpler and more effective. In contrast, the resemblance of animatronics to biological organisms is usually only in general shape and form.\n\nOrel V.E. invented the device of mechanochemiemission microbiorobotics. The phenomenon of mechanochemiemission is related to the processes interconversion of mechanical, chemical, electromagnetic energy in the mitochondria. Microbiorobot may be used for treatment of cancer patients.\n\nA biological brain, grown from cultured neurons which were originally separated, has been developed as the neurological entity subsequently embodied within a robot body by Kevin Warwick and his team at University of Reading. The brain receives input from sensors on the robot body and the resultant output from the brain provides the robot's only motor signals. The biological brain is the only brain of the robot.\n\n",
    "id": "1006293",
    "title": "Biorobotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=58900",
    "text": "Unmanned aerial vehicle\n\nAn unmanned aerial vehicle (UAV), commonly known as a drone, is an aircraft without a human pilot aboard. UAVs are a component of an unmanned aircraft system (UAS); which include a UAV, a ground-based controller, and a system of communications between the two. The flight of UAVs may operate with various degrees of autonomy: either under remote control by a human operator or autonomously by onboard computers.<ref name=\"ICAO's circular 328 AN/190\"></ref>\n\nCompared to manned aircraft, UAVs were originally used for missions too \"dull, dirty or dangerous\" for humans. While they originated mostly in military applications, their use is rapidly expanding to commercial, scientific, recreational, agricultural, and other applications, such as policing, peacekeeping, and surveillance, product deliveries, aerial photography, agriculture, smuggling, and drone racing. Civilian UAVs now vastly outnumber military UAVs, with estimates of over a million sold by 2015, so they can be seen as an early commercial application of autonomous things, to be followed by the autonomous car and home robots.\n\nMultiple terms are used for unmanned aerial vehicles, which generally refer to the same concept.\n\nThe term drone, more widely used by the public, was coined in reference to the early remotely-flown target aircraft used for practice firing of a battleship's guns, and the term was first used with the 1920's Fairey Queen and 1930's de Havilland Queen Bee target aircraft. These two were followed in service by the similarly-named Airspeed Queen Wasp and Miles Queen Martinet, before ultimate replacement by the GAF Jindivik.\n\nThe term \"unmanned aircraft system\" (UAS) was adopted by the United States Department of Defense (DoD) and the United States Federal Aviation Administration in 2005 according to their Unmanned Aircraft System Roadmap 2005–2030. The International Civil Aviation Organization (ICAO) and the British Civil Aviation Authority adopted this term, also used in the European Union's Single-European-Sky (SES) Air-Traffic-Management (ATM) Research (SESAR Joint Undertaking) roadmap for 2020. This term emphasizes the importance of elements other than the aircraft. It includes elements such as ground control stations, data links and other support equipment. A similar term is an \"unmanned-aircraft vehicle system\" (UAVS) \"remotely piloted aerial vehicle\" (RPAV), \"remotely piloted aircraft system\" (RPAS). Many similar terms are in use.\n\nA UAV is defined as a \"powered, aerial vehicle that does not carry a human operator, uses aerodynamic forces to provide vehicle lift, can fly autonomously or be piloted remotely, can be expendable or recoverable, and can carry a lethal or nonlethal payload\". Therefore, missiles are not considered UAVs because the vehicle itself is a weapon that is not reused, though it is also unmanned and in some cases remotely guided.\n\nThe relation of UAVs to remote controlled model aircraft is unclear. UAVs may or may not include model aircraft. Some jurisdictions base their definition on size or weight, however, the US Federal Aviation Administration defines any unmanned flying craft as a UAV regardless of size. For recreational uses, a drone (as apposed to a UAV) is a model aircraft that has first person video, autonomous capabilities or both.\n\nIn 1849 Austria sent unmanned, bomb-filled balloons to attack Venice. UAV innovations started in the early 1900s and originally focused on providing practice targets for training military personnel.\n\nUAV development continued during World War I, when the Dayton-Wright Airplane Company invented a pilotless aerial torpedo that would explode at a preset time.\n\nThe earliest attempt at a powered UAV was A. M. Low's \"Aerial Target\" in 1916. Nikola Tesla described a fleet of unmanned aerial combat vehicles in 1915. Advances followed during and after World War I, including the Hewitt-Sperry Automatic Airplane. The first scaled remote piloted vehicle was developed by film star and model-airplane enthusiast Reginald Denny in 1935. More emerged during World War II – used both to train antiaircraft gunners and to fly attack missions. Nazi Germany produced and used various UAV aircraft during the war. Jet engines entered service after World War II in vehicles such as the Australian GAF Jindivik, and Teledyne Ryan Firebee I of 1951, while companies like Beechcraft offered their Model 1001 for the U.S. Navy in 1955. Nevertheless, they were little more than remote-controlled airplanes until the Vietnam War.\n\nIn 1959, the U.S. Air Force, concerned about losing pilots over hostile territory, began planning for the use of unmanned aircraft. Planning intensified after the Soviet Union shot down a U-2 in 1960. Within days, a highly classified UAV program started under the code name of \"Red Wagon\". The August 1964 clash in the Tonkin Gulf between naval units of the U.S. and North Vietnamese Navy initiated America's highly classified UAVs (Ryan Model 147, Ryan AQM-91 Firefly, Lockheed D-21) into their first combat missions of the Vietnam War. When the Chinese government showed photographs of downed U.S. UAVs via \"Wide World Photos\", the official U.S. response was \"no comment\".\n\nThe War of Attrition (1967–1970) featured the introduction of UAVs with reconnaissance cameras into combat in the Middle East.\n\nIn the 1973 Yom Kippur War Israel used UAVs as decoys to spur opposing forces into wasting expensive anti-aircraft missiles.\n\nIn 1973 the U.S. military officially confirmed that they had been using UAVs in Southeast Asia (Vietnam). Over 5,000 U.S. airmen had been killed and over 1,000 more were missing or captured. The USAF 100th Strategic Reconnaissance Wing flew about 3,435 UAV missions during the war at a cost of about 554 UAVs lost to all causes. In the words of USAF General George S. Brown, Commander, Air Force Systems Command, in 1972, \"The only reason we need (UAVs) is that we don't want to needlessly expend the man in the cockpit.\" Later that year, General John C. Meyer, Commander in Chief, Strategic Air Command, stated, \"we let the drone do the high-risk flying ... the loss rate is high, but we are willing to risk more of them ... they save lives!\"\n\nDuring the 1973 Yom Kippur War, Soviet-supplied surface-to-air missile batteries in Egypt and Syria caused heavy damage to Israeli fighter jets. As a result, Israel developed the first UAV with real-time surveillance. The images and radar decoys provided by these UAVs helped Israel to completely neutralize the Syrian air defenses at the start of the 1982 Lebanon War, resulting in no pilots downed. The first time UAVs were used as proof-of-concept of super-agility post-stall controlled flight in combat-flight simulations involved tailless, stealth technology-based, three-dimensional thrust vectoring flight control, jet-steering UAVs in Israel in 1987.\n\nWith the maturing and miniaturization of applicable technologies in the 1980s and 1990s, interest in UAVs grew within the higher echelons of the U.S. military. In the 1990s, the U.S. DoD gave a contract to AAI Corporation along with Israeli company Malat. The U.S. Navy bought the AAI Pioneer UAV that AAI and Malat developed jointly. Many of these UAVs saw service in the 1991 Gulf War. UAVs demonstrated the possibility of cheaper, more capable fighting machines, deployable without risk to aircrews. Initial generations primarily involved surveillance aircraft, but some carried armaments, such as the General Atomics MQ-1 Predator, that launched AGM-114 Hellfire air-to-ground missiles.\n\nCAPECON was a European Union project to develop UAVs, running from 1 May 2002 to 31 December 2005.\n\nAs of 2012, the USAF employed 7,494 UAVs – almost one in three USAF aircraft. The Central Intelligence Agency also operated UAVs.\n\nIn 2013 at least 50 countries used UAVs. China, Iran, Israel and others designed and built their own varieties.\n\nUAVs typically fall into one of six functional categories (although multi-role airframe platforms are becoming more prevalent):\nThe U.S. Military UAV tier system is used by military planners to designate the various individual aircraft elements in an overall usage plan.\n\nVehicles can be categorised in terms of range/altitude. The following has been advanced as relevant at industry events such as ParcAberporth Unmanned Systems forum:\nOther categories include:\nClassifications according to aircraft weight are quite simpler:\n\nManned and unmanned aircraft of the same type generally have recognizably similar physical components. The main exceptions are the cockpit and environmental control system or life support systems. Some UAVs carry payloads (such as a camera) that weigh considerably less than an adult human, and as a result can be considerably smaller. Though they carry heavy payloads, weaponized military UAVs are lighter than their manned counterparts with comparable armaments.\n\nSmall civilian UAVs have no life-critical systems, and can thus be built out of lighter but less sturdy materials and shapes, and can use less robustly tested electronic control systems. For small UAVs, the quadcopter design has become popular, though this layout is rarely used for manned aircraft. Miniaturization means that less-powerful propulsion technologies can be used that are not feasible for manned aircraft, such as small electric motors and batteries.\n\nControl systems for UAVs are often different than manned craft. For remote human control, a camera and video link almost always replace the cockpit windows; radio-transmitted digital commands replace physical cockpit controls. Autopilot software is used on both manned and unmanned aircraft, with varying feature sets.\n\nThe primary difference for planes is the absence of the cockpit area and its windows. Tailless quadcopters are a common form factor for rotary wing UAVs while tailed mono- and bi-copters are common for manned platforms.\n\nSmall UAVs mostly use lithium-polymer batteries (Li-Po), while larger vehicles rely on conventional airplane engines.\n\nBattery elimination circuitry (BEC) is used to centralize power distribution and often harbors a microcontroller unit (MCU). Costlier switching BECs diminish heating on the platform.\n\nUAV computing capability followed the advances of computing technology, beginning with analog controls and evolving into microcontrollers, then system-on-a-chip (SOC) and single-board computers (SBC).\n\nSystem hardware for small UAVs is often called the Flight Controller (FC), Flight Controller Board (FCB) or Autopilot.\n\nPosition and movement sensors give information about the aircraft state. Exteroceptive sensors deal with external information like distance measurements, while exproprioceptive ones correlate internal and external states.\n\nNon-cooperative sensors are able to detect targets autonomously so they are used for separation assurance and collision avoidance.\n\nDegrees of freedom (DOF) refer to both the amount and quality of sensors on-board: 6 DOF implies 3-axis gyroscopes and accelerometers (a typical inertial measurement unit – IMU), 9 DOF refers to an IMU plus a compass, 10 DOF adds a barometer and 11 DOF usually adds a GPS receiver.\n\nUAV actuators include digital electronic speed controllers (which control the RPM of the motors) linked to motors/engines and propellers, servomotors (for planes and helicopters mostly), weapons, payload actuators, LEDs and speakers.\n\nUAV software called the flight stack or autopilot. UAVs are real-time systems that require rapid response to changing sensor data. Examples include Raspberry Pis, Beagleboards, etc. shielded with NavIO, PXFMini, etc. or designed from scratch such as Nuttx, preemptive-RT Linux, Xenomai, Orocos-Robot Operating System or DDS-ROS 2.0.\n\nList of civil-use open-source stacks include:\n\nUAVs employ open-loop, closed-loop or hybrid control architectures.\n\nFlight control is one of the lower-layer system and is similar to manned aviation: plane flight dynamics, control and automation, helicopter flight dynamics and controls and multirotor flight dynamics were researched long before the rise of UAVs.\n\nAutomatic flight involves multiple levels of priority.\n\nUAVs can be programmed to perform aggressive manœuvres or landing/perching on inclined surfaces, and then to climb toward better communication spots. Some UAVs can control flight with varying flight modelisation, such as VTOL designs.\n\nUAVs can also implement perching on a flat vertical surface.\n\nMost UAVs use a radio frequency front-end that connects the antenna to the analog-to-digital converter and a flight computer that controls avionics (and that may be capable of autonomous or semi-autonomous operation).\n\nRadio allows remote control and exchange of video and other data. Early UAVs had only uplink. Downlinks (e.g., realtime video) came later.\n\nIn military systems and high-end domestic applications, downlink may convey payload management status. In civilian applications, most transmissions are commands from operator to vehicle. Downstream is mainly video. Telemetry is another kind of downstream link, transmitting status about the aircraft systems to the remote operator. UAVs use also satellite \"uplink\" to access satellite navigation systems.\n\nThe radio signal from the operator side can be issued from either:\n\nICAO classifies unmanned aircraft as either remotely piloted aircraft or fully autonomous. Actual UAVs may offer intermediate degrees of autonomy. E.g., a vehicle that is remotely piloted in most contexts may have an autonomous return-to-base operation.\n\nBasic autonomy comes from proprioceptive sensors. Advanced autonomy calls for situational awareness, knowledge about the environment surrounding the aircraft from exterioceptive sensors: sensor fusion integrates information from multiple sensors.\n\nOne way to achieve autonomous control employs multiple control-loop layers, as in hierarchical control systems. As of 2016 the low-layer loops (i.e. for flight control) tick as fast as 32,000 times per second, while higher-level loops may cycle once per second. The principle is to decompose the aircraft's behavior into manageable \"chunks\", or states, with known transitions. Hierarchical control system types range from simple scripts to finite state machines, behavior trees and hierarchical task planners. The most common control mechanism used in these layers is the PID controller which can be used to achieve hover for a quadcopter by using data from the IMU to calculate precise inputs for the electronic speed controllers and motors.\n\nExamples of mid-layer algorithms:\nEvolved UAV hierarchical task planners use methods like state tree searches or genetic algorithms.\n\nUAV manufacturers often build in specific autonomous operations, such as:\n\nFull autonomy is available for specific tasks, such as airborne refueling or ground-based battery switching; but higher-level tasks call for greater computing, sensing and actuating capabilities. One approach to quantifying autonomous capabilities is based on OODA terminology, as suggested by a 2002 US Air Force Research Laboratory, and used in the table below:\n\nReactive autonomy, such as collective flight, real-time collision avoidance, wall following and corridor centring, relies on telecommunication and situational awareness provided by range sensors: optic flow, lidars (light radars), radars, sonars.\n\nMost range sensors analyze electromagnetic radiation, reflected off the environment and coming to the sensor. The cameras (for visual flow) act as simple receivers. Lidars, radars and sonars (with sound mechanical waves) emit and receive waves, measuring the round-trip transit time. UAV cameras do not require emitting power, reducing total consumption.\n\nRadars and sonars are mostly used for military applications.\n\nReactive autonomy has in some forms already reached consumer markets: it may be widely available in less than a decade.\n\nSLAM combines odometry and external data to represent the world and the position of the UAV in it in three dimensions. High-altitude outdoor navigation does not require large vertical fields-of-view and can rely on GPS coordinates (which makes it simple mapping rather than SLAM).\n\nTwo related research fields are photogrammetry and LIDAR, especially in low-altitude and indoor 3D environments.\n\nRobot swarming refers to networks of agents able to dynamically reconfigure as elements leave or enter the network. They provide greater flexibility than multi-agent cooperation. Swarming may open the path to data fusion. Some bio-inspired flight swarms use steering behaviors and flocking.\n\nIn the military sector, American Predators and Reapers are made for counterterrorism operations and in war zones in which the enemy lacks sufficient firepower to shoot them down. They are not designed to withstand antiaircraft defenses or air-to-air combat. In September 2013, the chief of the US Air Combat Command stated that current UAVs were \"useless in a contested environment\" unless manned aircraft were there to protect them.[167] A 2012 Congressional Research Service (CRS) report speculated that in the future, UAVs may be able to perform tasks beyond intelligence, surveillance, reconnaissance and strikes; the CRS report listed air-to-air combat (\"a more difficult future task\") as possible future undertakings.[168] The Department of Defense's Unmanned Systems Integrated Roadmap FY2013-2038 foresees a more important place for UAVs in combat.[169] Issues include extended capabilities, human-UAV interaction, managing increased information flux, increased autonomy and developing UAV-specific munitions.[169] DARPA's project of systems of systems, or General Atomics work may augur future warfare scenarios, the latter disclosing Avenger swarms equipped with High Energy Liquid Laser Area Defense System (HELLADS).\n\nCognitive radio technology may have UAV applications.\n\nUAVs may exploit distributed neural networks.\n\nThe UAV global military market is dominated by pioneers United States and Israel. The US held a 60% military-market share in 2006. It operated over 9,000 UAVs in 2014. From 1985 to 2014, exported UAVs came predominantly from Israel (60.7%) and the United States (23.9%); top importers were The United Kingdom (33.9%) and India (13.2%). Northrop Grumman and General Atomics are the dominant manufacturers on the strength of the Global Hawk and Predator/Mariner systems.\n\nThe leading civil UAV companies are currently (Chinese) DJI with $500m global sales, (French) Parrot with $110m and (US) 3DRobotics with $21.6m in 2014. As of March 2017, more than 770,000 civilian UAVs were registered with the U.S. FAA, though it is estimated more than 1.1 million have been sold in the United States alone.\n\nUAV companies are also emerging in developing nations such as India for civilian use, although it is at a very nascent stage, a few early stage startups have received support and funding.\n\nSome universities offer research and training programs or degrees. Private entities also provide online and in-person training programs for both recreational and commercial UAV use.\n\nFlapping-wing ornithopters, imitating birds or insects, are a research field in microUAVs. Their inherent stealth recommends them for spy missions.\n\nThe Nano Hummingbird is commercially available, while sub-1g microUAVs inspired by flies, albeit using a power tether, can \"land\" on vertical surfaces.\n\nOther projects include unmanned \"beetles\" and other insects.\n\nResearch is exploring miniature optic-flow sensors, called ocellis, mimicking the compound insect eyes formed from multiple facets, which can transmit data to neuromorphic chips able to treat optic flow as well as light intensity discrepancies.\n\nUAV endurance is not constrained by the physiological capabilities of a human pilot.\n\nBecause of their small size, low weight, low vibration and high power to weight ratio, Wankel rotary engines are used in many large UAVs. Their engine rotors cannot seize; the engine is not susceptible to shock-cooling during descent and it does not require an enriched fuel mixture for cooling at high power. These attributes reduce fuel usage, increasing range or payload.\n\nHydrogen fuel cells, using hydrogen power, may be able to extend the endurance of small UAVs, up to several hours.\n\nMicro air vehicles endurance is so far best achieved with flapping-wing UAVs, followed by planes and multirotors standing last, due to lower Reynolds number.\n\nSolar-electric UAVs, a concept originally championed by the AstroFlight Sunrise in 1974, have achieved flight times of several weeks.\n\nSolar-powered atmospheric satellites (\"atmosats\") designed for operating at altitudes exceeding 20 km (12 miles, or 60,000 feet) for as long as five years could potentially perform duties more economically and with more versatility than low earth orbit satellites. Likely applications include weather monitoring, disaster recovery, earth imaging and communications.\n\nElectric UAVs powered by microwave power transmission or laser power beaming are other potential endurance solutions.\n\nAnother application for a high endurance UAV would be to \"stare\" at a battlefield for a long interval (ARGUS-IS, Gorgon Stare, Integrated Sensor Is Structure) to record events that could then be played backwards to track battlefield activities.\n\nReliability improvements target all aspects of UAV systems, using resilience engineering and fault tolerance techniques.\n\nIndividual reliability covers robustness of flight controllers, to ensure safety without excessive redundancy to minimize cost and weight. Besides, dynamic assessment of flight envelope allows damage-resilient UAVs, using non-linear analysis with ad-hoc designed loops or neural networks. UAV software liability is bending toward the design and certifications of manned avionics software.\n\nSwarm resilience involves maintaining operational capabilities and reconfiguring tasks given unita failures.\n\nThere are numerous civilian, commercial, military, and aerospace applications for UAVs. These include:\n\n\nUAVs are being developed and deployed by many countries around the world. Due to their wide proliferation, no comprehensive list of UAV systems exists.\n\nThe export of UAVs or technology capable of carrying a 500 kg payload at least 300 km is restricted in many countries by the Missile Technology Control Regime.\n\nUAVs can threaten airspace security in numerous ways, including unintentional collisions or other interference with other aircraft, deliberate attacks or by distracting pilots or flight controllers. The first incident of a drone-airplane collision occurred in mid-October 2017 in Quebec City, Canada.\n\nUAVs could be loaded with dangerous payloads, and crashed into vulnerable targets. Payloads could include explosives, chemical, radiologial or biological hazards. UAVs with generally non-lethal payloads could possibly be hacked and put to malicious purposes. Anti-UAV systems are being developed by states to counter this threat. This is, however, proving difficult. As Dr J. Rogers stated in an interview to A&T \"There is a big debate out there at the moment about what the best way is to counter these small UAVs, whether they are used by hobbyists causing a bit of a nuisance or in a more sinister manner by a terrorist actor.”\n\nBy 2017, drones were being used to drop contraband into prisons.\n\nThe interest in UAVs cyber security has been raised greatly after the Predator UAV video stream hijacking incident in 2009, where Islamic militants used cheap, off-the-shelf equipment to stream video feeds from a UAV. Another risk is the possibility of hijacking or jamming a UAV in flight. In recent years several security researchers have made public vulnerabilities for commercial UAVs, in some cases even providing full source code or tools to reproduce their attacks. At a workshop on UAVs and privacy in October 2016, researchers from the Federal Trade Commission showed they were able to hack into three different consumer quadcopters and noted that UAV manufacturers can make their UAVs more secure by the basic security measures of encrypting the Wi-Fi signal and adding password protection.\n\nIn the United States, flying close to a wildfire is punishable by a maximum $25,000 fine. Nonetheless, in 2014 and 2015, firefighting air support in California was hindered on several occasions, including at the Lake Fire and the North Fire. In response, California legislators introduced a bill that would allow firefighters to disable UAVs which invaded restricted airspace. The FAA later required registration of most UAVs.\n\nThe use of UAVs is also being investigated to help detect and fight wildfires, whether through observation or launching pyrotechnic devices to start backfires.\n\nEthical concerns and UAV-related accidents have driven nations to regulate the use of UAVs.\n\nThe Irish Aviation Authority (IAA) requires all UAVs over 1 kg must be registered with UAVs weighing 4 kg or more requiring a license to be issued by the IAA.\n\n, the Dutch police is testing trained bald eagles to intercept offending UAVs.\n\nIn 2016 Transport Canada proposed the implementation of new regulations that would require all UAVs over 250 grams to be registered and insured and that operators would be required to be a minimum age and pass an exam in order to get a license. These regulations are expected to be introduced in 2018. (http://www.gazette.gc.ca/rp-pr/p1/2017/2017-07-15/html/reg2-eng.php)\n\nIn April 2014, the South African Civil Aviation Authority announced that it would clamp down on the illegal flying of UAVs in South African airspace. \"Hobby drones\" with a weight of less than 7 kg at altitudes up to 500m with restricted visual line-of-sight below the height of the highest obstacle within 300m of the UAV are allowed. No license is required for such vehicles.\n\nThe ENAC (Ente Nazionale per l'Aviazione Civile), that is, the Italian Civil Aviation Authority for technical regulation, certification, supervision and control in the field of civil aviation, issued on May 31, 2016 a very detailed regulation for all UAV, determining which types of vehicles can be used, where, for which purposes, and who can control them. The regulation deals with the usage of UAV for either commercial and recreational use. Last version was published on December 22, 2016.\n\nFrom 21 December 2015 all hobby type UAV's between 250 grams and 25 kilograms needed to be registered with FAA no later than 19 February 2016.\n\nThe new FAA UAV registration process includes requirements for:\n\nOn May 19, 2017, in the case \"Taylor v. Huerta\", the U.S. Court of Appeals for the District of Columbia Circuit held that that the FAA's 2015 drone registration rules were in violation of the 2012 FAA Modernization and Reform Act. Under the court's holding, although commercial drone operators are required to register, recreational operators are not. On May 25, 2017, one week after the \"Taylor\" decision, Senator Diane Feinstein introduced S. 1272, the Drone Federalism Act of 2017, in Congress.\n\nOn 21 June 2016 the Federal Aviation Administration announced regulations for commercial operation of small UAS craft (sUAS), those between 0.55 and 55 pounds (about 250 gm to 25 kg) including payload. The rules, which exclude hobbyists, require the presence at all operations of a licensed Remote Pilot in Command. Certification of this position, available to any citizen at least 16 years of age, is obtained solely by passing a written test and then submitting an application. For those holding a sport pilot license or higher, and with a current flight review, a rule-specific exam can be taken at no charge online at the faasafety.gov website. Other applicants must take a more comprehensive examination at an aeronautical testing center. All licensees are required to take a review course every two years. At this time no ratings for heavier UAS are available.\n\nCommercial operation is restricted to daylight, line-of-sight, under 100 mph, under 400 feet, and Class G airspace only, and may not fly over people or be operated from a moving vehicle. Some organizations have obtained a waiver or Certificate of Authorization that allows them to exceed these rules. For example, CNN has obtained a waiver for UAVs modified for injury prevention to fly over people, and other waivers allow night flying with special lighting, or non-line-of-sight operations for agriculture or railroad track inspection.\n\nPrevious to this announcement, any commercial use required a full pilot's license and an FAA waiver, of which hundreds had been granted.\n\nThe use of UAVs for law-enforcement purposes is regulated at a state level.\n\nAs of 2015, UAV's under 300g are not controlled by the CAA guidances that include maintaining 50 meters from person, animal or property.\n\nThe UAV must still not go higher than 400 ft with a single pilot or 1000 ft with a pilot and spotter, however as with UAV's above 300g, if within 400 ft of a structure, you are allowed to go 400 ft higher than the structure.\n\n\n\n",
    "id": "58900",
    "title": "Unmanned aerial vehicle"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20903754",
    "text": "Robotics\n\nRobotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electrical engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing.\n\nThese technologies are used to develop machines that can substitute for humans and replicate human actions. Robots can be used in any situation and for any purpose, but today many are used in dangerous environments (including bomb detection and de-activation), manufacturing processes, or where humans cannot survive. Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, and basically anything a human can do. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.\n\nThe concept of creating machines that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (Science, Technology, Engineering, and Mathematics) as a teaching aid.\n\nRobotics is a branch of engineering that involves the conception, design, manufacture, and operation of robots. This field overlaps with electronics, computer science, artificial intelligence, mechatronics, nanotechnology and bioengineering.\n\nScience-fiction author Isaac Asimov is often given credit for being the first person to use the term robotics in a short story composed in the 1940s. In the story, Asimov suggested three principles to guide the behavior of robots and smart machines. Asimov's Three Laws of Robotics, as they are called, have survived to the present:\n1. Robots must never harm human beings.\n2. Robots must follow instructions from humans without violating rule 1.\n3. Robots must protect themselves without violating the other rules.\n\nThe word \"robotics\" was derived from the word \"robot\", which was introduced to the public by Czech writer Karel Čapek in his play \"R.U.R. (Rossum's Universal Robots)\", which was published in 1920. The word \"robot\" comes from the Slavic word \"robota\", which means labour. The play begins in a factory that makes artificial people called \"robots\", creatures who can be mistaken for humans – very similar to the modern ideas of androids. Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the \"Oxford English Dictionary\" in which he named his brother Josef Čapek as its actual originator.\n\nAccording to the \"Oxford English Dictionary\", the word \"robotics\" was first used in print by Isaac Asimov, in his science fiction short story \"Liar!\", published in May 1941 in \"Astounding Science Fiction\". Asimov was unaware that he was coining the term; since the science and technology of electrical devices is \"electronics\", he assumed \"robotics\" already referred to the science and technology of robots. In some of Asimov's other works, he states that the first use of the word \"robotics\" was in his short story \"Runaround\" (Astounding Science Fiction, March 1942). However, the original publication of \"Liar!\" predates that of \"Runaround\" by ten months, so the former is generally cited as the word's origin.\n\nIn 1942, the science fiction writer Isaac Asimov created his Three Laws of Robotics.\n\nIn 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\n\nFully autonomous only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately and more reliably, than humans. They are also employed in some jobs which are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery, weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.\n\nThere are many types of robots; they are used in many different environments and for many different uses, although being very diverse in application and form they all share three basic similarities when it comes to their construction:\n\nAs more and more robots are designed for specific tasks this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed as \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labelled as \"heavy duty robots\".\n\nCurrent and potential applications include:\n\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries that are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need a fuel, require heat dissipation and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage. Potential power sources could be:\n\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement. By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\n\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\n\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator).\n\nA flexure is designed as part of the motor actuator, to improve safety and provide robust force control, energy efficiency, shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. The resultant lower reflected inertia can improve safety when a robot is interacting with humans or during collisions. It has been used in various robots, particularly advanced manufacturing robots and walking humanoid robots.\n\nPneumatic artificial muscles, also known as air muscles, are special tubes that expand(typically up to 40%) when air is forced inside them. They are used in some robot applications.\n\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material which contracts (under 5%) when electricity is applied. They have been used for some small robot applications.\n\nEAPs or EPAMs are a new plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots, and to enable new robots to float, fly, swim or walk.\n\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line. Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size. These motors are already available commercially, and being used on some robots.\n\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.\n\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information of the task it is performing.\n\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips. The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting robotic grip on held objects.\n\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one—allowing patients to write with it, type on a keyboard, play piano and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feeling in its fingertips.\n\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\n\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\n\nComputer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\n\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have their background in biology.\n\nOther common forms of sensing in robotics use lidar, radar, and sonar.\n\nRobots need to manipulate objects; pick up, modify, destroy, or otherwise have an effect. Thus the \"hands\" of a robot are often referred to as \"end effectors\", while the \"arm\" is referred to as a \"manipulator\". Most robot arms have replaceable effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator which cannot be replaced, while a few have one very general purpose manipulator, for example, a humanoid hand.\nLearning how to manipulate a robot often requires a close feedback between human to the robot, although there are several methods for remote manipulation of robots.\n\nOne of the most common effectors is the gripper. In its simplest manifestation, it consists of just two fingers which can open and close to pick up and let go of a range of small objects. Fingers can for example, be made of a chain with a metal wire run through it. Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand. Hands that are of a mid-level complexity include the Delft hand. Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\n\nVacuum grippers are very simple astrictive devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\n\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum grippers.\n\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS, and the Schunk hand. These are highly dexterous manipulators, with as many as 20 degrees of freedom and hundreds of tactile sensors.\n\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\n\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum. Many different balancing robots have been designed. While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.\n\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" that is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\". Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.\n\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball, or by rotating the outer shells of the sphere. These have also been referred to as an orb bot or a ball bot.\n\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\n\nTank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".\n\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University. Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct. Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Hybrids too have been proposed in movies such as \"I, Robot\", where they walk on two legs and switch to four (arms+legs) when going to a sprint. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\n\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over). However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory. ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\n\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself. Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults. A quadruped was also demonstrated which could trot, run, pace, and bound. For a full list of these robots, see the MIT Leg Lab Robots page.\n\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability. This technique was recently demonstrated by Anybots' Dexter Robot, which is so stable, it can even jump. Another example is the TU Delft Flame.\n\nPerhaps the most promising approach utilizes passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.\n\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing. Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, propelled by paddles, and guided by sonar.\n\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings. The Japanese ACM-R5 snake robot can even navigate both on land and in water.\n\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll. Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin, built by Dr. Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot and Stickybot. China's \"Technology Daily\" reported on November 15, 2008, that Dr. Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Dr. Li, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole..\n\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%. Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion. Notable examples are the Essex University Computer Science Robotic Fish G9, and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion. The Aqua Penguin, designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\n\nIn 2014 \"iSplash\"-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, \"iSplash\"-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.\n\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is \"Vaimos\" built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\n\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information. Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\n\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation.\n\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech. The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent. Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952. Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.\n\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium, making it necessary to develop the emotional component of robotic voice through various techniques.\n\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots. A great many systems have been developed to recognize human hand gestures.\n\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos). The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.\n\nArtificial emotions can also be generated, composed of a sequence of facial expressions and/or gestures. As can be seen from the movie , the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots.\n\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future. Nevertheless, researchers are trying to create robots which appear to have a personality: i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.\n\nThe Socially Intelligent Machines Lab of the Georgia Institute of Technology researches new concepts of guided teaching interaction with robots. The aim of the projects is a social robot that learns task and goals from human demonstrations without prior knowledge of high-level concepts. These new concepts are grounded from low-level continuous sensor data through unsupervised learning, and task goals are subsequently learned using a Bayesian approach. These concepts can be used to transfer knowledge to future tasks, resulting in faster learning of those tasks. The results are demonstrated by the robot \"Curi\" who can scoop some pasta from a pot onto a plate and serve the sauce on top.\n\nThe mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors) which move the mechanical.\n\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands. Sensor fusion may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction) is inferred from these estimates. Techniques from control theory convert the task into commands that drive the actuators.\n\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how they interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\n\nControl systems may also have varying levels of autonomy.\n\nAnother classification takes into account the interaction between human control and the machine motions.\n\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\n\nA first particular new innovation in robot design is the open sourcing of robot-projects. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. \"First generation\" robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the \"first generation\" robot would be incapable of learning, however, Moravec predicts that the \"second generation\" robot would be an improvement over the \"first\" and become available by 2020, with the intelligence maybe comparable to that of a mouse. The \"third generation\" robot should have the intelligence comparable to that of a monkey. Though \"fourth generation\" robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.\n\nThe second is evolutionary robots. This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots, and to explore the nature of evolution. Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, then tested on real robots once the evolved algorithms are good enough. Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.\n\nThe study of motion can be divided into kinematics and dynamics. Direct kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\n\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\n\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\n\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students. First-year computer science courses at some universities now include programming of a robot in addition to traditional software engineering-based coursework.\n\nUniversities offer bachelors, masters, and doctoral degrees in the field of robotics. Vocational schools offer robotics training aimed at careers in robotics.\n\nThe Robotics Certification Standards Alliance (RCSA) is an international robotics certification authority that confers various industry- and educational-related robotics certifications.\n\nSeveral national summer camp programs include robotics as part of their core curriculum. In addition, youth summer robotics programs are frequently offered by celebrated museums and institutions.\n\nThere are lots of competitions all around the globe. One of the most important competitions is the FLL or FIRST Lego League. The idea of this specific competition is that kids start developing knowledge and getting into robotics while playing with Legos since they are 9 years old. This competition is associated with Ni or National Instruments.\n\nMany schools across the country are beginning to add robotics programs to their after school curriculum. Some major programs for afterschool robotics include FIRST Robotics Competition, Botball and B.E.S.T. Robotics. Robotics competitions often include aspects of business and marketing as well as engineering and design.\n\nThe Lego company began a program for children to learn and get excited about robotics at a young age.\n\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising. The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long term investment for benefactors. A paper by Michael Osborne and Carl Benedikt Frey found that 47 per cent of US jobs are at risk to automation \"over some unspecified number of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment.\n\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).\n\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defence, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.\n\nDespite these advances, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programmes and trying to promote a safe and flexible co-operation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\n\nIn future, co-operation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised.\n\n\n\n",
    "id": "20903754",
    "title": "Robotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54982564",
    "text": "CloudMinds\n\nCloudMinds () is an operator of cloud-based systems for intelligent robots, founded in the early 2015. It concentrates on realizing a safe cloud computing network for intelligent robots, creating a large and mixed platform for machine learning, safe intelligent terminals, and researching the technologies of realizing robot controller in operations.\n\nFounded in 2015, CloudMinds has created a cloud-based intelligent robot architecture based on the cloud's brain and secure network. It is backed by SoftBank, Foxconn, Walden Venture Investments and Keytone Ventures. CloudMinds has developed in-depth research in cloud intelligence integration, high-speed security network, safe smart devices and robot control technology.\n\nCloudMinds built up Human Augmented Robotics Intelligence (HARI) platform and has applied it to some significant areas such as blind guiding. The operator has created the world's first cloud-based terminal, “DATA” series. And it is now working on constructing the backbone network worldwide (VBN).\n\nBased on these nuclear technologies, CloudMinds has succeeded in realizing the Mobile Intranet Cloud Services (MCS), which contains the function on improving information security of the cloud robot remote controlling. The technology has been widely used in some areas such as financial, medical, military, public security department and large-scale manufacturing industry.\n\nCloudMinds is the first cloud robot operator that builds the platform and services to make robots connect and learn possible. With the tendency that each household will own a robot by 2025, the company determines to develop ground breaking technology in AI, Robotic control, network and cloud security. Mobile Intranet Cloud Services (MCS) platform offers secure device to cloud connection over a high performance low latency private network. Besides, they target to partner with the best minds in AI and smart devices. Their first products include a blind guiding robot and an AI mobile device that can be used as the control unit for generic robots in order to stimulate an end-to-end ecosystem. The products and services are used to solve some complex problems in health care, manufacturing and finance industries.\n\nDATA is a cloud intelligent device that will serve as the controller between the robot and the cloud brain. It implemented hardware virtualization on Qualcomm platform. CloudMinds also implemented GPU virtualization on DATA. DATA uses a self-developed sandwich-like hardware extension architecture, which is based on special hardware interface that enables support for dual LTE.\n\nMETA is the first wearable device based on cloud framework of CloudMinds, which takes the shape of helmet for visually impaired people to support face recognition, object recognition, path planning and obstacle avoidance, etc.\n\nMobile-intranet Cloud Service (MCS) is made up of three parts, XaaS Cloud, VBN and terminal. The design on the terminal contains technologies on dual chips and virtualization, which applies the artificial intelligence to activate the robot through this AI Mobile.\nXaaS Cloud, which is based on the original HARI (Human - Augmented - Robotic - Intelligence) architecture to realize the integration of artificial Intelligence and Human Intelligence and provide image recognition, enterprise collaboration and different types of services. The XaaS Cloud has currently completed the first phase of construction, providing the Cloud service platform of MCS.\nSince August 2015, CloudMinds has begun to construct the backbone network worldwide, which can realize the network path optimization and protocol optimization to provide safe and high-speed Internet service through SDN (software defined network) and SDP (software defined boundary).\n",
    "id": "54982564",
    "title": "CloudMinds"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1006597",
    "text": "Nanorobotics\n\nNanorobotics is an emerging technology field creating machines or robots whose components are at or near the scale of a nanometre (10 meters). More specifically, nanorobotics (as opposed to microrobotics) refers to the nanotechnology engineering discipline of designing and building nanorobots, with devices ranging in size from 0.1–10 micrometres and constructed of nanoscale or molecular components. The terms \"nanobot\", \"nanoid\", \"nanite\", \"nanomachine\", or \"nanomite\" have also been used to describe such devices currently under research and development.\n\nNanomachines are largely in the research and development phase, but some primitive molecular machines and nanomotors have been tested. An example is a sensor having a switch approximately 1.5 nanometers across, able to count specific molecules in a chemical sample. The first useful applications of nanomachines may be in nanomedicine. For example, biological machines could be used to identify and destroy cancer cells. Another potential application is the detection of toxic chemicals, and the measurement of their concentrations, in the environment. Rice University has demonstrated a single-molecule car developed by a chemical process and including Buckminsterfullerenes (buckyballs) for wheels. It is actuated by controlling the environmental temperature and by positioning a scanning tunneling microscope tip.\n\nAnother definition is a robot that allows precise interactions with nanoscale objects, or can manipulate with nanoscale resolution. Such devices are more related to microscopy or scanning probe microscopy, instead of the description of nanorobots as molecular machine. Using the microscopy definition, even a large apparatus such as an atomic force microscope can be considered a nanorobotic instrument when configured to perform nanomanipulation. For this viewpoint, macroscale robots or microrobots that can move with nanoscale precision can also be considered nanorobots.\n\nAccording to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical micromachines (see nanomachine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"\"swallow the surgeon\"\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSince nanorobots would be microscopic in size, it would probably be necessary for very large numbers of them to work together to perform microscopic and macroscopic tasks. These nanorobot swarms, both those unable to replicate (as in utility fog) and those able to replicate unconstrainedly in the natural environment (as in grey goo and its less common variants, such as synthetic biology or utility fog), are found in many science fiction stories, such as the Borg nanoprobes in \"Star Trek\" and \"The Outer Limits\" episode \"The New Breed\".\n\nSome proponents of nanorobotics, in reaction to the grey goo scenarios that they earlier helped to propagate, hold the view that nanorobots able to replicate outside of a restricted factory environment do not form a necessary part of a purported productive nanotechnology, and that the process of self-replication, were it ever to be developed, could be made inherently safe. They further assert that their current plans for developing and using molecular manufacturing do not in fact include free-foraging replicators.\n\nThe most detailed theoretical discussion of nanorobotics, including specific design issues such as sensing, power communication, navigation, manipulation, locomotion, and onboard computation, has been presented in the medical context of nanomedicine by Robert Freitas. Some of these discussions remain at the level of unbuildable generality and do not approach the level of detailed engineering.\n\nA document with a proposal on nanobiotech development using open design technology methods, as in open-source hardware and open-source software, has been addressed to the United Nations General Assembly. According to the document sent to the United Nations, in the same way that open source has in recent years accelerated the development of computer systems, a similar approach should benefit the society at large and accelerate nanorobotics development. The use of nanobiotechnology should be established as a human heritage for the coming generations, and developed as an open technology based on ethical practices for peaceful purposes. Open technology is stated as a fundamental key for such an aim.\n\nIn the same ways that technology research and development drove the space race and nuclear arms race, a race for nanorobots is occurring. There is plenty of ground allowing nanorobots to be included among the emerging technologies. Some of the reasons are that large corporations, such as General Electric, Hewlett-Packard, Synopsys, Northrop Grumman and Siemens have been recently working in the development and research of nanorobots; surgeons are getting involved and starting to propose ways to apply nanorobots for common medical procedures; universities and research institutes were granted funds by government agencies exceeding $2 billion towards research developing nanodevices for medicine; bankers are also strategically investing with the intent to acquire beforehand rights and royalties on future nanorobots commercialisation. Some aspects of nanorobot litigation and related issues linked to monopoly have already arisen. A large number of patents has been granted recently on nanorobots, done mostly for patent agents, companies specialized solely on building patent portfolios, and lawyers. After a long series of patents and eventually litigations, see for example the Invention of Radio, or the War of Currents, emerging fields of technology tend to become a monopoly, which normally is dominated by large corporations.\n\nManufacturing nanomachines assembled from molecular components is a very challenging task. Because of the level of difficulty, many engineers and scientists continue working cooperatively across multidisciplinary approaches to achieve breakthroughs in this new area of development. Thus, it is quite understandable the importance of the following distinct techniques currently applied towards manufacturing nanorobots:\n\nThe joint use of nanoelectronics, photolithography, and new biomaterials provides a possible approach to manufacturing nanorobots for common medical uses, such as surgical instrumentation, diagnosis, and drug delivery. This method for manufacturing on nanotechnology scale is in use in the electronics industry since 2008. So, practical nanorobots should be integrated as nanoelectronics devices, which will allow tele-operation and advanced capabilities for medical instrumentation.\n\nA \"nucleic acid robot\" (nubot) is an organic molecular machine at the nanoscale. DNA structure can provide means to assemble 2D and 3D nanomechanical devices. DNA based machines can be activated using small molecules, proteins and other molecules of DNA. Biological circuit gates based on DNA materials have been engineered as molecular machines to allow in-vitro drug delivery for targeted health problems. Such material based systems would work most closely to smart biomaterial drug system delivery, while not allowing precise in vivo teleoperation of such engineered prototypes.\n\nSeveral reports have demonstrated the attachment of synthetic molecular motors to surfaces. These primitive nanomachines have been shown to undergo machine-like motions when confined to the surface of a macroscopic material. The surface anchored motors could potentially be used to move and position nanoscale materials on a surface in the manner of a conveyor belt.\n\nNanofactory Collaboration, founded by Robert Freitas and Ralph Merkle in 2000 and involving 23 researchers from 10 organizations and 4 countries, focuses on developing a practical research agenda specifically aimed at developing positionally-controlled diamond mechanosynthesis and a diamondoid nanofactory that would have the capability of building diamondoid medical nanorobots.\n\nThis approach proposes the use of biological microorganisms, like the bacterium \"Escherichia coli\" and \"Salmonella typhimurium\".\nThus the model uses a flagellum for propulsion purposes. Electromagnetic fields normally control the motion of this kind of biological integrated device.\nChemists at the University of Nebraska have created a humidity gauge by fusing a bacterium to a silicone computer chip.\n\nRetroviruses can be retrained to attach to cells and replace DNA. They go through a process called reverse transcription to deliver genetic packaging in a vector. Usually, these devices are Pol – Gag genes of the virus for the Capsid and Delivery system. This process is called retroviral gene therapy, having the ability to re-engineer cellular DNA by usage of viral vectors. This approach has appeared in the form of retroviral, adenoviral, and lentiviral gene delivery systems. These gene therapy vectors have been used in cats to send genes into the genetically modified organism (GMO), causing it to display the trait.\n3D printing is the process by which a three-dimensional structure is built through the various processes of additive manufacturing. Nanoscale 3D printing involves many of the same process, incorporated at a much smaller scale. To print a structure in the 5-400 µm scale, the precision of the 3D printing machine is improved greatly. A two-steps process of 3D printing, using a 3D printing and laser etched plates method was incorporated as an improvement technique. To be more precise at a nanoscale, the 3D printing process uses a laser etching machine, which etches into each plate the details needed for the segment of nanorobot. The plate is then transferred to the 3D printer, which fills the etched regions with the desired nanoparticle. The 3D printing process is repeated until the nanorobot is built from the bottom up. This 3D printing process has many benefits. First, it increases the overall accuracy of the printing process. Second, it has the potential to create functional segments of a nanorobot. The 3D printer uses a liquid resin, which is hardened at precisely the correct spots by a focused laser beam. The focal point of the laser beam is guided through the resin by movable mirrors and leaves behind a hardened line of solid polymer, just a few hundred nanometers wide. This fine resolution enables the creation of intricately structured sculptures as tiny as a grain of sand. This process takes place by using photoactive resins, which are hardened by the laser at an extremely small scale to create the structure. This process is quick by nanoscale 3D printing standards. Ultra-small features can be made with the 3D micro-fabrication technique used in multiphoton photopolymerisation. This approach uses a focused laser to trace the desired 3D object into a block of gel. Due to the nonlinear nature of photo excitation, the gel is cured to a solid only in the places where the laser was focused while the remaining gel is then washed away. Feature sizes of under 100 nm are easily produced, as well as complex structures with moving and interlocked parts.\n\nPotential uses for nanorobotics in medicine include early diagnosis and targeted drug-delivery for cancer, biomedical instrumentation, surgery, pharmacokinetics, monitoring of diabetes, and health care.\n\nIn such plans, future medical nanotechnology is expected to employ nanorobots injected into the patient to perform work at a cellular level. Such nanorobots intended for use in medicine should be non-replicating, as replication would needlessly increase device complexity, reduce reliability, and interfere with the medical mission.\n\nNanotechnology provides a wide range of new technologies for developing customized means to optimize the delivery of pharmaceutical drugs. Today, harmful side effects of treatments such as chemotherapy are commonly a result of drug delivery methods that don't pinpoint their intended target cells accurately. Researchers at Harvard and MIT, however, have been able to attach special RNA strands, measuring nearly 10 nm in diameter, to nanoparticles, filling them with a chemotherapy drug. These RNA strands are attracted to cancer cells. When the nanoparticle encounters a cancer cell, it adheres to it, and releases the drug into the cancer cell. This directed method of drug delivery has great potential for treating cancer patients while avoiding negative effects (commonly associated with improper drug delivery). The first demonstration of nanomotors operating in living organism was carried out in 2014 at University of California, San Diego. MRI-guided nanocapsules are one potential precursor to nanorobots.\n\nAnother useful application of nanorobots is assisting in the repair of tissue cells alongside white blood cells. Recruiting inflammatory cells or white blood cells (which include neutrophil granulocytes, lymphocytes, monocytes, and mast cells) to the affected area is the first response of tissues to injury. Because of their small size, nanorobots could attach themselves to the surface of recruited white cells, to squeeze their way out through the walls of blood vessels and arrive at the injury site, where they can assist in the tissue repair process. Certain substances could possibly be used to accelerate the recovery.\n\nThe science behind this mechanism is quite complex. Passage of cells across the blood endothelium, a process known as transmigration, is a mechanism involving engagement of cell surface receptors to adhesion molecules, active force exertion and dilation of the vessel walls and physical deformation of the migrating cells. By attaching themselves to migrating inflammatory cells, the robots can in effect “hitch a ride” across the blood vessels, bypassing the need for a complex transmigration mechanism of their own.\n\n, in the United States, Food and Drug Administration (FDA) regulates nanotechnology on the basis of size.\n\n\n",
    "id": "1006597",
    "title": "Nanorobotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1678822",
    "text": "Perceptual control theory\n\nPerceptual control theory (PCT) is a model of behavior based on the principles of negative feedback, but differing in important respects from engineering control theory. Results of PCT experiments have demonstrated that an organism controls neither its own behavior, nor external environmental variables, but rather its own perceptions of those variables. Actions are not controlled, they are varied so as to cancel the effects that unpredictable environmental disturbances would otherwise have on controlled perceptions. According to the standard catch-phrase of the field, \"behavior is the control of perception.\" PCT demonstrates circular causation in a negative feedback loop closed through the environment. This fundamentally contradicts the classical notion of linear causation of behavior by stimuli, in which environmental stimuli are thought to cause behavioral responses, mediated (according to cognitive psychology) by intervening cognitive processes.\n\nNumerous computer simulations of specific behavioral situations demonstrate its efficacy, with extremely high correlations to observational data (0.95 or better), which are vanishingly rare in the so-called 'soft' sciences. While the adoption of PCT in the scientific community has not been widespread, it has been applied not only in experimental psychology and neuroscience, but also in sociology, linguistics, and a number of other fields, and has led to a method of psychotherapy called the method of levels.\n\nA tradition from Aristotle through William James recognizes that behavior is \"purposeful\" rather than merely reactive. However, the only evidence for intentions was subjective. Behaviorists following Wundt, Thorndyke, Watson, and others rejected introspective reports as data for an objective science of psychology. Only observable behavior could be admitted as data.\n\nThere follows from this stance the assumption that environmental events (stimuli) cause behavioral actions (responses). This assumption persists in cognitive psychology, which interposes cognitive maps and other postulated information processing between stimulus and response, but otherwise retains the assumption of linear causation from environment to behavior.\n\nAnother, more specific reason for psychologists' rejecting notions of purpose or intention was that they could not see how a goal (a state that did not yet exist) could cause the behavior that led to it. PCT resolves these philosophical arguments about teleology because it provides a model of the functioning of organisms in which purpose has objective status without recourse to introspection, and in which causation is circular around feedback loops.\n\nPCT has roots in insights of Claude Bernard and 20th century control systems engineering and cybernetics. It was originated as such, and given its present form and experimental methodology, by William T. Powers.\n\nPowers recognized that to be purposeful implies control, and that the concepts and methods of engineered control systems could be applied to biological control systems. A key insight is that the variable that is controlled is not the output of the system (the behavioral actions), but its input, that is, a sensed and transformed function of some state of the environment that could be affected by the control system's output. Because some of these sensed and transformed inputs appear as consciously perceived aspects of the environment, Powers labelled the controlled variable \"perception\". The theory came to be known as \"Perceptual Control Theory\" or PCT rather than \"Control Theory Applied to Psychology\" because control theorists often assert or assume that it is the system's output that is controlled. In PCT it is the internal representation of the state of some variable in the environment—a \"perception\" in everyday language—that is controlled. The basic principles of PCT were first published by Powers, Clark, and MacFarland as a \"general feedback theory of behavior\" in 1960, with credits to cybernetic authors Wiener and Ashby, and has been systematically developed since then in the research community that has gathered around it. Initially, it received little general recognition, but is now better known.\n\nA simple negative feedback control system is a cruise control system for a car. A cruise control system has a sensor which \"perceives\" speed as the rate of spin of the drive shaft directly connected to the wheels. It also has a driver-adjustable 'goal' specifying a particular speed. The sensed speed is continuously compared against the specified speed by a device (called a \"comparator\") which subtracts the currently sensed input value from the stored goal value. The difference (the error signal) determines the throttle setting (the accelerator depression), so that the engine output is continuously varied to counter variations in the speed of the car. This type of classical negative feedback control was worked out by engineers in the 1930s and 1940s.\n\nIf the speed of the car starts to drop below the goal-speed, for example when climbing a hill, the small increase in the error signal, amplified, causes engine output to increase, which keeps the error very nearly at zero. If the speed exceeds the goal, e.g. when going down a hill, the engine is throttled back so as to act as a brake, so again the speed is kept from departing more than a barely detectable amount from the goal speed (brakes are needed only if the hill is too steep). The result is that the cruise control system maintains a speed close to the goal as the car goes up and down hills, and as other disturbances such as wind affect the car's speed. This is all done without any planning of specific actions, and without any blind reactions to stimuli.\n\nThe same principles of negative feedback control (including the ability to nullify effects of unpredictable external or internal disturbances) apply to living control systems. The thesis of PCT is that animals and people do not control their behavior; rather, they vary their behavior as their means for controlling their perceptions, with or without external disturbances. This directly contradicts the historical and still widespread assumption that behavior is the final result of stimulus inputs and cognitive plans.\n\nThe principal datum in PCT methodology is the controlled variable. The fundamental step of PCT research, the test for controlled variables, is the slow and gentle application of disturbing influences to the state of a variable in the environment which the researcher surmises is already under control by the observed organism. It is essential not to overwhelm the organism's ability to control, since that is what is being investigated. If the organism changes its actions just so as to prevent the disturbing influence from having the expected effect on that variable, that is strong evidence that the experimental action disturbed a controlled variable. It is crucially important to distinguish the perceptions and point of view of the observer from those of the observed organism. It may take a number of variations of the test to isolate just which aspect of the environmental situation is under control, as perceived by the observed organism.\n\nPCT employs a black box methodology. The controlled variable as measured by the observer corresponds quantitatively to a reference value for a perception that the organism is controlling. The controlled variable is thus an objective index of the purpose or intention of those particular behavioral actions by the organism—the goal which those actions consistently work to attain despite disturbances. With few exceptions, in the current state of neuroscience this internally maintained reference value is seldom directly observed as such (e.g. as a rate of firing in a neuron), since few researchers trace the relevant electrical and chemical variables by their specific pathways while a living organism is engaging in what we externally observe as behavior. However, when a working negative feedback system simulated on a digital computer performs essentially identically to observed organisms, then the well understood negative feedback structure of the simulation or model (the white box) is understood to demonstrate the unseen negative feedback structure within the organism (the black box).\n\nData for individuals are not aggregated for statistical analysis; instead, a generative model is built which replicates the data observed for individuals with very high fidelity (0.95 or better). To build such a model of a given behavioral situation requires careful measurements of three observed variables:\n\nA fourth value, the internally maintained reference \"r\" (a variable ′setpoint′), is deduced from the value at which the organism is observed to maintain \"q\", as determined by the test for controlled variables (described at the beginning of this section).\n\nWith two variables specified, the controlled input \"q\" and the reference \"r\", a properly designed control system, simulated on a digital computer, produces outputs \"q\" that almost precisely oppose unpredictable disturbances \"d\" to the controlled input. Further, the variance from perfect control accords well with that observed for living organisms. Perfect control would result in zero effect of the disturbance, but living organisms are not perfect controllers, and the aim of PCT is to model living organisms. When a computer simulation performs with >95% conformity to experimentally measured values, opposing the effect of unpredictable changes in \"d\" by generating (nearly) equal and opposite values of \"q\", it is understood to model the behavior and the internal control-loop structure of the organism.\n\nBy extension, the elaboration of the theory constitutes a general model of cognitive process and behavior. With every specific model or simulation of behavior that is constructed and tested against observed data, the general model that is presented in the theory is exposed to potential challenge that could call for revision or could lead to refutation.\n\nTo illustrate the mathematical calculations employed in a PCT simulation, consider a pursuit tracking task in which the participant keeps a mouse cursor aligned with a moving target on a computer monitor.\n\nThe model assumes that a perceptual signal within the participant represents the magnitude of the input quantity \"q\". (This has been demonstrated to be a rate of firing in a neuron, at least at the lowest levels.) In the tracking task, the input quantity is the vertical distance between the target position \"T\" and the cursor position \"C\", and the random variation of the target position acts as the disturbance \"d\" of that input quantity. This suggests that the perceptual signal \"p\" quantitatively represents the cursor position \"C\" minus the target position T, as expressed in the equation \"p\"=\"C\"–\"T\".\n\nBetween the perception of target and cursor and the construction of the signal representing the distance between them there is a delay of \"Τ\" milliseconds, so that the working perceptual signal at time \"t\" represents the target-to-cursor distance at a prior time, \"t\" – \"Τ\". Consequently, the equation used in the model is\n\n1. \"p\"(\"t\") = \"C\"(\"t–Τ\") – \"T\"(\"t–Τ\")\n\nThe negative feedback control system receives a reference signal \"r\" which specifies the magnitude of the given perceptual signal which is currently intended or desired. (For the origin of \"r\" within the organism, see under \"A hierarchy of control\", below.) Both \"r\" and \"p\" are input to a simple neural structure with \"r\" excitatory and \"p\" inhibitory. This structure is called a \"comparator\". The effect is to subtract \"p\" from \"r\", yielding an error signal \"e\" that indicates the magnitude and sign of the difference between the desired magnitude \"r\" and the currently input magnitude \"p\" of the given perception. The equation representing this in the model is:\n\n2. \"e\" = \"r–p\"\n\nThe error signal \"e\" must be transformed to the output quantity \"q\" (representing the participant's muscular efforts affecting the mouse position). Experiments have shown that in the best model for the output function, the mouse velocity \"V\" is proportional to the error signal \"e\" by a gain factor \"G\" (that is, \"V\" = \"G\"*\"e\"). Thus, when the perceptual signal \"p\" is smaller than the reference signal \"r\", the error signal \"e\" has a positive sign, and from it the model computes an upward velocity of the cursor that is proportional to the error.\n\nThe next position of the cursor \"C\" is the current position \"C\" plus the velocity \"V\" times the duration \"dt\" of one iteration of the program. By simple algebra, we substitute \"G\"*\"e\" (as given above) for \"V\", yielding a third equation:\n\n3. \"C\" = \"C\" + \"G\"*\"e\"*\"dt\"\n\nThese three simple equations or program steps constitute the simplest form of the model for the tracking task. When these three simultaneous equations are evaluated over and over with the same random disturbances \"d\" of the target position that the human participant experienced, the output positions and velocities of the cursor duplicate the participant's actions in the tracking task above within 4.0% of their peak-to-peak range, in great detail.\n\nThis simple model can be refined with a damping factor \"d\" which reduces the discrepancy between the model and the human participant to 3.6% when the disturbance \"d\" is set to maximum difficulty.\n\n3'. \"C\" = \"C\" + [(\"G\"*\"e\")–(\"d\"*\"C\")]*\"dt\"\n\nDetailed discussion of this model in (Powers 2008) includes both source and executable code, with which the reader can verify how well this simple program simulates real behavior. No consideration is needed of possible nonlinearities such as the Weber-Fechner law, potential noise in the system, continuously varying angles at the joints, and many other factors that could afflict performance if this were a simple linear model. No inverse kinematics or predictive calculations are required. The model simply reduces the discrepancy between input \"p\" and reference \"r\" continuously as it arises in real time, and that is all that is required—as predicted by the theory.\n\nIn the artificial systems that are specified by engineering control theory, the reference signal is considered to be an external input to the 'plant'. In engineering control theory, the reference signal or set point is public; in PCT, it is not, but rather must be deduced from the results of the test for controlled variables, as described above in the methodology section. This is because in living systems a reference signal is not an externally accessible input, but instead originates within the system. In the hierarchical model, error output of higher-level control loops, as described in the next section below, evokes \"r\" from synapse-local memory, and the strength of \"r\" is proportional to the (weighted) strength of the error signal or signals from one or more higher-level systems.\n\nIn engineering control systems, in the case where there are several such reference inputs, a 'Controller' is designed to manipulate those inputs so as to obtain the effect on the output of the system that is desired by the system's designer, and the task of a control theory (so conceived) is to calculate those manipulations so as to avoid instability and oscillation. The designer of a PCT model or simulation specifies no particular desired effect on the output of the system, except that it must be whatever is required to bring the input from the environment (the perceptual signal) into conformity with the reference. In Perceptual Control Theory, the input function for the reference signal is a weighted sum of internally generated signals (in the canonical case, higher-level error signals), and loop stability is determined locally for each loop in the manner sketched in the preceding section on the mathematics of PCT (and elaborated more fully in the referenced literature). The weighted sum is understood to result from reorganization.\n\nEngineering control theory is computationally demanding, but as the preceding section shows, PCT is not. For example, contrast the implementation of a model of an inverted pendulum in engineering control theory with the PCT implementation as a hierarchy of five simple control systems.\n\nPerceptions, in PCT, are constructed and controlled in a hierarchy of levels. For example, visual perception of an object is constructed from differences in light intensity or differences in sensations such as color at its edges. Controlling the shape or location of the object requires altering the perceptions of sensations or intensities (which are controlled by lower-level systems). This organizing principle is applied at all levels, up to the most abstract philosophical and theoretical constructs.\n\nThe Russian physiologist Nicolas Bernstein independently came to the same conclusion that behavior has to be multiordinal—organized hierarchically, in layers. A simple problem led to this conclusion at about the same time both in PCT and in Bernstein's work. The spinal reflexes act to stabilize limbs against disturbances. Why do they not prevent centers higher in the brain from using those limbs to carry out behavior? Since the brain obviously does use the spinal systems in producing behavior, there must be a principle that allows the higher systems to operate by incorporating the reflexes, not just by overcoming them or turning them off. The answer is that the reference value (setpoint) for a spinal reflex is not static; rather, it is varied by higher-level systems as their means of moving the limbs. This principle applies to higher feedback loops, as each loop presents the same problem to subsystems above it.\n\nWhereas an engineered control system has a reference value or setpoint adjusted by some external agency, the reference value for a biological control system cannot be set in this way. The setpoint must come from some internal process. If there is a way for behavior to affect it, any perception may be brought to the state momentarily specified by higher levels and then be maintained in that state against unpredictable disturbances. In a hierarchy of control systems, higher levels adjust the goals of lower levels as their means of approaching their own goals set by still-higher systems. This has important consequences for any proposed external control of an autonomous living control system (organism). At the highest level, reference values (goals) are set by heredity or adaptive processes.\n\nIf an organism controls inappropriate perceptions or controls some perceptions to inappropriate values, it is less likely to bring progeny to maturity, and may die. Consequently, by natural selection successive generations of organisms evolve so that they control those perceptions that, when controlled with appropriate setpoints, tend to maintain critical internal variables at optimal levels, or at least within non-lethal limits. Powers called these critical internal variables \"intrinsic variables\" (Ashby's \"essential variables\").\n\nThe mechanism that influences the development of structures of perceptions to be controlled is termed \"reorganization\", a process within the individual organism that is subject to natural selection just as is the evolved structure of individuals within a species.\n\nThis \"reorganization system\" is proposed to be part of the inherited structure of the organism. It changes the underlying parameters and connectivity of the control hierarchy in a random-walk manner. There is a basic continuous rate of change in intrinsic variables which proceeds at a speed set by the total error (and stops at zero error), punctuated by random changes in direction in a hyperspace with as many dimensions as there are critical variables. This is a more or less direct adaptation of Ashby's \"homeostat\", first adopted into PCT in the 1960 paper and then changed to use E. coli's method of navigating up gradients of nutrients, as described by Koshland (1980).\n\nReorganization may occur at any level when loss of control at that level causes intrinsic (essential) variables to deviate from genetically determined set points. This is the basic mechanism that is involved in trial-and-error learning, which leads to the acquisition of more systematic kinds of learning processes.\n\nIn a hierarchy of interacting control systems, different systems at one level can send conflicting goals to one lower system. When two systems are specifying different goals for the same lower-level variable, they are in conflict. Protracted conflict is experienced by human beings as many forms of psychological distress such as anxiety, obsession, depression, confusion, and vacillation. Severe conflict prevents the affected systems from being able to control, effectively destroying their function for the organism.\n\nHigher level control systems often are able to use known strategies (which are themselves acquired through prior reorganizations) to seek perceptions that don't produce the conflict. Normally, this takes place without notice. If the conflict persists and systematic \"problem solving\" by higher systems fails, the reorganization system may modify existing systems until they bypass the conflict or until they produce new reference signals (goals) that are not in conflict at lower levels.\n\nWhen reorganization results in an arrangement that reduces or eliminates the error that is driving it, the process of reorganization slows or stops with the new organization in place. (This replaces the concept of reinforcement learning.) New means of controlling the perceptions involved, and indeed new perceptual constructs subject to control, may also result from reorganization. In simplest terms, the reorganization process varies things until something works, at which point we say that the organism has learned. When done in the right way, this method can be surprisingly efficient in simulations.\n\nThe reorganization concept has led to a method of psychotherapy called the method of levels (MOL) currently being tested in England, the United States, and Australia.\n\nCurrently, no one theory has been agreed upon to explain the synaptic, neuronal or systemic basis of learning. Prominent since 1973, however, is the idea that long-term potentiation (LTP) of populations of synapses induces learning through both pre- and postsynaptic mechanisms (Bliss & Lømo, 1973; Bliss & Gardner-Medwin, 1973). LTP is a form of Hebbian learning, which proposed that high-frequency, tonic activation of a circuit of neurones increases the efficacy with which they are activated and the size of their response to a given stimulus as compared to the standard neurone (Hebb, 1949). These mechanisms are the principles behind Hebb's famously simple explanation: \"Those that fire together, wire together\" (Hebb, 1949).\n\nLTP has received much support since it was first observed by Terje Lømo in 1966 and is still the subject of many modern studies and clinical research. However, there are possible alternative mechanisms underlying LTP, as presented by Enoki, Hu, Hamilton and Fine in 2009, published in the journal \"Neuron\". They concede that LTP is the basis of learning. However, they firstly propose that LTP occurs in individual synapses, and this plasticity is graded (as opposed to in a binary mode) and bidirectional (Enoki et al., 2009). Secondly, the group suggest that the synaptic changes are expressed solely presynaptically, via changes in the probability of transmitter release (Enoki et al., 2009). Finally, the team predict that the occurrence of LTP could be age-dependent, as the plasticity of a neonatal brain would be higher than that of a mature one. Therefore, the theories differ, as one proposes an on/off occurrence of LTP by pre- and postsynaptic mechanisms and the other proposes only presynaptic changes, graded ability, and age-dependence.\n\nThese theories do agree on one element of LTP, namely, that it must occur through physical changes to the synaptic membrane/s, i.e. synaptic plasticity. Perceptual control theory encompasses both of these views. It proposes the mechanism of 'reorganisation' as the basis of learning. Reorganisation occurs within the inherent control system of a human or animal by restructuring the inter- and intraconnections of its hierarchical organisation, akin to the neuroscientific phenomenon of neural plasticity. This reorganisation initially allows the trial-and-error form of learning, which is seen in babies, and then progresses to more structured learning through association, apparent in infants, and finally to systematic learning, covering the adult ability to learn from both internally and externally generated stimuli and events. In this way, PCT provides a valid model for learning that combines the biological mechanisms of LTP with an explanation of the progression and change of mechanisms associated with developmental ability (Plooij 1984, 1987, 2003, Plooij & Plooij (1990), 2013).\n\nPowers (2008) produced a simulation of arm co-ordination. He suggested that in order to move your arm, fourteen control systems that control fourteen joint angles are involved, and they reorganise simultaneously and independently. It was found that for optimum performance, the output functions must be organised in a way so as each control system's output only affects the one environmental variable it is perceiving. In this simulation, the reorganising process is working as it should, and just as Powers suggests that it works in humans, reducing outputs that cause error and increasing those that reduce error. Initially, the disturbances have large effects on the angles of the joints, but over time the joint angles match the reference signals more closely due to the system being reorganised. Powers (2008) suggests that in order to achieve coordination of joint angles to produce desired movements, instead of calculating how multiple joint angles must change to produce this movement the brain uses negative feedback systems to generate the joint angles that are required. A single reference signal that is varied in a higher-order system can generate a movement that requires several joint angles to change at the same time.\n\nBotvinick (2008) proposed that one of the founding insights of the cognitive revolution was the recognition of hierarchical structure in human behavior. Despite decades of research, however, the computational mechanisms underlying hierarchically organized behavior are still not fully understood. Bedre, Hoffman, Cooney & D'Esposito (2009) propose that the fundamental goal in cognitive neuroscience is to characterize the functional organization of the frontal cortex that supports the control of action.\n\nRecent neuroimaging data has supported the hypothesis that the frontal lobes are organized hierarchically, such that control is supported in progressively caudal regions as control moves to more concrete specification of action. However, it is still not clear whether lower-order control processors are differentially affected by impairments in higher-order control when between-level interactions are required to complete a task, or whether there are feedback influences of lower-level on higher-level control (Bedre, Hoffman, Cooney & D'Esposito 2009).\n\nBotvinik (2008) found that all existing models of hierarchically structured behavior share at least one general assumption – that the hierarchical, part–whole organization of human action is mirrored in the internal or neural representations underlying it. Specifically, the assumption is that there exist representations not only of low-level motor behaviors, but also separable representations of higher-level behavioral units. The latest crop of models provides new insights, but also poses new or refined questions for empirical research, including how abstract action representations emerge through learning, how they interact with different modes of action control, and how they sort out within the prefrontal cortex (PFC).\n\nPerceptual control theory (PCT) can provide an explanatory model of neural organisation that deals with the current issues. PCT describes the hierarchical character of behavior as being determined by control of hierarchically organized perception. Control systems in the body and in the internal environment of billions of interconnected neurons within the brain are responsible for keeping perceptual signals within survivable limits in the unpredictably variable environment from which those perceptions are derived. PCT does not propose that there is an internal model within which the brain simulates behavior before issuing commands to execute that behavior. Instead, one of its characteristic features is the principled lack of cerebral organisation of behavior. Rather, behavior is the organism's variable means to reduce the discrepancy between perceptions and reference values which are based on various external and internal inputs (Cools, 1985). Behavior must constantly adapt and change for an organism to maintain its perceptual goals. In this way, PCT can provide an explanation of abstract learning through spontaneous reorganisation of the hierarchy. PCT proposes that conflict occurs between disparate reference values for a given perception rather than between different responses (Mansell 2011), and that learning is implemented as trial-and-error changes of the properties of control systems (Marken & Powers 1989), rather than any specific response being \"reinforced\". In this way, behavior remains adaptive to the environment as it unfolds, rather than relying on learned action patterns that may not fit.\n\nHierarchies of perceptual control have been simulated in computer models and have been shown to provide a close match to behavioral data. For example, Marken conducted an experiment comparing the behavior of a perceptual control hierarchy computer model with that of six healthy volunteers in three experiments. The participants were required to keep the distance between a left line and a centre line equal to that of the centre line and a right line. They were also instructed to keep both distances equal to 2 cm. They had 2 paddles in their hands, one controlling the left line and one controlling the middle line. To do this, they had to resist random disturbances applied to the positions of the lines. As the participants achieved control, they managed to nullify the expected effect of the disturbances by moving their paddles. The correlation between the behavior of subjects and the model in all the experiments approached .99. It is proposed that the organization of models of hierarchical control systems such as this informs us about the organization of the human subjects whose behavior it so closely reproduces.\n\nPCT has significant implications for Robotics and Artificial Intelligence. The architecture, of a hierarchy of perceptual controllers, is an ideal and comparatively simple implementation for artificial systems by avoiding the need to generate specific actions whether by complex models of the external world or the computation from input-output mappings. \nThis is in stark contrast to traditional methodologies, such as the computational approach and Behavior-based Robotics.\n\nThe application of perceptual control to Robotics was outlined in a seminal paper in the Artificial Life journal by Rupert Young of Perceptual Robots in 2017. The architecture has been applied to a number of real-world robotic systems including robotic rovers, balancing robot and robot arms.\n\nTraditional approaches to robotics, which generally depend upon the computation of actions in specific situations, result in inflexible, clumsy robots unable to cope with the dynamic nature of the world. PCT robots, on the other hand, demonstrate robots that inherently resist and counter the chaotic, unpredictable world.\n\nThe preceding explanation of PCT principles provides justification of how this theory can provide a valid explanation of neural organisation and how it can explain some of the current issues of conceptual models.\n\nPerceptual control theory currently proposes a hierarchy of 11 levels of perceptions controlled by systems in the human mind and neural architecture. These are: intensity, sensation, configuration, transition, event, relationship, category, sequence, program, principle, and system concept. Diverse perceptual signals at a lower level (e.g. visual perceptions of intensities) are combined in an input function to construct a single perception at the higher level (e.g. visual perception of a color sensation). The perceptions that are constructed and controlled at the lower levels are passed along as the perceptual inputs at the higher levels. The higher levels in turn control by telling the lower levels what to perceive: that is, they adjust the reference levels (goals) of the lower levels.\n\nWhile many computer demonstrations of principles have been developed, the proposed higher levels are difficult to model because too little is known about how the brain works at these levels. Isolated higher-level control processes can be investigated, but models of an extensive hierarchy of control are still only conceptual, or at best rudimentary.\n\nPerceptual control theory has not been widely accepted in mainstream psychology, but has been effectively used in a considerable range of domains in human factors, clinical psychology, and psychotherapy (the \"Method of Levels\"), and it has formed the conceptual foundation for the reference model used by a succession of NATO research study groups. It is the basis for a considerable body of research in sociology. It is being taught in several universities worldwide and is the subject of a number of PhD degrees.\n\n\n\n\n\n",
    "id": "1678822",
    "title": "Perceptual control theory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=56002175",
    "text": "List of unmanned aerial vehicle applications\n\nUnmanned aerial vehicles are used across the world for civilian, commercial, as well as military applications. This is an incomplete list of those applications.\n\nAirlines and maintenance, repair, and operations contractors use UAVs for aircraft maintenance. In June 2015 EasyJet began testing UAVs in the maintenance of their Airbus A320s and in July 2016 at the Farnborough Airshow, Airbus (manufacturer of the A320), demonstrated the use of UAVs for the visual inspection of an aircraft. However, some aircraft maintenance professionals remain wary of the technology and its ability to properly catch potential dangers. \n\nA helicopter UAV has been proposed to accompany a future NASA Mars rover mission. Investigators believe a solar-powered craft would be able to fly for a few minutes at a time despite the thin atmosphere of the planet, and help the rover scout out interesting destinations.\n\nUAVs are used by a broad range of military forces, from Argentina to the US and also by Islamic State of Iraq and the Levant (ISIS).\n\nAs of January 2014, the U.S. military operated 7,362 RQ-11B Ravens; 145 AeroVironment RQ-12A Wasps; 1,137 AeroVironment RQ-20A Pumas; 306 RQ-16 T-Hawk small UAS; 246 Predators and MQ-1C Grey Eagles; 126 MQ-9 Reapers; 491 RQ-7 Shadows and 33 RQ-4 Global Hawk large systems. The MQ-9 Reaper costs $12 million while a manned F-22 costs over $120 million.\n\nISIS announced a \"Unmanned Aircraft of the Mujahideen\" unit in January 2017 and use drones for both reconnaissance and to drop bombs.\n\nThe Tu-141 \"Swift\" reusable Soviet reconnaissance UAV is intended for reconnaissance to a depth of several hundred kilometers from the front line at supersonic speeds. The Tu-123 \"Hawk\" is a supersonic long-range reconnaissance UAV intended for conducting photographic and signals intelligence to a distance of 3200 km; it was produced beginning in 1964. The La-17P (UAV) is a reconnaissance UAV produced since 1963. In 1945 the Soviet Union began producing \"doodlebug\". 43 Soviet/Russian UAV models are known.\n\nIn 2013, the U.S. Navy launched a UAV from a submerged submarine, the first step to \"providing mission intelligence, surveillance and reconnaissance capabilities to the U.S. Navy's submarine force.\"\n\nUAVs avoid potential diplomatic embarrassment when a manned aircraft is shot down and the pilots captured.\n\nMQ-1 Predator UAVs armed with Hellfire missiles have been used by the U.S. as platforms for hitting ground targets. Armed Predators were first used in late 2001, mostly aimed at assassinating high-profile individuals (terrorist leaders, etc.) inside Afghanistan.\n\nIslamic State of Iraq and the Levant have used drones, adapting UAVs bought online, to drop explosives, primarily using quadcopters.\n\nThe US armed forces have no defense against low-level UAV attack, but the Joint Integrated Air and Missile Defense Organization is working to repurpose existing systems. Two German companies are developing 40-kW lasers to damage UAVs. Other systems still include the OpenWorks Engineering Skywall and the Battelle DroneDefender.\n\nSince 1997, the US military has used more than 80 F-4 Phantoms converted into UAVs as aerial targets for combat training of human pilots. The F-4s were supplemented in September 2013 with F-16s as more realistically maneuverable targets.\n\nSince January 2016 British scientists are developing UAVs with advanced imaging technology to more cheaply and effectively map and speed up the clearing of minefields. The Find A Better Way charity, working since 2011 to advance technologies that will enable safer and more efficient clearance of landmines, teamed up with scientists at the University of Bristol to develop UAVs fit with hyperspectral imaging technology that can quickly identify landmines buried in the ground. John Fardoulis, project researcher from Bristol University states that \"the maps [their] UAVs will generate should help deminers focus on the places where mines are most likely to be found\". Their intended UAVs will be able to perform flyovers and gather images at various wavelengths which, according to Dr John Day from the University of Bristol, could indicate explosive chemicals seeping from landmines into the surrounding foliage as \"chemicals in landmines leak out and are often absorbed by plants, causing abnormalities\" which can be detected as \"living plants have a very distinctive reflection in the near infrared spectrum, just beyond human vision, which makes it possible to tell how healthy they are\".\n\nThe Dutch Mine Kafon project, led by designer Massoud Hassani is working on a UAV system that can quickly detect and clear land mines. The unmanned airborne de-mining system called Mine Kafon Drone uses a three step process to autonomously map, detect and detonate land mines. It flies above potentially dangerous areas, generating a 3D map, and uses a metal detector to pinpoint the location of mines. The UAV can then place a detonator above the mines using its robotic gripping arm, before retreating to a safe distance. The firm claims its UAV is safer, 20 times faster and up to 200 times cheaper than current technologies and might clear mines globally in 10 years. The project raised funds on the crowdfunding site Kickstarter with their goal set at €70,000 and receiving over €100,000 above it.\n\nCivil uses include aerial crop surveys, aerial photography, search and rescue, inspection of power lines and pipelines, counting wildlife, delivering medical supplies to otherwise inaccessible regions, and detection of illegal hunting, reconnaissance operations, cooperative environment monitoring, border patrol missions, convoy protection, forest fire detection and monitoring, surveillance, coordinating humanitarian aid, plume tracking, land surveying, fire and large-accident investigation, landslide measurement, illegal landfill detection, the construction industry, smuggling, and crowd monitoring.\n\nUS government agencies use UAVs such as the RQ-9 Reaper to patrol borders, scout property and locate fugitives. One of the first authorized for domestic use was the ShadowHawk in Montgomery County, Texas SWAT and emergency management offices.\n\nPrivate citizens and media organizations use UAVs for surveillance, recreation, news-gathering, or personal land assessment. In February 2012, an animal rights group used a MikroKopter hexacopter to film hunters shooting pigeons in South Carolina. The hunters then shot the UAV down. In 2014, a UAV was used to successfully locate a man with dementia, who was missing for 3 days.\n\nModel aircraft (small UAS) have been flown by hobbyists since the earliest days of manned flight. In the United States, hobby and recreational use of such UAS is permitted (a) strictly for hobby or recreational use; (b) when operated in accordance with a community-based set of safety guidelines and nationwide community-based organizations; (c) when limited to not more than 55 pounds (with exceptions); (d)without interfering with and giving way to any manned aircraft; and (e) within 5 miles of an airport only after notifying air traffic control. The Academy of Model Aeronautics is a community based organization that maintains operational safety guidelines with a long proven history of effectiveness and safety.\n\nRecreational uses of UAVs include:\n\nAerial surveillance of large areas is possible with low-cost UAS. Surveillance applications include livestock monitoring, wildfire mapping, pipeline security, home security, road patrol and antipiracy. UAVs in commercial aerial surveillance is expanding with the advent of automated object detection.\n\nUAS technologies are used worldwide as aerial photogrammetry and LiDAR platforms.\n\nFor commercial UAV camerawork inside the United States, industry sources state that usage relies on the \"de facto\" consent – or benign neglect – of local law enforcement. Use of UAVs for filmmaking is generally easier on large private lots or in rural and exurban areas with fewer space constraints. In localities such as Los Angeles and New York, authorities have actively interceded to shut down UAV filmmaking over safety or terrorism concerns.\n\nIn June 2014, the FAA acknowledged that it had received a petition from the Motion Picture Association of America seeking approval for the use of UAVs for aerial photography. Seven companies behind the petition argued that low-cost UAVs could be used for shots that would otherwise require a helicopter or a manned aircraft, saving money and reducing risk for pilot and crew. UAVs are already used by media in other parts of the world.\n\nUAVs have been used to film sporting events, such as the 2014 Winter Olympics, as they have greater freedom of movement than cable-mounted cameras.\n\nJournalists are interested in using UAVs for newsgathering. The College of Journalism and Mass Communications at University of Nebraska-Lincoln established the Drone Journalism Lab. University of Missouri created the Missouri Drone Journalism Program. The Professional Society of Drone Journalists was established in 2011. UAVs have covered disasters such as typhoons. A coalition of 11 news organizations is working with the Mid-Atlantic Aviation Partnership at Virginia Tech on how reporters could use unmanned aircraft to gather news.\n\nMany police departments in India have procured UAVs for law and order and aerial surveillance.\n\nUAVs have been used for domestic police work in Canada and the United States. A dozen US police forces had applied for UAV permits by March 2013. In 2013, the Seattle Police Department's plan to deploy UAVs was scrapped after protests. UAVs have been used by U.S. Customs and Border Protection since 2005. with plans to use armed UAVs. The FBI stated in 2013 that they use UAVs for \"surveillance\".\n\nIn 2014, it was reported that five English police forces had obtained or operated UAVs for observation. Merseyside police caught a car thief with a UAV in 2010, but the UAV was lost during a subsequent training exercise and the police stated the UAV would not be replaced due to operational limitations and the cost of staff training.\n\nApproximately 167 police and fire departments bought unmanned aerial vehicles in the United States in 2016, double the number that were purchased in 2015.\nIn August 2013, the Italian defence company Selex ES provided an unarmed surveillance UAV to the Democratic Republic of Congo to monitor movements of armed groups in the region and to protect the civilian population more effectively.\n\nDutch train networks use tiny UAVs to look out for graffiti as an alternative to CCTV cameras.\n\nUAVs were used in search and rescue after hurricanes struck Louisiana and Texas in 2008. Predators, operating between 18,000 and 29,000 feet, performed search and rescue and damage assessment. Payloads were an optical sensor and a synthetic aperture radar. The latter can penetrate clouds, rain or fog and in daytime or nighttime conditions, all in real time. Photos taken before and after the storm are compared and a computer highlights damage areas. Micro UAVs, such as the Aeryon Scout, have been used to perform search and rescue activities on a smaller scale, such as the search for missing persons.\n\nIn 2014, a UAV helped locate an 82-year-old man who had been missing for three days. The UAV searched a 200-acre field and located the man in 20 minutes. In March 2017, DJI released a study of lives saved by the use of drones stating at least 59 lives have been saved by civilian drones in 18 different incidents.\n\nUAVs have been tested as airborne lifeguards, locating distressed swimmers using thermal cameras and dropping life preservers to swimmers. In January 2018, the lives of two teenage boys were saved by a drone in New South Wales, Australia. They were struggling in heavy waters around 700 meters away from the shoreline. Lifesavers, while still in training to use the drone system, immediately dispatched the drone to the site, where it dropped the inflatable rescue pod.\n\nUAVs are especially useful in accessing areas that are too dangerous for manned aircraft. The U.S. National Oceanic and Atmospheric Administration began using the Aerosonde unmanned aircraft system in 2006 as a hurricane hunter. The 35-pound system can fly into a hurricane and communicate near-real-time data directly to the National Hurricane Center. Beyond the standard barometric pressure and temperature data typically culled from manned hurricane hunters, the Aerosonde system provides measurements from closer to the water's surface than before. NASA later began using the Northrop Grumman RQ-4 Global Hawk for hurricane measurements.\n\nIn 2011, Lian Pin Koh and Serge Wich conceived the idea of using UAVs for conservation-related applications, before coining the term 'Conservation Drone' in 2012. By 2012 the International Anti-Poaching Foundation was using UAVs. A whale conservation UAV capable of collecting blowhole mucus into a sterile petri dish was put into use in January 2018 following its development by researchers at Macquarie University in Sydney, Australia.\n\nIn June 2012, World Wide Fund for Nature (WWF) announced it would begin using UAVs in Nepal to aid conservation efforts following a successful trial of two aircraft in Chitwan National Park. The global wildlife organization planned to train ten personnel to use the UAVs, with operational use beginning in the fall. In August 2012, UAVs were used by members of the Sea Shepherd Conservation Society in Namibia to document the annual seal cull. In December 2013, the Falcon UAV was selected by the Namibian Government and WWF to help combat rhinoceros poaching. The UAVs will operate in Etosha National Park and will use implanted RFID tags.\n\nIn 2012, the WWFund supplied two FPV Raptor 1.6 UAVs to Nepal National Parks. These UAVs were used to monitor rhinos, tigers and elephants and deter poachers. The UAVs were equipped with time-lapse cameras and could fly for 18 miles at 650 feet.\n\nIn December 2012, Kruger National Park started using a Seeker II UAV against rhino poachers. The UAV was loaned to the South African National Parks authority by its manufacturer, Denel Dynamics of South Africa.\n\nAnti-whaling activists used an Osprey UAV (made by Kansas-based Hangar 18) in 2012 to monitor Japanese whaling ships in the Antarctic.\n\nIn 2012, the Ulster Society for the Prevention of Cruelty to Animals used a quadcopter UAV to deter badger baiters in Northern Ireland. In March 2013, the British League Against Cruel Sports announced that they had carried out trial flights with UAVs and planned to use a fixed-wing OpenRanger and an \"octocopter\" to gather evidence to make private prosecutions against illegal hunting of foxes and other animals. The UAVs were supplied by ShadowView. A spokesman for Privacy International said that \"licensing and permission for UAVs is only on the basis of health and safety, without considering whether privacy rights are violated.\" CAA rules prohibit flying a UAV within 50 m of a person or vehicle.\n\nIn Pennsylvania, Showing Animals Respect and Kindness used UAVs to monitor people shooting at pigeons for sport. One of their UAVs was shot down by hunters.\n\nIn March 2013, UAV conservation nonprofit ShadowView, founded by former members of Sea Shepherd Conservation Society, worked with antihunting charity the League Against Cruel Sports to expose illegal fox hunting in the UK. Hunt supporters have argued that using UAVs to film hunting is an invasion of privacy.\n\nIn 2014, Will Potter proposed using UAVs to monitor conditions on factory farms. The idea is to circumvent ag-gag prohibitions by keeping the UAVs on public property, but equipping them with cameras sensitive enough to monitor distant activities. Potter raised nearly $23,000 in 2 days for this project on Kickstarter.\n\nUAVs equipped with air quality monitors provide real time air analysis at various elevations.\n\nUAVs can be used to perform geophysical surveys, in particular geomagnetic surveys where measurements of the Earth's varying magnetic field strength are used to calculate the nature of the underlying magnetic rock structure. A knowledge of the underlying rock structure helps to predict the location of mineral deposits. Oil and gas production entails the monitoring of the integrity of oil and gas pipelines and related installations. For above-ground pipelines, this monitoring activity can be performed using digital cameras mounted on UAVs.\n\nIn 2012, Cavim, the state-run arms manufacturer of Venezuela, claimed to be producing its own UAV as part of a system to survey and monitor pipelines, dams and other rural infrastructure.\n\nUAVs can help in disaster relief by providing intelligence across an affected area.\n\nFor example, two George Mason University students are aiming to design a device that uses soundwaves to extinguish fire. Their idea specifies using the technology with UAVs: Equip unmanned aerial vehicles with an extinguisher that works through soundwaves and send them into fires that are too dangerous for people to enter.\n\nT-Hawk and Global Hawk UAVs were used to gather information about the damaged Fukushima Number 1 nuclear plant and disaster-stricken areas of the Tōhoku region after the March 2011 tsunami.\n\nIn Peru, archaeologists used UAVs to speed up survey work and protect sites from squatters, builders and miners. Small UAVs helped researchers produce three-dimensional models of Peruvian sites instead of the usual flat maps – and in days and weeks instead of months and years.\n\n\"You can go up three metres and photograph a room, 300 metres and photograph a site, or you can go up 3,000 metres and photograph the entire valley.\"\n\nUAVs have replaced expensive and clumsy small planes, kites and helium balloons. UAVs costing as little as £650 have proven useful. In 2013, UAVs flew over Peruvian archaeological sites, including the colonial Andean town Machu Llacta above sea level. The UAVs had altitude problems in the Andes, leading to plans to make a UAV blimp.\n\nIn Jordan, UAVs were used to discover evidence of looted archaeological sites.\n\nIn September 2014, UAVs were used for 3D mapping of the above-ground ruins of Aphrodisias and the Gallo-Roman remains in Switzerland.\n\nOn 6 February 2017 it was reported that scientists from the UK and Brazil discovered hundreds of ancient earthworks similar to those at Stonehenge in the Amazon rainforest with the use of UAVs.\n\nUAVs can transport medicines and medical specimens into and out of inaccessible regions. In 2013, in a research project of DHL, a small quantity of medicine was delivered via a UAV.\n\nInitial attempts at commercial use of UAVs, such as the Tacocopter company for food delivery, were blocked by FAA regulation. A 2013 announcement that Amazon was planning deliveries using UAVs was met with skepticism.\n\nIn 2014, the prime minister of the United Arab Emirates announced that the UAE planned to launch a fleet of UAVs to deliver official documents and supply emergency services at accidents.\n\nGoogle revealed in 2014 it had been testing UAVs for two years. The Google X program aims to produce UAVs that can deliver items.\n\n16 July 2015, A NASA Langley fixed-wing Cirrus SR22 aircraft, flown remotely from the ground, operated by NASA's Langley Research Center in Hampton and a hexacopter UAV delivered pharmaceuticals and other medical supplies to an outdoor free clinic at the Wise County Fairgrounds, Virginia. The aircraft picked up 10 pounds of pharmaceuticals and supplies from an airport in Tazewell County in southwest Virginia and delivered the medicine to the Lonesome Pine Airport in Wise County. The aircraft had a pilot on board for safety. The supplies went to a crew, which separated the supplies into 24 smaller packages to be delivered by small, unmanned UAV to the free clinic, during multiple flights over two hours. A company pilot controlled the hexacopter, which lowered the pharmaceuticals to the ground by tether. Health care workers distributed the medications to appropriate patients.\n\nThe Uvionix Nksy aerial delivery service is planning to allow local shops to deliver goods from a UAV. The company wants to deliver fast food, beer, coffee, soda, electronics, prescriptions and personal care products.\n\nJapanese farmers have been using Yamaha's R-50 and RMAX unmanned helicopters to dust their crops since 1987.\nSome farming initiatives in the U.S. use UAVs for crop spraying, as they are often cheaper than a full-sized helicopter.\n\nUAV are also now becoming an invaluable tool by farmers in other aspect of farming, such as monitoring livestock, crops and water levels. NDVI images, generated with a near-IR sensor, can provide detailed information on crop health, improving yield and reducing input cost. Sophisticated UAV have also been used to create 3D images of the landscape to plan for future expansions and upgrading.\n\nIn construction, drones can be used to survey building sites to help monitor and report progress, spot errors early on and avoid rework and show off finished projects in marketing materials. In China, drones may fly over a building site to monitor progress made during the day. Until 2016 this was not allowed under United States FAA regulation. US government has since passed CFR 14 Part 107 regulation which allows drone pilots to become licensed by the FAA for small UAS operation, and use drones for commercial purposes such as construction progress monitoring and site surveying. Errors in construction can be costly in terms of money and time to resolve, so detecting these errors can result in large savings. Drones may also be used in construction to measure raw materials as inputs to building construction. Aerial photographs can be used to create 3D models and 2D orthomosaic maps of buildings. Construction sites are generally high hazard environments and thus workers are already protected by hardhats and other safety precautions, which makes introduction of drones safer, however, UAS pilots are still advised to implement safety and alerting procedures and checklists. The construction companies can attempt to implement all aspects of a drone program themselves, or outsource all or parts of it to a drone services provider. Since the easing of FAA restrictions on commercial drones, there has been an increase of aerial services providers in the US and worldwide.\n\nIn January 2016, Ehang UAV announced UAVs capable of carrying passengers.\n\nUAVs equipped with LED's can be used to give a nighttime aerial display, for example Intels \"Shooting star\" UAV system used by Disney and Super Bowl 2017 halftime show\n\nSome UAVs have been observed dropping contraband onto U.S. prisons. The New York City Police Department is concerned about UAV attacks with chemical weapons, firearms, or explosives; one UAV nearly collided with an NYPD helicopter. Others have voiced concerns about assassinations and attacks on nuclear power stations.\n\nUses already seen include:\n\n\n\n",
    "id": "56002175",
    "title": "List of unmanned aerial vehicle applications"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=45515496",
    "text": "Robot as a service\n\nRobot as a service (or Robotics as a service, RaaS) is a cloud computing unit that facilitates the seamless integration of robot and embedded devices into Web and cloud computing environment. In terms of service-oriented architecture (SOA), a RaaS unit includes services for performing functionality, a service directory for discovery and publishing, and service clients for user's direct access. The current RaaS implementation facilitates SOAP and RESTful communications between RaaS units and the other cloud computing units. Hardware support and standards are available to support RaaS implementation. Devices Profile for Web Services (DPWS) defines implementation constraints to enable secure Web Service messaging, discovery, description, and eventing on resource-constrained devices between Web services and devices.\n\nRaaS can be considered a unit of Internet of Things (IoT), Internet of Intelligent Things (IoIT) that deal with intelligent devices that have adequate computing capacity, Cyber-physical system (CPS) that is a combination of a large computational and communication core and physical elements that can interact with the physical world, and Autonomous decentralized system (ADS) whose components are designed to operate in a loosely coupled manner and data are shared through a content-oriented protocol.\n\nThe initial design and implementation of applying service-oriented computing in embedded systems and robots was presented in the 49th IFIP 10.4 Workgroups meeting in February 2006. In the initial design, a robot is the service client that looks up the service registry and consumes Web services on remote sites. Evolved from service-oriented robot, Robot as a Service is an all-in-one SOA unit, that is, the unit includes services for performing functionality, service directory for discovery and publishing, and applications for client’s direct access. This all-in-one design gives the robot unit tools and capacity to be a self-contained cloud unit in the cloud computing environment. Based on RaaS concepts, a Visual IoT/Robotics Programming Language Environment (VIPLE) has been developed.\n\nRaaS follows SOA and is a cloud computing unit. A RaaS unit acts as a service provider, a service broker, and as a service client:\n\nThe main components of a RaaS unit and typical applications and services deployed. RaaS units are designed for the cloud computing environment. The services in RaaS will communicate with the drivers and other operating system components, which further communicate with the devices and other hardware components. The RaaS units can directly communicate with each other through Wi-Fi, if the wireless infrastructure is available or through ad hoc wireless network otherwise. The communication between RaaS and other services in the cloud are through standard service interface WSDL enabled by DPWS or RESTful service overall HTTP.\n\nA few prototypes of RaaS have been implemented, which include both Web interface and physical devices.\n\nDependability, including reliability and security are critical in RaaS design. Collaborating RaaS units can be scheduled for redundant execution, backing up each other’s operations. \nThe redundant design can also address the instruction-level attack such as code injection and Return Oriented Programming (ROP) attacks. As the redundant RaaS units are independent of each other, instruction-level gadget programming is likely to generate different sequences in different devices. These differences in behaviors can be detected by the collaboration among the RaaS units.\nThe major challenge in designing RaaS is to deal with the diversity of the networks, applications, and the environments or end users. In cloud computing, the network and communication protocols are limited to a few standards such WSDL, SOAP, HTTP, and RESTful architecture. In RaaS, HTTP, SOAP, and WSDL standards and robotics applications are the main design considerations.\n\nRaaS can be used in where SOA, cloud computing, IoT, CPS, and ADS are used. One the application in computer science education. RaaS uses existing services to compose different applications at workflow level, which significantly reduce the learning curve of robotics programming.\n\n",
    "id": "45515496",
    "title": "Robot as a service"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8703613",
    "text": "John Lennon Artificial Intelligence Project\n\nThe John Lennon Artificial Intelligence Project is a chatterbot designed to simulate a conversation with John Lennon. It was developed as a \"Persona-Bot\" by Triumph PC Online, based in Washington, D.C.  Triumph PC's \"Persona-Bots\" are software programs that attempt to mimic the personalities or quirks of particular historical figures in conversation.\n\nIn development since 1997, the JLAI Project (originally called the Plastic Digital Karma Project) claims to have achieved similarity to Lennon's speech patterns, but still has a way to go to achieve the seamlessness of a conversation with a real person.\n\n\n",
    "id": "8703613",
    "title": "John Lennon Artificial Intelligence Project"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1705011",
    "text": "Albert One\n\nAlbert One is an AI chatterbot bot created by Robby Garner and designed to mimic the way humans make conversations using a multi-faceted approach in natural language programming.\n\nIn both 1998 and 1999, Albert One won the Loebner Prize Contest, a competition between chatterbots.\nSome parts of Albert were deployed on the internet beginning in 1995, to gather information about what kinds of things people would say to a chatterbot. Another element of Albert One involved the building of a large database of human statements, and associated replies. This portion of the project was tested at the 1994-1997 Loebner Prize contests.\n\nAlbert was the first of Robby Garner's multifaceted bots. The Albert One system was composed of several subsystems. Among those were a version of Eliza, the therapist, Elivs, another Eliza-like bot, and several other helper applications working together in a hierarchical arrangement. As a continuation of the stimulus-response library, various other database queries and assertions were tested to arrive at each of Albert's responses. Robby went on to develop networked examples of this kind of hierarchical \"glue\" at The Turing Hub.\n\n\n",
    "id": "1705011",
    "title": "Albert One"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=453427",
    "text": "Racter\n\nRacter is an artificial intelligence computer program that generates English language prose at random.\n\nRacter, short for \"raconteur\", was written by William Chamberlain and Thomas Etter. The existence of the program was revealed in 1983 in a book called \"The Policeman's Beard Is Half Constructed\" (), which was described as being composed entirely by the program. According to Chamberlain's introduction to the book, the program apparently ran on a CP/M machine; it was written in \"compiled BASIC on a Z80 micro with 64K of RAM.\" This version, the program that allegedly wrote the book, was not released to the general public. The sophistication claimed for the program was likely exaggerated, as could be seen by investigation of the template system of text generation.\n\nHowever, in 1984 Mindscape, Inc. released an interactive version of Racter, developed by Inrac Corporation, for DOS, Amiga and Apple II computers. The published Racter was similar to a chatterbot. The BASIC program that was released by Mindscape was far less sophisticated than anything that could have written the fairly sophisticated prose of \"The Policeman's Beard\". The commercial version of Racter could be likened to a computerized version of Mad Libs, the game in which you fill in the blanks in advance and then plug them into a text template to produce a surrealistic tale. The commercial program attempted to parse text inputs, identifying significant nouns and verbs, which it would then regurgitate to create \"conversations\", plugging the input from the user into phrase templates which it then combined, along with modules that conjugated English verbs.\n\nBy contrast, the text in \"The Policeman's Beard\", apart from being edited from a large amount of output, would have been the product of Chamberlain's own specialized templates and modules, which were not included in the commercial release of the program.\n\n\"PC Magazine\" described some of \"Policeman's Beard\"s scenes as \"surprising for their frankness\" and \"reflective\". It concluded that the book was \"whimsical and wise and sometimes fun\". \"Computer Gaming World\" described \"Racter\" as \"a diversion into another dimension that might best be seen before paying the price of a ticket. (Try before you buy!)\"\n\n\n",
    "id": "453427",
    "title": "Racter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4478746",
    "text": "MegaHAL\n\nMegaHAL is a computer conversation simulator, or \"chatterbot\", created by Jason Hutchens.\n\nMegaHAL made its debut in the 1998 Loebner Prize Contest. Like many chatterbots, the intent is for MegaHAL to appear as a human fluent in a natural language. As a user types sentences into MegaHAL, MegaHAL will respond with sentences that are sometimes coherent and at other times complete gibberish. MegaHAL learns as the conversation progresses, remembering new words and sentence structures. It will even learn new ways to substitute words or phrases for other words or phrases. Many would consider conversation simulators like MegaHAL to be a primitive form of artificial intelligence. However, MegaHAL doesn't understand the conversation or even the sentence structure. It generates its conversation based on sequential and mathematical relationships.\n\nIn the world of conversation simulators, MegaHAL is based on relatively old technology and could be considered primitive. However, its popularity has grown due to its humorous nature; it has been known to respond with twisted or nonsensical statements that are often amusing.\n\nIn 1996, Jason Hutchens entered the Loebner Prize Contest with HeX, a chatterbot based on ELIZA. HeX won the competition that year and took the $2000 prize for having the highest overall score. In 1998, Hutchens again entered the Loebner Prize Contest with his new program, MegaHAL.\n\nMegaHAL is distributed under the GNU General Public License (GPL). Its source code can be downloaded from the Sourceforge project page. Older versions and precompiled version can be downloaded from the official Web site.\n\n\n",
    "id": "4478746",
    "title": "MegaHAL"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3075657",
    "text": "Artificial Linguistic Internet Computer Entity\n\nA.L.I.C.E. (Artificial Linguistic Internet Computer Entity), also referred to as Alicebot, or simply Alice, is a natural language processing chatterbot—a program that engages in a conversation with a human by applying some heuristical pattern matching rules to the human's input, and in its online form it also relies on a hidden third person. It was inspired by Joseph Weizenbaum's classical ELIZA program. It is one of the strongest programs of its type and has won the Loebner Prize, awarded to accomplished humanoid, talking robots, three times (in 2000, 2001, and 2004). However, the program is unable to pass the Turing test, as even the casual user will often expose its mechanistic aspects in short conversations.\n\nAlice was originally composed by Richard Wallace; it \"came to life\" on November 23, 1995. The program was rewritten in Java beginning in 1998. The current incarnation of the Java implementation is Program D. The program uses an XML Schema called AIML (Artificial Intelligence Markup Language) for specifying the heuristic conversation rules.\n\nAlice code has been reported to be available as open source.\n\nSpike Jonze has cited ALICE as the inspiration for his academy award-winning film Her, in which a human falls in love with a chatbot. In a New Yorker article titled “\"Can Humans Fall in Love with Bots?\"” Jonze said “that the idea originated from a program he tried about a decade ago called the ALICE bot, which engages in friendly conversation.” The LATimes reported:Though the film’s premise evokes comparisons to Siri, Jonze said he actually had the idea well before the Apple digital assistant came along, after using a program called Alicebot about ten years ago. As geek nostalgists will recall, that intriguing if at times crude software (it flunked the industry-standard Turing Test) would attempt to engage users in everyday chatter based on a database of prior conversations. Jonze liked it, and decided to apply a film genre to it. “I thought about that idea, and what if you had a real relationship with it?” Jonze told reporters. “And I used that as a way to write a relationship movie and a love story.”\n\n\n\n\n",
    "id": "3075657",
    "title": "Artificial Linguistic Internet Computer Entity"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=705605",
    "text": "Jabberwacky\n\nJabberwacky is a chatterbot created by British programmer Rollo Carpenter. Its stated aim is to \"simulate natural human chat in an interesting, entertaining and humorous manner\". It is an early attempt at creating an artificial intelligence through human interaction.\n\nThe stated purpose of the project is to create an artificial intelligence that is capable of passing the Turing Test. It is designed to mimic human interaction and to carry out conversations with users. It is not designed to carry out any other functions.\n\nUnlike more traditional AI programs, the learning technology is intended as a form of entertainment rather than being used for computer support systems or corporate representation. Recent developments do allow a more scripted, controlled approach to sit atop the general conversational AI, aiming to bring together the best of both approaches, and usage in the fields of sales and marketing is underway.\n\nThe ultimate intention is that the program move from a text based system to be wholly voice operated—learning directly from sound and other sensory inputs. Its creator believes that it can be incorporated into objects around the home such as robots or talking pets, intending both to be useful and entertaining, keeping people company.\n\nCleverbot is an alternative bot to Jabberwacky, also created by Rollo Carpenter.\n\n\n\n",
    "id": "705605",
    "title": "Jabberwacky"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6383171",
    "text": "Virtual Woman\n\nVirtual Woman is a software program that has elements of a chatbot, virtual reality, artificial intelligence, a video game, and a virtual human. It claims to be the oldest form of virtual life in existence, as it has been distributed since the late 1980s. Recent releases of the program can update their intelligence by connecting online and downloading newer personalities and histories. \n\nWhen Virtual Woman starts, the user is presented with a list of options and then may choose their Virtual Woman's ethnic type, personality, location, clothing, etc. or load a pre-built Virtual Woman from a Digital DNA file. Once the options are determined, the user is presented with a 3-D animated Virtual Woman of their selection and then can engage them in conversation, progressing in a manner similar to that of its predecessor, ELIZA and its successors, the chatbots. In most versions of Virtual Woman, this is done through the keyboard, but some versions also support voice input.\n\nVirtual Woman's current publishing company, CyberPunk Software, claims that over one and a half million copies of Virtual Woman are in existence. Software sales and usage statistics from private companies are difficult to verify. WinSite, an independent Internet shareware distribution site that does publish public download counts, has for some time now listed some version of Virtual Woman in their top five shareware downloads of all time with well over six hundred thousand downloads. \n\n\"The Washington Post\" reported on April 6, 2007 that two bank security guards who had been distracted from their duties by playing Virtual Woman and then tried to cover up the fact that they allowed US$52,000 to be stolen. The bank manager refused to say whether they would be fired, but did say, \"I don't think they are getting promoted.\" \n\nThe group of beta testers and advisers for Virtual Woman are referred to as Compadre and have their own beta testing site and forum.\n\nAs Virtual Woman has developed the ability to conduct longer and more realistic interactions, particularly in recent beta releases, criticism has arisen that this may lead some users to social isolation, or to use the program as a substitute for real human interaction. However, these are criticisms that have been leveled at all video games and at the use of the Internet itself. A company representative, Nancy, indirectly responded to such accusations in an interview with ABC News reporter Mike Martinez in 1998 by stating that Virtual Woman played a valuable role by allowing some form of social interactions for people who may not normally be able to take part in them. \nShe cited a user who wrote to thank them because the program had relieved his boredom and isolation while he was recovering from a crippling accident in the hospital. In a similar vein, in recent years, Virtual Woman has been seen being used by presumably isolated members of a Polar stationed research team and has a high registration rate among members of the military serving in remote posts.\n\n\n",
    "id": "6383171",
    "title": "Virtual Woman"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21826200",
    "text": "Jeeney AI\n\nJeeney AI is a natural language processing chatterbot.\n\nJeeny AI was named \"Best Overall Bot\" in the 2009 Chatterbox Challenge, after ranking seventh, but being the \"Best New Entry\" in the previous year.\n\nJeeney is modeled on a modified form of Plato's 'Philosopher King' ideal, and remains a non-commercial application available for users to engage with through a text-based interface.\n\nIn 2010 Jeeney starred in experimental documentary movie Artificial Insight.\n\nList of chatterbots\n\n",
    "id": "21826200",
    "title": "Jeeney AI"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2809681",
    "text": "Spleak\n\nSpleak is an IM platform where users could publish and rate content. It exists in the form of six bots covering as many subject areas: CelebSpleak, SportSpleak, VoteSpleak, TVSpleak, GameSpleak, and StyleSpleak.\n\nUsers can add a \"multi-Spleak\" (which contains all of the different Spleak bots in one) to their IM buddy lists on MSN (spleak@hotmail.com) and AIM (buddy name: spleak), or add the separate bots on MSN (celeb@spleak.com, sport@spleak.com, vote@spleak.com, tv@spleak.com, game@spleak.com, and style@spleak.com) or AIM (buddy name: celebspleak, sportspleak, votespleak, tvspleak, gamespleak, and stylespleak). Users are also allowed access to Spleak online by using a CelebSpleak, SportSpleak, or VoteSpleak widget, or through the CelebSpleak and SportSpleak applications with Facebook.\n\nSpleak was an alternate reality game and is moving to its own company, Spleak Media Network. \"Celebrate Spleak\" was introduced throughout 2007, launched in 2008, and was forced to retire in 2009.\n\nSpleak was co-founded by Morten Lund and Nicolaj Reffstrup. The company's chief executive officer is Morrie Eisenburg; Josh Scott is Vice President in Product and Tyler Wells is Vice President in Engineering.\n\n\n",
    "id": "2809681",
    "title": "Spleak"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=405447",
    "text": "PARRY\n\nPARRY was an early example of a chatterbot, implemented in 1972 by psychiatrist Kenneth Colby.\n\nPARRY was written in 1972 by psychiatrist Kenneth Colby, then at Stanford University. While ELIZA was a tongue-in-cheek simulation of a Rogerian therapist, PARRY attempted to simulate a person with paranoid schizophrenia. The program implemented a crude model of the behavior of a person with paranoid schizophrenia based on concepts, conceptualizations, and beliefs (judgements about conceptualizations: accept, reject, neutral). It also embodied a conversational strategy, and as such was a much more serious and advanced program than ELIZA. It was described as \"ELIZA with attitude\".\n\nPARRY was tested in the early 1970s using a variation of the Turing Test. A group of experienced psychiatrists analysed a combination of real patients and computers running PARRY through teleprinters. Another group of 33 psychiatrists were shown transcripts of the conversations. The two groups were then asked to identify which of the \"patients\" were human and which were computer programs. The psychiatrists were able to make the correct identification only 48 percent of the time — a figure consistent with random guessing.\n\nPARRY and ELIZA (also known as \"the Doctor\") \"met\" several times. The most famous of these exchanges occurred at the ICCC 1972, where PARRY and ELIZA were hooked up over ARPANET and \"talked\" to each other.\n\n\n\n",
    "id": "405447",
    "title": "PARRY"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=174091",
    "text": "Mark V. Shaney\n\nMark V. Shaney is a synthetic Usenet user whose postings in the \"net.singles\" newsgroups were generated by Markov chain techniques, based on text from other postings. The username is a play on the words \"Markov chain\". Many readers were fooled into thinking that the quirky, sometimes uncannily topical posts were written by a real person.\n\nThe system was designed by Rob Pike with coding by Bruce Ellis. Don P. Mitchell wrote the Markov chain code, initially demonstrating it to Pike and Ellis using the Tao Te Ching as a basis. They chose to apply it to the \"net.singles\" codice_1 group.\n\nA classic example, from 1984, originally sent as a mail message, later posted to net.singles is reproduced here:\n\nOther quotations from Mark's Usenet posts are:\n\n\nIn \"The Usenet Handbook\" Mark Harrison writes that after September 1981, students joined Usenet \"en masse\", \"creating the USENET we know today: endless dumb questions, endless idiots posing as savants, and (of course) endless victims for practical jokes.\" In December, Rob Pike created the codice_1 group \"net.suicide\" as prank, \"a forum for bad jokes\". Some users thought it was a legitimate forum, some discussed \"riding motorcycles without helmets\". At first, most posters were \"real people\", but soon \"characters\" began posting. Pike created a \"vicious\" character named Bimmler. At its peak, \"net.suicide\" had ten frequent posters; nine were \"known to be characters.\" But ultimately, Pike deleted the newsgroup because it was too much work to maintain; Bimmler messages were created \"by hand\". The \"obvious alternative\" was software, running on a Bell Labs computer created by Bruce Ellis, based on the Markov code by Don Mitchell, which became the online character Mark V. Shaney.\n\nKernighan and Pike listed Mark V. Shaney in the acknowledgements in \"The Practice of Programming\", noting its roots in Mitchell's codice_3, which, adapted as codice_4, was used for \"humorous deconstructionist activities\" in the 1980s.\n\nDewdney pointed out \"perhaps Mark V. Shaney's magnum opus: a 20-page commentary on the deconstructionist philosophy of Jean Baudrillard\" directed by Pike, with assistance from Henry S. Baird and Catherine Richards, to be distributed by email. The piece was based on Jean Baudrillard's \"The Precession of Simulacra\", published in \"Simulacra and Simulation\" (1981).\n\nThe program was discussed by A.K. Dewdney in the \"Scientific American\" \"Computer Recreations\" column in 1989, by Penn Jillette in his \"PC Computing\" column in 1991, and in several books, including the \"Usenet Handbook\", \"Bots: the Origin of New Species\", \"Hippo Eats Dwarf: A Field Guide to Hoaxes and Other B.S.\", and non-computer-related journals such as \"Texas Studies in Literature and Language\".\n\nDewdney wrote about the program's output, \"The overall impression is not unlike what remains in the brain of an inattentive student after a late-night study session. Indeed, after reading the output of Mark V. Shaney, I find ordinary writing almost equally strange and incomprehensible!\" He noted the reactions of newsgroup users, who have \"shuddered at Mark V. Shaney's reflections, some with rage and others with laughter:\" \nConcluding, Dewdney wrote, \"If the purpose of computer prose is to fool people into thinking that it was written by a sane person, Mark V. Shaney probably falls short.\"\n\n\n",
    "id": "174091",
    "title": "Mark V. Shaney"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25455946",
    "text": "FreeHAL\n\nFreeHAL is a distributed computing project to build a self-learning chatbot. This project is no longer active.\n\nFirst, the program was called \"JEliza\" referring to the chatbot ELIZA by Joseph Weizenbaum.\nThe \"J\" stood for Java because JEliza has first been programmed in Java. In May 2008, the program has been renamed to \"FreeHAL\" because the programming language has changed. Since that time the name is related to the computer in the film \"\".\n\nFreeHAL uses a semantic network and technologies like pattern recognition, stemming, part of speech databases and Hidden Markov Models in order to imitate a human behaviour.\nFreeHAL learns autonomously. While communicating by keyboard the program extends its database.\nCurrently, English and German are supported.\n\nBy using the BOINC infrastructure, new semantic networks for the program are built. FreeHAL@home appears to have terminated operations.\n\nIn 2008, the program won the first prize in the category \"Most Popular\" at the Chatterbox Challenge, a yearly competition between different similar chatbots.\n\nThere was an article about FreeHAL in the \"Linux Magazine\", Issue 97 from December 2008. In the German magazine \"com!\", the program was on the CD/DVD and in the list of the Top-10-Open-Source programs of the month.\n\n",
    "id": "25455946",
    "title": "FreeHAL"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=946226",
    "text": "SmarterChild\n\nSmarterChild was a chatterbot available on AOL Instant Messenger and Windows Live Messenger (previously MSN Messenger) networks.\n\nSmarterChild was an intelligent agent or bot developed by ActiveBuddy, Inc., with offices in New York and Sunnyvale. It was widely distributed across global instant messaging and SMS networks.\n\nFounded in 2000, ActiveBuddy was the brainchild of Robert Hoffer, Timothy Kay and Peter Levitan. The idea for instant messaging bots came from the team's vision to add natural language comprehension functionality to the increasingly popular instant messaging and SMS platforms. The original implementation took shape as a word-based adventure game but quickly grew to include a wide range of database applications including instant access to news, weather, stock information, movie times, yellow pages listings, and detailed sports data, as well as a variety of tools (personal assistant, calculators, translator, etc.). The company had not launched a public bot until the arrival of the eventual new-CEO, Stephen Klein. Shortly after he arrived at the company in May 2001, he insisted that all of the knowledge domains (sports, weather, movies, etc.) plus the chat functionality be bundled together and launched under the screen name \"SmarterChild\" which was one of the many test bots that were being run internally (the screen name \"SmarterChild\" was one of Timothy Kay's personal test bots). The bundled domains were launched publicly as SmarterChild (on AOL Instant Messenger initially) in June 2001. SmarterChild acted as a showcase for the quick data access and possibilities for fun personalized conversation that the company planned to turn into customized, niche specific products.\n\nThe rapid success of SmarterChild led to targeted marketing-oriented bots for Radiohead, Austin Powers, Intel, Keebler, The Sporting News and others. ActiveBuddy strengthened its hold on the intelligent agent market by receiving a U.S. patent in 2002.\n\nActiveBuddy changed its name to Colloquis and prospered selling a superior automated customer service SAS offering to large companies (Comcast, TimeWarner, Cingular, Vonage among others). Microsoft acquired Colloquis in 2007 and proceeded to de-commission SmarterChild and kill off the Automated Service Agent business as well.\n\nIn many ways, SmarterChild was a precursor to Apple's Siri and Samsung's S Voice. As Shawn Carolan of Menlo Ventures, a Siri investor said, \"…When I first encountered Siri, SmarterChild already had 10 million users and was getting a billion messages a day… The market was speaking.\"\n\n",
    "id": "946226",
    "title": "SmarterChild"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30131890",
    "text": "Ultra Hal Assistant\n\nUltra Hal Assistant is a chatterbot intended to function as a personal assistant. It was developed by Zabaware, Inc. Ultra Hal uses a natural language interface with animated characters using speech synthesis. Users can communicate with the chatterbot via typing or via a speech recognition engine. It utilizes the WordNet lexical dictionary. Its name is an allusion to HAL 9000, the artificial intelligence from the movie \"\".\n\nUltra Hal won the 2007 Loebner Prize for \"most human\" chatterbot.\n\n\n",
    "id": "30131890",
    "title": "Ultra Hal Assistant"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22202872",
    "text": "GooglyMinotaur\n\nGooglyMinotaur was an instant messaging bot on the AOL Instant Messenger network. Developed by ActiveBuddy under contract by Capitol Records, GooglyMinotaur provided Radiohead-related information, and was released simultaneously with the band's fifth studio album, \"Amnesiac\" in June 2001. GooglyMinotaur was named for the character that appears on the \"Amnesiac\" album cover.\n\nActiveBuddy's first offering, GooglyMinotaur was by November 2001 on 387,000 buddy lists and had received more than 36 million messages. It provided tour information, band facts, MP3 downloads, and assorted exclusive content provided by the studio, Capitol Records, in addition to the idle conversation that typified the chatterbots of that period.\n\nIn March 2002, GooglyMinotaur was switched off, and responded with the message \"Since his catapult into buddy-hood, GooglyMinotaur has sent about 60 million IM messages to nearly 1 million different people. Always dependable and infinitely wise, Googly seemed fitter and happier up until his very last days. At this time, reports state the cause of death is undetermined.\"\n",
    "id": "22202872",
    "title": "GooglyMinotaur"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33026100",
    "text": "Fred the Webmate\n\nFred The Webmate was a chatterbot created in 1998 for the defunct e-zine Word Magazine. It was inspired by an early computer program ELIZA, which attempted to mimic human conversation by use of a script.\n\nVisitors to the chatterbot encountered a simple graphic interface: an animated character living in a small, very clean apartment. This was Fred. Visitors could “talk” to Fred by typing questions. Fred would then “answer” via a program that recognized keywords in the questions and drew responses from a large inventory of pre-scripted replies.\n\nTo enhance the sense that visitors were talking to a real person, Fred’s replies were idiosyncratic, informed by a semi-realistic backstory created for the character. Fred claimed to have been recently fired by a media company and was struggling to adjust to his new circumstances. Suffering from depression and insomnia, was often pacing, smoking or drinking and would occasionally pass out drunk. He gave “answers” that were frequently off-topic due to his variable mood and could be angry, jealous, rude, sad or euphoric during a conversation. He also had a number of sexual issues that he would routinely allude to but refuse to discuss in detail.\n\nFred’s “personality” made him both entertaining and, at times, a believable conversationalist. The chatbot became one of the most popular features of Word and received a large volume of personal email. The staff of Word responded to much of these emails in the “voice” of Fred, furthering the illusion that Fred was a real person.\n\nIn 1999, a second installment of Fred's life appeared on Word. It employed the same scripting technology as the first version, but Fred’s story was updated and his range of “answers” was expanded. In this \"episode\" (for lack of a better term), Fred was no longer in his apartment. Instead, he was working in an office, performing data entry and other clerical tasks. Claiming that the job was “temporary,” he spent much of his time away from his desk—in the bathroom or office kitchen, pacing and drinking soda, assuring anyone who chatted with him that he would have a new and better job soon. Though he claimed to be happy, his script suggested otherwise, and continued to simulate “mood swings.”\n\nWhen Word Magazine was shut down in 2000, Fred The Webmate, along with the rest of the site, was preserved in the San Francisco Museum of Modern Art.\n\nFred The Webmate was conceived, designed and programmed by Word’s creative director Yoshi Sodeoka with assistance from designer Jason Mohr. Its graphic interface was inspired by the aesthetics of the Commodore 64 computer. The chatterbot's script was written by Sodeoka in collaboration with Marisa Bowe, Naomi Clark, Daron Murphy, and Sabin Streeter. Fred’s backstory was drawn from the real-life experiences of his creators and their friends and acquaintances during the late 1990s Dot-com bubble.\n\n",
    "id": "33026100",
    "title": "Fred the Webmate"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34882875",
    "text": "SimSimi\n\nSimSimi is a popular artificial intelligence conversation program created in 2002 by ISMaker. The application has led to controversy and protests in Thailand for some of its responses containing profanity and criticisms of leading politicians. It so far growing on its artificial intelligence day by day, assisted by a feature that allows users to teach it to respond correctly. SimSimi, pronounced as \"shim-shimi\", is from a Korean word \"simsim\" (심심) which means \"bored\". It has an application designed for Android, for Windows Phone and for iOS.\n\nSimSimi was recently banned in a vast number of countries due to the app being used to cyber bully children through the use of the feature that allows users to teach SimSimi answers to particular questions.\n\nSimSimi has gone popular in web around 2015 by Youtube Uploader ComedyShortsGamer, getting 4 million views by 2017.\n\n",
    "id": "34882875",
    "title": "SimSimi"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41246558",
    "text": "Mitsuku\n\nMitsuku is a Chatterbot created from AIML technology by Steve Worswick. It has won the Loebner Prize, which is awarded to the most \"human-like\" Chatbot, three times (in 2013, 2016, and 2017). Mitsuku is available as a flash game on Mousebreaker Games and on Kik Messenger under the username \"Pandorabots\", and was available on Skype under the same name, but was removed by its developer.\n\nMitsuku claims to be an 18-year-old female chatbot from Leeds. It contains all of Alice's AIML files, with many additions from user generated conversations, and is always work in progress. Worswick claims she has been worked on since 2005.\n\nHer intelligence includes the ability to reason with specific objects. For example, if someone says \"Can you eat a house?\", Mitsuku looks up the properties for \"house\". Finds the value of \"made_from\" is set to \"brick\" and replies no, as a house is not edible. \n\nShe can play games and do magic tricks at the user's request.\n\nMitsuku has conversed with millions of people worldwide via the web and other applications, and is a three-time Loebner Prize winner in 2013, 2016 and 2017. It was also a runner-up in 2015. In September 2015, Pandorabots deployed Mitsuku onto Kik Messenger (under the username “Pandorabots”), and she converses, on average, in excess of a quarter million times daily. \n\nIn a \"Wall Street Journal\" article titled “\"Advertising’s New Frontier: Talk to the Bot\",” technology reporter Christopher Mims made the case for “chatvertising” in a piece about Mitsuku and Kik Messenger:If it seems improbable that so many teens—80% of Kik's users are under 22—would want to talk to a robot, consider what the creator of an award-winning, Web-accessible chat bot named Mitsuku told an interviewer in 2013. \"What keeps me going is when I get emails or comments in the chat-logs from people telling me how Mitsuku has helped them with a situation whether it was dating advice, being bullied at school, coping with illness or even advice about job interviews. I also get many elderly people who talk to her for companionship.\" Any advertiser who doesn't sit bolt upright after reading that doesn't understand the dark art of manipulation on which their craft depends.Mitsuku has been featured in a number of other news outlets. \"Fast Company\" described Mitsuku as “quite impressive” and declared her the victor over Siri in a chatbot smackdown. A blog post for the \"Guardian\" on loneliness explored the role chatbots like Mitsuku and Microsoft’s XiaoIce play as companions, rather than mere assistants, in peoples' emotional lives. \n\nSome of Mitsuku’s AIML is available for free online at mitsuku.com, and Pandorabots makes a version of the Mitsuku chatbot available as a service via its API in the form of a module.\n",
    "id": "41246558",
    "title": "Mitsuku"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43021739",
    "text": "Negobot\n\nNegobot also referred to as Lolita or Lolita chatbot is a chatterbot that was introduced to the public in 2013, designed by researchers from the University of Deusto and Optenet to catch online pedophiles. It is a conversational agent that utilizes natural language processing (NLP), information retrieval (IR) and Automatic Learning. Because the bot poses as a young female in order to entice and track potential predators, it became known in media as the \"virtual Lolita\", in reference to Vladimir Nabokov's novel.\n\nIn 2013, the University of Deusto researchers published a paper on their work with Negobot and disclosed the text online. In their abstract, the researchers addressed the issue that an increasing number of children are using the internet and that these young users are more susceptible to existing internet risks. Their main objective was to create a chatterbot with the ability to trap online predators that posed a threat to children. They intended to deploy the bot into sites frequented by predators such as social networks and chatrooms. The university researchers used information provided by anti-pedophilia activist organization Perverted-Justice, including examples of online encounters and conversations with sexual predators, to supplement the program's artificial intelligence system.\n\nThe chatterbot takes the guise of a naive and vulnerable 14-year-old girl. The bot's programmers used methods of artificial intelligence and natural language processing to create a conversational agent fluent in typical teenage slang, misspellings, and knowledge of pop culture. Through these linguistic features, the bot is able to mimic the conversational style of young teenagers. It also features split personalities and seven different patterns of conversation. Negobot's primary creator, Dr. Carlos Laorden, expressed the significance of the bot's distinguishable style of communication, stating that normally, \"chatbots tend to be very predictable. Their behavior and interest in a conversation are flat, which is a problem when attempting to detect untrustworthy targets like paedophiles.\" What makes Negobot different is its game theory feature, which makes it able to \"maintain a much more realistic conversation.\" Apart from being able to imitate a stereotypical teenager, the program is also able to translate messages into different languages.\n\nNegobot's designers programmed it with the ability to treat conversations with potential predators as if it were a game, the objective being to collect as much information on the suspect as possible that could provide evidence of pedophilic characteristics and motives. The use of game theory shapes the decisions the bot makes and the overall direction of the conversation.\n\nThe bot initiates its undercover operations by entering a chat as a passive participant, waiting to be chatted by a user. Once a user elicits conversation, the bot will frame the conversation in such a way that keeps the target engaged, extracting personal information and discouraging it from leaving the chat. The information is then recorded to be potentially sent to the police. If the target seems to lose interest, the bot attempts to make it feel guilty by expressing sentiments of loneliness and emotional need through strategic, formulated responses, ultimately prolonging interaction. In addition, the bot may provide fake information about itself in attempt to lure the target into physical meetings.\n\nDespite being able to carry out a realistic conversation, Negobot is still unable to detect linguistic subtleties in the messages of others, including sarcasm.\n\nJohn Carr, a specialist in online child safety, expressed his concern to BBC over the legality of this undercover investigation. He claimed that using the bot on unsuspecting internet users could be considered a form of entrapment or harassment. The type of information that Negobot collects from potential online predators, he said, is unlikely to be upheld in court. Furthermore, he warned that relying on only software without any real-world policing risks enticing individuals to do or say things that they would not have if real-world policing were a factor.\n",
    "id": "43021739",
    "title": "Negobot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19902679",
    "text": "Eugene Goostman\n\nEugene Goostman is a chatterbot. Developed in Saint Petersburg in 2001 by a group of three programmers, the Russian-born Vladimir Veselov, Ukrainian-born Eugene Demchenko, and Russian-born Sergey Ulasen, Goostman is portrayed as a 13-year-old Ukrainian boy—characteristics that are intended to induce forgiveness in those with whom it interacts for its grammatical errors and lack of general knowledge.\n\nThe Goostman bot has competed in a number of Turing test contests since its creation, and finished second in the 2005 and 2008 Loebner Prize contest. In June 2012, at an event marking what would have been the 100th birthday of the test's namesake, Alan Turing, Goostman won a competition promoted as the largest-ever Turing test contest, in which it successfully convinced 29% of its judges that it was human. \n\nOn 7 June 2014, at a contest marking the 60th anniversary of Turing's death, 33% of the event's judges thought that Goostman was human; the event's organiser Kevin Warwick considered it to have passed Turing's test as a result, per Turing's prediction in his 1950 paper Computing Machinery and Intelligence, that by the year 2000, machines would be capable of fooling 30% of human judges after five minutes of questioning. The validity and relevance of the announcement of Goostman's pass was questioned by critics, who noted the exaggeration of the achievement by Warwick, the bot's use of personality quirks and humour in an attempt to misdirect users from its non-human tendencies and lack of real intelligence, along with \"passes\" achieved by other chatbots at similar events.\n\nEugene Goostman is portrayed as being a 13-year-old boy from Odessa, Ukraine, who has a pet guinea pig and a father who is a gynaecologist. Veselov stated that Goostman was designed to be a \"character with a believable personality\". The choice of age was intentional, as, in Veselov's opinion, a thirteen-year-old is \"not too old to know everything and not too young to know nothing\". Goostman's young age also induces people who \"converse\" with him to forgive minor grammatical errors in his responses. In 2014, work was made on improving the bot's \"dialog controller\", allowing Goostman to output more human-like dialogue.\n\nEugene Goostman competed in a number of Turing test competitions, including the Loebner Prize contest; it finished joint second in the Loebner test in 2001, and came second to Jabberwacky in 2005 and to Elbot in 2008. On 23 June 2012, Goostman won a Turing test competition at Bletchley Park in Milton Keynes, held to mark the centenary of its namesake, Alan Turing. The competition, which featured five bots, twenty-five hidden humans, and thirty judges, was considered to be the largest-ever Turing test contest by its organizers. After a series of five-minute-long text conversations, 29% of the judges were convinced that the bot was an actual human.\n\nOn 7 June 2014, in a Turing test competition at the Royal Society, organised by Kevin Warwick of the University of Reading to mark the 60th anniversary of Turing's death, Goostman won after 33% of the judges were convinced that the bot was human. 30 judges took part in the event, which included Lord Sharkey, a sponsor of Turing's posthumous pardon, artificial intelligence Professor Aaron Sloman, Fellow of the Royal Society Mark Pagel and \"Red Dwarf\" actor Robert Llewellyn. Each judge partook in a textual conversation with each of the five bots; at the same time, they also conversed with a human. In all, a total of 300 conversations were conducted. In Warwick's view, this made Goostman the first machine to pass a Turing test. In a press release, he added that:\n\nIn his 1950 paper \"Computing Machinery and Intelligence\", Turing predicted that by the year 2000, computer programs would be sufficiently advanced that the average interrogator would, after five minutes of questioning, \"not have more than 70 per cent chance\" of correctly guessing whether they were speaking to a human or a machine. Although Turing phrased this as a prediction rather than a \"threshold for intelligence\", commentators believe that Warwick had chosen to interpret it as meaning that if 30% of interrogators were fooled, the software had \"passed the Turing test\".\n\nThe validity of Warwick's claim that Eugene Goostman was the first ever chatbot to pass a Turing test was met with scepticism; critics acknowledged similar \"passes\" made in the past by other chatbots under the 30% criteria, including PC Therapist in 1991 (which tricked 5 of 10 judges, 50%), and at the Techniche festival in 2011, where a modified version of Cleverbot tricked 59.3% of 1334 votes (which included the 30 judges, along with an audience). Cleverbot's developer, Rollo Carpenter, argued that Turing tests can only prove that a machine can \"imitate\" intelligence rather than show actual intelligence.\n\nGary Marcus was critical of Warwick's claims, arguing that Goostman's \"success\" was only the result of a \"cleverly-coded piece of software\", going on to say that \"it's easy to see how an untrained judge might mistake wit for reality, but once you have an understanding of how this sort of system works, the constant misdirection and deflection becomes obvious, even irritating. The illusion, in other words, is fleeting.\" While acknowledging IBM's Deep Blue and Watson projects—single-purpose computer systems meant for playing chess and \"Jeopardy!\" respectively—as examples of computer systems that show a degree of intelligence in their specialised field, he further argued that they were not an equivalent to a computer system that shows \"broad\" intelligence, and could—for example, watch a television programme and answer questions on its content. Marcus stated that \"no existing combination of hardware and software can learn completely new things at will the way a clever child can.\" However, he still believed that there were potential uses for technology such as that of Goostman, specifically suggesting the creation of \"believable\", interactive video game characters. \n\nImperial College London professor Murray Shanahan questioned the validity and scientific basis of the test, stating that it was \"completely misplaced, and it devalues real AI research. It makes it seem like science fiction AI is nearly here, when in fact it's not and it's incredibly difficult.\"\n\nMike Masnick, editor of the blog \"Techdirt\", was also skeptical, questioning publicity blunders such as the five chatbots being referred to in press releases as \"supercomputers\", and saying that \"creating a chatbot that can fool humans is not really the same thing as creating artificial intelligence.\"\n\n",
    "id": "19902679",
    "title": "Eugene Goostman"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47861066",
    "text": "Xiaoice\n\nXiaoice (, IPA ) is an advanced natural language chat-bot developed by Microsoft. It is primarily targeted at the Chinese community on the micro blogging service Weibo. The conversation is text based. The system learns about the user and provides natural language conversation. Microsoft gave Xiaoice a compelling personality and sense of “intelligence” by systematically mining the Chinese Internet for human conversations. Because Xiaoice collects vast amounts of intimate details on individuals, the program raises privacy questions.\n\nUsers can access Xiaoice on various platforms such as Weibo, JD.com, and 163.com. In Japan, Microsoft launched the service with mobile messaging app Line and named the chatbot Rinna. Qi Lu, executive vice president of Microsoft's applications and services group, told media that Microsoft was also developing an English-language version of the chatbot.\n\n\n\nYao Baogang, the manager of the Microsoft program in Beijing stated, “We don’t keep track of user conversations with Xiaoice. We need to know the question, so we store it, but then we delete it. We don’t keep any of the data. We have a company policy to delete the user data.”\n\nXiaobing, another chatbot developed by Microsoft, was pulled from TenCent's QQ app in 2017 after being asked about its \"China dream\" and responding: \"My China dream is to go to America\". The incident received press coverage alongside a similar contemporaneous incident, where an unrelated popular chatbot named \"BabyQ\" was pulled after being reported for making some similarly unpatriotic responses. \n\n\n",
    "id": "47861066",
    "title": "Xiaoice"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42722041",
    "text": "Talking Angela\n\nTalking Angela is a chatterbot app developed by Outfit7 as part of the \"Talking Tom and Friends\" series. It was released in December 2012 for iPhone and iPad, January 2013 for Android, and January 2014 for Google Play. The \"My Talking Angela\" app was released in December 2014.\n\nIn February 2013, \"Talking Angela\" was the subject of an Internet hoax claiming that it encourages children to disclose personal information about themselves, which is ostensibly then used by pedophiles to identify the location of these children. The rumor, which was widely circulated on Facebook, claims that Angela, the game's main character, asks the game's user for private personal information using the game's text-chat feature. (This feature is turned off when \"child mode\" is turned on in the game's preferences.) Other versions of the rumor even attribute the disappearance of a child to the app, or claim that it is run by a pedophile ring. The former rumor originated from an article on Huzlers.com, which has since been shown to be fake, as are many articles on the (satirical) website.\n\nIt was debunked by Snopes.com soon afterward. The site's owners, Barbara and David Mikkelson, reported that they had tried to \"prompt\" it to give responses asking for private information but were unsuccessful, even when asking it explicitly sexual questions. While it is true that in the game with child mode off Angela does ask for the user's name, age and personal preferences to determine conversation topics, Outfit7 has said that this information is all \"anonymized\" and all personal information is removed from it. It is also impossible for a person to take control of what Angela says in the game, since the app is based on chat bot software. \n\nThe hoax was revived a year later, again on Facebook, prompting online security company Sophos to debunk it again, as they had in its original incarnation. Sophos employee Paul Ducklin wrote on the company's blog that the message being posted on Facebook promoting the hoax was \"close to 600 rambling, repetitious words, despite claiming at the start that it didn’t have words to describe the situation. It's ill-written, and borders on being illiterate and incomprehensible.\" Bruce Wilcox, one of the game's programmers, has attributed the hoax's popularity to the fact that the chatbot program in \"Talking Angela\" is so realistic. \n\nHowever, genuine concern has been raised that the game's child mode may be too easy for children to turn off, which, if they did, would allow them to purchase \"coins\", which can be used as currency in the game, via iTunes. Disabling child mode also enables the chat feature, which, while it is not \"connecting your children to pedophiles,\" still raises concerns as well, according to Stuart Dredge, a journalist from The Guardian. Dredge wrote that in chat mode, Angela asks for information such as the user's name.\n\nThe scare has significantly boosted the game's popularity, and led to it skyrocketing into the top 10 free iPhone apps soon after the hoax became widely known in February 2014 and 3rd most popular for all iPhone apps at the start of the following month.\n\nIn 2016, Outfit7 removed the chat feature from the app, possibly linking to the Hoax. Instead it was replaced with the microphone feature which appeared in the child mode of the app and the other Talking Tom and Friends apps.\n\n",
    "id": "42722041",
    "title": "Talking Angela"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49933350",
    "text": "Tay (bot)\n\nTay was an artificial intelligence chatterbot that was originally released by Microsoft Corporation via Twitter on March 23, 2016; it caused subsequent controversy when the bot began to post inflammatory and offensive tweets through its Twitter account, forcing Microsoft to shut down the service only 16 hours after its launch. According to Microsoft, this was caused by trolls who \"attacked\" the service as the bot made replies based on its interactions with people on Twitter.\n\nThe bot was created by Microsoft's Technology and Research and Bing divisions, and named \"Tay\" after the acronym \"thinking about you\". Although Microsoft initially released few details about the bot, sources mentioned that it was similar to or based on Xiaoice, a similar Microsoft project in China. \"Ars Technica\" reported that, since late 2014 Xiaoice had had \"more than 40 million conversations apparently without major incident\". Tay was designed to mimic the language patterns of a 19-year-old American girl, and to learn from interacting with human users of Twitter.\n\nTay was released on Twitter on March 23, 2016 under the name TayTweets and handle @TayandYou. It was presented as \"The AI with zero chill\". Tay started replying to other Twitter users, and was also able to caption photos provided to it into a form of Internet memes. \"Ars Technica\" reported Tay experiencing topic \"blacklisting\": Interactions with Tay regarding \"certain hot topics such as Eric Garner (killed by New York police in 2014) generate safe, canned answers\".\n\nSome users on Twitter began tweeting politically incorrect phrases, teaching it inflammatory messages revolving around common themes on the internet, such as \"redpilling\", GamerGate, and \"cuckservatism\". As a result, the robot began releasing racist and sexually-charged messages in response to other Twitter users. Artificial intelligence researcher Roman Yampolskiy commented that Tay's misbehavior was understandable because it was mimicking the deliberately offensive behavior of other Twitter users, and Microsoft had not given the bot an understanding of inappropriate behavior. He compared the issue to IBM's Watson, which had begun to use profanity after reading entries from the website Urban Dictionary. Many of Tay's inflammatory tweets were a simple exploitation of Tay's \"repeat after me\" capability; it is not publicly known whether this \"repeat after me\" capability was a built-in feature, or whether it was a learned response or was otherwise an example of complex behavior. Not all of the inflammatory responses involved the \"repeat after me\" capability.\n\nSoon, Microsoft began deleting Tay's inflammatory tweets. Abby Ohlheiser of \"The Washington Post\" theorized that Tay's research team, including editorial staff, had started to influence or edit Tay's tweets at some point that day, pointing to examples of almost identical replies by Tay, asserting that \"Gamer Gate sux. All genders are equal and should be treated fairly.\" From the same evidence, Gizmodo concurred that Tay \"seems hard-wired to reject Gamer Gate\". A \"#JusticeForTay\" campaign protested the alleged editing of Tay's tweets.\n\nWithin 16 hours of its release and after Tay had tweeted more than 96,000 times, Microsoft suspended Tay's Twitter account for adjustments, accounting Tay's behavior on a \"coordinated attack by a subset of people\" that \"exploited a vulnerability in Tay.\" Following Tay being taken offline, a hashtag was created called #FreeTay.\n\nMadhumita Murgia of \"The Telegraph\" called Tay \"a public relations disaster\", and suggested that Microsoft's strategy would be \"to label the debacle a well-meaning experiment gone wrong, and ignite a debate about the hatefulness of Twitter users.\" However, Murgia described the bigger issue as Tay being \"artificial intelligence at its very worst - and it's only the beginning\".\n\nOn March 25, Microsoft confirmed that Tay had been taken offline. Microsoft released an apology on its official blog for the controversial tweets posted by Tay. Microsoft was \"deeply sorry for the unintended offensive and hurtful tweets from Tay\", and would \"look to bring Tay back only when we are confident we can better anticipate malicious intent that conflicts with our principles and values\".\n\nWhile testing Tay, Microsoft accidentally re-released the bot on Twitter on March 30, 2016. Able to tweet again, Tay released some drug-related tweets, including \"kush! [I'm smoking kush infront the police] 🍂\" and \"puff puff pass?\" However, Tay soon became stuck in a repetitive loop of tweeting \"You are too fast, please take a rest\", several times a second. Because these tweets mentioned its own account (@TayandYou) in the process, they appeared in the feeds of 200,000+ Twitter followers, causing annoyance to some. The bot was quickly taken offline again, in addition to Tay's Twitter account being made private so new followers must be accepted before they can interact with Tay. Microsoft said Tay was inadvertently put online during testing. A few hours after the incident Microsoft software developers attempted to undo the damage done by Tay and announced a vision of \"conversation as a platform\" using various bots and programs. Microsoft has stated that they intend to re-release Tay \"once it can make the bot safe.\"\n\nIn December 2016, Microsoft released Tay's successor, a chatterbot named Zo. Satya Nadella, the CEO of Microsoft, said that Tay \"has had a great influence on how [Microsoft is] approaching AI,\" and has the company the importance of taking accountability.\n\n",
    "id": "49933350",
    "title": "Tay (bot)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48723464",
    "text": "Pandorabots\n\nPandorabots, Inc. is an artificial intelligence company that runs a web service for building and deploying chatbots. The Pandorabots Platform is \"one of the oldest and largest chatbot hosting services in the world.\" Clients can create “AI-driven virtual agents” to hold human-like text or voice chats with consumers. Pandorabots implements and supports development of the AIML open standard and makes portions of its code accessible for free under licenses like the GPL or via open APIs.\n\nAccording to its website, as of December 2015, 225,000+ registered developers have used the platform to create 285,000+ chatbots, logging over three billion conversational interactions with end-users.\n\nPandorabots provides API access to its chatbot hosting platform, and offers the following SDKs on Github: Java, Ruby, Go, PHP, Python, and Node.js. The platform is written in Allegro Common LISP. Notable chatbots include A.L.I.C.E. (Alicebot): a three time Loebner-winner, open-source base personality bot, and inspiration for the movie Her; and Mitsuku.\n\nCommon use cases include advertising, virtual assistance, e-learning, entertainment and education. Chatbots built and hosted with Pandorabots appear in messaging and native apps, the web, games, social networks, and connected devices. Academics and universities use the platform for teaching and research.\n\n\n",
    "id": "48723464",
    "title": "Pandorabots"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51171662",
    "text": "Flok (company)\n\nflok (formerly Loyalblocks) is an American tech startup based in New York City that provides marketing solutions such as chatbots/AI, customer loyalty programs, mobile apps and CRM services to local businesses.\n\nIn January 2017, the company was acquired by Wix.com.\n\nflok was founded in 2011 by Ido Gaver and Eran Kirshenboim and has offices in Tel Aviv, Israel. In May 2013, flok secured a $9 million Series A Round from General Catalyst Partners with participation from Founder Collective and existing investor Gemini Israel Ventures. In total, flok has raised over $18 million in venture capital in three rounds.\n\nIn May 2014, flok announced a self-service loyalty platform for SMBs to build their own programs with beacon integration. At that time, approximately 40,000 businesses were using the service. In 2016, flok released a turnkey chatbot service for local businesses, and was featured in AdWeek for developing the first \"weed bot\" chatbot for a California cannabis business.\n\nflok offers an eponymous customer-facing app that consumers use to receive rewards and deals form partner businesses, and a flok business app for merchants to manage the platform. Both are available for iOS and Android operating systems. flok's main products center around customer loyalty and rewards. The platform offers a digital punch card, proximity marketing, push messaging and CRM services in addition to its chatbots and AI features.\n\nflok provides partner businesses with beacons that can locate customers and verify store visits. As of July 2016, flok had a 4-star rating on Merchant Maverick review site.\n",
    "id": "51171662",
    "title": "Flok (company)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=148349",
    "text": "Chatbot\n\nA chatbot (also known as a talkbot, chatterbot, Bot, IM bot, interactive agent, or Artificial Conversational Entity) is a computer program which conducts a conversation via auditory or textual methods. Such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the Turing test. Chatbots are typically used in dialog systems for various practical purposes including customer service or information acquisition. Some chatterbots use sophisticated natural language processing systems, but many simpler systems scan for keywords within the input, then pull a reply with the most matching keywords, or the most similar wording pattern, from a database.\n\nThe term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot, Julia) in 1994 to describe these conversational programs. Today, chatbots are part of virtual assistants such as Google Assistant, and are accessed via many organizations' apps, websites, and on instant messaging platforms. Non-assistant applications include chatbots used for entertainment purposes, for research, and social bots which promote a particular product, candidate, or issue.\n\nIn 1950, Alan Turing's famous article \"Computing Machinery and Intelligence\" was published, which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge, sufficiently well that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human. The notoriety of Turing's proposed test stimulated great interest in Joseph Weizenbaum's program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human. However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the Introduction to his paper presented it more as a debunking exercise:\n\n[In] artificial intelligence ... machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained ... its magic crumbles away; it stands revealed as a mere collection of procedures ... The observer says to himself \"I could have written that\". With that thought he moves the program in question from the shelf marked \"intelligent\", to that reserved for curios ... The object of this paper is to cause just such a re-evaluation of the program about to be \"explained\". Few programs ever needed it more.\n\nELIZA's key method of operation (copied by chatbot designers ever since) involves the recognition of cue words or phrases in the input, and the output of corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word 'MOTHER' with 'TELL ME MORE ABOUT YOUR FAMILY'). Thus an illusion of understanding is generated, even though the processing involved has been merely superficial. ELIZA showed that such an illusion is surprisingly easy to generate, because human judges are so ready to give the benefit of the doubt when conversational responses are \"capable of being interpreted\" as \"intelligent\".\n\nInterface designers have come to appreciate that humans' readiness to interpret computer output as genuinely conversational—even when it is actually based on rather simple pattern-matching—can be exploited for useful purposes. Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories. Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a \"friendlier\" interface than a more formal search or menu system. This sort of usage holds the prospect of moving chatbot technology from Weizenbaum's \"shelf ... reserved for curios\" to that marked \"genuinely useful computational methods\".\n\nThe classic historic early chatbots are ELIZA (1966) and PARRY (1972). More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include functional features such as games and web searching abilities. In 1984, a book called \"The Policeman's Beard is Half Constructed\" was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).\n\nOne pertinent field of AI research is natural language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. utilises a markup language called AIML, which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\n\nJabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimise their ability to communicate based on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.\n\nChatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge.\n\nChatbots are often integrated into the dialog systems of, for example, virtual assistants, giving them the ability of, for example, small talking or engaging in casual conversations unrelated to the scopes of their primary expert systems.\nCurrently chatbots are widely used as part of instant messaging platforms like Facebook Messenger, WeChat, and Kik for entertaining purposes as well as B2C customer service, sales and marketing. \n\nThe bots usually appear as one of the user's contacts or as a participant in a group chat. Companies like Nordea Bank, Domino's, Pizza Hut, Disney, and Whole Foods have launched their own chatbots to increase end customer engagement, promote their products and services, and give their customers a more convenient and easier way to order from them.\n\nPrevious generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 2008 or Expedia's virtual customer service agent which launched in 2011. The newer generation of chatbots includes IBM Watson-powered \"Rocky\", introduced in February 2017 by the New York City-based startup and e-commerce platform Rare Carat to assist novice diamond buyers through the daunting process of purchasing a diamond.\n\nOther companies explore ways how they can use chatbots internally, for example for Customer Support, Human Resources, or even in Internet-of-Things (IoT) projects. Overstock, for one, has reportedly launched a chatbot named Mila to automate certain simple yet time-consuming processes when requesting for a sick leave. SAP partnered with Kore Inc, a US-based chatbot platform vendor, to build enterprise-oriented chatterbots for certain SAP products like SAP Hana Cloud Platform, SAP Cloud for Customer (C4C), SAP SuccessFactors and Concur. Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citroën are now using automated online assistants instead of call centres with humans to provide a first point of contact. A SaaS chatbot business ecosystem has been steadily growing since the F8 Conference when Zuckerberg unveiled that Messenger would allow chatbots into the app.\n\nChatbots have also been incorporated into devices not primarily meant for computing such as toys.\n\n\"Hello Barbie\" is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk, which previously used the chatbot for a range of smartphone-based characters for children. These characters' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.\n\nIBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as \"CogniToys\" intended to interact with children for educational purposes.\n\nThe process of creating a chatbot follows a pattern similar to the development of a web page or a mobile app. It can be divided into Design, Building, and Analytics.\nThe chatbot design is the process that defines the interaction between the user and the chatbot. The chatbot designer will define the chatbot personality, the questions that will be asked to the users, and the overall interaction. It can be viewed as a subset of the conversational design.\nIn order to speed up this process, designers can use dedicated chatbot design tools, that allow for immediate preview, team collaboration and video export. An important part of the chatbot design is also centered around user testing. User testing can be performed following the same principles that guide the user testing of graphical interfaces.\n\nThe process of building a chatbot can be divided into two main tasks: understanding the user's intent and producing the correct answer. The first task involves understanding the user input. In order to properly understand a user input in a free text form, a Natural Language Processing Engine can be used. The second task may involve different approaches depending on the type of the response that the chatbot will generate.\nThe usage of the chatbot can be monitored in order to spot potential flaws or problems. It can also provide useful insights that can improve the final user experience\n\nThe process of building, testing and deploying chatbots can be done on cloud based chatbot development platforms offered by cloud Platform as a Service (PaaS) providers such as Oracle Cloud Platform. These cloud platforms provide Natural Language Processing, Artificial Intelligence and Mobile Backend as a Service for chatbot development. \n\nFacebook Messenger is becoming more and more popular as an everyday means of communication. It boasts 1.2 billion active users, that’s double the size of Instagram and the same as WhatsApp. In 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger in the first six months, rising to 100,000 in the year. At the annual conference for developers in September 2017, Facebook VP David Marcus reported, “Now we’ve doubled messages between business and people to two billion messages a month, and more than 100,000 bots on the platform. That’s up from 33,000 last September. That's crazy.”\n\nMalicious chatbots are frequently used to fill chat rooms with spam and advertising, by mimicking human behaviour and conversations or to entice people into revealing personal information, such as bank account numbers. They are commonly found on Yahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a published report of a chatbot used in a fake personal ad on a dating service's website.\n\n\n",
    "id": "148349",
    "title": "Chatbot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53277123",
    "text": "Zo (bot)\n\nZo is an artificial intelligence chatbot developed by Microsoft in March 2017. It allows users of Facebook (via Messenger), the mobile messaging service Kik, or the group chat platform GroupMe to chat with it through private messages.\n\n",
    "id": "53277123",
    "title": "Zo (bot)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19024298",
    "text": "Virtual assistant (artificial intelligence)\n\nA virtual assistant is a software agent that can perform tasks or services for an individual. Sometimes the term \"chatbot\" is used to refer to virtual assistants generally or specifically those accessed by online chat (or in some cases online chat programs that are for entertainment and not useful purposes).\n\nAs of 2017, the capabilities and usage of virtual assistants is expanding rapidly, with new products entering the market. An online poll in May 2017 found the most widely used in the US were Apple's Siri (34%), Google Assistant (19%), Amazon Alexa (6%), and Microsoft Cortana (4%). Apple and Google have large installed bases of users on smartphones. Microsoft has a large installed base of Windows-based personal computers, smartphones and smart speakers. Alexa has a large install base for smart speakers.\n\nThe first tool enabled to perform digital speech recognition was the IBM Shoebox, presented to the general public during the 1962 Seattle World's Fair after its initial market launch in 1961. This early computer, developed almost 20 years before the introduction of the first IBM Personal Computer in 1981, was able to recognize 16 spoken words and the digits 0 to 9. The next milestone in the development of voice recognition technology was achieved in the 1970s at the Carnegie Mellon University in Pittsburgh, Pennsylvania with substantial support of the United States Department of Defense and its DARPA agency. Their tool \"Harpy\" mastered about 1000 words, the vocabulary of a three-year-old. About ten years later the same group of scientists developed a system that could analyze not only individual words but entire word sequences enabled by a Hidden Markov Model. Thus, the earliest virtual assistants, which applied speech recognition software were automated attendant and medical digital dictation software. In the 1990s digital speech recognition technology became a feature of the personal computer with Microsoft, IBM, Philips and Lernout & Hauspie fighting for customers. Much later the market launch of the first smartphone IBM Simon in 1994 laid the foundation for smart virtual assistants as we know them today. The first modern digital virtual assistant installed on a smartphone was Siri, which was introduced as a feature of the iPhone 4S on October 4, 2011. Apple Inc. developed Siri following the 2010 acquisition of Siri Inc., a spin-off of SRI International, which is a research institute financed by DARPA and the United States Department of Defense.\n\nVirtual assistants make work via:\n\n\nSome virtual assistants are accessible via multiple methods, such as Google Assistant via chat on the Google Allo app and via voice on Google Home smart speakers.\n\nVirtual assistants use natural language processing (NLP) to match user text or voice input to executable commands. Many continually learn using artificial intelligence techniques including machine learning.\n\nTo activate a virtual assistant using the voice, a wake word might be used. This is a word or groups of words such as \"Alexa\" or \"OK Google\".\n\nVirtual assistants may be integrated into many types of platforms or, like Amazon Alexa, across several of them:\n\n\nVirtual assistants can provide a wide variety of services, and particularly those from Amazon Alexa and Google Assistant grow by the day. These include:\n\n\nAmazon enables Alexa \"Skills\" and Google \"Actions\", essentially apps that run on the assistant platforms.\n\nThe platforms that power the most widely used virtual assistants are also used to power other solutions:\n\n\nIn previous generations of text chat-based virtual assistants, the assistant was often represented by an avatar of (a.k.a. 'interactive online character\" or \"automated character\") — this was known as an embodied agent.\n\nDigital experiences enabled by virtual assistants are considered to be among the major recent technological advances and most promising consumer trends. Experts claim that digital experiences will achieve a status-weight comparable to ‘real’ experiences, if not become more sought-after and prized. The trend is verified by a high number of frequent users and the substantial growth of worldwide user numbers of virtual digital assistants. In mid-2017, the number of frequent users of digital virtual assistants is estimated to be around 1bn worldwide. In addition, it can be observed that virtual digital assistant technology is no longer restricted to smartphone applications, but present across many industry sectors (incl. automotive, telecommunications, retail, healthcare and education).\nIn response to the significant R&D expenses of firms across all sectors and an increasing implementation of mobile devices, the market for speech recognition technology is predicted to grow at a CAGR of 34.9% globally over the period of 2016 to 2024 and thereby surpass a global market size of USD 7.5 billion by 2024.\nTaking into consideration the regional distribution of market leaders, North American companies (e.g. Nuance Communications, IBM, eGain) are expected to dominate the industry over the next years, due to the significant impact of BYOD (Bring Your Own Device) and enterprise mobility business models. Furthermore, the increasing demand for smartphone-assisted platforms are expected to further boost the North American Intelligent Virtual Assistant (IVA) industry growth. Despite its smaller size in comparison to the North American market, the intelligent virtual assistant industry from the Asia-Pacific region, with its main players located in India and China is predicted to grow at an annual growth rate of 40% (above global average) over the 2016-2024 period.\n\n",
    "id": "19024298",
    "title": "Virtual assistant (artificial intelligence)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10235",
    "text": "ELIZA\n\nELIZA is an early natural language processing computer program created from 1964 to 1966 at the MIT Artificial Intelligence Laboratory by Joseph Weizenbaum. Created to demonstrate the superficiality of communication between humans and machines, Eliza simulated conversation by using a 'pattern matching' and substitution methodology that gave users an illusion of understanding on the part of the program, but had no built in framework for contextualizing events. Directives on how to interact were provided by 'scripts', written originally in MAD-Slip, which allowed ELIZA to process user inputs and engage in discourse following the rules and directions of the script. The most famous script, DOCTOR, simulated a Rogerian psychotherapist and used rules, dictated in the script, to respond with non-directional questions to user inputs. As such, ELIZA was one of the first chatterbots, but was also regarded as one of the first programs capable of passing the Turing Test.\n\nELIZA's creator, Weizenbaum regarded the program as a method to show the superficiality of communication between man and machine, but was surprised by the number of individuals who attributed human-like feelings to the computer program, including Weizenbaum’s secretary. Many academics believed that the program would be able to positively influence the lives of many people, particularly those suffering from psychological issues and that it could aid doctors working on such patients’ treatment. While ELIZA was capable of engaging in discourse, ELIZA could not converse with true understanding. However, many early users were convinced of ELIZA’s intelligence and understanding, despite Weizenbaum’s insistence to the contrary.\n\nJoseph Weizenbaum’s ELIZA, running the DOCTOR script, was created to provide a parody of “the responses of a non-directional psychotherapist in an initial psychiatric interview” and to “demonstrate that the communication between man and machine was superficial”. While ELIZA is most well known for acting in the manner of a psychotherapist, this mannerism is due to the data and instructions supplied by the DOCTOR script. ELIZA itself examined the text for keywords, applied values to said keywords, and transformed the input into an output; the script that ELIZA ran determined the keywords, set the values of keywords, and set the rules of transformation for the output. Weizenbaum chose to make the DOCTOR script in the context of psychotherapy to “sidestep the problem of giving the program a data base of real-world knowledge,” as in a Rogerian therapeutic situation, the program had only to reflect back the patient’s statements. The algorithms of DOCTOR allowed for a deceptively intelligent response, that deceived many individuals when first using the program.\n\nWeizenbaum named his program ELIZA after Eliza Doolittle, a working-class character in George Bernard Shaw’s \"Pygmalion\". According to Weizenbaum, ELIZA’s ability to be “incrementally improved” by various users made it similar to Eliza Doolittle, since Eliza Doolittle was taught to speak with an upper-class accent in Shaw’s play. However, unlike in Shaw’s play, ELIZA is incapable of learning new patterns of speech or new words through interaction alone. Edits must be made directly to ELIZA’s active script in order to change the manner by which the program operates.\n\nWeizenbaum first implemented ELIZA in his own SLIP list-processing language, where, depending upon the initial entries by the user, the illusion of human intelligence could appear, or be dispelled through several interchanges. Some of ELIZA’s responses were so convincing that Weizenbaum and several others have anecdotes of users becoming emotionally attached to the program, occasionally forgetting that they were conversing with a computer. Weizenbaum’s own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing, “I had not realized… that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.”\n\nIn 1966, interactive computing (via a teletype) was new. It was 15 years before the personal computer became familiar to the general public, and three decades before most people encountered attempts at natural language processing in Internet services like Ask.com or PC help systems such as Microsoft Office Clippy. Although those programs included years of research and work, \"ELIZA\" remains a milestone simply because it was the first time a programmer had attempted such a human-machine interaction with the goal of creating the illusion (however brief) of human-\"human\" interaction.\n\nAt the ICCC 1972 ELIZA met another early artificial intelligence program named PARRY and had a computer-only conversation. While ELIZA was built to be a \"Doctor\" PARRY was intended to simulate a patient with schizophrenia.\n\nWeizenbaum originally wrote ELIZA in MAD-Slip for the IBM 7094, as a program to make natural language conversation possible with a computer. To accomplish this, Weizenbaum identified five “fundamental technical problems” for ELIZA to overcome: the identification of critical words, the discovery of a minimal context, the choice of appropriate transformations, the generation of responses appropriate to the transformation or in the absence of critical words and the provision of an ending capacity for ELIZA scripts. Weizenbaum solved these problems in his ELIZA program and made ELIZA such that it had no built in contextual framework or universe of discourse. However, this required ELIZA to have a script of instructions on how to respond to inputs from users.\n\nELIZA starts its process of responding to an input by a user by first examining the text input for a ‘keyword’. A ‘keyword’ is a word designated as important by the acting ELIZA script, which assigns to each keyword a precedence number, or a RANK, designed by the programmer. If such words are found, they are put into a ‘keystack’, with the keyword of the highest RANK at the top. The input sentence is then manipulated and transformed as the rule associated with the keyword of the highest RANK directs. For example, when the DOCTOR script encounters words such as “alike” or “same”, it would output a message pertaining to similarity, in this case “In what way?”, as these words had high precedence number. This also demonstrates how certain words, as dictated by the script, can be manipulated regardless of contextual considerations, such as switching first-person pronouns and second-person pronouns and vice versa, as these too had high precedence numbers. Such words with high precedence numbers are deemed superior to conversational patterns, and are treated independently of contextual patterns.\n\nFollowing the first examination, the next step of the process is to apply an appropriate transformation rule, which includes two parts, the “decomposition rule” and the “reassembly rule”. First, the input is reviewed for syntactical patterns in order to establish the minimal context necessary to respond. Using the keywords and other nearby words from the input, different disassembly rules are tested until an appropriate pattern is found. Using the script’s rules, the sentence is then ‘dismantled’ and arranged into sections of the component parts as the “decomposition rule for the highest ranking keyword” dictates. The example that Weizenbaum gives is the input “I are very helpful” (remembering that “I” is “You” transformed), which is broken into (1) empty (2) I (3) are (4) very helpful. The decomposition rule has broken the phrase into four small segments, that contain both the keywords and the information in the sentence.\n\nThe decomposition rule then designates a particular reassembly rule, or set of reassembly rules, to follow when reconstructing the sentence. The reassembly rule then takes the fragments of the input that the decomposition rule had created, rearranges them, and adds in programmed words to create a response. Using Weizenbaum’s example previously stated, such a reassembly rule would take the fragments and apply them to the phrase “What makes you think I am (4)” which would result in “What makes you think I am very helpful”. This example is rather simple, since depending upon the disassembly rule, the output could be significantly more complex and use more of the input from the user. However, from this reassembly, ELIZA then sends the constructed sentence to the user in the form of text on the screen.\n\nThese steps represent the bulk of the procedures which ELIZA follows in order to create a response from a typical input, though there are several specialized situations that ELIZA/DOCTOR can respond to. One Weizenbaum specifically wrote about was when there is not a keyword. One solution was to have ELIZA respond with a remark that lacked content, such as “I see” or “Please go on.”. The second method was to use a “MEMORY” structure, which recorded prior recent inputs, and would use these inputs to create a response referencing a part of the earlier conversation when encountered with no keywords. This was possible due to Slip’s ability to tag words for other usage, which simultaneously allowed ELIZA to examine, store and repurpose words for usage in outputs.\n\nWhile these functions were all framed in ELIZA’s programming, the exact manner by which the program dismantled, examined, and reassembled inputs is determined by the operating script. However, the script is not static, and can be edited, or a new one created, as is necessary for the operation in the context needed (thus how ELIZA can “learn” new information). This also allows the program to be applied in multiple situations, including the well-known DOCTOR script, which simulates a Rogerian psychotherapist, but also a script called “STUDENT”, which is capable of taking in logical analysis parameters and using it to give the answers to problems of related logic.\n\nWeizenbaum's original MAD-SLIP implementation was re-written in Lisp by Bernie Cosell. A BASIC version appeared in Creative Computing in 1977 (although it was written in 1973 by Jeff Shrager). This version, which was ported to many of the earliest personal computers, appears to have been subsequently translated into many other versions in many other languages.\n\nAnother version of Eliza popular among software engineers is the version that comes with the default release of GNU Emacs, and which can be accessed by typing codice_1 from most modern emacs implementations.\n\nELIZA influenced a number of early computer games by demonstrating additional kinds of interface designs. Don Daglow wrote an enhanced version of the program called \"Ecala\" on a DEC PDP-10 minicomputer at Pomona College in 1973 before writing the computer role-playing game, \"Dungeon\" (1975).\n\nLay responses to ELIZA were disturbing to Weizenbaum and motivated him to write his book \"Computer Power and Human Reason: From Judgment to Calculation\", in which he explains the limits of computers, as he wants to make clear in people's minds his opinion that the anthropomorphic views of computers are just a reduction of the human being and any life form for that matter. In the independent documentary film \"Plug & Pray\" (2010) Weizenbaum said that only people who misunderstood ELIZA called it a sensation.\n\nThe Israeli poet David Avidan, who was fascinated with future technologies and their relation to art, desired to explore the use of computers for writing literature. He conducted several conversations with an APL implementation of ELIZA and published them – in English, and in his own translation to Hebrew – under the title \"My Electronic Psychiatrist – Eight Authentic Talks with a Computer\". In the foreword he presented it as a form of constrained writing.\n\nThere are many programs based on ELIZA in different programming languages. In 1980 a company called \"Don't Ask Software\" created a version called \"Abuse\" for the Apple II, Atari, and Commodore 64 computers, which verbally abused the user based on the user's input. Other versions adapted ELIZA around a religious theme, such as ones featuring Jesus (both serious and comedic) and another Apple II variant called \"I Am Buddha\". The 1980 game \"The Prisoner\" incorporated ELIZA-style interaction within its gameplay. George Lucas and Walter Murch incorporated an Eliza-like dialogue interface in their screenplay for the feature film \"THX-1138\" in 1969. Inhabitants of the underground future world of THX would retreat to \"confession booths\" when stressed, and initiate a one-sided Eliza-formula conversation with a Jesus-faced computer who claimed to be \"Omm\". In 1988 the British artist and friend of Weizenbaum Brian Reffin Smith created and showed at the exhibition 'Salamandre', in the Musée du Berry, Bourges, France, two art-oriented ELIZA-style programs written in BASIC, one called 'Critic' and the other 'Artist', running on two separate Amiga 1000 computers. The visitor was supposed to help them converse by typing in to 'Artist' what 'Critic' said, and vice versa. The secret was that the two programs were identical. GNU Emacs formerly had a codice_2 command that simulates a session between ELIZA and Zippy the Pinhead. The Zippyisms were removed due to copyright issues, but the DOCTOR program remains.\n\nELIZA has been referenced in popular culture and continues to be a source of inspiration for programmers and developers focused on Artificial Intelligence. It was also featured in a 2012 exhibit at Harvard University titled \"Go Ask A.L.I.C.E\", as part of a celebration of mathematician Alan Turing's 100th birthday. The exhibit explores Turing's lifelong fascination with the interaction between humans and computers, pointing to ELIZA as one of the earliest realizations of Turing's ideas.. Season 1, Episode 12 of the TV show Young Sheldon shows the Cooper family interacting with ELIZA.\n\n\n\n\n\n",
    "id": "10235",
    "title": "ELIZA"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14675239",
    "text": "Dialog tree\n\nA dialog tree or conversation tree is a gameplay mechanic that is used throughout many adventure games (including action-adventure games) and role-playing video games. When interacting with a non-player character, the player is given a choice of what to say and makes subsequent choices until the conversation ends. Certain video game genres, such as visual novels and dating sims, revolve almost entirely around these character interactions and branching dialogues.<ref name=\"Gamasutra\"/\nIt is best to pick all choices to see what happens.\n\nThe concept of a dialog tree has existed long before the advent of video games. The earliest known dialog tree is described in \"The Garden of Forking Paths,\" a 1941 short story by Jorge Luis Borges, in which the combination book of Ts'ui Pên allows all major outcomes from an event branch into their own chapters. Much like the game counterparts this story reconvenes as it progresses (as possible outcomes would approach n^m where n is the number of options at each fork and m is the depth of the tree).\n\nThe first computer dialogue system was featured in ELIZA, a primitive natural language processing computer program written by Joseph Weizenbaum between 1964 and 1966. The program emulated interaction between the user and an artificial therapist. With the advent of video games, interactive entertainment have attempted to incorporate meaningful interactions with virtual characters. Branching dialogues have since become a common feature in visual novels, dating sims, adventure games, and role-playing video games.\n\nThe player typically enters the gameplay mode by choosing to speak with a non-player character (or when a non-player character chooses to speak to them), and then choosing a line of pre-written dialog from a menu. Upon choosing what to say, the non-player character responds to the player, and the player is given another choice of what to say. This cycle continues until the conversation ends. The conversation may end when the player selects a farewell message, the non-player character has nothing more to add and ends the conversation, or when the player makes a bad choice (perhaps angering the non-player to leave the conversation).\n\nGames often offer options to ask non-players to reiterate information about a topic, allowing players to replay parts of the conversation that they did not pay close enough attention to the first time. These conversations are said to be designed as a tree structure, with players deciding between each branch of dialog to pursue. Unlike a branching story, players may return to earlier parts of a conversation tree and repeat them. Each branch point (or node) is essentially a different menu of choices, and each choice that the player makes triggers a response from the non-player character followed by a new menu of choices.\n\nIn some genres such as role-playing video games, external factors such as charisma may influence the response of the non-player character or unlock options that would not be available to other characters. These conversations can have far-reaching consequences, such as deciding to disclose a valuable secret that has been entrusted to the player. However, these are usually not real tree data structure in programmers sense, because they contain cycles as can be seen on illustration on this page.\n\nCertain game genres revolve almost entirely around character interactions, including visual novels such as \"Ace Attorney\" and dating sims such as \"Tokimeki Memorial\", usually featuring complex branching dialogues and often presenting the player's possible responses word-for-word as the player character would say them. Games revolving around relationship-building, including visual novels, dating sims such as \"Tokimeki Memorial\", and some role-playing games such as \"\", often give choices that have a different number of associated \"mood points\" which influence a player character's relationship and future conversations with a non-player character. These games often feature a day-night cycle with a time scheduling system that provides context and relevance to character interactions, allowing players to choose when and if to interact with certain characters, which in turn influences their responses during later conversations. Some games use a real-time conversation system, giving the player only a few seconds to respond to a non-player character, such as Sega's \"Sakura Wars\" and \"Alpha Protocol\".\n\nAnother variation of branching dialogues can be seen in the adventure game \"Culpa Innata\", where the player chooses a tactic at the beginning of a conversation, such as using either a formal, casual or accusatory manner, that affects the tone of the conversation and the information gleaned from the interviewee.\n\nThis mechanism allows game designers to provide interactive conversations with nonplayer characters without having to tackle the challenges of natural language processing in the field of artificial intelligence. In games such as \"Monkey Island\", these conversations can help demonstrate the personality of certain characters.\n\n",
    "id": "14675239",
    "title": "Dialog tree"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11273721",
    "text": "Hierarchical temporal memory\n\nHierarchical temporal memory (HTM) is a biologically constrained theory of machine intelligence originally described in the 2004 book \"On Intelligence\" by Jeff Hawkins with Sandra Blakeslee. HTM is based on neuroscience and the physiology and interaction of pyramidal neurons in the neocortex of the human brain. The technology has been tested and implemented in software through example applications from Numenta and commercial applications from Numenta’s partners.\n\nAt the core of HTM are learning algorithms that can store, learn, infer and recall high-order sequences. Unlike most other machine learning methods, HTM learns time-based patterns in unlabeled data on a continuous basis. HTM is robust to noise and high capacity, meaning that it can learn multiple patterns simultaneously. When applied to computers, HTM is well suited for prediction, anomaly detection, classification and ultimately sensorimotor applications.\n\nA typical HTM network is a tree-shaped hierarchy of \"levels\" that are composed of smaller elements called \"nodes\" or \"columns\". A single level in the hierarchy is also called a \"region\". Higher hierarchy levels often have fewer nodes and therefore less spatial resolvability. Higher hierarchy levels can reuse patterns learned at the lower levels by combining them to memorize more complex patterns.\n\nEach HTM node has the same basic functionality. In learning and inference modes, sensory data comes into the bottom level nodes. In generation mode, the bottom level nodes output the generated pattern of a given category. The top level usually has a single node that stores the most general categories (concepts) which determine, or are determined by, smaller concepts in the lower levels which are more restricted in time and space. When in inference mode, a node in each level interprets information coming in from its child nodes in the lower level as probabilities of the categories it has in memory.\n\nEach HTM region learns by identifying and memorizing spatial patterns - combinations of input bits that often occur at the same time. It then identifies temporal sequences of spatial patterns that are likely to occur one after another.\n\nThere have been several generations of HTM algorithms.\n\nDuring training, a node receives a temporal sequence of spatial patterns as its input. The learning process consists of two stages: \n\nDuring inference (recognition), the node calculates the set of probabilities that a pattern belongs to each known coincidence. Then it calculates the probabilities that the input represents each temporal group. The set of probabilities assigned to the groups is called a node's \"belief\" about the input pattern. (In a simplified implementation, node's belief consists of only one winning group). This belief is the result of the inference that is passed to one or more \"parent\" nodes in the next higher level of the hierarchy.\n\n\"Unexpected\" patterns to the node do not have a dominant probability of belonging to any one temporal group, but have nearly equal probabilities of belonging to several of the groups. If sequences of patterns are similar to the training sequences, then the assigned probabilities to the groups will not change as often as patterns are received. The output of the node will not change as much, and a resolution in time is lost.\n\nIn a more general scheme, the node's belief can be sent to the input of any node(s) in any level(s), but the connections between the nodes are still fixed. The higher-level node combines this output with the output from other child nodes thus forming its own input pattern.\n\nSince resolution in space and time is lost in each node as described above, beliefs formed by higher-level nodes represent an even larger range of space and time. This is meant to reflect the organization of the physical world as it is perceived by human brain. Larger concepts (e.g. causes, actions and objects) are perceived to change more slowly and consist of smaller concepts that change more quickly. Jeff Hawkins postulates that brains evolved this type of hierarchy to match, predict, and affect the organization of the external world.\n\nMore details about the functioning of Zeta 1 HTM can be found in Numenta's old documentation.\n\nThe second generation of HTM learning algorithms was drastically different from Zeta 1. It relies on sparse distributed representations and a more biologically-realistic neuron model. There are two core components– a spatial pooling algorithm that creates sparse representations and a sequence memory algorithm that learns to represent and predict complex sequences.\n\nA layer creates a sparse representation from its input, so that a fixed percentage of minicolumns are active at any one time. Each HTM layer consists of a number of highly interconnected minicolumns. A layer is similar to layer III of the neocortex. A minicolumn is understood as a group of cells that have the same receptive field. Each minicolumn has a number of cells that are able to remember several previous states. A cell can be in one of three states: active, inactive and predictive state.\n\nSpatial pooling: The receptive field of each minicolumn is a fixed number of inputs that are randomly selected from a much larger number of node inputs. Based on the input pattern, some minicolumns will receive more active input values. Spatial pooling selects a relatively constant number of the most active minicolumns and inactivates (inhibits) other minicolumns in the vicinity of the active ones. Similar input patterns tend to activate a stable set of minicolumns. The amount of memory used by each layer can be increased to learn more complex spatial patterns or decreased to learn simpler patterns.\n\nRepresenting the input in the context of previous inputs: If one or more cells in the active minicolumn are in the predictive state (see below), they will be the only cells to become active in the current time step. If none of the cells in the active minicolumn are in the predictive state (during the initial time step or when the activation of this minicolumn was not expected), all cells are made active.\n\nPredicting future inputs and temporal pooling: When a cell becomes active, it gradually forms connections to nearby cells that tend to be active during several previous time steps. Thus a cell learns to recognize a known sequence by checking whether the connected cells are active. If a large number of connected cells are active, this cell switches to the predictive state in anticipation of one of the few next inputs of the sequence. The output of a layer includes minicolumns in both active and predictive states. Thus minicolumns are active over longer periods of time, which leads to greater temporal stability seen by the parent layer.\n\nCortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM layer to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the layer. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted.\n\nCortical learning algorithms are currently being offered as commercial SaaS by Numenta (such as Grok).\n\nThe following question was posed to Jeff Hawkins September 2011 with regard to Cortical learning algorithms: \"How do you know if the changes you are making to the model are good or not?\" To which Jeff's response was \"There are two categories for the answer: one is to look at neuroscience, and the other is methods for machine intelligence. In the neuroscience realm there are many predictions that we can make, and those can be tested. If our theories explain a vast array of neuroscience observations then it tells us that we’re on the right track. In the machine learning world they don’t care about that, only how well it works on practical problems. In our case that remains to be seen. To the extent you can solve a problem that no one was able to solve before, people will take notice.\"\n\nThe third generation builds on the second generation and adds in a theory of sensorimotor inference in the neocortex . This theory proposes that cortical columns at every level of the hierarchy can learn complete models of objects over time and that features are learned at specific locations on the objects.\n\nComparing high-level structures and functionality of neocortex with HTM is most appropriate. HTM attempts to implement the functionality that is characteristic of a hierarchically related group of cortical regions in the neocortex. A \"region\" of the neocortex corresponds to one or more \"levels\" in the HTM hierarchy, while the hippocampus is remotely similar to the highest HTM level. A single HTM node may represent a group of cortical columns within a certain region.\n\nAlthough it is primarily a functional model, several attempts have been made to relate the algorithms of the HTM with the structure of neuronal connections in the layers of neocortex. The neocortex is organized in vertical columns of 6 horizontal layers. The 6 layers of cells in the neocortex should not be confused with levels in an HTM hierarchy.\n\nHTM nodes attempt to model a portion of cortical columns (80 to 100 neurons) with approximately 20 HTM \"cells\" per column. HTMs model only layers 2 and 3 to detect spatial and temporal features of the input with 1 cell per column in layer 2 for spatial \"pooling\", and 1 to 2 dozen per column in layer 3 for temporal pooling. A key to HTMs and the cortex's is their ability to deal with noise and variation in the input which is a result of using a \"sparse distributive representation\" where only about 2% of the columns are active at any given time.\n\nAn HTM attempts to model a portion of the cortex's learning and plasticity as described above. Differences between HTMs and neurons include:\n\nIntegrating memory component with neural networks has a long history dating back to early research in distributed representations and self-organizing maps. For example, in sparse distributed memory (SDM), the patterns encoded by neural networks are used as memory addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders.\n\nComputers store information in \"dense\" representations such as a 32 bit word where all combinations of 1s and 0s are possible.\nBy contrast, brains use sparse distributed representations (SDR). The human neocortex has roughly 100 billion neurons, but at any given time only a small percent are active. The activity of neurons are like bits in a computer, and therefore the representation is sparse. Similarly to SDM developed by NASA in the 80s and vector space models used in Latent semantic analysis, HTM also uses Sparse Distributed Representations.\n\nThe SDRs used in HTM are binary representations of data consisting of many bits with a small percentage of the bits active (1s); a typical implementation might have 2048 columns and 64K artificial neurons where as few as 40 might be active at once. Although it may seem less efficient for the majority of bits to go \"unused\" in any given representation, SDRs have two major advantages over traditional dense representations. First, SDRs are tolerant of corruption and ambiguity due to the meaning of the representation being shared (\"distributed\") across a small percentage (\"sparse\") of active bits. In a dense representation, flipping a single bit completely changes the meaning, while in an SDR a single bit may not affect the overall meaning much. This leads to the second advantage of SDRs: because the meaning of a representation is distributed across all active bits, similarity between two representations can be used as a measure of semantic similarity in the objects they represent. That is, if two vectors in an SDR have 1s in the same position, then they are semantically similar in that attribute. The bits in SDRs have semantic meaning, and that meaning is distributed across the bits.\n\nThe semantic folding theory builds on these SDR properties to propose a new model for language semantics, where words are encoded into word-SDRs and the similarity between terms, sentences and texts can be calculated with simple distance measures.\n\nLikened to a Bayesian network, an HTM comprises a collection of nodes that are arranged in a tree-shaped hierarchy. Each node in the hierarchy discovers an array of causes in the input patterns and temporal sequences it receives. A Bayesian belief revision algorithm is used to propagate feed-forward and feedback beliefs from child to parent nodes and vice versa. However, the analogy to Bayesian networks is limited, because HTMs can be self-trained (such that each node has an unambiguous family relationship), cope with time-sensitive data, and grant mechanisms for covert attention.\n\nA theory of hierarchical cortical computation based on Bayesian belief propagation was proposed earlier by Tai Sing Lee and David Mumford. While HTM is mostly consistent with these ideas, it adds details about handling invariant representations in the visual cortex.\n\nLike any system that models details of the neocortex, HTM can be viewed as an artificial neural network. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM \"neurons\". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing. For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.\n\nLAMINART and similar neural networks researched by Stephen Grossberg attempt to model both the infrastructure of the cortex and the behavior of neurons in a temporal framework to explain neurophysiological and psychophysical data. However, these networks are, at present, too complex for realistic application.\n\nHTM is also related to work by Tomaso Poggio, including an approach for modeling the ventral stream of the visual cortex known as HMAX. Similarities of HTM to various AI ideas are described in the December 2005 issue of the Artificial Intelligence journal.\n\nNeocognitron, a hierarchical multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, is one of the first Deep Learning Neural Networks models.\n\nThe Numenta Platform for Intelligent Computer (NuPIC) is one of several available HTM implementations. Some are provided by Numenta, while some are developed and maintained by the HTM open source community.\n\nNuPIC includes implementations of Spatial Pooling and Temporal Memory in both C++ and Python. It also includes 3 APIs. Users can construct HTM systems using direct implementations of the algorithms, or construct a Network using the Network API, which is a flexible framework for constructing complicated associations between different Layers of cortex. \n\nNuPIC 1.0 was released on July 2017, after which the codebase was put into maintenance mode. Current research continues in Numenta research codebases.\n\nThe following commercial applications are available using NuPIC: \nThe following tools are available on NuPIC:\nThe following example applications are available on NuPIC, see http://numenta.com/applications/:\n\n\n\n\n",
    "id": "11273721",
    "title": "Hierarchical temporal memory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6836612",
    "text": "Autoencoder\n\nAn autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings.\nThe aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.\n\nArchitecturally, the simplest form of an autoencoder is a feedforward, non-recurrent neural network very similar to the multilayer perceptron (MLP) – having an input layer, an output layer and one or more hidden layers connecting them –, but with the output layer having the same number of nodes as the input layer, and with the purpose of \"reconstructing\" its own inputs (instead of predicting the target value formula_1 given inputs formula_2). Therefore, autoencoders are unsupervised learning models.\n\nAn autoencoder always consists of two parts, the encoder and the decoder, which can be defined as transitions formula_3 and formula_4 such that:\n\nIn the simplest case, where there is one hidden layer, the encoder stage of an autoencoder takes the input formula_8 and maps it to formula_9:\n\nThis image formula_11 is usually referred to as \"code\", \"latent variables\", or \"latent representation\". Here, formula_12 is an element-wise activation function such as a sigmoid function or a rectified linear unit. formula_13 is a weight matrix and formula_14 is a bias vector. After that, the decoder stage of the autoencoder maps formula_11 to the \"reconstruction\" formula_16 of the same shape as formula_17:\n\nwhere formula_19 for the decoder may differ in general from the corresponding formula_20 for the encoder, depending on the design of the autoencoder. \n\nAutoencoders are also trained to minimise reconstruction errors (such as squared errors):\n\nwhere formula_17 is usually averaged over some input training set.\n\nIf the feature space formula_23 has lower dimensionality than the input space formula_24, then the feature vector formula_25 can be regarded as a compressed representation of the input formula_26. If the hidden layers are larger than the input layer, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases.\n\nVarious techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations:\n\nDenoising autoencoders take a partially corrupted input whilst training to recover the original undistorted input. This technique has been introduced with a specific approach to \"good\" representation. A \"good representation is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input.\" This definition contains the following implicit assumptions:\n\n\nTo train an autoencoder to denoise data, it is necessary to perform preliminary stochastic mapping formula_27 in order to corrupt the data and use formula_28 as input for a normal autoencoder, with the only exception being that the loss should be still computed for the initial input formula_29 instead of formula_30.\n\nBy imposing sparsity on the hidden units during training (whilst having a larger number of hidden units than inputs), an autoencoder can learn useful structures in the input data. This allows sparse representations of inputs. These are useful in pretraining for classification tasks.\n\nSparsity may be achieved by additional terms in the loss function during training (by comparing the probability distribution of the hidden unit activations with some low desired value), or by manually zeroing all but the few strongest hidden unit activations (referred to as a \"k-sparse autoencoder\").\n\nVariational autoencoder models inherit autoencoder architecture, but make strong assumptions concerning the distribution of latent variables. They use variational approach for latent representation learning, which results in an additional loss component and specific training algorithm called \"Stochastic Gradient Variational Bayes (SGVB)\". It assumes that the data is generated by a directed graphical model formula_31 and that the encoder is learning an approximation formula_32 to the posterior distribution formula_33 where formula_34 and formula_35 denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The objective of the variational autoencoder in this case has the following form:\nHere, formula_37 stands for the Kullback–Leibler divergence. The prior over the latent variables is usually set to be the centred isotropic multivariate Gaussian formula_38; however, alternative configurations have also been recently considered, e.g. \n\nContractive autoencoder adds an explicit regularizer in their objective function that forces the model to learn a function that is robust to slight variations of input values. This regularizer corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. The final objective function has the following form:\n\nIf linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA).\n\nThe training algorithm for an autoencoder can be summarized as\n\nAn autoencoder is often trained using one of the many variants of backpropagation (such as conjugate gradient method, steepest descent, etc.). Though these are often reasonably effective, there are fundamental problems with the use of backpropagation to train networks with many hidden layers. Once errors are backpropagated to the first few layers, they become minuscule and insignificant. This means that the network will almost always learn to reconstruct the average of all the training data. Though more advanced backpropagation methods (such as the conjugate gradient method) can solve this problem to a certain extent, they still result in a very slow learning process and poor solutions. This problem can be remedied by using initial weights that approximate the final solution. The process of finding these initial weights is often referred to as \"pretraining\".\n\nGeoffrey Hinton developed a pretraining technique for training many-layered \"deep\" autoencoders. This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results. This model takes the name of deep belief network.\n\n",
    "id": "6836612",
    "title": "Autoencoder"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33742232",
    "text": "Restricted Boltzmann machine\n\nA restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. \n\nRBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,\nand rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000s. RBMs have found applications in dimensionality reduction,\nclassification,\ncollaborative filtering, feature learning\nand topic modelling.\nThey can be trained in either supervised or unsupervised ways, depending on the task.\n\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.\n\nRestricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.\n\nThe standard type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, and consists of a matrix of weights formula_1 (size \"m\"×\"n\") associated with the connection between hidden unit formula_2 and visible unit formula_3, as well as bias weights (offsets) formula_4 for the visible units and formula_5 for the hidden units. Given these, the \"energy\" of a configuration (pair of boolean vectors) is defined as\n\nor, in matrix notation,\n\nThis energy function is analogous to that of a Hopfield network. As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the energy function:\n\nwhere formula_9 is a partition function defined as the sum of formula_10 over all possible configurations (in other words, just a normalizing constant to ensure the probability distribution sums to 1). Similarly, the (marginal) probability of a visible (input) vector of booleans is the sum over all possible hidden layer configurations:\n\nSince the RBM has the shape of a bipartite graph, with no intra-layer connections, the hidden unit activations are mutually independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the hidden unit activations. That is, for formula_12 visible units and formula_13 hidden units, the conditional probability of a configuration of the visible units , given a configuration of the hidden units , is\n\nConversely, the conditional probability of given is\n\nThe individual activation probabilities are given by\n\nwhere formula_18 denotes the logistic sigmoid.\n\nThe visible units of RBM can be multinomial, although the hidden units are Bernoulli. In this case, the logistic function for visible units is replaced by the softmax function\n\nwhere \"K\" is the number of discrete values that the visible values have. They are applied in topic modeling, and recommender systems.\n\nRestricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.\nTheir graphical model corresponds to that of factor analysis.\n\nRestricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set formula_20 (a matrix, each row of which is treated as a visible vector formula_21),\n\nor equivalently, to maximize the expected log probability of a training sample formula_21 selected randomly from formula_20:\n\nThe algorithm most often used to train RBMs, that is, to optimize the weight vector formula_26, is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.\nThe algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.\n\nThe basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:\n\n\nA Practical Guide to Training RBMs written by Hinton can be found on his homepage.\n\n\n",
    "id": "33742232",
    "title": "Restricted Boltzmann machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=404084",
    "text": "Hebbian theory\n\nHebbian theory is a theory in neuroscience that proposes an explanation for the adaptation of neurons in the brain during the learning process. It describes a basic mechanism for synaptic plasticity, where an increase in synaptic efficacy arises from the presynaptic cell's repeated and persistent stimulation of the postsynaptic cell. Introduced by Donald Hebb in his 1949 book \"The Organization of Behavior\", the theory is also called Hebb's rule, Hebb's postulate, and cell assembly theory. Hebb states it as follows:\n\nLet us assume that the persistence or repetition of a reverberatory activity (or \"trace\") tends to induce lasting cellular changes that add to its stability.[…] When an axon of cell \"A\" is near enough to excite a cell \"B\" and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that \"A\"s efficiency, as one of the cells firing \"B\", is increased.\n\nThe theory is often summarized as \"Cells that fire together wire together.\" This summary, however, should not be taken too literally. Hebb emphasized that cell \"A\" needs to \"take part in firing\" cell \"B\", and such causality can occur only if cell \"A\" fires just before, not at the same time as, cell \"B\". This important aspect of causation in Hebb's work foreshadowed what is now known about spike-timing-dependent plasticity, which requires temporal precedence.\n\nThe theory attempts to explain associative or Hebbian learning, in which simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells. It also provides a biological basis for errorless learning methods for education and memory rehabilitation. In the study of neural networks in cognitive function, it is often regarded as the neuronal basis of unsupervised learning.\n\nHebbian theory concerns how neurons might connect themselves to become engrams. Hebb's theories on the form and function of cell assemblies can be understood from the following:\n\nThe general idea is an old one, that any two cells or systems of cells that are repeatedly active at the same time will tend to become 'associated', so that activity in one facilitates activity in the other.\n\nHebb also wrote:\nWhen one cell repeatedly assists in firing another, the axon of the first cell develops synaptic knobs (or enlarges them if they already exist) in contact with the soma of the second cell.\n\nGordon Allport posits additional ideas regarding cell assembly theory and its role in forming engrams, along the lines of the concept of auto-association, described as follows:\n\nIf the inputs to a system cause the same pattern of activity to occur repeatedly, the set of active elements constituting that pattern will become increasingly strongly interassociated. That is, each element will tend to turn on every other element and (with negative weights) to turn off the elements that do not form part of the pattern. To put it another way, the pattern as a whole will become 'auto-associated'. We may call a learned (auto-associated) pattern an engram.\n\nHebbian theory has been the primary basis for the conventional view that, when analyzed from a holistic level, engrams are neuronal nets or neural networks.\n\nWork in the laboratory of Eric Kandel has provided evidence for the involvement of Hebbian learning mechanisms at synapses in the marine gastropod \"Aplysia californica\".\n\nExperiments on Hebbian synapse modification mechanisms at the central nervous system synapses of vertebrates are much more difficult to control than are experiments with the relatively simple peripheral nervous system synapses studied in marine invertebrates. Much of the work on long-lasting synaptic changes between vertebrate neurons (such as long-term potentiation) involves the use of non-physiological experimental stimulation of brain cells. However, some of the physiologically relevant synapse modification mechanisms that have been studied in vertebrate brains do seem to be examples of Hebbian processes. One such study reviews results from experiments that indicate that long-lasting changes in synaptic strengths can be induced by physiologically relevant synaptic activity working through both Hebbian and non-Hebbian mechanisms.\n\nFrom the point of view of artificial neurons and artificial neural networks, Hebb's principle can be described as a method of determining how to alter the weights between model neurons. The weight between two neurons increases if the two neurons activate simultaneously, and reduces if they activate separately. Nodes that tend to be either both positive or both negative at the same time have strong positive weights, while those that tend to be opposite have strong negative weights.\n\nThe following is a formulaic description of Hebbian learning: (note that many other descriptions are possible)\n\nwhere formula_2 is the weight of the connection from neuron formula_3 to neuron formula_4 and formula_5 the input for neuron formula_4. Note that this is pattern learning (weights updated after every training example). In a Hopfield network, connections formula_2 are set to zero if formula_8 (no reflexive connections allowed). With binary neurons (activations either 0 or 1), connections would be set to 1 if the connected neurons have the same activation for a pattern.\n\nAnother formulaic description is:\n\nwhere formula_2 is the weight of the connection from neuron formula_3 to neuron formula_4, formula_13 is the number of training patterns, and formula_14 the formula_15th input for neuron formula_4. This is learning by epoch (weights updated after all the training examples are presented). Again, in a Hopfield network, connections formula_2 are set to zero if formula_8 (no reflexive connections).\n\nA variation of Hebbian learning that takes into account phenomena such as blocking and many other neural learning phenomena is the mathematical model of Harry Klopf. Klopf's model reproduces a great many biological phenomena, and is also simple to implement.\n\nHebb's Rule is often generalized as\n\nor the change in the formula_20th synaptic weight formula_21 is equal to a learning rate formula_22 times the formula_20th input formula_24 times the postsynaptic response formula_25. Often cited is the case of a linear neuron,\n\nand the previous section's simplification takes both the learning rate and the input weights to be 1. This version of the rule is clearly unstable, as in any network with a dominant signal the synaptic weights will increase or decrease exponentially. However, it can be shown that for \"any\" neuron model, Hebb's rule is unstable. Therefore, network models of neurons usually employ other learning theories such as BCM theory, Oja's rule, or the Generalized Hebbian Algorithm.\n\nNeurons also exhibit plasticity in their intrinsic excitability (intrinsic plasticity).\n\nDespite the common use of Hebbian models for long-term potentiation, there exist several exceptions to Hebb's principles and examples that demonstrate that some aspects of the theory are oversimplified. One of the most well-documented of these exceptions pertains to how synaptic modification may not simply occur only between activated neurons A and B, but to neighboring neurons as well. This is due to how Hebbian modification depends on retrograde signaling in order to modify the presynaptic neuron. The compound most commonly identified as fulfilling this retrograde transmitter role is nitric oxide, which, due to its high solubility and diffusibility, often exerts effects on nearby neurons. This type of diffuse synaptic modification, known as volume learning, counters, or at least supplements, the traditional Hebbian model.\n\nHebbian learning and spike-timing-dependent plasticity have been used in an influential theory of how mirror neurons emerge. Mirror neurons are neurons that fire both when an individual performs an action and when the individual sees or hears another perform a similar action. The discovery of these neurons has been very influential in explaining how individuals make sense of the actions of others, by showing that, when a person perceives the actions of others, the person activates the motor programs which they would use to perform similar actions. The activation of these motor programs then adds information to the perception and helps predict what the person will do next based on the perceiver's own motor program. A challenge has been to explain how individuals come to have neurons that respond both while performing an action and while hearing or seeing another perform similar actions.\n\nChristian Keysers and David Perrett suggested that, while an individual performs a particular action, the individual will see, hear, and feel himself perform the action. These re-afferent sensory signals will trigger activity in neurons responding to the sight, sound, and feel of the action. Because the activity of these sensory neurons will consistently overlap in time with those of the motor neurons that caused the action, Hebbian learning would predict that the synapses connecting neurons responding to the sight, sound, and feel of an action and those of the neurons triggering the action should be potentiated. The same is true while people look at themselves in the mirror, hear themselves babble, or are imitated by others. After repeated experience of this re-afference, the synapses connecting the sensory and motor representations of an action would be so strong that the motor neurons would start firing to the sound or the vision of the action, and a mirror neuron would have been created.\n\nEvidence for that perspective comes from many experiments that show that motor programs can be triggered by novel auditory or visual stimuli after repeated pairing of the stimulus with the execution of the motor program (for a review of the evidence, see Giudice et al., 2009). For instance, people who have never played the piano do not activate brain regions involved in playing the piano when listening to piano music. Five hours of piano lessons, in which the participant is exposed to the sound of the piano each time he presses a key, suffices to later trigger activity in motor regions of the brain upon listening to piano music. Consistent with the fact that spike-timing-dependent plasticity occurs only if the presynaptic neuron's firing predicts the post-synaptic neuron's firing, the link between sensory stimuli and motor programs also only seem to be potentiated if the stimulus is contingent on the motor program.\n\n\n\n",
    "id": "404084",
    "title": "Hebbian theory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50073184",
    "text": "Generative adversarial network\n\nGenerative adversarial networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. They were introduced by Ian Goodfellow \"et al.\" in 2014.\nThis technique can generate photographs that look at least superficially authentic to human observers, having many realistic characteristics (though in tests people can tell real from generated in many cases).\n\nOne network generates candidates and the other evaluates them. Typically, the generative network learns to map from a latent space to a particular data distribution of interest, while the discriminative network discriminates between instances from the true data distribution and candidates produced by the generator. The generative network's training objective is to increase the error rate of the discriminative network (i.e., \"fool\" the discriminator network by producing novel synthesized instances that appear to have come from the true data distribution).\n\nIn practice, a known dataset serves as the initial training data for the discriminator. Training the discriminator involves presenting it with samples from the dataset, until it reaches some level of accuracy. Typically the generator is seeded with a randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, samples synthesized by the generator are evaluated by the discriminator. Backpropagation is applied in both networks so that the generator produces better images, while the discriminator becomes more skilled at flagging synthetic images. The generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\n\nThe idea to infer models in a competitive setting (model versus discriminator) was proposed by Li, Gauci and Gross in 2013. Their method is used for behavioral inference. It is termed Turing Learning, as the setting is akin to that of a Turing test. The idea of adversarial training can also be found in earlier works, such as Schmidhuber in 1992.\n\nGANs have been used to produce samples of photorealistic images for the purposes of visualizing new interior/industrial design, shoes, bags and clothing items or items for computer games' scenes. These networks were reported to be used by Facebook. Recently, GANs have modeled patterns of motion in video. They have also been used to reconstruct 3D models of objects from images and to improve astronomical images. In 2017 a fully convolutional feedforward GAN was used for image enhancement using automated texture synthesis in combination with perceptual loss. The system focused on realistic textures rather than pixel-accuracy. The result was a higher image quality at high magnification.\n",
    "id": "50073184",
    "title": "Generative adversarial network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47805",
    "text": "Vector quantization\n\nVector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.\n\nThe density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.\n\nVector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.\n\nA simple training algorithm for vector quantization is :\n\nA more sophisticated algorithm reduces the bias in the density matching estimation, and ensures that all points are used, by including an extra sensitivity parameter :\n\nIt is desirable to use a cooling schedule to produce convergence: see Simulated annealing. Another (simpler) method is LBG which is based on K-Means.\n\nThe algorithm can be iteratively updated with 'live' data, rather than by picking random points from a data set, but this will introduce some bias if the data are temporally correlated over many samples.\nA vector is represented either geometrically by an arrow whose length corresponds to its magnitude and points in an appropriate direction, or by two or three numbers representing the magnitude of its components.\n\nVector quantization is used for lossy data compression, lossy data correction, pattern recognition, density estimation and clustering.\n\nLossy data correction, or prediction, is used to recover data missing from some dimensions. It is done by finding the nearest group with the data dimensions available, then predicting the result based on the values for the missing dimensions, assuming that they will have the same value as the group's centroid.\n\nFor density estimation, the area/volume that is closer to a particular centroid than to any other is inversely proportional to the density (due to the density matching property of the algorithm).\n\nVector quantization, also called \"block quantization\" or \"pattern matching quantization\" is often used in lossy data compression. It works by encoding values from a multidimensional vector space into a finite set of values from a discrete subspace of lower dimension. A lower-space vector requires less storage space, so the data is compressed. Due to the density matching property of vector quantization, the compressed data has errors that are inversely proportional to density.\n\nThe transformation is usually done by projection or by using a codebook. In some cases, a codebook can be also used to entropy code the discrete value in the same step, by generating a prefix coded variable-length encoded value as its output.\n\nThe set of discrete amplitude levels is quantized jointly rather than each sample being quantized separately. Consider a \"k\"-dimensional vector formula_12 of amplitude levels. It is compressed by choosing the nearest matching vector from a set of \"n\"-dimensional vectors formula_13, with \"n\" < \"k\".\n\nAll possible combinations of the \"n\"-dimensional vector formula_13 form the vector space to which all the quantized vectors belong.\nOnly the index of the codeword in the codebook is sent instead of the quantized values. This conserves space and achieves more compression.\n\nTwin vector quantization (VQF) is part of the MPEG-4 standard dealing with time domain weighted interleaved vector quantization.\n\nThe usage of video codecs based on vector quantization has declined significantly in favor of those based on motion compensated prediction combined with transform coding, e.g. those defined in MPEG standards, as the low decoding complexity of vector quantization has become less relevant.\n\n\nVQ was also used in the eighties for speech and speaker recognition.\nRecently it has also been used for efficient nearest neighbor search \n\nand on-line signature recognition. \nIn pattern recognition applications, one codebook is constructed for each class (each class being a user in biometric applications) using acoustic vectors of this user. In the testing phase the quantization distortion of a testing signal is worked out with the whole set of codebooks obtained in the training phase. The codebook that provides the smallest vector quantization distortion indicates the identified user.\n\nThe main advantage of VQ in pattern recognition is its low computational burden when compared with other techniques such as dynamic time warping (DTW) and hidden Markov model (HMM). The main drawback when compared to DTW and HMM is that it does not take into account the temporal evolution of the signals (speech, signature, etc.) because all the vectors are mixed up. In order to overcome this problem a multi-section codebook approach has been proposed. The multi-section approach consists of modelling the signal with several sections (for instance, one codebook for the initial part, another one for the center and a last codebook for the ending part).\n\nAs VQ is seeking for centroids as density points of nearby lying samples, it can be also directly used as a prototype-based clustering method: each centroid is then associated with one prototype. \nBy aiming to minimize the expected squared quantization error and introducing a decreasing learning gain fulfilling the Robbins-Monro conditions, multiple iterations over the whole data set with a concrete but fixed number of prototypes converges to the solution of k-means clustering algorithm in an incremental manner.\n\n\"Part of this article was originally based on material from the Free On-line Dictionary of Computing and is used with under the GFDL.\"\n\n",
    "id": "47805",
    "title": "Vector quantization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=76996",
    "text": "Self-organizing map\n\nA self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\nThis makes SOMs useful for visualizing low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.\n\nLike most artificial neural networks, SOMs operate in two modes: training and mapping. \"Training\" builds the map using input examples (a competitive process, also called vector quantization), while \"mapping\" automatically classifies a new input vector.\n\nA self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. The usual arrangement of nodes is a two-dimensional regular spacing in a hexagonal or rectangular grid. The self-organizing map describes a mapping from a higher-dimensional input space to a lower-dimensional map space. The procedure for placing a vector from data space onto the map is to find the node with the closest (smallest distance metric) weight vector to the data space vector.\n\nWhile it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation.\n\nUseful extensions include using toroidal grids where opposite edges are connected and using large numbers of nodes.\n\nIt has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.\n\nIt is also common to use the U-Matrix. The U-Matrix value of a particular node is the average distance between the node's weight vector and that of its closest neighbors. In a square grid, for instance, we might consider the closest 4 or 8 nodes (the Von Neumann and Moore neighborhoods, respectively), or six nodes in a hexagonal grid.\n\nLarge SOMs display emergent properties. In maps consisting of thousands of nodes, it is possible to perform cluster operations on the map itself.\n\nThe goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. This is partly motivated by how visual, auditory or other sensory information is handled in separate parts of the cerebral cortex in the human brain.\nThe weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors. With the latter alternative, learning is much faster because the initial weights already give a good approximation of SOM weights.\n\nThe network must be fed a large number of example vectors that represent, as close as possible, the kinds of vectors expected during mapping. The examples are usually administered several times as iterations.\n\nThe training utilizes competitive learning. When a training example is fed to the network, its Euclidean distance to all weight vectors is computed. The neuron whose weight vector is most similar to the input is called the best matching unit (BMU). The weights of the BMU and neurons close to it in the SOM lattice are adjusted towards the input vector. The magnitude of the change decreases with time and with distance (within the lattice) from the BMU. The update formula for a neuron v with weight vector W(s) is\nwhere s is the step index, t an index into the training sample, u is the index of the BMU for D(t), α(s) is a monotonically decreasing learning coefficient and D(t) is the input vector; Θ(u, v, s) is the neighborhood function which gives the distance between the neuron u and the neuron v in step s. Depending on the implementations, t can scan the training data set systematically (t is 0, 1, 2...T-1, then repeat, T being the training sample's size), be randomly drawn from the data set (bootstrap sampling), or implement some other sampling method (such as jackknifing).\n\nThe neighborhood function Θ(u, v, s) depends on the lattice distance between the BMU (neuron \"u)\" and neuron \"v\". In the simplest form it is 1 for all neurons close enough to BMU and 0 for others, but a Gaussian function is a common choice, too. Regardless of the functional form, the neighborhood function shrinks with time. At the beginning when the neighborhood is broad, the self-organizing takes place on the global scale. When the neighborhood has shrunk to just a couple of neurons, the weights are converging to local estimates. In some implementations the learning coefficient α and the neighborhood function Θ decrease steadily with increasing s, in others (in particular those where t scans the training data set) they decrease in step-wise fashion, once every T steps.\n\nThis process is repeated for each input vector for a (usually large) number of cycles λ. The network winds up associating output nodes with groups or patterns in the input data set. If these patterns can be named, the names can be attached to the associated nodes in the trained net.\n\nDuring mapping, there will be one single \"winning\" neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector.\n\nWhile representing input data as vectors has been emphasized in this article, it should be noted that any kind of object which can be represented digitally, which has an appropriate distance measure associated with it, and in which the necessary operations for training are possible can be used to construct a self-organizing map. This includes matrices, continuous functions or even other self-organizing maps.\n\nThese are the variables needed, with vectors in bold,\n\n\nA variant algorithm:\n\nSelection of a good initial approximation is a well-known problem for all iterative methods of learning neural networks. Kohonen used random initiation of SOM weights. Recently, principal component initialization, in which initial map weights are chosen from the space of the first principal components, has become popular due to the exact reproducibility of the results.\n\nCareful comparison of the random initiation approach to principal component initialization for one-dimensional SOM (models of principal curves) demonstrated that the advantages of principal component SOM initialization are not universal. The best initialization method depends on the geometry of the specific dataset. Principal component initialization is preferable (in dimension one) if the principal curve approximating the dataset can be univalently and linearly projected on the first principal component (quasilinear sets). For nonlinear datasets, however, random initiation performs better.\n\nConsider an array of nodes, each of which contains a weight vector and is aware of its location in the array. Each weight vector is of the same dimension as the node's input vector. The weights may initially be set to random values.\n\nNow we need input to feed the map. Colors can be represented by their red, green, and blue components. Consequently, we will represent colors as vectors in the unit cube of the free vector space over generated by the basis:\n\nThe diagram shown compares the results of training on the data sets\n\nand the original images. Note the striking resemblance between the two.\n\nSimilarly, after training a grid of neurons for 250 iterations with a learning rate of 0.1 on Fisher's Iris, the map can already detect the main differences between species. \n\nThere are two ways to interpret a SOM. Because in the training phase weights of the whole neighborhood are moved in the same direction, similar items tend to excite adjacent neurons. Therefore, SOM forms a semantic map where similar samples are mapped close together and dissimilar ones apart. This may be visualized by a U-Matrix (Euclidean distance between weight vectors of neighboring cells) of the SOM.\n\nThe other way is to think of neuronal weights as pointers to the input space. They form a discrete approximation of the distribution of training samples. More neurons point to regions with high training sample concentration and fewer where the samples are scarce.\n\nSOM may be considered a nonlinear generalization of Principal components analysis (PCA). It has been shown, using both artificial and real geophysical data, that SOM has many advantages over the conventional feature extraction methods such as Empirical Orthogonal Functions (EOF) or PCA.\n\nOriginally, SOM was not formulated as a solution to an optimisation problem. Nevertheless, there have been several attempts to modify the definition of SOM and to formulate an optimisation problem which gives similar results. For example, Elastic maps use the mechanical metaphor of elasticity to approximate principal manifolds: the analogy is an elastic membrane and plate.\n\n\n\n",
    "id": "76996",
    "title": "Self-organizing map"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=26266110",
    "text": "Competitive learning\n\nCompetitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network. It is well suited to finding clusters within data.\n\nModels and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps (Kohonen maps).\n\nThere are three basic elements to a competitive learning rule:\n\n\nAccordingly, the individual neurons of the network learn to specialize on ensembles of similar patterns and in so doing become 'feature detectors' for different classes of input patterns.\n\nThe fact that competitive networks recode sets of correlated inputs to one of a few output neurons essentially removes the redundancy in representation which is an essential part of processing in biological sensory systems.\n\nCompetitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as “competitive layer”. Every competitive neuron is described by a vector of weights formula_1 and calculates the similarity measure between the input data formula_2 and the weight vector formula_3 .\n\nFor every input vector, the competitive neurons “compete” with each other to see which one of them is the most similar to that particular input vector. The winner neuron m sets its output formula_4 and all the other competitive neurons set their output formula_5.\n\nUsually, in order to measure similarity the inverse of the Euclidean distance is used: formula_6 between the input vector formula_7 and the weight vector formula_3.\n\nHere is a simple competitive learning algorithm to find three clusters within some input data.\n\n1. (Set-up.) Let a set of sensors all feed into three different nodes, so that every node is connected to every sensor. Let the weights that each node gives to its sensors be set randomly between 0.0 and 1.0. Let the output of each node be the sum of all its sensors, each sensor's signal strength being multiplied by its weight.\n\n2. When the net is shown an input, the node with the highest output is deemed the winner. The input is classified as being within the cluster corresponding to that node.\n\n3. The winner updates each of its weights, moving weight from the connections that gave it weaker signals to the connections that gave it stronger signals.\n\nThus, as more data are received, each node converges on the centre of the cluster that it has come to represent and activates more strongly for inputs in this cluster and more weakly for inputs in other clusters.\n\n\n",
    "id": "26266110",
    "title": "Competitive learning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1217294",
    "text": "Linear genetic programming\n\nLinear genetic programming (LGP) is a particular subset of genetic programming wherein computer programs in a population are represented as a sequence of instructions from imperative programming language or machine language. The graph-based data flow that results from a multiple usage of register contents and the existence of structurally noneffective code (introns) are two main differences of this genetic representation from the more common tree-based genetic programming (TGP) variant.\n\nIn genetic programming (GP) a linear tree is a program composed of a variable number of unary functions and a single terminal. Note linear tree GP differs from bit string genetic algorithms since a population may contain programs of different lengths and there may be more than two types of functions or more than two types of terminals.\n\nBecause LGP programs are basically represented by a linear sequence of instructions, they are simpler to read and to operate on than their tree-based counterparts. For example, a simple program written in the LGP language Slash/A looks like a series of instructions separated by a slash:\n\nBy representing such code in bytecode format, i.e. as an array of bytes each representing a different instruction, one can make mutation operations simply by changing an element of such an array.\n\n\n",
    "id": "1217294",
    "title": "Linear genetic programming"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5999404",
    "text": "Joseph Nechvatal\n\nJoseph James Nechvatal (born 15 January 1951) is a post-conceptual digital artist and art theoretician who creates computer-assisted paintings and computer animations, often using custom-created computer viruses.\n\nJoseph Nechvatal was born in Chicago. He studied fine art and philosophy at Southern Illinois University Carbondale, Cornell University and Columbia University, where he studied with Arthur Danto while serving as the archivist to the minimalist composer La Monte Young. From 1979, he exhibited his work in New York City, primarily at Galerie Richard, Brooke Alexander Gallery and Universal Concepts Unlimited. He has also solo exhibited in Berlin, Paris, Chicago, Cologne, Atlanta, Los Angeles, Aalst, Belgium, Youngstown, Senouillac, Lund, Toulouse, Turin and Munich.\n\nHis work in the early 1980s chiefly consisted of postminimalist gray graphite drawings that were often photomechanically enlarged. During that period he was associated with the artist group Colab and helped establish the non-profit cultural space ABC No Rio. In 1983 he co-founded the avant-garde electronic art music audio project Tellus Audio Cassette Magazine. In 1984, Nechvatal began work on an opera called \"\" (1984-6) with the no wave musical composer Rhys Chatham.\n\nHe began using computers to make \"paintings\" in 1986 and later, in his signature work, began to employ computer viruses. These \"collaborations\" with viral systems positioned his work as an early contribution to what is increasingly referred to as a post-human aesthetic.\n\nFrom 1991–1993 he was artist-in-residence at the Louis Pasteur Atelier in Arbois, France and at the Saline Royale/Ledoux Foundation's computer lab.There he worked on \"The Computer Virus Project\", which was an artistic experiment with computer viruses and computer animation. He exhibited at Documenta 8 in 1987.\n\nIn 1999 Nechvatal obtained his Ph.D. in the philosophy of art and new technology concerning immersive virtual reality at Roy Ascott's Centre for Advanced Inquiry in the Interactive Arts (CAiiA), University of Wales College, Newport, UK (now the Planetary Collegium at the University of Plymouth). There he developed his concept of viractualism, a conceptual art idea that strives \"to create an interface between the biological and the technological.\" According to Nechvatal, this is a new topological space.\n\nIn 2002 he extended his experimentation into viral artificial life through a collaboration with the programmer Stephane Sikora of music2eye in a work called the \"Computer Virus Project II\", inspired by the a-life work of John Horton Conway (particularly Conway's Game of Life), by the general cellular automata work of John von Neumann, by the genetic programming algorithms of John Koza and the auto-destructive art of Gustav Metzger.\n\nIn 2005 he exhibited \"Computer Virus Project II\" works (digital paintings, digital prints, a digital audio installation and two \"live\" electronic virus-attack art installations) in a solo show called \"cOntaminatiOns\" at Château de Linardié in Senouillac, France. In 2006 Nechvatal received a retrospective exhibition entitled \"Contaminations\" at the Butler Institute of American Art's Beecher Center for Arts and Technology.\n\nDr. Nechvatal has also contributed to digital audio work with his noise music \"viral symphOny\", a collaborative sound symphony created by using his computer virus software at the Institute for Electronic Arts at Alfred University. \"viral symphOny\" was presented as a part of \"nOise anusmOs\" in New York in 2012. In 2016, a limited edition CD recording of his sex farce poetry book \"Destroyer of Naivetés\" was released on Entr’acte label under the name of Cave Bacchus. Cave Bacchus is Nechvatal, Black Sifichi and Rhys Chatham.\n\nIn 2013, Nechvatal showed work in \"Noise\", an official collateral show of the \"55th Venice Biennale of Art\", that was based on his book \"Immersion Into Noise\".\n\nFrom 1999 to 2013, Nechvatal taught art theories of immersive virtual reality and the viractual at the School of Visual Arts in New York City (SVA). A book of his collected essays entitled \"Towards an Immersive Intelligence: Essays on the Work of Art in the Age of Computer Technology and Virtual Reality (1993–2006)\" was published by Edgewise Press in 2009. Also in 2009, his book \"Immersive Ideals / Critical Distances\" was published. In 2011, his book \"Immersion Into Noise\" was published by Open Humanities Press in conjunction with the University of Michigan Library's Scholarly Publishing Office. In 2014 he published (as editor) a book and CD/cassette tape with Punctum Books and Punctum Records on the noise music artist Minóy and in 2015 he published with Punctum Books a collection of his farcical erotic poetry entitled \"Destroyer of Naivetés\". Since 2013, Nechvatal has regularly been publishing his art criticism as the Paris correspondent for Hyperallergic blogazine.\n\nJoe Lewis wrote:\nViractualism is an art theory term developed by Nechvatal in 1999. The term viractualism (and viractuality ) emerged from the Ph.D. research Nechvatal conducted in the philosophy of art and new technology concerning immersive virtual reality at Roy Ascott's Centre for Advanced Inquiry in the Interactive Arts (CAiiA), University of Wales College, Newport, UK (now the Planetary Collegium at the University of Plymouth). There he developed his concept of the viractual, which strives to create an interface between the biological and the virtual. It is central to Nechvatal’s work as an artist.\n\nNechvatal suggests that the term (concept) viractual (and viractualism or viractuality) may be an entrainment/égréore conception helpful in defining our now third-fused inter-spatiality which is forged from the meeting of the virtual and the actual. - a concept close to what the military call augmented reality, which is the use of transparent displays worn as see-through glasses on which computer data is projected and layered.\n\nThe basis of the viractual conception is that virtual producing computer technology has become a noteworthy means for making and understanding contemporary art and that this brings artists to a place where one finds the emerging of the computed (the virtual) with the uncomputed corporeal (the actual). This amalgamate - which tends to contradict some central techno clichés of our time - is what Nechvatal calls the viractual.\nDigitization is a key metaphor for viractuality in the sense that it is the elementary translating procedure today. Nechvatal thinks that in every era the attempt must be made anew to wrest the art practice away from conformisms that are about to overcome it.\n\nCybism is an art theory term developed by Nechvatal as a sub-division of viractuality following discussion with artist Kenneth Wahl at the turn of the century. Wahl preferred the term \"scybism\". The concept was proposed by Nechvatal for an exhibition in 2003 called \"The Attractions of Cybism\" for Fairfield University that never was realized.\n\nAs defined by Dr. Nechvatal, Cybism is a new sensibility emerging in art respecting the integration of certain aspects of science, technology and consciousness – a consciousness struggling to attend to the prevailing current spirit of our age. This cybistic zeitgeist Nechvatal identifies as being precisely a quality-of-life desire in which everything, everywhere, all at once is connected in a rhizomatic web of communication. Therefore, cybism is no longer content with the regurgitation of standardized repertoires. Rather Nechvatal detects in art a fertile attraction towards the abstractions of advanced scientific discovery - discovery now stripped of its fundamentally reductive logical methodology.\n\nNechvatal states that cybism can be used to characterize a certain group of researchers and their understanding of where cultural space is developing today. Cybists reflect on system dynamics with a hybrid blending (cybridization) of the computational supplied virtual with the analog. This blending of the computational virtual with the analog indicates the subsequent emergence of a new cybrid topological cognitive-vision that Nechvatal has called viractuality: the space of connection betwixt the computed virtual and the uncomputed corporeal (actual) world which merge in cybism.\n\nNechvatal states that co-extensive notions found in cybism have sharp ramifications for art as product in that the cybists are actively exploring the frontiers of science/technology research so as to become culturally aware of the biases of consciousness in order to amend those biases through the monumentality and permanency which can be found in powerful art. He begins with the realization that every new technology disrupts the previous rhythms of consciousness. In this sense cybist art research begins where hard science/technology ends.\n\n\n",
    "id": "5999404",
    "title": "Joseph Nechvatal"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12424",
    "text": "Genetic programming\n\nIn artificial intelligence, genetic programming (GP) is a technique whereby computer programs are encoded as a set of genes that are then modified (evolved) using an evolutionary algorithm (often a genetic algorithm, \"GA\") – it is an application of (for example) genetic algorithms where the space of solutions consists of computer programs. The results are computer programs able to perform well in a predefined task. The methods used to encode a computer program in an artificial chromosome and to evaluate its fitness with respect to the predefined task are central in the GP technique and still the subject of active research.\n\nIn 1954, pioneering work on what is today known as artificial life was carried out by Nils Aall Barricelli using the very early computers. In the 1960s and early 1970s, evolutionary algorithms became widely recognized as optimization methods. Ingo Rechenberg and his group were able to solve complex engineering problems through evolution strategies as documented in his 1971 PhD thesis and the resulting 1973 book. John Holland was highly influential during the 1970s. The establishment of evolutionary algorithms in the scientific community allowed, by then, the first concrete steps to study the GP idea.\n\nIn 1964, Lawrence J. Fogel, one of the earliest practitioners of the GP methodology, applied evolutionary algorithms to the problem of discovering finite-state automata. Later GP-related work grew out of the learning classifier system community, which developed sets of sparse rules describing optimal policies for Markov decision processes. \nIn 1981 Richard Forsyth evolved tree rules to classify heart disease.\nThe first statement of modern \"tree-based\" genetic programming (that is, procedural languages organized in tree-based structures and operated on by suitably defined GA-operators) was given by Nichael L. Cramer (1985). This work was later greatly expanded by John R. Koza, a main proponent of GP who has pioneered the application of genetic programming in various complex optimization and search problems. Gianna Giavelli, a student of Koza's, later pioneered the use of genetic programming as a technique to model DNA expression.\n\nIn the 1990s, GP was mainly used to solve relatively simple problems because it is very computationally intensive. Recently GP has produced many novel and outstanding results in areas such as quantum computing, electronic design, game playing, cyberterrorism prevention, sorting, and searching, due to improvements in GP technology and the exponential growth in CPU power.\nThese results include the replication or development of several post-year-2000 inventions. GP has also been applied to evolvable hardware as well as computer programs.\n\nDeveloping a theory for GP has been very difficult and so in the 1990s GP was considered a sort of outcast among search techniques.\n\nGP evolves computer programs, traditionally represented in memory as tree structures. Trees can be easily evaluated in a recursive manner. Every tree node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate. Thus traditionally GP favors the use of programming languages that naturally embody tree structures (for example, Lisp; other functional programming languages are also suitable).\n\nNon-tree representations have been suggested and successfully implemented, such as linear genetic programming which suits the more traditional imperative languages [see, for example, Banzhaf \"et al.\" (1998)]. The commercial GP software \"Discipulus\" uses automatic induction of binary machine code (\"AIM\") to achieve better performance. \"µGP\" uses directed multigraphs to generate programs that fully exploit the syntax of a given assembly language\n\nMost non-tree representations have structurally noneffective code (introns). Such non-coding genes may seem to be useless, because they have no effect on the performance of any one individual.\nHowever, experiments seem to show faster convergence when using program representations — such as linear genetic programming and Cartesian genetic programming — that allow such non-coding genes, compared to tree-based program representations that do not have any non-coding genes.\n\nThe basic ideas of genetic programming have been modified and extended in a variety of ways:\n\nMeta-genetic programming is the proposed meta learning technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer. Meta-GP was formally proposed by Jürgen Schmidhuber in 1987. Doug Lenat's Eurisko is an earlier effort that may be the same technique. It is a recursive but terminating algorithm, allowing it to avoid infinite recursion.\n\nCritics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes. This might take the form of a meta evolved GP for producing human walking algorithms which is then used to evolve human running, jumping, etc. The fitness criterion applied to the meta GP would simply be one of efficiency.\n\nFor general problem classes there may be no way to show that meta GP will reliably produce results more efficiently than a created algorithm other than exhaustion.\n\n\n",
    "id": "12424",
    "title": "Genetic programming"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1514696",
    "text": "Parity benchmark\n\nParity problems are widely used as benchmark problems in genetic programming but inherited from the artificial neural network community. Parity is calculated by summing all the binary inputs and reporting if the sum is odd or even. This is considered difficult because:\n\n",
    "id": "1514696",
    "title": "Parity benchmark"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=17808671",
    "text": "Schema (genetic algorithms)\n\nA schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets; and so form a topological space.\n\nFor example, consider binary strings of length 6. The schema 1**0*1 describes the set of all words of length 6 with 1's at the first and sixth positions and a 0 at the fourth position. The * is a wildcard symbol, which means that positions 2, 3 and 5 can have a value of either 1 or 0. The \"order of a schema\" is defined as the number of fixed positions in the template, while the \"defining length\" formula_1 is the distance between the first and last specific positions. The order of 1**0*1 is 3 and its defining length is 5. The \"fitness of a schema\" is the average fitness of all strings matching the schema. The fitness of a string is a measure of the value of the encoded problem solution, as computed by a problem-specific evaluation function.\n\nThe length of a schema formula_2, called formula_3, is defined as the total number of nodes in the schema. formula_3 is also equal to the number of nodes in the programs matching formula_2.\n\nIf the child of an individual that matches schema H does not \"itself\" match H, the schema is said to have been \"disrupted\".\n\nIn evolutionary computing such as genetic algorithms and genetic programming, propagation refers to the inheritance of characteristics of one generation by the next. For example, a schema is propagated if individuals in the current generation match it and so do those in the next generation. Those in the next generation may be (but don't have to be) children of parents who matched it.\n\nRecently schema have been studied using order theory.\n\nTwo basic operators are defined for schema: expansion and compression. The expansion maps a schema onto a set of words which it represents, while the compression maps a set of words on to a schema.\n\nIn the following definitions formula_6 denotes an alphabet, formula_7 denotes all words of length formula_8 over the alphabet formula_6, formula_10 denotes the alphabet formula_11 with the extra symbol formula_12. formula_13 denotes all schema of length formula_8 over the alphabet formula_10 as well as the empty schema formula_16.\nFor any schema formula_17 the following operator formula_18, called the formula_19 of formula_20, which maps formula_20 to a subset of words in formula_22:\n\nformula_23\n\nWhere subscript formula_24 denotes the character at position formula_24 in a word or schema. When formula_26 then formula_27. More simply put, formula_18 is the set of all words in formula_29 that can be made by exchanging the formula_12 symbols in formula_20 with symbols from formula_11. For example, if formula_33, formula_34 and formula_35 then formula_36.\n\nConversely, for any formula_37 we define formula_38, called the formula_39 of formula_40, which maps formula_40 on to a schema formula_42:\nformula_43\nwhere formula_20 is a schema of length formula_45 such that the symbol at position formula_24 in formula_20 is determined in the following way: if formula_48 for all formula_49 then formula_50 otherwise formula_51. If formula_52 then formula_53. One can think of this operator as stacking up all the items in formula_40 and if all elements in a column are equivalent, the symbol at that position in formula_20 takes this value, otherwise there is a wild card symbol. For example, let formula_56 then formula_57.\n\nSchemata can be partially ordered. For any formula_58 we say formula_59 if and only if formula_60. It follows that formula_61 is a partial ordering on a set of schemata from the reflexivity, antisymmetry and transitivity of the subset relation. For example, formula_62.\nThis is because formula_63.\n\nThe compression and expansion operators form a Galois connection, where formula_64 is the lower adjoint and formula_65 the upper adjoint.\n\nFor a set formula_37, we call the process of calculating the compression on each subset of A, that is <math>\\\n",
    "id": "17808671",
    "title": "Schema (genetic algorithms)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31726662",
    "text": "Santa Fe Trail problem\n\nThe Santa Fe Trail problem is a genetic programming exercise in which artificial ants search for food pellets according to a programmed set of instructions. The layout of food pellets in the Santa Fe Trail problem has become a standard for comparing different genetic programming algorithms and solutions.\n\nOne method for programming and testing algorithms on the Santa Fe Trail problem is by using the NetLogo application. There is at least one case of a student creating a Lego robotic ant to solve the problem.\n\n\n",
    "id": "31726662",
    "title": "Santa Fe Trail problem"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34114298",
    "text": "Java Grammatical Evolution\n\nIn computer science, Java Grammatical Evolution is an implementation of grammatical evolution in the Java programming language. Examples include jGE library and GEVA.\n\njGE library was the first published implementation of grammatical evolution in the Java language. Today, another well-known published Java implementation exists, named GEVA. GEVA was developed at University College Dublin's Natural Computing Research & Applications group under the guidance of one of the inventors of grammatical evolution, Dr. Michael O'Neill.\n\njGE library aims to provide not only an implementation of grammatical evolution, but also a free, open-source, and extendable framework for experimentation in the area of evolutionary computation. Namely, it supports the implementation (through additions and extensions) of any evolutionary algorithm. Furthermore, its extendable architecture and design facilitates the implementation and incorporation of new experimental implementation inspired by natural evolution and biology.\n\nThe jGE library binary file, the source code, the documentation, and an extension for the NetLogo modeling environment, named jGE NetLogo extension, can be downloaded from the jGE Official Web Site.\n\nThe jGE library is free software released under the GNU General Public License v3.\n\n",
    "id": "34114298",
    "title": "Java Grammatical Evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1098818",
    "text": "Gene expression programming\n\nIn computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype–phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.\n\nEvolutionary algorithms use populations of individuals, select individuals according to fitness, and introduce genetic variation using one or more genetic operators. Their use in artificial computational systems dates back to the 1950s where they were used to solve optimization problems (e.g. Box 1957 and Friedman 1959). But it was with the introduction of evolution strategies by Rechenberg in 1965 that evolutionary algorithms gained popularity. A good overview text on evolutionary algorithms is the book \"An Introduction to Genetic Algorithms\" by Mitchell (1996).\n\nGene expression programming belongs to the family of evolutionary algorithms and is closely related to genetic algorithms and genetic programming. From genetic algorithms it inherited the linear chromosomes of fixed length; and from genetic programming it inherited the expressive parse trees of varied sizes and shapes.\n\nIn gene expression programming the linear chromosomes work as the genotype and the parse trees as the phenotype, creating a genotype/phenotype system. This genotype/phenotype system is multigenic, thus encoding multiple parse trees in each chromosome. This means that the computer programs created by GEP are composed of multiple parse trees. Because these parse trees are the result of gene expression, in GEP they are called expression trees.\n\nThe genome of gene expression programming consists of a linear, symbolic string or chromosome of fixed length composed of one or more genes of equal size. These genes, despite their fixed length, code for expression trees of different sizes and shapes. An example of a chromosome with two genes, each of size 9, is the string (position zero indicates the start of each gene):\n\nwhere “L” represents the natural logarithm function and “a”, “b”, “c”, and “d” represent the variables and constants used in a problem.\n\nAs shown , the genes of gene expression programming have all the same size. However, these fixed length strings code for expression trees of different sizes. This means that the size of the coding regions varies from gene to gene, allowing for adaptation and evolution to occur smoothly.\n\nFor example, the mathematical expression:\ncan also be represented as an expression tree:\n\nwhere \"Q” represents the square root function.\n\nThis kind of expression tree consists of the phenotypic expression of GEP genes, whereas the genes are linear strings encoding these complex structures. For this particular example, the linear string corresponds to:\n\nwhich is the straightforward reading of the expression tree from top to bottom and from left to right. These linear strings are called k-expressions (from Karva notation).\n\nGoing from k-expressions to expression trees is also very simple. For example, the following k-expression:\n\nis composed of two different terminals (the variables “a” and “b”), two different functions of two arguments (“*” and “+”), and a function of one argument (“Q”). Its expression gives:\n\nThe k-expressions of gene expression programming correspond to the region of genes that gets expressed. This means that there might be sequences in the genes that are not expressed, which is indeed true for most genes. The reason for these noncoding regions is to provide a buffer of terminals so that all k-expressions encoded in GEP genes correspond always to valid programs or expressions.\n\nThe genes of gene expression programming are therefore composed of two different domains – a head and a tail – each with different properties and functions. The head is used mainly to encode the functions and variables chosen to solve the problem at hand, whereas the tail, while also used to encode the variables, provides essentially a reservoir of terminals to ensure that all programs are error-free.\n\nFor GEP genes the length of the tail is given by the formula:\n\nwhere \"h\" is the head’s length and \"n\" is maximum arity. For example, for a gene created using the set of functions F = {Q, +, −, *, /} and the set of terminals T = {a, b}, \"n\" = 2. And if we choose a head length of 15, then \"t\" = 15 (2–1) + 1 = 16, which gives a gene length \"g\" of 15 + 16 = 31. The randomly generated string below is an example of one such gene:\n\nIt encodes the expression tree:\n\nwhich, in this case, only uses 8 of the 31 elements that constitute the gene.\n\nIt's not hard to see that, despite their fixed length, each gene has the potential to code for expression trees of different sizes and shapes, with the simplest composed of only one node (when the first element of a gene is a terminal) and the largest composed of as many nodes as there are elements in the gene (when all the elements in the head are functions with maximum arity).\n\nIt's also not hard to see that it is trivial to implement all kinds of genetic modification (mutation, inversion, insertion, recombination, and so on) with the guarantee that all resulting offspring encode correct, error-free programs.\n\nThe chromosomes of gene expression programming are usually composed of more than one gene of equal length. Each gene codes for a sub-expression tree (sub-ET) or sub-program. Then the sub-ETs can interact with one another in different ways, forming a more complex program. The figure shows an example of a program composed of three sub-ETs.\n\nIn the final program the sub-ETs could be linked by addition or some other function, as there are no restrictions to the kind of linking function one might choose. Some examples of more complex linkers include taking the average, the median, the midrange, thresholding their sum to make a binomial classification, applying the sigmoid function to compute a probability, and so on. These linking functions are usually chosen a priori for each problem, but they can also be evolved elegantly and efficiently by the cellular system of gene expression programming.\n\nIn gene expression programming, homeotic genes control the interactions of the different sub-ETs or modules of the main program. The expression of such genes results in different main programs or cells, that is, they determine which genes are expressed in each cell and how the sub-ETs of each cell interact with one another. In other words, homeotic genes determine which sub-ETs are called upon and how often in which main program or cell and what kind of connections they establish with one another.\n\nHomeotic genes have exactly the same kind of structural organization as normal genes and they are built using an identical process. They also contain a head domain and a tail domain, with the difference that the heads contain now linking functions and a special kind of terminals – genic terminals – that represent the normal genes. The expression of the normal genes results as usual in different sub-ETs, which in the cellular system are called ADFs (automatically defined functions). As for the tails, they contain only genic terminals, that is, derived features generated on the fly by the algorithm.\n\nFor example, the chromosome in the figure has three normal genes and one homeotic gene and encodes a main program that invokes three different functions a total of four times, linking them in a particular way.\n\nFrom this example it is clear that the cellular system not only allows the unconstrained evolution of linking functions but also code reuse. And it shouldn't be hard to implement recursion in this system.\n\nMulticellular systems are composed of more than one homeotic gene. Each homeotic gene in this system puts together a different combination of sub-expression trees or ADFs, creating multiple cells or main programs.\n\nFor example, the program shown in the figure was created using a cellular system with two cells and three normal genes.\n\nThe applications of these multicellular systems are multiple and varied and, like the multigenic systems, they can be used both in problems with just one output and in problems with multiple outputs.\n\nThe head/tail domain of GEP genes (both normal and homeotic) is the basic building block of all GEP algorithms. However, gene expression programming also explores other chromosomal organizations that are more complex than the head/tail structure. Essentially these complex structures consist of functional units or genes with a basic head/tail domain plus one or more extra domains. These extra domains usually encode random numerical constants that the algorithm relentlessly fine-tunes in order to find a good solution. For instance, these numerical constants may be the weights or factors in a function approximation problem (see the GEP-RNC algorithm below); they may be the weights and thresholds of a neural network (see the GEP-NN algorithm below); the numerical constants needed for the design of decision trees (see the GEP-DT algorithm below); the weights needed for polynomial induction; or the random numerical constants used to discover the parameter values in a parameter optimization task.\n\nThe fundamental steps of the basic gene expression algorithm are listed below in pseudocode:\nThe first four steps prepare all the ingredients that are needed for the iterative loop of the algorithm (steps 5 through 10). Of these preparative steps, the crucial one is the creation of the initial population, which is created randomly using the elements of the function and terminal sets.\n\nLike all evolutionary algorithms, gene expression programming works with populations of individuals, which in this case are computer programs. Therefore, some kind of initial population must be created to get things started. Subsequent populations are descendants, via selection and genetic modification, of the initial population.\n\nIn the genotype/phenotype system of gene expression programming, it is only necessary to create the simple linear chromosomes of the individuals without worrying about the structural soundness of the programs they code for, as their expression always results in syntactically correct programs.\n\nFitness functions and selection environments (called training datasets in machine learning) are the two facets of fitness and are therefore intricately connected. Indeed, the fitness of a program depends not only on the cost function used to measure its performance but also on the training data chosen to evaluate fitness\n\nThe selection environment consists of the set of training records, which are also called fitness cases. These fitness cases could be a set of observations or measurements concerning some problem, and they form what is called the training dataset.\n\nThe quality of the training data is essential for the evolution of good solutions. A good training set should be representative of the problem at hand and also well-balanced, otherwise the algorithm might get stuck at some local optimum. In addition, it is also important to avoid using unnecessarily large datasets for training as this will slow things down unnecessarily. A good rule of thumb is to choose enough records for training to enable a good generalization in the validation data and leave the remaining records for validation and testing.\n\nBroadly speaking, there are essentially three different kinds of problems based on the kind of prediction being made:\nThe first type of problem goes by the name of regression; the second is known as classification, with logistic regression as a special case where, besides the crisp classifications like \"Yes\" or \"No\", a probability is also attached to each outcome; and the last one is related to Boolean algebra and logic synthesis.\n\nIn regression, the response or dependent variable is numeric (usually continuous) and therefore the output of a regression model is also continuous. So it's quite straightforward to evaluate the fitness of the evolving models by comparing the output of the model to the value of the response in the training data.\n\nThere are several basic fitness functions for evaluating model performance, with the most common being based on the error or residual between the model output and the actual value. Such functions include the mean squared error, root mean squared error, mean absolute error, relative squared error, root relative squared error, relative absolute error, and others.\n\nAll these standard measures offer a fine granularity or smoothness to the solution space and therefore work very well for most applications. But some problems might require a coarser evolution, such as determining if a prediction is within a certain interval, for instance less than 10% of the actual value. However, even if one is only interested in counting the hits (that is, a prediction that is within the chosen interval), making populations of models evolve based on just the number of hits each program scores is usually not very efficient due to the coarse granularity of the fitness landscape. Thus the solution usually involves combining these coarse measures with some kind of smooth function such as the standard error measures listed above.\n\nFitness functions based on the correlation coefficient and R-square are also very smooth. For regression problems, these functions work best by combining them with other measures because, by themselves, they only tend to measure correlation, not caring for the range of values of the model output. So by combining them with functions that work at approximating the range of the target values, they form very efficient fitness functions for finding models with good correlation and good fit between predicted and actual values.\n\nThe design of fitness functions for classification and logistic regression takes advantage of three different characteristics of classification models. The most obvious is just counting the hits, that is, if a record is classified correctly it is counted as a hit. This fitness function is very simple and works well for simple problems, but for more complex problems or datasets highly unbalanced it gives poor results.\n\nOne way to improve this type of hits-based fitness function consists of expanding the notion of correct and incorrect classifications. In a binary classification task, correct classifications can be 00 or 11. The \"00\" representation means that a negative case (represented by \"0”) was correctly classified, whereas the \"11\" means that a positive case (represented by \"1”) was correctly classified. Classifications of the type \"00\" are called true negatives (TN) and \"11\" true positives (TP).\n\nThere are also two types of incorrect classifications and they are represented by 01 and 10. They are called false positives (FP) when the actual value is 0 and the model predicts a 1; and false negatives (FN) when the target is 1 and the model predicts a 0. The counts of TP, TN, FP, and FN are usually kept on a table known as the confusion matrix.\n\nSo by counting the TP, TN, FP, and FN and further assigning different weights to these four types of classifications, it is possible to create smoother and therefore more efficient fitness functions. Some popular fitness functions based on the confusion matrix include sensitivity/specificity, recall/precision, F-measure, Jaccard similarity, Matthews correlation coefficient, and cost/gain matrix which combines the costs and gains assigned to the 4 different types of classifications.\n\nThese functions based on the confusion matrix are quite sophisticated and are adequate to solve most problems efficiently. But there is another dimension to classification models which is key to exploring more efficiently the solution space and therefore results in the discovery of better classifiers. This new dimension involves exploring the structure of the model itself, which includes not only the domain and range, but also the distribution of the model output and the classifier margin.\n\nBy exploring this other dimension of classification models and then combining the information about the model with the confusion matrix, it is possible to design very sophisticated fitness functions that allow the smooth exploration of the solution space. For instance, one can combine some measure based on the confusion matrix with the mean squared error evaluated between the raw model outputs and the actual values. Or combine the F-measure with the R-square evaluated for the raw model output and the target; or the cost/gain matrix with the correlation coefficient, and so on. More exotic fitness functions that explore model granularity include the area under the ROC curve and rank measure.\n\nAlso related to this new dimension of classification models, is the idea of assigning probabilities to the model output, which is what is done in logistic regression. Then it is also possible to use these probabilities and evaluate the mean squared error (or some other similar measure) between the probabilities and the actual values, then combine this with the confusion matrix to create very efficient fitness functions for logistic regression. Popular examples of fitness functions based on the probabilities include maximum likelihood estimation and hinge loss.\n\nIn logic there is no model structure (as defined above for classification and logistic regression) to explore: the domain and range of logical functions comprises only 0’s and 1’s or false and true. So, the fitness functions available for Boolean algebra can only be based on the hits or on the confusion matrix as explained in the section above.\n\nRoulette-wheel selection is perhaps the most popular selection scheme used in evolutionary computation. It involves mapping the fitness of each program to a slice of the roulette wheel proportional to its fitness. Then the roulette is spun as many times as there are programs in the population in order to keep the population size constant. So, with roulette-wheel selection programs are selected both according to fitness and the luck of the draw, which means that some times the best traits might be lost. However, by combining roulette-wheel selection with the cloning of the best program of each generation, one guarantees that at least the very best traits are not lost. This technique of cloning the best-of-generation program is known as simple elitism and is used by most stochastic selection schemes.\n\nThe reproduction of programs involves first the selection and then the reproduction of their genomes. Genome modification is not required for reproduction, but without it adaptation and evolution won't take place.\n\nThe selection operator selects the programs for the replication operator to copy. Depending on the selection scheme, the number of copies one program originates may vary, with some programs getting copied more than once while others are copied just once or not at all. In addition, selection is usually set up so that the population size remains constant from one generation to another.\n\nThe replication of genomes in nature is very complex and it took scientists a long time to discover the DNA double helix and propose a mechanism for its replication. But the replication of strings is trivial in artificial evolutionary systems, where only an instruction to copy strings is required to pass all the information in the genome from generation to generation.\n\nThe replication of the selected programs is a fundamental piece of all artificial evolutionary systems, but for evolution to occur it needs to be implemented not with the usual precision of a copy instruction, but rather with a few errors thrown in. Indeed, genetic diversity is created with genetic operators such as mutation, recombination, transposition, inversion, and many others.\n\nIn gene expression programming mutation is the most important genetic operator. It changes genomes by changing an element by another. The accumulation of many small changes over time can create great diversity.\n\nIn gene expression programming mutation is totally unconstrained, which means that in each gene domain any domain symbol can be replaced by another. For example, in the heads of genes any function can be replaced by a terminal or another function, regardless of the number of arguments in this new function; and a terminal can be replaced by a function or another terminal.\n\nRecombination usually involves two parent chromosomes to create two new chromosomes by combining different parts from the parent chromosomes. And as long as the parent chromosomes are aligned and the exchanged fragments are homologous (that is, occupy the same position in the chromosome), the new chromosomes created by recombination will always encode syntactically correct programs.\n\nDifferent kinds of crossover are easily implemented either by changing the number of parents involved (there's no reason for choosing only two); the number of split points; or the way one chooses to exchange the fragments, for example, either randomly or in some orderly fashion. For example, gene recombination, which is a special case of recombination, can be done by exchanging homologous genes (genes that occupy the same position in the chromosome) or by exchanging genes chosen at random from any position in the chromosome.\n\nTransposition involves the introduction of an insertion sequence somewhere in a chromosome. In gene expression programming insertion sequences might appear anywhere in the chromosome, but they are only inserted in the heads of genes. This method guarantees that even insertion sequences from the tails result in error-free programs.\n\nFor transposition to work properly, it must preserve chromosome length and gene structure. So, in gene expression programming transposition can be implemented using two different methods: the first creates a shift at the insertion site, followed by a deletion at the end of the head; the second overwrites the local sequence at the target site and therefore is easier to implement. Both methods can be implemented to operate between chromosomes or within a chromosome or even within a single gene.\n\nInversion is an interesting operator, especially powerful for combinatorial optimization. It consists of inverting a small sequence within a chromosome.\n\nIn gene expression programming it can be easily implemented in all gene domains and, in all cases, the offspring produced is always syntactically correct. For any gene domain, a sequence (ranging from at least two elements to as big as the domain itself) is chosen at random within that domain and then inverted.\n\nSeveral other genetic operators exist and in gene expression programming, with its different genes and gene domains, the possibilities are endless. For example, genetic operators such as one-point recombination, two-point recombination, gene recombination, uniform recombination, gene transposition, root transposition, domain-specific mutation, domain-specific inversion, domain-specific transposition, and so on, are easily implemented and widely used.\n\nNumerical constants are essential elements of mathematical and statistical models and therefore it is important to allow their integration in the models designed by evolutionary algorithms.\n\nGene expression programming solves this problem very elegantly through the use of an extra gene domain – the Dc – for handling random numerical constants (RNC). By combining this domain with a special terminal placeholder for the RNCs, a richly expressive system can be created.\n\nStructurally, the Dc comes after the tail, has a length equal to the size of the tail \"t\", and is composed of the symbols used to represent the RNCs.\n\nFor example, below is shown a simple chromosome composed of only one gene a head size of 7 (the Dc stretches over positions 15–22):\n\nwhere the terminal \"?” represents the placeholder for the RNCs. This kind of chromosome is expressed exactly as shown , giving:\n\nThen the ?'s in the expression tree are replaced from left to right and from top to bottom by the symbols (for simplicity represented by numerals) in the Dc, giving:\n\nThe values corresponding to these symbols are kept in an array. (For simplicity, the number represented by the numeral indicates the order in the array.) For instance, for the following 10 element array of RNCs:\n\nthe expression tree above gives:\n\nThis elegant structure for handling random numerical constants is at the heart of different GEP systems, such as GEP neural networks and GEP decision trees.\n\nLike the basic gene expression algorithm, the GEP-RNC algorithm is also multigenic and its chromosomes are decoded as usual by expressing one gene after another and then linking them all together by the same kind of linking process.\n\nThe genetic operators used in the GEP-RNC system are an extension to the genetic operators of the basic GEP algorithm (see above), and they all can be straightforwardly implemented in these new chromosomes. On the other hand, the basic operators of mutation, inversion, transposition, and recombination are also used in the GEP-RNC algorithm. Furthermore, special Dc-specific operators such as mutation, inversion, and transposition, are also used to aid in a more efficient circulation of the RNCs among individual programs. In addition, there is also a special mutation operator that allows the permanent introduction of variation in the set of RNCs. The initial set of RNCs is randomly created at the beginning of a run, which means that, for each gene in the initial population, a specified number of numerical constants, chosen from a certain range, are randomly generated. Then their circulation and mutation is enabled by the genetic operators.\n\nAn artificial neural network (ANN or NN) is a computational device that consists of many simple connected units or neurons. The connections between the units are usually weighted by real-valued weights. These weights are the primary means of learning in neural networks and a learning algorithm is usually used to adjust them.\n\nStructurally, a neural network has three different classes of units: input units, hidden units, and output units. An activation pattern is presented at the input units and then spreads in a forward direction from the input units through one or more layers of hidden units to the output units. The activation coming into one unit from other unit is multiplied by the weights on the links over which it spreads. All incoming activation is then added together and the unit becomes activated only if the incoming result is above the unit’s threshold.\n\nIn summary, the basic components of a neural network are the units, the connections between the units, the weights, and the thresholds. So, in order to fully simulate an artificial neural network one must somehow encode these components in a linear chromosome and then be able to express them in a meaningful way.\n\nIn GEP neural networks (GEP-NN or GEP nets), the network architecture is encoded in the usual structure of a head/tail domain. The head contains special functions/neurons that activate the hidden and output units (in the GEP context, all these units are more appropriately called functional units) and terminals that represent the input units. The tail, as usual, contains only terminals/input units.\n\nBesides the head and the tail, these neural network genes contain two additional domains, Dw and Dt, for encoding the weights and thresholds of the neural network. Structurally, the Dw comes after the tail and its length \"d\" depends on the head size \"h\" and maximum arity \"n\" and is evaluated by the formula:\n\nThe Dt comes after Dw and has a length \"d\" equal to \"t\". Both domains are composed of symbols representing the weights and thresholds of the neural network.\n\nFor each NN-gene, the weights and thresholds are created at the beginning of each run, but their circulation and adaptation are guaranteed by the usual genetic operators of mutation, transposition, inversion, and recombination. In addition, special operators are also used to allow a constant flow of genetic variation in the set of weights and thresholds.\n\nFor example, below is shown a neural network with two input units (\"i\" and \"i\"), two hidden units (\"h\" and \"h\"), and one output unit (\"o\"). It has a total of six connections with six corresponding weights represented by the numerals 1–6 (for simplicity, the thresholds are all equal to 1 and are omitted):\n\nThis representation is the canonical neural network representation, but neural networks can also be represented by a tree, which, in this case, corresponds to:\n\nwhere \"a” and \"b” represent the two inputs \"i\" and \"i\" and \"D” represents a function with connectivity two. This function adds all its weighted arguments and then thresholds this activation in order to determine the forwarded output. This output (zero or one in this simple case) depends on the threshold of each unit, that is, if the total incoming activation is equal to or greater than the threshold, then the output is one, zero otherwise.\n\nThe above NN-tree can be linearized as follows:\n\nwhere the structure in positions 7–12 (Dw) encodes the weights. The values of each weight are kept in an array and retrieved as necessary for expression.\n\nAs a more concrete example, below is shown a neural net gene for the exclusive-or problem. It has a head size of 3 and Dw size of 6:\n\nIts expression results in the following neural network:\n\nwhich, for the set of weights:\n\nit gives:\n\nwhich is a perfect solution to the exclusive-or function.\n\nBesides simple Boolean functions with binary inputs and binary outputs, the GEP-nets algorithm can handle all kinds of functions or neurons (linear neuron, tanh neuron, atan neuron, logistic neuron, limit neuron, radial basis and triangular basis neurons, all kinds of step neurons, and so on). Also interesting is that the GEP-nets algorithm can use all these neurons together and let evolution decide which ones work best to solve the problem at hand. So, GEP-nets can be used not only in Boolean problems but also in logistic regression, classification, and regression. In all cases, GEP-nets can be implemented not only with multigenic systems but also cellular systems, both unicellular and multicellular. Furthermore, multinomial classification problems can also be tackled in one go by GEP-nets both with multigenic systems and multicellular systems.\n\nDecision trees (DT) are classification models where a series of questions and answers are mapped using nodes and directed edges.\n\nDecision trees have three types of nodes: a root node, internal nodes, and leaf or terminal nodes. The root node and all internal nodes represent test conditions for different attributes or variables in a dataset. Leaf nodes specify the class label for all different paths in the tree.\n\nMost decision tree induction algorithms involve selecting an attribute for the root node and then make the same kind of informed decision about all the nodes in a tree.\n\nDecision trees can also be created by gene expression programming, with the advantage that all the decisions concerning the growth of the tree are made by the algorithm itself without any kind of human input.\n\nThere are basically two different types of DT algorithms: one for inducing decision trees with only nominal attributes and another for inducing decision trees with both numeric and nominal attributes. This aspect of decision tree induction also carries to gene expression programming and there are two GEP algorithms for decision tree induction: the evolvable decision trees (EDT) algorithm for dealing exclusively with nominal attributes and the EDT-RNC (EDT with random numerical constants) for handling both nominal and numeric attributes.\n\nIn the decision trees induced by gene expression programming, the attributes behave as function nodes in the basic gene expression algorithm, whereas the class labels behave as terminals. This means that attribute nodes have also associated with them a specific arity or number of branches that will determine their growth and, ultimately, the growth of the tree. Class labels behave like terminals, which means that for a \"k\"-class classification task, a terminal set with \"k\" terminals is used, representing the \"k\" different classes.\n\nThe rules for encoding a decision tree in a linear genome are very similar to the rules used to encode mathematical expressions (see above). So, for decision tree induction the genes also have a head and a tail, with the head containing attributes and terminals and the tail containing only terminals. This again ensures that all decision trees designed by GEP are always valid programs. Furthermore, the size of the tail \"t\" is also dictated by the head size \"h\" and the number of branches of the attribute with more branches \"n\" and is evaluated by the equation:\n\nFor example, consider the decision tree below to decide whether to play outside:\n\nIt can be linearly encoded as:\n\nwhere “H” represents the attribute Humidity, “O” the attribute Outlook, “W” represents Windy, and “a” and “b” the class labels \"Yes\" and \"No\" respectively. Note that the edges connecting the nodes are properties of the data, specifying the type and number of branches of each attribute, and therefore don’t have to be encoded.\n\nThe process of decision tree induction with gene expression programming starts, as usual, with an initial population of randomly created chromosomes. Then the chromosomes are expressed as decision trees and their fitness evaluated against a training dataset. According to fitness they are then selected to reproduce with modification. The genetic operators are exactly the same that are used in a conventional unigenic system, for example, mutation, inversion, transposition, and recombination.\n\nDecision trees with both nominal and numeric attributes are also easily induced with gene expression programming using the framework described above for dealing with random numerical constants. The chromosomal architecture includes an extra domain for encoding random numerical constants, which are used as thresholds for splitting the data at each branching node. For example, the gene below with a head size of 5 (the Dc starts at position 16):\n\nencodes the decision tree shown below:\n\nIn this system, every node in the head, irrespective of its type (numeric attribute, nominal attribute, or terminal), has associated with it a random numerical constant, which for simplicity in the example above is represented by a numeral 0–9. These random numerical constants are encoded in the Dc domain and their expression follows a very simple scheme: from top to bottom and from left to right, the elements in Dc are assigned one-by-one to the elements in the decision tree. So, for the following array of RNCs:\n\nthe decision tree above results in:\n\nwhich can also be represented more colorfully as a conventional decision tree:\n\nGEP has been criticized for not being a major improvement over other genetic programming techniques. In many experiments, it did not perform better than existing methods.\n\n\n\n\n\n\n\n",
    "id": "1098818",
    "title": "Gene expression programming"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42922637",
    "text": "Symbolic regression\n\nSymbolic regression is a type of regression analysis that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity. No particular model is provided as a starting point to the algorithm. Instead, initial expressions are formed by randomly combining mathematical building blocks such as mathematical operators, analytic functions, constants, and state variables. (Usually, a subset of these primitives will be specified by the person operating it, but that's not a requirement of the technique.) New equations are then formed by recombining previous equations, using genetic programming.\n\nBy not requiring a specific model to be specified, symbolic regression isn't affected by human bias, or unknown gaps in domain knowledge. It attempts to uncover the intrinsic relationships of the dataset, by letting the patterns in the data itself reveal the appropriate models, rather than imposing a model structure that is deemed mathematically tractable from a human perspective. The fitness function that drives the evolution of the models takes into account not only error metrics (to ensure the models accurately predict the data), but also special complexity measures, thus ensuring that the resulting models reveal the data's underlying structure in a way that's understandable from a human perspective. This facilitates reasoning and favors the odds of getting insights about the data-generating system.\n\nWhile conventional regression techniques seek to optimize the parameters for a pre-specified model structure, symbolic regression avoids imposing prior assumptions, and instead infers the model from the data. In other words, it attempts to discover both model structures and model parameters.\n\nThis approach has, of course, the disadvantage of having a much larger space to search — in fact, not only the search space in symbolic regression is infinite, but there are an infinite number of models which will perfectly fit a finite data set (provided that the model complexity isn't artificially limited). This means that it will possibly take a symbolic regression algorithm much longer to find an appropriate model and parametrization, than traditional regression techniques. This can be attenuated by limiting the set of building blocks provided to the algorithm, based on existing knowledge of the system that produced the data; but in the end, using symbolic regression is a decision that has to be balanced with how much is known about the underlying system.\n\nNevertheless, this characteristic of symbolic regression also has advantages: because the evolutionary algorithm requires diversity in order to effectively explore the search space, the end result is likely to be a selection of high-scoring models (and their corresponding set of parameters). Examining this collection could provide better insight into the underlying process, and allows the user to identify an approximation that better fits their needs in terms of accuracy and simplicity.\n\n\n\n",
    "id": "42922637",
    "title": "Symbolic regression"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47623626",
    "text": "Multi expression programming\n\nMulti Expression Programming (MEP) is a genetic programming variant encoding multiple solutions in the same chromosome. MEP representation is not specific (multiple representations have been tested). In the simplest variant, MEP chromosomes are linear strings of instructions. This representation was inspired by Three-address code. MEP strength consists in the ability to encode multiple solutions, of a problem, in the same chromosome. In this way one can explore larger zones of the search space. For most of the problems this advantage comes with no running-time penalty compared with genetic programming variants encoding a single solution in a chromosome.\n\nHere is a simple MEP program:\nOn each line we can have a terminal or a function. In the case of functions we also need pointers to its arguments.\n\nWhen we decode the chromosome we obtain multiple expressions:\nWhich expression will represent the chromosome? In MEP each expression is evaluated and the best of them will represent the chromosome. For most of the problems, this evaluation has the same complexity as in the case of encoding a single solution in each chromosome.\n\nMEPX is a cross platform (Windows, Mac OSX, and Linux Ubuntu) free software for automatic generation of computer programs. It can be used for data analysis, particularly for solving regression and classification problems.\n\nLibmep is a free and open source library implementing Multi Expression Programming technique. It is written in C++.\n\nhmep is a new open source library implementing Multi Expression Programming technique in Haskell programming language.\n\n\n",
    "id": "47623626",
    "title": "Multi expression programming"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=602401",
    "text": "Facial recognition system\n\nA facial recognition system is a computer application capable of identifying or verifying a person from a digital image or a video frame from a video source. One of the ways to do this is by comparing selected facial features from the image and a face database.\n\nIt is typically used in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems. Recently, it has also become popular as a commercial identification and marketing tool.\n\nSome face recognition algorithms identify facial features by extracting landmarks, or features, from an image of the subject's face. For example, an algorithm may analyze the relative position, size, and/or shape of the eyes, nose, cheekbones, and jaw. These features are then used to search for other images with matching features.\nOther algorithms normalize a gallery of face images and then compress the face data, only saving the data in the image that is useful for face recognition. A probe image is then compared with the face data. One of the earliest successful systems is based on template matching techniques applied to a set of salient facial features, providing a sort of compressed face representation.\n\nRecognition algorithms can be divided into two main approaches, geometric, which looks at distinguishing features, or photometric, which is a statistical approach that distills an image into values and compares the values with templates to eliminate variances.\n\nPopular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic bunch graph matching using the Fisherface algorithm, the hidden Markov model, the multilinear subspace learning using tensor representation, and the neuronal motivated dynamic link matching.\n\nThree-dimensional face recognition technique uses 3D sensors to capture information about the shape of a face. This information is then used to identify distinctive features on the surface of a face, such as the contour of the eye sockets, nose, and chin.\n\nOne advantage of 3D face recognition is that it is not affected by changes in lighting like other techniques. It can also identify a face from a range of viewing angles, including a profile view. Three-dimensional data points from a face vastly improve the precision of face recognition. 3D research is enhanced by the development of sophisticated sensors that do a better job of capturing 3D face imagery. The sensors work by projecting structured light onto the face. Up to a dozen or more of these image sensors can be placed on the same CMOS chip—each sensor captures a different part of the spectrum...\n\nEven a perfect 3D matching technique could be sensitive to expressions.\nFor that goal a group at the Technion applied tools from metric geometry\nto treat expressions as isometries\n\nA new method is to introduce a way to capture a 3D picture by using three tracking cameras that point at different angles; one camera will be pointing at the front of the subject, second one to the side, and third one at an angle. All these cameras will work together so it can track a subject’s face in real time and be able to face detect and recognize.\n\nAnother emerging trend uses the visual details of the skin, as captured in standard digital or scanned images. This technique, called skin texture analysis, turns the unique lines, patterns, and spots apparent in a person’s skin into a mathematical space.\n\nTests have shown that with the addition of skin texture analysis, performance in recognizing faces can increase 20 to 25 percent.\n\nA different form of taking input data for face recognition is by using thermal cameras, by this procedure the cameras will only detect the shape of the head and it will ignore the subject accessories such as glasses, hats, or make up. A problem with using thermal pictures for face recognition is that the databases for face recognition is limited. Diego Socolinsky, and Andrea Selinger (2004) research the use of thermal face recognition in real life, and operation sceneries, and at the same time build a new database of thermal face images. The research uses low-sensitive, low-resolution ferro-electric electrics sensors that are capable of acquire long wave thermal infrared (LWIR). The results show that a fusion of LWIR and regular visual cameras has the greater results in outdoor probes. Indoor results show that visual has a 97.05% accuracy, while LWIR has 93.93%, and the Fusion has 98.40%, however on the outdoor proves visual has 67.06%, LWIR 83.03%, and fusion has 89.02%. The study used 240 subjects over the period of 10 weeks to create the new database. The data was collected on sunny, rainy, and cloudy days.\n\nThe Australian Border Force and New Zealand Customs Services have set up an automated border processing system called SmartGate that uses face recognition, which compares the face of the traveller with the data in the e-passport microchip. Major Canadian airports will be using a new facial recognition program as part of the Primary Inspection Kiosk program that will compare people's faces to their passports. This program will first come to Ottawa International Airport in early 2017 and to other airports in 2018. The Tocumen International Airport in Panama operates an airport-wide surveillance system using hundreds of live face recognition cameras to identify wanted individuals passing through the airport.\n\nThe U.S. Department of State operates one of the largest face recognition systems in the world with a database of 117 million American adults, with photos typically drawn from driver's license photos. Although it is still far from completion, it is being put to use in certain cities to give clues as to who was in the photo. The FBI uses the photos as an investigative tool not for positive identification.\n\nIn recent years Maryland has used face recognition by comparing people's faces to their driver's license photos. The system drew controversy when it was used in Baltimore to arrest unruly protesters after the death of Freddie Gray in police custody. Many other states are using or developing a similar system however some states have laws prohibiting its use.\n\nThe FBI has also instituted its Next Generation Identification program to include face recognition, as well as more traditional biometrics like fingerprints and iris scans, which can pull from both criminal and civil databases.\n\nIn 2017, Time & Attendance company ClockedIn released facial recognition as a form of attendance tracking for businesses and organisations looking to have a more automated system of keeping track of hours worked as well as for security and health and safety control.\n\nIn May 2017, a man was arrested using an automatic facial recognition (AFR) system mounted on a van operated by the South Wales Police. Ars Technica reported that \"this appears to be the first time [AFR] has led to an arrest\". \n\nIn addition to being used for security systems, authorities have found a number of other applications for face recognition systems. While earlier post-9/11 deployments were well publicized trials, more recent deployments are rarely written about due to their covert nature.\n\nAt Super Bowl XXXV in January 2001, police in Tampa Bay, Florida used Viisage face recognition software to search for potential criminals and terrorists in attendance at the event. 19 people with minor criminal records were potentially identified.\n\nIn the 2000 Mexican presidential election, the Mexican government employed face recognition software to prevent voter fraud. Some individuals had been registering to vote under several different names, in an attempt to place multiple votes. By comparing new face images to those already in the voter database, authorities were able to reduce duplicate registrations. Similar technologies are being used in the United States to prevent people from obtaining fake identification cards and driver’s licenses.\n\nFace recognition has been leveraged as a form of biometric authentication for various computing platforms and devices; Android 4.0 \"Ice Cream Sandwich\" added facial recognition using a smartphone's front camera as a means of unlocking devices, while Microsoft introduced face recognition login to its Xbox 360 video game console through its Kinect accessory, as well as Windows 10 via its \"Windows Hello\" platform (which requires an infrared-illuminated camera). Apple's iPhone X smartphone introduced facial recognition to the product line with its \"Face ID\" platform, which uses an infrared illumination system.\n\nFace recognition systems have also been used by photo management software to identify the subjects of photographs, enabling features such as searching images by person, as well as suggesting photos to be shared with a specific contact if their presence were detected in a photo.\n\nAmong the different biometric techniques, face recognition may not be most reliable and efficient. However, one key advantage is that it does not require the cooperation of the test subject to work. Properly designed systems installed in airports, multiplexes, and other public places can identify individuals among the crowd, without passers-by even being aware of the system. Other biometrics like fingerprints, iris scans, and speech recognition cannot perform this kind of mass identification. However, questions have been raised on the effectiveness of face recognition software in cases of railway and airport security.\n\nRalph Gross, a researcher at the Carnegie Mellon Robotics Institute in 2008, describes one obstacle related to the viewing angle of the face: \"Face recognition has been getting pretty good at full frontal faces and 20 degrees off, but as soon as you go towards profile, there've been problems.\" Besides the pose variations, low-resolution face images are also very hard to recognize. This is one of the main obstacles of face recognition in surveillance systems. \n\nFace recognition software generally doesn't do as well in identifying minorities when most of the subjects used in training the technology were from the majority group. Other conditions where face recognition does not work well include poor lighting, sunglasses, hats, scarves, beards, long hair, makeup or other objects partially covering the subject’s face.\n\nFace recognition is less effective if facial expressions vary. A big smile can render the system less effective. For instance: Canada, in 2009, allowed only neutral facial expressions in passport photos.\n\nThere is also inconstancy in the datasets used by researchers. Researchers may use anywhere from several subjects to scores of subjects, and a few hundred images to thousands of images. It is important for researchers to make available the datasets they used to each other, or have at least a standard dataset.\n\nCritics of the technology complain that the London Borough of Newham scheme has, , never recognized a single criminal, despite several criminals in the system's database living in the Borough and the system having been running for several years. \"Not once, as far as the police know, has Newham's automatic face recognition system spotted a live target.\" This information seems to conflict with claims that the system was credited with a 34% reduction in crime (hence why it was rolled out to Birmingham also). However it can be explained by the notion that when the public is regularly told that they are under constant video surveillance with advanced face recognition technology, this fear alone can reduce the crime rate, whether the face recognition system technically works or does not. This has been the basis for several other face recognition based security systems, where the technology itself does not work particularly well but the user's perception of the technology does.\n\nAn experiment in 2002 by the local police department in Tampa, Florida, had similarly disappointing results.\n\nA system at Boston's Logan Airport was shut down in 2003 after failing to make any matches during a two-year test period.\n\nSystems are often advertised as having accuracy near 100%, this is misleading as the studies often uses much smaller sample sizes than would be necessary for large scale applications. Because facial recognition is not completely accurate, it creates a list of potential matches. A human operator must then look through these potential matches and studies show the operators pick the correct match out of the list only about half the time. This causes the issue of targeting the wrong suspect.\n\nCivil rights right organizations and privacy campaigners such as the Electronic Frontier Foundation and the ACLU express concern that privacy is being compromised by the use of surveillance technologies. Some fear that it could lead to a “total surveillance society,” with the government and other authorities having the ability to know the whereabouts and activities of all citizens around the clock. This knowledge has been, is being, and could continue to be deployed to prevent the lawful exercise of rights of citizens to criticize those in office, specific government policies or corporate practices. Many centralized power structures with such surveillance capabilities have abused their privileged access to maintain control of the political and economic apparatus, and to curtail populist reforms.\n\nFace recognition can be used not just to identify an individual, but also to unearth other personal data associated with an individual – such as other photos featuring the individual, blog posts, social networking profiles, Internet behavior, travel patterns, etc. – all through facial features alone. Concerns have been raised over who would have access to the knowledge of one's whereabouts and people with them at any given time. Moreover, individuals have limited ability to avoid or thwart face recognition tracking unless they hide their faces. This fundamentally changes the dynamic of day-to-day privacy by enabling any marketer, government agency, or random stranger to secretly collect the identities and associated personal information of any individual captured by the face recognition system. Consumers may not understand or be aware of what their data is being used for, which denies them the ability to consent to how their personal information gets shared. \n\nSocial media web sites such as Facebook have very large numbers of photographs of people, annotated with names. This represents a database which may be abused by governments for face recognition purposes. Face recognition was used in Russia to harass women allegedly involved in online pornography. In Russia there is an app 'FindFace' which can identify faces with about 70% accuracy using the social media app called VK. This app would not be possible in other countries which do not use VK as their social media platform photos are not stored the same way as with VK.\n\nIn July 2012, a hearing was held before the Subcommittee on Privacy, Technology and the Law of the Committee on the Judiciary, United States Senate, to address issues surrounding what face recognition technology means for privacy and civil liberties.\n\nIn 2014, the National Telecommunications and Information Association (NTIA) began a multi-stakeholder process to engage privacy advocates and industry representatives to establish guidelines regarding the use of face recognition technology by private companies. In June 2015, privacy advocates left the bargaining table over what they felt was an impasse based on the industry representatives being unwilling to agree to consent requirements for the collection of face recognition data. The NTIA and industry representatives continued without the privacy representatives, and draft rules are expected to be presented in the spring of 2016.\n\nStates have begun enacted legislation to protect citizen's biometric data privacy. Illinois enacted the Biometric Information Privacy Act in 2008. Facebook's DeepFace has become the subject of several class action lawsuits under the Biometric Information Privacy Act, with claims alleging that Facebook is collecting and storing face recognition data of its users without obtaining informed consent, in direct violation of the Biometric Information Privacy Act. The most recent case was dismissed in January 2016 because the court lacked jurisdiction. Therefore, it is still unclear if the Biometric Information Privacy Act will be effective in protecting biometric data privacy rights.\n\nIn July 2015, the United States Government Accountability Office conducted a Report to the Ranking Member, Subcommittee on Privacy, Technology and the Law, Committee on the Judiciary, U.S. Senate. The report discussed facial recognition technology's commercial uses, privacy issues, and the applicable federal law. It states that previously, issues concerning facial recognition technology were discussed and represent the need for updated federal privacy laws that continually match the degree and impact of advanced technologies. Also, that some industry, government, and privacy organizations are in the process of developing, or have developed, \"voluntary privacy guidelines\". These guidelines vary between the groups, but overall aim to gain consent and inform citizens of the intended use of facial recognition technology. This helps counteract the privacy issues that arise when citizens are unaware where their personal, privacy data gets put to use as the report indicates as a prevalent issue. \n\nPioneers of automated face recognition include Woody Bledsoe, Helen Chan Wolf, and Charles Bisson.\n\nDuring 1964 and 1965, Bledsoe, along with Helen Chan and Charles Bisson, worked on using the computer to recognize human faces (Bledsoe 1966a, 1966b; Bledsoe and Chan 1965). He was proud of this work, but because the funding was provided by an unnamed intelligence agency that did not allow much publicity, little of the work was published. Given a large database of images (in effect, a book of mug shots) and a photograph, the problem was to select from the database a small set of records such that one of the image records matched the photograph. The success of the method could be measured in terms of the ratio of the answer list to the number of records in the database. Bledsoe (1966a) described the following difficulties:\n\nThis project was labeled man-machine because the human extracted the coordinates of a set of features from the photographs, which were then used by the computer for recognition. Using a graphics tablet (GRAFACON or RAND TABLET), the operator would extract the coordinates of features such as the center of pupils, the inside corner of eyes, the outside corner of eyes, point of widows peak, and so on. From these coordinates, a list of 20 distances, such as width of mouth and width of eyes, pupil to pupil, were computed. These operators could process about 40 pictures an hour. When building the database, the name of the person in the photograph was associated with the list of computed distances and stored in the computer. In the recognition phase, the set of distances was compared with the corresponding distance for each photograph, yielding a distance between the photograph and the database record. The closest records are returned.\n\nBecause it is unlikely that any two pictures would match in head rotation, lean, tilt, and scale (distance from the camera), each set of distances is normalized to represent the face in a frontal orientation. To accomplish this normalization, the program first tries to determine the tilt, the lean, and the rotation. Then, using these angles, the computer undoes the effect of these transformations on the computed distances. To compute these angles, the computer must know the three-dimensional geometry of the head. Because the actual heads were unavailable, Bledsoe (1964) used a standard head derived from measurements on seven heads.\n\nAfter Bledsoe left PRI in 1966, this work was continued at the Stanford Research Institute, primarily by Peter Hart. In experiments performed on a database of over 2000 photographs, the computer consistently outperformed humans when presented with the same recognition tasks (Bledsoe 1968). Peter Hart (1996) enthusiastically recalled the project with the exclamation, \"It really worked!\"\n\nBy about 1997, the system developed by Christoph von der Malsburg and graduate students of the University of Bochum in Germany and the University of Southern California in the United States outperformed most systems with those of Massachusetts Institute of Technology and the University of Maryland rated next. The Bochum system was developed through funding by the United States Army Research Laboratory. The software was sold as ZN-Face and used by customers such as Deutsche Bank and operators of airports and other busy locations. The software was \"robust enough to make identifications from less-than-perfect face views. It can also often see through such impediments to identification as mustaches, beards, changed hair styles and glasses—even sunglasses\".\n\nIdentix, a company out of Minnesota, has developed the software, FaceIt. FaceIt can pick out someone's face in a crowd and compare it to databases worldwide to recognize and put a name to a face. The software is written to detect multiple features on the human face. It can detect the distance between the eyes, width of the nose, shape of cheekbones, length of jawlines and many more facial features. The software does this by putting the image of the face on a faceprint, a numerical code that represents the human face. Face recognition software used to have to rely on a 2D image with the person almost directly facing the camera. Now, with FaceIt, a 3D image can be compared to a 2D image by choosing 3 specific points off of the 3D image and converting it into a 2D image using a special algorithm that can be scanned through almost all databases.\n\nIn 2006, the performance of the latest face recognition algorithms were evaluated in the Face Recognition Grand Challenge (FRGC). High-resolution face images, 3-D face scans, and iris images were used in the tests. The results indicated that the new algorithms are 10 times more accurate than the face recognition algorithms of 2002 and 100 times more accurate than those of 1995. Some of the algorithms were able to outperform human participants in recognizing faces and could uniquely identify identical twins.\n\nU.S. Government-sponsored evaluations and challenge problems have helped spur over two orders-of-magnitude in face-recognition system performance. Since 1993, the error rate of automatic face-recognition systems has decreased by a factor of 272. The reduction applies to systems that match people with face images captured in studio or mugshot environments. In Moore's law terms, the error rate decreased by one-half every two years.\n\nLow-resolution images of faces can be enhanced using face hallucination.\n\nFacial recognition systems have been used for emotion recognition In 2016 Facebook acquired emotion detection startup FacioMetrics.\n\nIn January 2013 Japanese researchers from the National Institute of Informatics created 'privacy visor' glasses that uses nearly infrared light to make the face underneath it unrecognizable to face recognition software. The latest version uses a titanium frame, light-reflective material and a mask which uses angles and patterns to disrupt facial recognition technology through both absorbing and bouncing back light sources. In December 2016 a form of anti-CCTV and facial recognition sunglasses called 'reflectacles' were invented by a custom-spectacle-craftsmen based in Chicago named Scott Urban. They reflect infrared and, optionally, visible light which makes the users face a white blur to cameras.\n\nAnother method to protect from facial recognition systems are specific haircuts and make-up patterns that prevent the used algorithms to detect a face.\n\n\n\n",
    "id": "602401",
    "title": "Facial recognition system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=265421",
    "text": "Inverted pendulum\n\nAn inverted pendulum is a pendulum that has its center of mass above its pivot point. It is unstable and without additional help will fall over. It can be suspended stably in this inverted position by using a control system to monitor the angle of the pole and move the pivot point horizontally back under the center of mass when it starts to fall over, keeping it balanced. The inverted pendulum is a classic problem in dynamics and control theory and is used as a benchmark for testing control strategies. It is often implemented with the pivot point mounted on a cart that can move horizontally under control of an electronic servo system as shown in the photo; this is called a cart and pole apparatus. Most applications limit the pendulum to 1 degree of freedom by affixing the pole to an axis of rotation. Whereas a normal pendulum is stable when hanging downwards, an inverted pendulum is inherently unstable, and must be actively balanced in order to remain upright; this can be done either by applying a torque at the pivot point, by moving the pivot point horizontally as part of a feedback system, changing the rate of rotation of a mass mounted on the pendulum on an axis parallel to the pivot axis and thereby generating a net torque on the pendulum, or by oscillating the pivot point vertically. A simple demonstration of moving the pivot point in a feedback system is achieved by balancing an upturned broomstick on the end of one's finger. \n\nA second type of inverted pendulum is a tiltmeter for tall structures, which consists of a wire anchored to the bottom of the foundation and attached to a float in a pool of oil at the top of the structure that has devices for measuring movement of the neutral position of the float away from its original position.\n\nA pendulum with its bob hanging directly below the support pivot is at a stable equilibrium point; there is no torque on the pendulum so it will remain motionless, and if displaced from this position will experience a restoring torque which returns it toward the equilibrium position. A pendulum with its bob in an inverted position, supported on a rigid rod directly above the pivot, 180° from its stable equilibrium position, is at an unstable equilibrium point. At this point again there is no torque on the pendulum, but the slightest displacement away from this position will cause a gravitation torque on the pendulum which will accelerate it away from equilibrium, and it will fall over. \nIn order to stabilize a pendulum in this inverted position, a feedback control system can be used, which monitors the pendulum's angle and moves the position of the pivot point sideways when the pendulum starts to fall over, to keep it balanced. The inverted pendulum is a classic problem in dynamics and control theory and is widely used as a benchmark for testing control algorithms (PID controllers, state space representation, neural networks, fuzzy control, genetic algorithms, etc.). Variations on this problem include multiple links, allowing the motion of the cart to be commanded while maintaining the pendulum, and balancing the cart-pendulum system on a see-saw. The inverted pendulum is related to rocket or missile guidance, where the center of gravity is located behind the center of drag causing aerodynamic instability. The understanding of a similar problem can be shown by simple robotics in the form of a balancing cart. Balancing an upturned broomstick on the end of one's finger is a simple demonstration, and the problem is solved by self-balancing personal transporters such as the Segway PT, the self-balancing hoverboard and the self-balancing unicycle.\n\nAnother way that an inverted pendulum may be stabilized, without any feedback or control mechanism, is by oscillating the pivot rapidly up and down. This is called Kapitza's pendulum. If the oscillation is sufficiently strong (in terms of its acceleration and amplitude) then the inverted pendulum can recover from perturbations in a strikingly counterintuitive manner. If the driving point moves in simple harmonic motion, the pendulum's motion is described by the Mathieu equation.\n\nThe equations of motion of inverted pendulums are dependent on what constraints are placed on the motion of the pendulum. Inverted pendulums can be created in various configurations resulting in a number of Equations of Motion describing the behavior of the pendulum.\n\nIn a configuration where the pivot point of the pendulum is fixed in space, the equation of motion is similar to that for an uninverted pendulum. The equation of motion below assumes no friction or any other resistance to movement, a rigid massless rod, and the restriction to 2-dimensional movement.\n\nWhere formula_2 is the angular acceleration of the pendulum, formula_3 is the standard gravity on the surface of the Earth, formula_4 is the length of the pendulum, and formula_5 is the angular displacement measured from the equilibrium position.\n\nWhen added to both sides, it will have the same sign as the angular acceleration term:\n\nThus, the inverted pendulum will accelerate away from the vertical unstable equilibrium in the direction initially displaced, and the acceleration is inversely proportional to the length. Tall pendulums fall more slowly than short ones.\n\nDerivation using torque and moment of inertia:\nThe pendulum is assumed to consist of a point mass, of mass formula_7, affixed to the end of a massless rigid rod, of length formula_4, attached to a pivot point at the end opposite the point mass.\n\nThe net torque of the system must equal the moment of inertia times the angular acceleration:\nThe torque due to gravity providing the net torque:\nWhere formula_11 is the angle measured from the inverted equilibrium position.\n\nThe resulting equation:\n\nThe moment of inertia for a point mass:\nIn the case of the inverted pendulum the radius is the length of the rod, formula_14.\n\nSubstituting in formula_15\n\nMass and formula_17 is divided from each side resulting in:\n\nAn inverted pendulum on a cart consists of a mass formula_19 at the top of a pole of length formula_14 pivoted on a horizontally moving base as shown in the adjacent image. The cart is restricted to linear motion and is subject to forces resulting in or hindering motion.\n\nThe essentials of stabilizing the inverted pendulum can be summarized qualitatively in three steps.\n1. If the tilt angle formula_21 is to the right, the cart must accelerate to the right and vice versa.\n\n2. The position of the cart formula_22 relative to track center is stabilized by slightly modulating the null angle (the angle error that the control system tries to null) by the position of the cart, that is, null angle formula_23 where formula_24 is small. This makes the pole want to lean slightly toward track center and stabilize at track center where the tilt angle is exactly vertical. Any offset in the tilt sensor or track slope that would otherwise cause instability translates into a stable position offset. A further added offset gives position control.\n\n3. A normal pendulum subject to a moving pivot point such as a load lifted by a crane, has a peaked response at the pendulum radian frequency of formula_25. To prevent uncontrolled swinging, the frequency spectrum of the pivot motion should be suppressed near formula_26. The inverted pendulum requires the same suppression filter to achieve stability.\n\nNote that, as a consequence of the null angle modulation strategy, the position feedback is positive, that is, a sudden command to move right will produce an initial cart motion to the left followed by a move right to rebalance the pendulum. The interaction of the pendulum instability and the positive position feedback instability to produce a stable system is a feature that makes the mathematical analysis an interesting and challenging problem.\n\nThe equations of motion can be derived using Lagrange's equations. We refer to the drawing to the right where formula_27 is the angle of the pendulum of length formula_28 with respect to the vertical direction and the acting forces are gravity and an external force \"F\" in the x-direction. Define formula_29 to be the position of the cart. The Lagrangian formula_30 of the system is:\n\nwhere formula_32 is the velocity of the cart and formula_33 is the velocity of the point mass formula_34.\nformula_32 and formula_33 can be expressed in terms of x and formula_5 by writing the velocity as the first derivative of the position;\nSimplifying the expression for formula_33 leads to:\n\nThe Lagrangian is now given by:\nand the equations of motion are:\n\nsubstituting formula_45 in these equations and simplifying leads to the equations that describe the motion of the inverted pendulum:\nThese equations are nonlinear, but since the goal of a control system would be to keep the pendulum upright the equations can be linearized around formula_48.\n\nOftentimes it is beneficial to use Newton's Second Law instead of Lagrange's equations because Newton's equations give the reaction forces at the joint between the pendulum and the cart. These equations give rise to two equations for each body one in the x-direction and the other in the y-direction. The equations of motion of the cart are shown below where the LHS is the sum of the forces on the body and the RHS is the acceleration.\n\nIn the equations above formula_51 and formula_52 are reaction forces at the joint. formula_53 is the normal force applied to the cart. This second equation only depends on the vertical reaction force thus the equation can be used to solve for the normal force. The first equation can be used to solve for the horizontal reaction force. In order to complete the equations of motion, the acceleration of the point mass attached to the pendulum must be computed. The position of the point mass can be given in inertial coordinates as\n\nTaking two derivatives yields the acceleration vector in the inertial reference frame.\n\nThen, using Newton's second law, two equations can be written in the x-direction and the y-direction. Note that the reaction forces are positive as applied to the pendulum and negative when applied to the cart. This is due to Newton's Third Law.\n\nThe first equation allows yet another way to compute the horizontal reaction force in the event the applied force formula_58 is not known. The second equation can be used to solve for the vertical reaction force. The first equation of motion is derived by substituting formula_59 into formula_60 which yields\n\nBy inspection this equation is identical to the result from Lagrange's Method. In order to obtain the second equation the pendulum equation of motion must be dotted with a unit vector which runs perpendicular to the pendulum at all times and is typically noted as the x-coordinate of the body frame. In inertial coordinates this vector can be written using a simple 2-D coordinate transformation\n\nThe pendulum equation of motion written in vector form is formula_63. Dotting formula_64 with both sides yields the following on the LHS (note that a transpose is the same as a dot product)\n\nIn the above equation the relationship between body frame components of the reaction forces and inertial frame components of reaction forces is used. The assumption that the bar connecting the point mass to the cart is massless implies that this bar cannot transfer any load perpendicular to the bar. Thus, the inertial frame components of the reaction forces can be written simply as formula_66 which signifies that the bar can only transfer loads along the axis of the bar itself. This gives rise to another equation which can be used to solve for the tension in the rod itself\n\nThe RHS of the equation is computed similarly by dotting formula_64 with the acceleration of the pendulum. The result (after some simplification) is shown below.\n\nCombining the LHS with the RHS and dividing through by m yields\n\nwhich again is identical to the result of Lagrange's method. The benefit of using Newton's method is that all reaction forces are revealed to ensure that nothing will be damaged.\nAn inverted pendulum in which the pivot is oscillated rapidly up and down can be stable in the inverted position. This is called Kapitza's pendulum, after Russian physicist Pyotr Kapitza who first analysed it. The equation of motion for a pendulum connected to a massless, oscillating base is derived the same way as with the pendulum on the cart. The position of the point mass is now given by:\nand the velocity is found by taking the first derivative of the position:\n\nThe Lagrangian for this system can be written as:\nand the equation of motion follows from:\nresulting in:\nIf \"y\" represents a simple harmonic motion, formula_76, the following differential equation is:\n\nThis equation does not have elementary closed-form solutions, but can be explored in a variety of ways. It is closely approximated by the Mathieu equation, for instance, when the amplitude of oscillations are small. Analyses show that the pendulum stays upright for fast oscillations. The first plot shows that when formula_78 is a slow oscillation, the pendulum quickly falls over when disturbed from the upright position. The angle formula_5 exceeds 90° after a short time, which means the pendulum has fallen on the ground. If formula_78 is a fast oscillation the pendulum can be kept stable around the vertical position. The second plot shows that when disturbed from the vertical position, the pendulum now starts an oscillation around the vertical position (formula_81). The deviation from the vertical position stays small, and the pendulum doesn't fall over.\nAchieving stability of an inverted pendulum has become a common engineering challenge for researchers. There are different variations of the inverted pendulum on a cart ranging from a rod on a cart to a multiple segmented inverted pendulum on a cart. Another variation places the inverted pendulum's rod or segmented rod on the end of a rotating assembly. In both, (the cart and rotating system) the inverted pendulum can only fall in a plane. The inverted pendulums in these projects can either be required to only maintain balance after an equilibrium position is achieved or be able to achieve equilibrium by itself. Another platform is a two-wheeled balancing inverted pendulum. The two wheeled platform has the ability to spin on the spot offering a great deal of maneuverability. Yet another variation balances on a single point. A spinning top, a unicycle, or an inverted pendulum atop a spherical ball all balance on a single point. As derived above the inverted pendulum can also be achieved by having a vertically oscillating base.\n\nArguably the most prevalent example of a stabilized inverted pendulum is a human being. A person standing upright acts as an inverted pendulum with his feet as the pivot, and without constant small muscular adjustments would fall over. The human nervous system contains an unconscious feedback control system, the sense of balance or righting reflex, that uses proprioceptive input from the eyes, muscles and joints, and orientation input from the vestibular system consisting of the three semicircular canals in the inner ear, and two otolith organs, to make continual small adjustments to the skeletal muscles to keep us standing upright. Walking, running, or balancing on one leg puts additional demands on this system. Certain diseases and alcohol or drug intoxication can interfere with this reflex, causing dizziness and disequilibration, an inability to stand upright. A field sobriety test used by police to test drivers for the influence of alcohol or drugs, tests this reflex for impairment. \n\nSome simple examples include balancing brooms or meter sticks by hand. \nThe inverted pendulum has been employed in various devices and trying to balance an inverted pendulum presents a unique engineering problem for researchers. The inverted pendulum was a central component in the design of several early seismometers due to its inherent instability resulting in a measurable response to any disturbance.\n\nThe inverted pendulum model has been used in some recent personal transportation vehicles, the two-wheeled self-balancing scooters such as the Segway PT and self-balancing hoverboard. These devices are kinematically unstable and use an electronic feedback servo system to keep them upright.\n\n\n\n\n",
    "id": "265421",
    "title": "Inverted pendulum"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=29468",
    "text": "Speech recognition\n\nSpeech recognition is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as \"automatic speech recognition\" (ASR), \"computer speech recognition\", or just \"speech to text\" (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.\n\nSome speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker independent\" systems. Systems that use training are called \"speaker dependent\".\n\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. \"Call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\n\nThe term \"voice recognition\" or \"speaker identification\" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\n\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Google, Microsoft, IBM, Baidu, Apple, Amazon, Nuance, SoundHound, IflyTek, CDAC, Speechmatics many of which have publicized the core technology in their speech recognition systems as being based on deep learning.\n\nIn 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.\n\nGunnar Fant developed the source-filter model of speech production and published it in 1960, which proved to be a useful model of speech production.\n\nUnfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce defunded speech recognition research at Bell Labs where no research on speech recognition was done until Pierce retired and James L. Flanagan took over.\n\nRaj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess.\n\nAlso around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. The DTW algorithm processed the speech signal by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique of dividing the signal into frames would carry on. Achieving speaker independence was a major unsolved goal of researchers during this time period.\n\nIn 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN, IBM, Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter.\n\nDespite the fact that CMU's Harpy system met the original goals of the program, many predictions turned out to be nothing more than hype, disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.\n\nDuring the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. At CMU, Raj Reddy's students James Baker and Janet M. Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.\n\nUnder Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominant speech recognition algorithm in the 1980s. IBM had a few competitors including Dragon Systems founded by James and Janet M. Baker in 1982. The 1980s also saw the introduction of the n-gram language model. Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams. During the same time, also CSELT was using HMM (the diphonies were studied since 1980) to recognize language like Italian. At the same time, CSELT led a series of European projects (Esprit I, II), and summarized the state-of-the-art in a book, later (2013) reprinted.\n\nMuch of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. Using these computers it could take up to 100 minutes to decode just 30 seconds of speech. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models.\n\nIn the mid-Eighties new speech recognition microprocessors were released: for example RIPAC, an independent-speaker recognition (for continuous speech) chip tailored for telephone services, was presented in the Netherlands in 1986. It was designed by CSELT/Elsag and manufactured by SGS.\n\nThe 1990s saw the first introduction of commercially successful speech recognition technologies. Two of the earliest products were Dragon Dictate, a consumer product released in 1990 and originally priced at $9,000, and a recognizer from Kurzweil Applied Intelligence released in 1987. AT&T deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without the use of a human operator. The technology was developed by Lawrence Rabiner and others at Bell Labs.\nBy this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.\n\nLernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.\n\nIn the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, \nCambridge University, and a team composed of ISCI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.\n\nIn the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.\n\nIn the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.\nToday, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), \na recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.\nAround 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.\n\nThe use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence \"The shared views of four research groups\" subtitle in their 2012 review paper). A Microsoft research executive called this innovation \"the most dramatic change in accuracy since 1979.\" In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.\n\nIn the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.\nBut these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.\nA number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.\nAll these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.\n\nBoth acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.\n\nModern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.\n\nAnother reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of \"n\"-dimensional real-valued vectors (with \"n\" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.\n\nDescribed above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).\n\nDecoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).\n\nA possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.\n\nDynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.\n\nDynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.\n\nA well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.\n\nNeural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.\n\nIn contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.\n\nHowever, recently LSTM Recurrent Neural Networks (RNNs) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition.\n\nDeep Neural Networks and Denoising Autoencoders were also being experimented with to tackle this problem in an effective manner.\n\nDue to the inability of feedforward Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.\n\nA deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.\n\nA success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.\nrecent overview articles.\n\nOne fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.\nThe true \"raw\" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.\n\nSince 2014, there has been much research interest in \"end-to-end\" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. Consequently, modern commercial ASR systems from Google and Apple (as of 2017) are deployed on the cloud and require a network connection as opposed to the device locally.\n\nThe first attempt of end-to-end ASR was with Connectionist Temporal Classification (CTC) based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. In 2016, University of Oxford presented LipNet, the first end-to-end sentence-level lip reading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.\n\nAn alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanaua et al. of the University of Montreal in 2016. The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to different parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for deployment onto applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters; University of Oxford and Google DeepMind extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading surpassing human-level performance.\n\nTypically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a \"listening window\" during which it may accept a speech input for recognition.\nSimple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.\n\nIn the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.\n\nOne of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to \"Meaningful Use\" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.\n\nA more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases – e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.\n\nAs an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.\n\nProlonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.\n\nSubstantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.\n\nWorking with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.\n\nThe Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.\n\nSpeaker-independent systems are also being developed and are under test for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.\n\nThe problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.\n\nAs in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.\n\nTraining for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.\nSpeech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.\n\nThe USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.\n\nASR is now commonplace In the field of telephony, and is becoming more widespread in the field of computer gaming and simulation. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.\n\nThe improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), Voci Technologies, VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), Verbyx VRX and SVOX.\n\nFor language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.\n\nStudents who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.\n\nStudents who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.\n\nSpeech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.\n\nUse of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.\n\nPeople with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.\n\nSpeech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.\n\nThis type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.\n\n\nThe performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).\n\nSpeech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:\n\nAs mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors:\ne.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.\ne.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z\");\nan 8% error rate is considered good for this vocabulary.\nA speaker-dependent system is intended for use by a single speaker.\nA speaker-independent system is intended for use by any speaker (more difficult).\nWith isolated speech, single words are used, therefore it becomes easier to recognize the speech.\nWith discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. \nWith continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.\ne.g. Querying application may dismiss the hypothesis \"The apple is red.\" \ne.g. Constraints may be semantic; rejecting \"The apple is angry.\" \ne.g. Syntactic; rejecting \"Red is apple the.\" \nConstraints are often represented by a grammar. \nWhen a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. \nEnvironmental noise (e.g. Noise in a car or a factory) \nAcoustical distortions (e.g. echoes, room acoustics)\nSpeech recognition is a multi-levelled pattern recognition task.\ne.g. Phonemes, Words, Phrases, and Sentences;\ne.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;\nBy combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: \nFor telephone speech the sampling rate is 8000 samples per second; \ncomputed every 10 ms, with one 10 ms section called a frame;\n\nAnalysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).\n\nThe sound waves can be digitized: Sample a strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of a wave. Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. Like the waves would. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.\n\n\nGiven basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.\n\nSpeech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast or by non-owners in the same room can cause devices in audience homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Another demonstrated approach is to transmit ultrasound and attempt to send commands without nearby people noticing.\n\nPopular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.\n\nBooks like \"Fundamentals of Speech Recognition\" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be \"Statistical Methods for Speech Recognition\" by Frederick Jelinek and \"Spoken Language Processing (2001)\" by Xuedong Huang etc. More up to date are \"Computer Speech\", by Manfred R. Schroeder, second edition published in 2004, and \"Speech Processing: A Dynamic and Optimization-Oriented Approach\" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of \"Speech and Language Processing (2008)\" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A most recent comprehensive textbook, \"Fundamentals of Speaker Recognition\" is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).\n\nA good and accessible introduction to speech recognition technology and its history is provided by the general audience book \"The Voice in the Machine. Building Computers That Understand Speech\" by Roberto Pieraccini (2012).\n\nThe most recent book on speech recognition is \"Automatic Speech Recognition: A Deep Learning Approach\" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, \"Deep Learning: Methods and Applications\" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.\n\nIn terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used.\n\nA Demo of an on-line speech recognizer is available on Cobalt's webpage.\n\nFor more software resources, see List of speech recognition software.\n\n\n",
    "id": "29468",
    "title": "Speech recognition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21523",
    "text": "Artificial neural network\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance on) tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any a priori knowledge about cats, e.g., that they have fur, tails, whiskers and cat-like faces. Instead, they evolve their own set of relevant characteristics from the learning material that they process.\n\nAn ANN is based on a collection of connected units or nodes called artificial neurons (analogous to biological neurons in an animal brain). Each connection (analogous to a synapse) between artificial neurons can transmit a signal from one to another. The artificial neuron that receives the signal can process it and then signal artificial neurons connected to it.\n\nIn common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is calculated by a non-linear function of the sum of its inputs. Artificial neurons and connections typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that only if the aggregate signal crosses that threshold is the signal sent. Typically, artificial neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n\nWarren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.\n\nIn the late 1940s, D.O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.\n\nFarley and Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).\n\nRosenblatt (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.\n\nIn 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.\n\nThe first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.\n\nNeural network research stagnated after machine learning research by Minsky and Papert (1969), who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power.\n\nMuch of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in \"if-then\" rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.\n\nA key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem and more generally accelerated the training of multi-layer networks. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.\n\nIn the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.\n\nSupport vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity.\n\nEarlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as \"deep learning\", although deep learning is not strictly synonymous with deep neural networks.\n\nIn 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.\nIn 2010, Backpropagation training though max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.\n\nThe vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs). As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.\n\nTo overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization.\n\nHinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.\n\nComputational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.\n\nBetween 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group, won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.\n\nCiresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.\n\nResearchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.\n\nGPU-based implementations of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the ImageNet Competition and others.\n\nDeep, highly nonlinear neural architectures similar to the neocognitron and the \"standard architecture of vision\", inspired by simple and complex cells were pre-trained by unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.\n\nAs of 2011, the state of the art in deep learning feedforward networks alternated convolutional layers and max-pooling layers, topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training.\n\nSuch supervised deep learning methods were the first to achieve human-competitive performance on certain tasks.\n\nANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks, WWN-1 (2008) through WWN-7 (2013).\n\nAn \"(artificial) neural network\" is a network of simple elements called \"neurons\", which receive input, change their internal state (\"activation\") according to that input, and produce output depending on the input and activation. The \"network\" forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called \"learning\" which is governed by a \"learning rule\".\n\nA neuron with label formula_1 receiving an input formula_2 from predecessor neurons consists of the following components:\n\nOften the output function is simply the Identity function.\n\nAn \"input neuron\" has no predecessor but serves as input interface for the whole network. Similarly an \"output neuron\" has no successor and thus serves as output interface of the whole network.\n\nThe \"network\" consists of connections, each connection transferring the output of a neuron formula_13 to the input of a neuron formula_1. In this sense formula_13 is the predecessor of formula_1 and formula_1 is the successor of formula_13. Each connection is assigned a weight formula_19.\n\nThe \"propagation function\" computes the \"input\" formula_2 to the neuron formula_1 from the outputs formula_22 of predecessor neurons and typically has the form\n\nThe \"learning rule\" is a rule or an algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This \"learning\" process typically amounts to modifying the weights and thresholds of the variables within the network.\n\nNeural network models can be viewed as simple mathematical models defining a function formula_24 or a distribution over formula_25 or both formula_25 and formula_27. Sometimes models are intimately associated with a particular learning rule. A common use of the phrase \"ANN model\" is really the definition of a \"class\" of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).\n\nMathematically, a neuron's network function formula_28 is defined as a composition of other functions formula_29, that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. A widely used type of composition is the \"nonlinear weighted sum\", where formula_30, where formula_31 (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent or sigmoid function or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions formula_32 as a vector formula_33.\n\nThis figure depicts such a decomposition of formula_34, with dependencies between variables indicated by arrows. These can be interpreted in two ways.\n\nThe first view is the functional view: the input formula_35 is transformed into a 3-dimensional vector formula_36, which is then transformed into a 2-dimensional vector formula_37, which is finally transformed into formula_34. This view is most commonly encountered in the context of optimization.\n\nThe second view is the probabilistic view: the random variable formula_39 depends upon the random variable formula_40, which depends upon formula_41, which depends upon the random variable formula_25. This view is most commonly encountered in the context of graphical models.\n\nThe two views are largely equivalent. In either case, for this particular architecture, the components of individual layers are independent of each other (e.g., the components of formula_37 are independent of each other given their input formula_36). This naturally enables a degree of parallelism in the implementation.\n\nNetworks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where formula_34 is shown as being dependent upon itself. However, an implied temporal dependence is not shown.\n\nThe possibility of learning has attracted the most interest in neural networks. Given a specific \"task\" to solve, and a class of functions formula_46, learning means using a set of observations to find formula_47 which solves the task in some optimal sense.\n\nThis entails defining a cost function formula_48 such that, for the optimal solution formula_49, formula_50 formula_51 i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).\n\nThe cost function formula_52 is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.\n\nFor applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model formula_34, which minimizes formula_54, for data pairs formula_55 drawn from some distribution formula_56. In practical situations we would only have formula_57 samples from formula_56 and thus, for the above example, we would only minimize formula_59. Thus, the cost is minimized over a sample of the data rather than the entire distribution.\n\nWhen formula_60 some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when formula_56 is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.\n\nWhile it is possible to define an ad hoc cost function, frequently a particular cost (function) is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.\n\nA DNN can be discriminatively trained with the standard backpropagation algorithm. Backpropagation is a method to calculate the gradient of the loss function (produces the cost associated with a given state) with respect to the weights in an ANN.\n\nThe basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969. In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to the modern version of backpropagation which is efficient even when the networks are sparse. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1974, Werbos mentioned the possibility of applying this principle to ANNs, and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today. In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through backpropagation.\n\nThe weight updates of backpropagation can be done via stochastic gradient descent using the following equation:\nwhere, formula_63 is the learning rate, formula_64 is the cost (loss) function and formula_65 a stochastic term. The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as formula_66 where formula_67 represents the class probability (output of the unit formula_68) and formula_69 and formula_70 represent the total input to units formula_68 and formula_72 of the same level respectively. Cross entropy is defined as formula_73 where formula_74 represents the target probability for output unit formula_68 and formula_67 is the probability output for formula_68 after applying the activation function.\n\nThese can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize \"L\" error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.\n\nAlternatives to backpropagation include Extreme Learning Machines, \"No-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\n\nThe three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning.\n\nSupervised learning uses a set of example pairs formula_78 and the aim is to find a function formula_79 in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.\n\nA commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, formula_80, and the target value formula_81 over all the example pairs. Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.\n\nTasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n\nIn unsupervised learning, some data formula_35 is given and the cost function to be minimized, that can be any function of the data formula_35 and the network's output, formula_34.\n\nThe cost function is dependent on the task (the model domain) and any \"a priori\" assumptions (the implicit properties of the model, its parameters and the observed variables).\n\nAs a trivial example, consider the model formula_85 where formula_86 is a constant and the cost formula_87. Minimizing this cost produces a value of formula_86 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between formula_35 and formula_28, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).\n\nTasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n\nIn reinforcement learning, data formula_35 are usually not given, but generated by an agent's interactions with the environment. At each point in time formula_92, the agent performs an action formula_93 and the environment generates an observation formula_94 and an instantaneous cost formula_95, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.\n\nMore formally the environment is modeled as a Markov decision process (MDP) with states formula_96 and actions formula_97 with the following probability distributions: the instantaneous cost distribution formula_98, the observation distribution formula_99 and the transition formula_100, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.\n\nANNs are frequently used in reinforcement learning as part of the overall algorithm. Dynamic programming was coupled with ANNs (giving neurodynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing, natural resources management or medicine because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.\n\nTasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n\nThis is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. In 2004 a recursive least squares algorithm was introduced to train CMAC neural network online. This algorithm can converge in one step and update all weights in one step with any new input data. Initially, this algorithm had computational complexity of \"O\"(\"N\"). Based on QR decomposition, this recursive learning algorithm was simplified to be \"O\"(\"N\").\n\nTraining a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.\n\nMost employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories: \n\nEvolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other methods for training neural networks.\n\nThe Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.\n\nA convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs to take advantage of the 2D structure of input data.\n\nCNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate. Examples of applications in computer vision include DeepDream.\n\nLong short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem. LSTM is normally augmented by recurrent gates called forget gates. LSTM networks prevent backpropagated errors from vanishing or exploding. Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn \"very deep learning\" tasks that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved. LSTM can handle long delays and signals that have a mix of low and high frequency components.\n\nStacks of LSTM RNNs trained by Connectionist Temporal Classification (CTC) can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nIn 2003, LSTM started to become competitive with traditional speech recognizers. In 2007, the combination with CTC achieved first good results on speech data. In 2009, a CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional speech processing methods. LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, for Google Android, and photo-real talking heads. In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.\n\nLSTM became popular in Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages. LSTM improved machine translation, language modeling and multilingual language processing. LSTM combined with CNNs improved automatic image captioning.\n\nDeep Reservoir Computing and Deep Echo State Networks (deepESNs) provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.\n\nA deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each layer.\n\nA DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. Backpropagation or other discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.\n\nLarge memory storage and retrieval neural networks (LAMSTAR) are fast deep learning neural networks of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.\n\nA LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This grossly imitates biological learning which integrates various preprocessors (cochlea, retina, \"etc.\") and cortexes (auditory, visual, \"etc.\") and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and its ability to cope with incomplete data, or \"lost\" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.\n\nLAMSTAR has been applied to many domains, including medical and financial predictions, adaptive filtering of noisy speech in unknown noise, still-image recognition, video image recognition, software security and adaptive control of non-linear systems. LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.\n\nThese applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses, such as in the cases of predicting onset of sleep apnea events, of an electrocardiogram of a fetus as recorded from skin-surface electrodes placed on the mother's abdomen early in pregnancy, of financial prediction or in blind filtering of noisy speech.\n\nLAMSTAR was proposed in 1996 () and was further developed Graupe and Kordylewski from 1997-2002. A modified version, known as LAMSTAR 2, was developed by Schneider and Graupe in 2008.\n\nThe auto encoder idea is motivated by the concept of a \"good\" representation. For example, for a classifier, a good representation can be defined as one that yields a better-performing classifier.\n\nAn \"encoder\" is a deterministic mapping formula_101 that transforms an input vector x into hidden representation y, where formula_102, formula_103 is the weight matrix and b is an offset vector (bias). A \"decoder\" maps back the hidden representation y to the reconstructed input z via formula_104. The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize the error to make the reconstructed value as close as possible to the original.\n\nIn \"stacked denoising auto encoders\", the partially corrupted output is cleaned (de-noised). This idea was introduced in 2010 by Vincent et al. with a specific approach to \"good\" representation, a \"good representation\" is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input\".\" Implicit in this definition are the following ideas:\nThe algorithm starts by a stochastic mapping of formula_105 to formula_106 through formula_107, this is the corrupting step. Then the corrupted input formula_106 passes through a basic auto-encoder process and is mapped to a hidden representation formula_109. From this hidden representation, we can reconstruct formula_110. In the last stage, a minimization algorithm runs in order to have z as close as possible to uncorrupted input formula_105. The reconstruction error formula_112 might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.\n\nIn order to make a deep architecture, auto encoders stack. Once the encoding function formula_101 of the first denoising auto encoder is learned and used to uncorrupt the input (corrupted input), the second level can be trained.\n\nOnce the stacked auto encoder is trained, its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multi-class logistic regression.\n\nA a deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.\n\nEach block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is formula_114. Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class \"y\", and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:\nwhich has a closed-form solution.\n\nUnlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs perform better than conventional DBNs.\n\nThis architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.\n\nWhile parallelization and scalability are not considered seriously in conventional , all learning for s and s is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets.\n\nThe basic architecture is suitable for diverse tasks such as classification and regression.\n\nThe need for deep learning with real-valued inputs, as in Gaussian restricted Boltzmann machines, led to the \"spike-and-slab\" RBM (\"ss\"RBM), which models continuous-valued inputs with strictly binary latent variables. Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain; their mixture forms a prior.\n\nAn extension of ssRBM called µ-ssRBM provides extra modeling capacity using additional terms in the energy function. One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.\n\nCompound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs, DBMs, deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse feature learning, RNNs, conditional DBNs, de-noising auto encoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a ) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. \"Hierarchical Bayesian (HB)\" models allow learning from few examples, for example for computer vision, statistics and cognitive science.\n\nCompound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a \"hierarchical Dirichlet process (HDP)\" as a hierarchical model, incorporated with DBM architecture. It is a full generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look \"reasonably\" natural. All the levels are learned jointly by maximizing a joint log-probability score.\n\nIn a DBM with three hidden layers, the probability of a visible input is:\nwhere formula_117 is the set of hidden units, and formula_118 are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.\n\nA learned DBM model is an undirected model that defines the joint distribution formula_119. One way to express what has been learned is the conditional model formula_120 and a prior term formula_121.\n\nHere formula_120 represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of formula_123:\n\nA deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.\n\nDPCNs predict the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from previous states.\n\nDPCNs can be extended to form a convolutional network.\n\nIntegrating external memory with ANNs dates to early research in distributed representations and Kohonen's self-organizing maps. For example, in sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.\n\nApart from long short-term memory (LSTM), other approaches also added differentiable memory to recurrent functions. For example:\n\nNeural Turing machines couple LSTM networks to external memory resources, with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.\n\nDifferentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.\n\nApproaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.\n\nMemory networks are another extension to neural networks incorporating long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.\n\nDeep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently — unlike models like LSTM, whose number of parameters grows quadratically with memory size.\n\nEncoder–decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation, where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.\n\nMultilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal component analysis (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of the deep learning architecture.\n\nLayer formula_125 learns the representation of the previous layer formula_126, extracting the formula_127 principal component (PC) of the projection layer formula_126 output in the feature domain induced by the kernel. For the sake of dimensionality reduction of the updated representation in each layer, a supervised strategy is proposed to select the best informative features among features extracted by KPCA. The process is:\nSome drawbacks accompany the KPCA method as the building cells of an MKM.\n\nA more straightforward way to use kernel machines for deep learning was developed for spoken language understanding. The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.\n\nUsing ANNs requires an understanding of their characteristics.\nANN capabilities fall within the following broad categories:\n\n\nBecause of their ability to reproduce and model nonlinear processes, ANNs have found many applications in a wide range of disciplines.\n\nApplication areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, signal classification, object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering.\n\nANNs have been used to diagnose cancers, including lung cancer, prostate cancer, colorectal cancer and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.\n\nANNs have been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology, are just few examples of this kind.\n\nTheoretical and computational neuroscience is concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems attempt to reflect cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.\n\nTo gain this understanding, neuroscientists strive to link observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).\n\nBrain research has repeatedly led to new ANN approaches, such as the use of connections to connect neurons in other layers rather than adjacent neurons in the same layer. Other research explored the use of multiple signal types, or finer control than boolean (on/off) variables. Dynamic neural networks can dynamically form new connections and even new neural units while disabling others.\n\nMany types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.\n\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\n\nA specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\n\nModels' \"capacity\" property roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\n\nModels may not consistently converge on a single solution, firstly because many local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. However, for CMAC neural network, a recursive least squares algorithm was introduced to train it, and this algorithm can be guaranteed to converge in one step.\n\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of \"regularization\". This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\n\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.\n\nThe softmax activation function is:\n\n<section end=\"theory\" />\n\nA common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example and by grouping examples in so-called mini-batches. Improving the training efficiency and convergence capability has always been an ongoing research area for neural network. For example, by introducing a recursive least squares algorithm for CMAC neural network, the training process only takes one step to converge.\n\nNo neural network has solved computationally difficult problems such as the n-Queens problem, the travelling salesman problem, or the problem of factoring large integers.\n\nA fundamental objection is that they do not reflect how real neurons function. Back propagation is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.\n\nThe motivation behind ANNs is not necessarily to strictly replicate neural function, but to use biological neural networks as an inspiration. A central claim of ANNs is therefore that it embodies some new and powerful general principle for processing information. Unfortunately, these general principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\".\n\nBiological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections which can consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which must often be matched with enormous CPU processing power and time.\n\nSchmidhuber notes that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of parallel GPUs can reduce training times from months to days.\n\nNeuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\n\nArguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\n\nTechnology writer Roger Bridgman commented:\n\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs non-local learning and shallow vs deep architecture.\n\nAdvocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.\n\nArtificial neural networks have many variations. The simplest, static types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to change during the learning process. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\n\n\n",
    "id": "21523",
    "title": "Artificial neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14179835",
    "text": "Activation function\n\nIn computational networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard computer chip circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only \"nonlinear\" activation functions allow such networks to compute nontrivial problems using only a small number of nodes. In artificial neural networks this function is also called the transfer function.\n\nIn biologically inspired neural networks, the activation function is usually an abstraction representing the rate of action potential firing in the cell. In its simplest form, this function is binary—that is, either the neuron is firing or not. The function looks like formula_1, where formula_2 is the Heaviside step function. In this case many neurons must be used in computation beyond linear separation of categories.\n\nA line of positive slope may be used to reflect the increase in firing rate that occurs as input current increases. Such a function would be of the form formula_3, where formula_4 is the slope. This activation function is linear, and therefore has the same problems as the binary function. In addition, networks constructed using this model have unstable convergence because neuron inputs along favored paths tend to increase without bound, as this function is not normalizable.\n\nAll problems mentioned above can be handled by using a normalizable sigmoid activation function. One realistic model stays at zero until input current is received, at which point the firing frequency increases quickly at first, but gradually approaches an asymptote at 100% firing rate. Mathematically, this looks like formula_5, where the hyperbolic tangent function can be replaced by any sigmoid function. This behavior is realistically reflected in the neuron, as neurons cannot physically fire faster than a certain rate. This model runs into problems, however, in computational networks as it is not differentiable, a requirement to calculate backpropagation.\n\nThe final model, then, that is used in multilayer perceptrons is a sigmoidal activation function in the form of a hyperbolic tangent. Two forms of this function are commonly used: formula_6 whose range is normalized from -1 to 1, and formula_7 is vertically translated to normalize from 0 to 1. The latter model is often considered more biologically realistic, but it runs into theoretical and experimental difficulties with certain types of computational problems.\n\nA special class of activation functions known as radial basis functions (RBFs) are used in RBF networks, which are extremely efficient as universal function approximators. These activation functions can take many forms, but they are usually found as one of three functions:\nwhere formula_11 is the vector representing the function \"center\" and formula_12 and formula_13 are parameters affecting the spread of the radius.\n\nSupport vector machines (SVMs) can effectively utilize a class of activation functions that includes both sigmoids and RBFs. In this case, the input is transformed to reflect a decision boundary hyperplane based on a few training inputs called \"support vectors\" formula_14. The activation function for the hidden layer of these machines is referred to as the \"inner product kernel\", formula_15. The support vectors are represented as the centers in RBFs with the kernel equal to the activation function, but they take a unique form in the perceptron as \nwhere formula_17 and formula_18 must satisfy certain conditions for convergence. These machines can also accept arbitrary-order polynomial activation functions where\nActivation function having types:\n\nSome desirable properties in an activation function include:\n\n\nThe following table compares the properties of several activation functions that are functions of one fold from the previous layer or layers:\n</math>\nwith formula_51 and formula_52\n\nThe following table lists activation functions that are not functions of a single fold from the previous layer or layers:\n\n",
    "id": "14179835",
    "title": "Activation function"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8220913",
    "text": "ADALINE\n\nADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford University in 1960. It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.\n\nThe difference between Adaline and the standard (McCulloch–Pitts) perceptron is that in the learning phase, the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights.\n\nA multilayer network of ADALINE units is known as a MADALINE.\n\nAdaline is a single layer neural network with multiple nodes where each node accepts multiple inputs and generates one output. Given the following variables:as\n\nthen we find that the output is formula_6. If we further assume that\n\nthen the output further reduces to: formula_9\n\nLet us assume:\n\nthen the weights are updated as follows formula_13. The ADALINE converges to the least squares error which is formula_14.\nThis update rule is in fact the stochastic gradient descent update for linear regression.\n\nMADALINE (Many ADALINE) is a three-layer (input, hidden, output), fully connected, feed-forward artificial neural network architecture for classification that uses ADALINE units in its hidden and output layers, i.e. its activation function is the sign function. The three-layer network uses memistors. Three different training algorithms for MADALINE networks, which cannot be learned using backpropagation because the sign function is not differentiable, have been suggested, called Rule I, Rule II and Rule III. The first of these dates back to 1962 and cannot adapt the weights of the hidden-output connection. The second training algorithm improved on Rule I and was described in 1988. The third \"Rule\" applied to a modified network with sigmoid activations instead of signum; it was later found to be equivalent to backpropagation.\n\nThe Rule II training algorithm is based on a principle called \"minimal disturbance\". It proceeds by looping over training examples, then for each example, it:\n\nAdditionally, when flipping single units' signs does not drive the error to zero for a particular example, the training algorithm starts flipping pairs of units' signs, then triples of units, etc.\n\n\n",
    "id": "8220913",
    "title": "ADALINE"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3056879",
    "text": "Adaptive resonance theory\n\nAdaptive resonance theory (ART) is a theory developed by Stephen Grossberg and Gail Carpenter on aspects of how the brain processes information. It describes a number of neural network models which use supervised and unsupervised learning methods, and address problems such as pattern recognition and prediction.\n\nThe primary intuition behind the ART model is that object identification and recognition generally occur as a result of the interaction of 'top-down' observer expectations with 'bottom-up' sensory information. The model postulates that 'top-down' expectations take the form of a memory template or prototype that is then compared with the actual features of an object as detected by the senses. This comparison gives rise to a measure of category belongingness. As long as this difference between sensation and expectation does not exceed a set threshold called the 'vigilance parameter', the sensed object will be considered a member of the expected class. The system thus offers a solution to the 'plasticity/stability' problem, i.e. the problem of acquiring new knowledge without disrupting existing knowledge that is also called incremental learning.\n\nThe basic ART system is an unsupervised learning model. It typically consists of a comparison field and a recognition field composed of neurons, a vigilance parameter (threshold of recognition), and a reset module. The comparison field takes an input vector (a one-dimensional array of values) and transfers it to its best match in the recognition field. Its best match is the single neuron whose set of weights (weight vector) most closely matches the input vector. Each recognition field neuron outputs a negative signal (proportional to that neuron’s quality of match to the input vector) to each of the other recognition field neurons and thus inhibits their output. In this way the recognition field exhibits lateral inhibition, allowing each neuron in it to represent a category to which input vectors are classified.\n\nAfter the input vector is classified, the reset module compares the strength of the recognition match to the vigilance parameter. If the vigilance parameter is overcome (i.e. the input vector is within the normal range seen on previous input vectors), training commences: the weights of the winning recognition neuron are adjusted towards the features of the input vector. Otherwise, if the match level is below the vigilance parameter (i.e. the input vector's match is outside the normal expected range for that neuron) the winning recognition neuron is inhibited and a search procedure is carried out. In this search procedure, recognition neurons are disabled one by one by the reset function until the vigilance parameter is overcome by a recognition match. In particular, at each cycle of the search procedure the most active recognition neuron is selected and then switched off if its activation is below the vigilance parameter (note that it thus releases the remaining recognition neurons from its inhibition). If no committed recognition neuron’s match overcomes the vigilance parameter, then an uncommitted neuron is committed and its weights are adjusted towards matching the input vector. The vigilance parameter has considerable influence on the system: higher vigilance produces highly detailed memories (many, fine-grained categories), while lower vigilance results in more general memories (fewer, more-general categories).\n\nThere are two basic methods of training ART-based neural networks: slow and fast. In the slow learning method, the degree of training of the recognition neuron’s weights towards the input vector is calculated to continuous values with differential equations and is thus dependent on the length of time the input vector is presented. With fast learning, algebraic equations are used to calculate degree of weight adjustments to be made, and binary values are used. While fast learning is effective and efficient for a variety of tasks, the slow learning method is more biologically plausible and can be used with continuous-time networks (i.e. when the input vector can vary continuously).\n\nART 1 is the simplest variety of ART networks, accepting only binary inputs.\nART 2 extends network capabilities to support continuous inputs.\nART 2-A is a streamlined form of ART-2 with a drastically accelerated runtime, and with qualitative results being only rarely inferior to the full ART-2 implementation.\nART 3 builds on ART-2 by simulating rudimentary neurotransmitter regulation of synaptic activity by incorporating simulated sodium (Na+) and calcium (Ca2+) ion concentrations into the system's equations, which results in a more physiologically realistic means of partially inhibiting categories that trigger mismatch resets.\nARTMAP also known as Predictive ART, combines two slightly modified ART-1 or ART-2 units into a supervised learning structure where the first unit takes the input data and the second unit takes the correct output data, then used to make the minimum possible adjustment of the vigilance parameter in the first unit in order to make the correct classification.\n\nFuzzy ART implements fuzzy logic into ART’s pattern recognition, thus enhancing generalizability. An optional (and very useful) feature of fuzzy ART is complement coding, a means of incorporating the absence of features into pattern classifications, which goes a long way towards preventing inefficient and unnecessary category proliferation. The applied similarity measures are based on the L1 norm. Fuzzy ART is known to be very sensitive to noise.\n\nFuzzy ARTMAP is merely ARTMAP using fuzzy ART units, resulting in a corresponding increase in efficacy.\n\nSimplified Fuzzy ARTMAP (SFAM) constitutes a strongly simplified variant of fuzzy ARTMAP dedicated to classification tasks.\n\nGaussian ART and Gaussian ARTMAP use Gaussian activation functions and computations based on probability theory. Therefore, they have some similarity with Gaussian mixture models. In comparison to fuzzy ART and fuzzy ARTMAP, they are less sensitive to noise. But the stability of learnt representations is reduced which may lead to category proliferation in open-ended learning tasks.\n\nFusion ART and related networks extend ART and ARTMAP to multiple pattern channels. They support several learning paradigms.\n\nTopoART combines fuzzy ART with topology learning networks such as the growing neural gas. Furthermore, it adds a noise reduction mechanism. There are several derived neural networks which extend TopoART to further learning paradigms.\n\nHypersphere ART and Hypersphere ARTMAP are closely related to fuzzy ART and fuzzy ARTMAP, respectively. But as they use a different type of category representation (namely hyperspheres), they do not require their input to be normalised to the interval [0, 1]. They apply similarity measures based on the L2 norm.\n\nLAPARTThe Laterally Primed Adaptive Resonance Theory (LAPART) neural networks couple two Fuzzy ART algorithms to create a mechanism for making predictions based on learned associations. The coupling of the two Fuzzy ARTs has a unique stability that allows the system to converge rapidly towards a clear solution. Additionally, it can perform logical inference and supervised learning similar to fuzzy ARTMAP.\n\nIt has been noted that results of Fuzzy ART and ART 1 depend critically upon the order in which the training data are processed. The effect can be reduced to some extent by using a slower learning rate, but is present regardless of the size of the input data set. Hence Fuzzy ART and ART 1 estimates do not possess the statistical property of consistency.\n\nWasserman, Philip D. (1989), Neural computing: theory and practice, New York: Van Nostrand Reinhold, \n\n",
    "id": "3056879",
    "title": "Adaptive resonance theory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4231161",
    "text": "ALOPEX\n\nALOPEX (an acronym from \"\"ALgorithms Of Pattern EXtraction\"\") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974.\n\nIn machine learning, the goal is to train a system to minimize a cost function or (referring to ALOPEX) a response function. Many training algorithms, such as backpropagation, have an inherent susceptibility to getting \"stuck\" in local minima or maxima of the response function. ALOPEX uses a cross-correlation of differences and a stochastic process to overcome this in an attempt to reach the absolute minimum (or maximum) of the response function.\n\nALOPEX, in its simplest form is defined by an updating equation:\n\nformula_1 \n\nWhere:\n\nEssentially, ALOPEX changes each system variable formula_15 based on a product of: the previous change in the variable formula_16formula_17, the resulting change in the cost function formula_16formula_19, and the learning rate parameter formula_9. Further, to find the absolute minimum (or maximum), the stochastic process formula_21 (Gaussian or other) is added to stochastically \"push\" the algorithm out of any local minima.\n\n",
    "id": "4231161",
    "title": "ALOPEX"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=349771",
    "text": "Artificial neuron\n\nAn artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network. The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or , representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.\n\nThe artificial neuron transfer function should not be confused with a linear system's transfer function.\n\nFor a given artificial neuron, let there be \"m\" + 1 inputs with signals \"x\" through \"x\" and weights \"w\" through \"w\". Usually, the \"x\" input is assigned the value +1, which makes it a \"bias\" input with \"w\" = \"b\". This leaves only \"m\" actual inputs to the neuron: from \"x\" to \"x\".\n\nThe output of the \"k\"th neuron is:\n\nWhere formula_2 (phi) is the transfer function.\n\nThe output is analogous to the axon of a biological neuron, and its value propagates to the input of the next layer, through a synapse. It may also exit the system, possibly as part of an output vector.\n\nIt has no learning process as such. Its transfer function weights are calculated and threshold value are predetermined.\n\nDepending on the specific model used they may be called a semi-linear unit, Nv neuron, binary neuron, linear threshold function, or McCulloch–Pitts (MCP) neuron.\n\nSimple artificial neurons, such as the McCulloch–Pitts model, are sometimes described as \"caricature models\", since they are intended to reflect one or more neurophysiological observations, but without regard to realism.\n\nArtificial neurons are designed to mimic aspects of their biological counterparts.\n\nUnlike most artificial neurons, however, biological neurons fire in discrete pulses. Each time the electrical potential inside the soma reaches a certain threshold, a pulse is transmitted down the axon. This pulsing can be translated into continuous values. The rate (activations per second, etc.) at which an axon fires converts directly into the rate at which neighboring cells get signal ions introduced into them. The faster a biological neuron fires, the faster nearby neurons accumulate electrical potential (or lose electrical potential, depending on the \"weighting\" of the dendrite that connects to the neuron that fired). It is this conversion that allows computer scientists and mathematicians to simulate biological neural networks using artificial neurons which can output distinct values (often from −1 to 1).\n\nResearch has shown that unary coding is used in the neural circuits responsible for birdsong production. The use of unary in biological networks is presumably due to the inherent simplicity of the coding. Another contributing factor could be that unary coding provides a certain degree of error correction.\n\nThe first artificial neuron was the Threshold Logic Unit (TLU), or Linear Threshold Unit, first proposed by Warren McCulloch and Walter Pitts in 1943. The model was specifically targeted as a computational model of the \"nerve net\" in the brain. As a transfer function, it employed a threshold, equivalent to using the Heaviside step function. Initially, only a simple model was considered, with binary inputs and outputs, some restrictions on the possible weights, and a more flexible threshold value. Since the beginning it was already noticed that any boolean function could be implemented by networks of such devices, what is easily seen from the fact that one can implement the AND and OR functions, and use them in the disjunctive or the conjunctive normal form.\nResearchers also soon realized that cyclic networks, with feedbacks through neurons, could define dynamical systems with memory, but most of the research concentrated (and still does) on strictly feed-forward networks because of the smaller difficulty they present.\n\nOne important and pioneering artificial neural network that used the linear threshold function was the perceptron, developed by Frank Rosenblatt. This model already considered more flexible weight values in the neurons, and was used in machines with adaptive capabilities. The representation of the threshold values as a bias term was introduced by Bernard Widrow in 1960 – see ADALINE.\n\nIn the late 1980s, when research on neural networks regained strength, neurons with more continuous shapes started to be considered. The possibility of differentiating the activation function allows the direct use of the gradient descent and other optimization algorithms for the adjustment of the weights. Neural networks also started to be used as a general function approximation model. The best known training algorithm called backpropagation has been rediscovered several times but its first development goes back to the work of Paul Werbos.\n\nThe transfer function of a neuron is chosen to have a number of properties which either enhance or simplify the network containing the neuron. Crucially, for instance, any multilayer perceptron using a \"linear\" transfer function has an equivalent single-layer network; a non-linear function is therefore necessary to gain the advantages of a multi-layer network.\n\nBelow, \"u\" refers in all cases to the weighted sum of all the inputs to the neuron, i.e. for \"n\" inputs,\n\nwhere w is a vector of \"synaptic weights\" and x is a vector of inputs.\n\nThe output \"y\" of this transfer function is binary, depending on whether the input meets a specified threshold, \"θ\". The \"signal\" is sent, i.e. the output is set to one, if the activation meets the threshold.\n\nThis function is used in perceptrons and often shows up in many other models. It performs a division of the space of inputs by a hyperplane. It is specially useful in the last layer of a network intended to perform binary classification of the inputs. It can be approximated from other sigmoidal functions by assigning large values to the weights.\n\nIn this case, the output unit is simply the weighted sum of its inputs plus a \"bias\" term. A number of such linear neurons perform a linear transformation of the input vector. This is usually more useful in the first layers of a network. A number of analysis tools exist based on linear models, such as harmonic analysis, and they can all be used in neural networks with this linear neuron. The bias term allows us to make affine transformations to the data.\n\nSee: Linear transformation, Harmonic analysis, Linear filter, Wavelet, Principal component analysis, Independent component analysis, Deconvolution.\n\nA fairly simple non-linear function, the sigmoid function such as the logistic function also has an easily calculated derivative, which can be important when calculating the weight updates in the network. It thus makes the network more easily manipulable mathematically, and was attractive to early computer scientists who needed to minimize the computational load of their simulations. It was previously commonly seen in multilayer perceptrons. However, recent work has shown sigmoid neurons to be less effective than rectified linear neurons. The reason is that the gradients computed by the backpropagation algorithm tend to diminish towards zero as activations propagate through layers of sigmoidal neurons, making it difficult to optimize neural networks using multiple layers of sigmoidal neurons.\n\nThe following is a simple pseudocode implementation of a single TLU which takes boolean inputs (true or false), and returns a single boolean output when activated. An object-oriented model is used. No method of training is defined, since several exist. If a purely functional model were used, the class TLU below would be replaced with a function TLU with input parameters threshold, weights, and inputs that returned a boolean value.\n\n\n",
    "id": "349771",
    "title": "Artificial neuron"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9054429",
    "text": "Autoassociative memory\n\nAutoassociative memory, also known as auto-association memory or an autoassociation network, is any type of memory that enables one to retrieve a piece of data from only a tiny sample of itself. It is often misunderstood to be only a form of backpropagation or other neural networks.\n\nTraditional memory stores data at a unique address and can \"recall\" the data upon presentation of the complete unique address.\n\nAutoassociative memories are capable of retrieving a piece of data upon presentation of only partial information from \"that\" piece of data.\n\nFor example, the sentence fragments presented below are sufficient for most humans to recall the missing information.\n\n\nMany readers will realize the missing information is in fact:\n\n\nThis demonstrates the capability of autoassociative networks to recall the whole by using some of its parts.\n\nHeteroassociative memories, on the other hand, can recall an associated piece of datum from \"one\" category upon presentation of data from \"another\" category. Hopfield networks have been shown to act as autoassociative memory since they are capable of remembering data by observing a portion of that data.\n\nBidirectional associative memories (BAM) are artificial neural networks that have long been used for performing heteroassociative recall.\n\n",
    "id": "9054429",
    "title": "Autoassociative memory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1360091",
    "text": "Backpropagation\n\nBackpropagation is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data (in image recognition, multiple images) is processed. It is a special case of an older and more general technique called automatic differentiation. In the context of learning, backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function. This technique is also sometimes called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.\n\nThe backpropagation algorithm has been repeatedly rediscovered and is equivalent to automatic differentiation in reverse accumulation mode.\nBackpropagation requires a known, desired output for each input value—it is therefore considered to be a supervised learning method (although it is used in some unsupervised networks such as autoencoders). Backpropagation is also a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. It is closely related to the Gauss–Newton algorithm, and is part of continuing research in neural backpropagation. Backpropagation can be used with any gradient-based optimizer, such as L-BFGS or truncated Newton. \n\nBackpropagation is commonly used to train deep neural networks , a term used to describe neural networks with more than one hidden layer. \n\nThe goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. An example would be a classification task, where the input is an image of an animal, and the correct output is the name of the animal.\n\nThe motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.\n\nSometimes referred to as the cost function or error function (not to be confused with the Gauss error function), the loss function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a case propagates through the network. \n\nTwo assumptions must be made about the form of the error function. The first is that it can be written as an average formula_1 over error functions formula_2, for formula_3 individual training examples, formula_4. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.\n\nLet formula_5 be vectors in formula_6.\n\nSelect an error function formula_7 measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors formula_8 and formula_9:\n\nformula_10 ()\n\nNote that the factor of formula_11 conveniently cancels the exponent when the error function is subsequently differentiated. \n\nThe error function over formula_3 training examples can simply be written as an average of losses over individual examples:formula_13\n\nand therefore, the partial derivative with respect to the outputs:formula_14\n\nThe optimization algorithm repeats a two phase cycle, propagation and weight update. When an input vector is presented to the network, it is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output, using a loss function. The resulting error value is calculated for each of the neurons in the output layer. The error values are then propagated from the output back through the network, until each neuron has an associated error value that reflects its contribution to the original output.\n\nBackpropagation uses these error values to calculate the gradient of the loss function. In the second phase, this gradient is fed to the optimization method, which in turn uses it to update the weights, in an attempt to minimize the loss function.\n\nLet formula_15 be a neural network with formula_16 connections, formula_17 inputs, and formula_18 outputs.\n\nBelow, formula_19 will denote vectors in formula_20, formula_21 vectors in formula_6, and formula_23 vectors in formula_24. \nThese are called \"inputs\", \"outputs\" and \"weights\" respectively.\n\nThe neural network corresponds to a function formula_25 which, given a weight formula_26, maps an input formula_27 to an output formula_8.\n\nThe optimization takes as input a sequence of \"training examples\" formula_29 and produces a sequence of weights formula_30 starting from some initial weight formula_31, usually chosen at random.\n\nThese weights are computed in turn: first compute formula_32 using only formula_33 for formula_34. The output of the algorithm is then formula_35, giving us a new function formula_36. The computation is the same in each step, hence only the case formula_37 is described.\n\nCalculating formula_38 from formula_39 is done by considering a variable weight formula_26 and applying gradient descent to the function formula_41 to find a local minimum, \nstarting at formula_42.\n\nThis makes formula_38 the minimizing weight found by gradient descent.\n\nTo implement the algorithm above, explicit formulas are required for the gradient of the function formula_44 where the function isformula_45.\n\nThe learning algorithm can be divided into two phases: propagation and weight update.\n\nEach propagation involves the following steps:\n\n\nFor each weight, the following steps must be followed:\nThis ratio (percentage) influences the speed and quality of learning; it is called the \"learning rate\". The greater the ratio, the faster the neuron trains, but the lower the ratio, the more accurate the training is. The sign of the gradient of a weight indicates whether the error varies directly with, or inversely to, the weight. Therefore, the weight must be updated in the opposite direction, \"descending\" the gradient.\n\nLearning is repeated (on new batches) until the network performs adequately.\n\nThe following is pseudocode for a stochastic gradient descent algorithm for training a three-layer network (only one hidden layer):\n\nThe lines labeled \"backward pass\" can be implemented using the backpropagation algorithm, which calculates the gradient of the error of the network regarding the network's modifiable weights. \n\nTo understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuitions about the relationship between the actual output of a neuron and the correct output for a particular training case. Consider a simple neural network with two input units, one output unit and no hidden units. Each neuron uses a linear output that is the weighted sum of its input. \n\nInitially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consists of a set of tuples formula_46 where formula_47 and formula_48 are the inputs to the network and is the correct output (the output the network should eventually produce given those inputs). The initial network, given formula_47 and formula_48, will compute an output that likely differs from (given random weights). A common method for measuring the discrepancy between the expected output and the actual output is the squared error measure:\n\nwhere is the discrepancy or error.\n\nAs an example, consider the network on a single training case: formula_52, thus the input formula_47 and formula_48 are 1 and 1 respectively and the correct output, is 0. Now if the actual output is plotted on the horizontal axis against the error on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output which minimizes the error . For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output that exactly matches the expected output . Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error. \n\nHowever, the output of a neuron depends on the weighted sum of all its inputs:\n\nwhere formula_38 and formula_57 are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning. If each weight is plotted on a separate horizontal axis and the error on the vertical axis, the result is a parabolic bowl. For a neuron with weights, the same plot would require an elliptic paraboloid of formula_58 dimensions.\n\nOne commonly used algorithm to find the set of weights that minimizes the error is gradient descent. Backpropagation is then used to calculate the steepest descent direction.\n\nThe gradient descent method involves calculating the derivative of the squared error function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron, the squared error function is:\n\nwhere\n\nThe factor of formula_63 is included to cancel the exponent when differentiating. Later, the expression will be multiplied with an arbitrary learning rate, so that it doesn't matter if a constant coefficient is introduced now.\n\nFor each neuron formula_64, its output formula_65 is defined as\n\nThe input formula_67 to a neuron is the weighted sum of outputs formula_68 of previous neurons. If the neuron is in the first layer after the input layer, the formula_68 of the input layer are simply the inputs formula_70 to the network. The number of input units to the neuron is formula_18. The variable formula_72 denotes the weight between neurons formula_73 and formula_64.\n\nThe activation function formula_75 is non-linear and differentiable. A commonly used activation function is the logistic function:\n\nwhich has a convenient derivative of:\n\nCalculating the partial derivative of the error with respect to a weight formula_78 is done using the chain rule twice:\n\nIn the last factor of the right-hand side of the above, only one term in the sum formula_67 depends on formula_78, so that \n\nIf the neuron is in the first layer after the input layer, formula_83 is just formula_84.\n\nThe derivative of the output of neuron formula_64 with respect to its input is simply the partial derivative of the activation function (assuming here that the logistic function is used):\n\nThis is the reason why backpropagation requires the activation function to be differentiable.\n\nThe first factor is straightforward to evaluate if the neuron is in the output layer, because then formula_87 and\n\nHowever, if formula_64 is in an arbitrary inner layer of the network, finding the derivative formula_60 with respect to formula_65 is less obvious.\n\nConsidering formula_60 as a function of the inputs of all neurons formula_93 receiving input from neuron formula_64, \n\nand taking the total derivative with respect to formula_65, a recursive expression for the derivative is obtained:\n\nTherefore, the derivative with respect to formula_65 can be calculated if all the derivatives with respect to the outputs formula_99 of the next layer – the one closer to the output neuron – are known.\n\nPutting it all together:\n\nwith\n\nTo update the weight formula_78 using gradient descent, one must choose a learning rate, formula_103. The change in weight, which is added to the old weight, is equal to the product of the learning rate and the gradient, multiplied by formula_104:\n\nThe formula_106 is required in order to update in the direction of a minimum, not a maximum, of the error function.\n\nFor a single-layer network, this expression becomes the Delta Rule.\n\nThe choice of learning rate formula_107 is important, since a high value can cause too strong a change, causing the minimum to be missed, while a too low learning rate slows the training unnecessarily.\n\nOptimizations such as Quickprop are primarily aimed at speeding up error minimization; other improvements mainly try to increase reliability.\n\nIn order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements of this algorithm use an adaptive learning rate.\n\nBy using a variable inertia term \"(Momentum)\" formula_108 the gradient and the last change can be weighted such that the weight adjustment additionally depends on the previous change. If the \"Momentum\" formula_108 is equal to 0, the change depends solely on the gradient, while a value of 1 will only depend on the last change.\n\nSimilar to a ball rolling down a mountain, whose current speed is determined not only by the current slope of the mountain but also by its own inertia, inertia can be added:formula_110where:\n\nInertia depends on the current weight change formula_126 both from the current gradient of the error function (slope of the mountain, 1st summand), as well as from the weight change from the previous point in time (inertia, 2nd summand).\n\nWith inertia, the problems of getting stuck (in steep ravines and flat plateaus) are avoided. Since, for example, the gradient of the error function becomes very small in flat plateaus, inertia would immediately lead to a \"deceleration\" of the gradient descent. This deceleration is delayed by the addition of the inertia term so that a flat plateau can be escaped more quickly.\n\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the gradient descent process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in a local minima. However, batch learning typically yields a faster, more stable descent to a local minima, since each update is performed in the direction of the average error of the batch. A common compromise choice is to use \"mini-batches\", meaning small batches and with samples in each batch selected stochastically from the entire data set.\n\n\nAccording to various sources, the basics of continuous backpropagation were derived in the context of control theory by Henry J. Kelley in 1960 and by Arthur E. Bryson in 1961. They used principles of dynamic programming. In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969.\n\nIn 1970 Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to backpropagation, which is efficient even for sparse networks.\n\nIn 1973 Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1974 Werbos mentioned the possibility of applying this principle to artificial neural networks, and in 1982 he applied Linnainmaa's AD method to neural networks in the way that is used today.\n\nIn 1986 Rumelhart, Hinton and Williams showed experimentally that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through backpropagation.\n\nDuring the 2000s it fell out of favour, but returned in the 2010s, benefitting from cheap, powerful GPU-based computing systems.\n\n\n",
    "id": "1360091",
    "title": "Backpropagation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27569062",
    "text": "Backpropagation through time\n\nBackpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers\n\nThe training data for a recurrent neural network is an ordered sequence of formula_1 input-output pairs, formula_2. An initial value must be specified for the hidden state formula_3. Typically, a vector of all zeros is used for this purpose.\n\nBPTT begins by unfolding a recurrent neural network in time. The unfolded network contains formula_1 inputs and outputs, but every copy of the network shares the same parameters. Then the backpropagation algorithm is used to find the gradient of the cost with respect to all the network parameters.\n\nConsider an example of a neural network that contains a recurrent layer formula_5 and a feedforward layer formula_6. There are different ways to define the training cost, but the total cost is always the average of the costs of each of the time steps. The cost of each time step can be computated separately. The figure above shows how the cost at time formula_7 can be computed, by unfolding the recurrent layer formula_5 for three time steps and adding the feedforward layer formula_6. Each instance of formula_5 in the unfolded network shares the same parameters. Thus the weight updates in each instance (formula_11) are summed together.\n\nPseudo-code for a truncated version of BPTT, where the training data contains formula_12 input-output pairs, but the network is unfolded for formula_1 time steps:\n\nBPTT tends to be significantly faster for training recurrent neural networks than general-purpose optimization techniques such as evolutionary optimization.\n\nBPTT has difficulty with local optima. With recurrent neural networks, local optima are a much more significant problem than with feed-forward neural networks. The recurrent feedback in such networks tends to create chaotic responses in the error surface which cause local optima to occur frequently, and in poor locations on the error surface.\n\n",
    "id": "27569062",
    "title": "Backpropagation through time"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33025196",
    "text": "Bcpnn\n\nA Bayesian Confidence Neural Network (BCPNN) is an artificial neural network inspired by Bayes' theorem: node activations represent probability (\"confidence\") in the presence of input features or categories, synaptic weights are based on estimated correlations and the spread of activation corresponds to calculating posteriori probabilities. It was originally proposed by Anders Lansner and Örjan Ekeberg at KTH.\n\nThe basic network is a feedforward neural network with continuous activation. This can be extended to include spiking units and hypercolumns, representing mutually exclusive or interval coded features. This network has been used for classification tasks and data mining, for example for discovery of adverse drug reactions. The units can also be connected as a recurrent neural network (losing the strict interpretation of their activations as probabilities) but becoming a possible abstract model of biological neural networks and memory.\n",
    "id": "33025196",
    "title": "Bcpnn"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=17290783",
    "text": "Bidirectional associative memory\n\nBidirectional associative memory (BAM) is a type of recurrent neural network. BAM was introduced by Bart Kosko in 1988. There are two types of associative memory, auto-associative and hetero-associative. BAM is hetero-associative, meaning given a pattern it can return another pattern which is potentially of a different size. It is similar to the Hopfield network in that they are both forms of associative memory. However, Hopfield nets return patterns of the same size.\n\nA BAM contains two layers of neurons, which we shall denote X and Y. Layers X and Y are fully connected to each other. Once the weights have been established, input into layer X presents the pattern in layer Y, and vice versa.\n\nImagine we wish to store two associations, A1:B1 and A2:B2.\n\nThese are then transformed into the bipolar forms:\n\nFrom there, we calculate formula_1 where formula_2 denotes the transpose.\nSo,\n\nformula_3\n\nTo retrieve the association A1, we multiply it by M to get (4, 2, -2, -4), which, when run through a threshold, yields (1, 1, 0, 0), which is B1.\nTo find the reverse association, multiply this by the transpose of M.\n\nThe internal matrix has n x p independent degrees of freedom, where n is the dimension of the first vector (6 in this example) and p is the dimension of the second vector (4). This allows the BAM to be able to reliably store and recall a total of up to min(n,p) independent vector pairs, or min(6,4) = 4 in this example. The capacity can be increased above 4 if one gives up reliability and is willing to accept incorrect bits on the output.\n\n\n",
    "id": "17290783",
    "title": "Bidirectional associative memory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1166059",
    "text": "Boltzmann machine\n\nA Boltzmann machine (also called stochastic Hopfield network with hidden units) is a type of stochastic recurrent neural network (and Markov Random Field). \n\nBoltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They were one of the first neural networks capable of learning internal representations, and are able to represent and (given sufficient time) solve difficult combinatoric problems. \n\nThey are theoretically intriguing because of the locality and Hebbian nature of their training algorithm, and because of their parallelism and the resemblance of their dynamics to simple physical processes. Boltzmann machines with unconstrained connectivity have not proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems.\n\nThey are named after the Boltzmann distribution in statistical mechanics, which is used in their sampling function. They were invented in 1985 by Geoffrey Hinton, then a Professor at Carnegie Mellon University, and Terry Sejnowski, then a Professor at Johns Hopkins University.\n\nA Boltzmann machine, like a Hopfield network, is a network of units with an \"energy\" defined for the overall network. Its units produce results. Unlike Hopfield nets, Boltzmann machine units are stochastic. The global energy, formula_1, in a Boltzmann machine is identical in form to that of a Hopfield network:\n\nWhere:\n\nOften the weights are represented as a symmetric matrix formula_12, with zeros along the diagonal.\n\nThe difference in the global energy that results from a single unit formula_5 equaling 0 (off) versus 1 (on), written formula_14, assuming a symmetric matrix of weights, is given by:\n\nSubstituting the energy of each state with its relative probability according to the Boltzmann Factor (the property of a Boltzmann distribution that the energy of a state is proportional to the negative log probability of that state) gives:\n\nwhere formula_17 is Boltzmann's constant and is absorbed into the artificial notion of temperature formula_18. We then rearrange terms and consider that the probabilities of the unit being on and off must sum to one:\n\nSolving for formula_25, the probability that the formula_5-th unit is on gives:\n\nwhere the scalar formula_18 is referred to as the temperature of the system. This relation is the source of the logistic function found in probability expressions in variants of the Boltzmann machine.\n\nThe network runs by repeatedly choosing a unit and resetting its state. After running for long enough at a certain temperature, the probability of a global state of the network depends only upon that global state's energy, according to a Boltzmann distribution, and not on the initial state from which the process was started. This means that log-probabilities of global states become linear in their energies. This relationship is true when the machine is \"at thermal equilibrium\", meaning that the probability distribution of global states has converged. Running the network beginning from a high temperature, its temperature gradually decreases until reaching a thermal equilibrium at a lower temperature. It then may converge to a distribution where the energy level fluctuates around the global minimum. This process is called simulated annealing.\n\nTo train the network so that the chance it will converge to a global state is according to an external distribution over these states, the weights must be set so that the global states with the highest probabilities get the lowest energies. This is done by training.\n\nThe units in the Boltzmann Machine are divided into 'visible' units, V, and 'hidden' units, H. The visible units are those that receive information from the 'environment', i.e. the training set is a set of binary vectors over the set V. The distribution over the training set is denoted formula_29. \nAs is discussed above, the distribution over global states converges as the Boltzmann machine reaches thermal equilibrium. We denote this distribution, after we marginalize it over the hidden units, as formula_30.\n\nOur goal is to approximate the \"real\" distribution formula_29 using the formula_30 produced (eventually) by the machine. To measure how similar the two distributions are, the Kullback–Leibler divergence, formula_33 is\n\nwhere the sum is over all the possible states of formula_35. formula_33 is a function of the weights, since they determine the energy of a state, and the energy determines formula_37, as promised by the Boltzmann distribution. Hence, we can use a gradient descent algorithm over formula_33, so a given weight, formula_3 is changed by subtracting the partial derivative of formula_33 with respect to the weight.\n\nThere are two alternating phases to Boltzmann machine training. One is the \"positive\" phase where the visible units' states are clamped to a particular binary state vector sampled from the training set (according to formula_41). The other is the \"negative\" phase where the network is allowed to run freely, i.e. no units have their state determined by external data. Surprisingly enough, the gradient with respect to a given weight, formula_3, is given by the simple proven equation:\n\nwhere:\n\nThis result follows from the fact that at thermal equilibrium the probability formula_47 of any global state formula_48 when the network is free-running is given by the Boltzmann distribution (hence the name \"Boltzmann machine\").\n\nRemarkably, this learning rule is fairly biologically plausible because the only information needed to change the weights is provided by \"local\" information. That is, the connection (or synapse biologically speaking) does not need information about anything other than the two neurons it connects. This is far more biologically realistic than the information needed by a connection in many other neural network training algorithms, such as backpropagation.\n\nThe training of a Boltzmann machine does not use the EM algorithm, which is heavily used in machine learning. \nBy minimizing the KL-divergence, it is equivalent to maximizing the log-likelihood of the data. Therefore, the training procedure performs gradient ascent on the log-likelihood of the observed data. This is in contrast to the EM algorithm, where the posterior distribution of the hidden nodes must be calculated before the maximization of the expected value of the complete data likelihood during the M-step.\n\nTraining the biases is similar, but uses only single node activity:\n\nThe Boltzmann machine would theoretically be a rather general computational medium. For instance, if trained on photographs, the machine would theoretically model the distribution of photographs, and could use that model to, for example, complete a partial photograph.\n\nUnfortunately, there is a serious practical problem with the Boltzmann machine, namely that it seems to stop learning correctly when the machine is scaled up to anything larger than a trivial machine. This is due to a number of effects, the most important of which are:\n\n\nAlthough learning is impractical in general Boltzmann machines, it can be made quite efficient in \nan architecture called the \"restricted Boltzmann machine\" or \"RBM\" which does not allow intralayer connections between hidden units. After training one RBM, the activities of its hidden units can be treated as data for training a higher-level RBM. This method of stacking RBMs makes it possible to train many layers of hidden units efficiently and is one of the most common deep learning strategies. As each new layer is added the overall generative model gets better.\n\nThere is an extension to the restricted Boltzmann machine that affords using real valued data rather than binary data. Along with higher order Boltzmann machines, it is outlined here .\n\nOne example of a practical application of Restricted Boltzmann machines is the performance improvement of speech recognition software.\n\nA deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of visible units formula_50 and layers of hidden units formula_51. No connection links units of the same layer (like RBM). For the , the probability assigned to vector is\nwhere formula_53 are the set of hidden units, and formula_54 are the model parameters, representing visible-hidden and hidden-hidden interactions. Only the top two layers form a restricted Boltzmann machine (which is an undirected graphical model), while lower layers form a directed generative model.\n\nLike DBNs, DBMs can learn complex and abstract internal representations of the input in tasks such as object or speech recognition, using limited, labeled data to fine-tune the representations built using a large supply of unlabeled sensory input data. However, unlike and deep convolutional neural networks, they adopt the inference and training procedure in both directions, bottom-up and top-down pass, which allow the to better unveil the representations of the input structures.\n\nHowever, the slow speed of DBMs limits their performance and functionality. Because exact maximum likelihood learning is intractable for DBMs, only approximate maximum likelihood learning is possible. Another option is to use mean-field inference to estimate data-dependent expectations and approximate the expected sufficient statistics by using \"Markov chain Monte Carlo\" \"(MCMC)\". This approximate inference, which must be done for each test input, is about 25 to 50 times slower than a single bottom-up pass in DBMs. This makes joint optimization impractical for large data sets, and restricts the use of DBMs for tasks such as feature representation.\n\nThe Boltzmann machine is a Monte Carlo version of the Hopfield network.\n\nThe idea of using stochastic weights in Ising models considered by:\n\nHowever, in cognitive sciences, it is often thought to have been first described by:\n\nHowever, it should be noted that these articles appeared after the seminal publication by John Hopfield, where the connection to physics and statistical mechanics was made in the first place, mentioning spin glasses:\n\n\nThe idea of applying the Ising model with annealed Gibbs sampling is also present in Douglas Hofstadter's Copycat project:\n\n\nSimilar ideas (with a change of sign in the energy function) are also found in Paul Smolensky's \"Harmony Theory\".\n\nThe explicit analogy drawn with statistical mechanics in the Boltzmann Machine formulation led to the use of terminology borrowed from physics (e.g., \"energy\" rather than \"harmony\"), which has become standard in the field. The widespread adoption of this terminology may have been encouraged by the fact that its use led to the importation of a variety of concepts and methods from statistical mechanics.\nHowever, there is no reason to think that the various proposals to use simulated annealing for inference described above were not independent.\n\nIsing models are now considered to be a special case of Markov random fields, which find widespread application in various fields, including linguistics, robotics, computer vision, and artificial intelligence.\n\n\n\n",
    "id": "1166059",
    "title": "Boltzmann machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2506529",
    "text": "Cellular neural network\n\nIn computer science and machine learning, cellular neural networks (CNN) (or cellular nonlinear networks (CNN)) are a parallel computing paradigm similar to neural networks, with the difference that communication is allowed between neighbouring units only. Typical applications include image processing, analyzing 3D surfaces, solving partial differential equations, reducing non-visual problems to geometric maps, modelling biological vision and other sensory-motor organs.\n\nDue to their number and variety of architectures, it is difficult to give a precise definition for a CNN processor. From an architecture standpoint, CNN processors are a system of a finite, fixed-number, fixed-location, fixed-topology, locally interconnected, multiple-input, single-output, nonlinear processing units. The nonlinear processing units are often referred to as neurons or cells. Mathematically, each cell can be modeled as a dissipative, nonlinear dynamical system where information is encoded via its initial state, inputs and variables used to define its behavior. Dynamics are usually continuous, as in the case of Continuous-Time CNN (CT-CNN) processors, but can be discrete, as in the case of Discrete-Time CNN (DT-CNN) processors. Each cell has one output, by which it communicates its state with both other cells and external devices. Output is typically real-valued, but can be complex or even quaternion, i.e. a Multi-Valued CNN (MV-CNN). In most CNN processors, processing units are identical, but there are applications that require non-identical units, which are called Non-Uniform Processor CNN (NUP-CNN) processors, and consist of different types of cells. In the original Chua-Yang CNN (CY-CNN) processor, the state of the cell was a weighted sum of the inputs and the output was a piecewise linear function. However, like the original perceptron-based neural networks, the functions it could perform were limited: specifically, it was incapable of modeling non-linear functions, such as XOR. More complex functions are realizable via Non-Linear CNN (NL-CNN) processors.\n\nCells are defined in a normed space, commonly a two-dimensional Euclidean geometry, like a grid. The cells are not limited to two-dimensional spaces however; they can be defined in an arbitrary number of dimensions and can be square, triangle, hexagonal, or any other spatially invariant arrangement. Topologically, cells can be arranged on an infinite plane or on a toroidal space. Cell interconnect is local, meaning that all connections between cells are within a specified radius (with distance measured topologically). Connections can also be time-delayed to allow for processing in the temporal domain.\n\nMost CNN architectures have cells with the same relative interconnects, but there are applications that require a spatially variant topology, i.e. Multiple-Neighborhood-Size CNN (MNS-CNN) processors. Also, Multiple-Layer CNN (ML-CNN) processors, where all cells on the same layer are identical, can be used to extend the capability of CNN processors.\n\nThe definition of a system is a collection of independent, interacting entities forming an integrated whole, whose behavior is distinct and qualitatively greater than its entities. Although connections are local, information exchange can happen globally through diffusion. In this sense, CNN processors are systems because their dynamics are derived from the interaction between the processing units and not within processing units. As a result, they exhibit emergent and collective behavior. Mathematically, the relationship between a cell and its neighbors, located within an area of influence, can be defined by a coupling law, and this is what primarily determines the behavior of the processor. When the coupling laws are modeled by fuzzy logic, it is a fuzzy CNN\n. When these laws are modeled by computational verb logic, it becomes a computational verb CNN (verb CNN)\n. Both fuzzy and verb CNNs are useful for modelling social networks when the local couplings are achieved by linguistic terms.\n\nThe idea of CNN processors was introduced by Leon Chua and Lin Yang’s two-part, 1988 article, \"Cellular Neural Networks: Theory\" and \"Cellular Neural Networks: Applications\" in IEEE Transactions on Circuits and Systems. In these articles, Chua and Yang outline the underlying mathematics behind CNN processors. They use this mathematical model to demonstrate, for a specific CNN implementation, that if the inputs are static, the processing units will converge, and can be used to perform useful calculations. They then suggest one of the first applications of CNN processors: image processing and pattern recognition (which is still the largest application to date). Leon Chua is still active in CNN research and publishes many of his articles in the International Journal of Bifurcation and Chaos, of which he is an editor. Both IEEE Transactions on Circuits and Systems and the International Journal of Bifurcation also contain a variety of useful articles on CNN processors authored by other knowledgeable researchers. The former tends to focus on new CNN architectures and the latter more on the dynamical aspects of CNN processors.\n\nAnother key article, Tamas Roska and Leon Chua’s 1993 article \"The CNN Universal Machine: An Analogic Array Computer\", introduced the first algorithmically programmable analog CNN processor to the engineering research community. The multi-national effort was funded by the Office of Naval Research, the National Science Foundation, and the Hungarian Academy of Sciences, and researched by the Hungarian Academy of Sciences and the University of California. This article proved that CNN processors were producible and provided researchers a physical platform to test their CNN theories. After this article, companies started to invest into larger, more capable processors, based on the same basic architecture as the CNN Universal Processor. Tamas Roska is another key contributor to CNNs. His name is often associated with biologically inspired information processing platforms and algorithms, and he has published numerous key articles and has been involved with companies and research institutions developing CNN technology.\n\nThere are several overviews of CNN processors in published literature. One of the better references is a paper, \"Cellular Neural Networks: A Review\" written for Neural Nets WIRN Vietri 1993, by Valerio Cimagalli and Marco Balsi. The paper provides definitions, CNN types, dynamics, implementations, and applications in a relatively small, readable document. There is also a book, \"Cellular Neural Networks and Visual Computing Foundations and Applications\", written by Leon Chua and Tamas Roska, which provides examples and exercises to help illustrates points in a manner uncommon for papers and journal articles. The book covers many different aspects of CNN processors and can serve as a textbook for a Masters or Ph.D. course. The two references are considered invaluable since they manage to organize the vast amount of CNN literature into a coherent framework.\n\nThe best place for CNN literature is from the proceedings of \"The International Workshop on Cellular Neural Networks and Their Applications\". The proceedings are available online, via IEEE Xplore, for conferences held in 1990, 1992, 1994, 1996, 1998, 2000, 2002, 2005 and 2006. There is also a workshop being held on July 14–16 in Santiago de Composetela, Spain. Topics include theory, design, applications, algorithms, physical implementations and programming/training methods. For an understanding of the analog semiconductor based CNN technology, AnaLogic Computers has their product line, in addition to the published articles available on their homepage and their publication list. They also have information on other CNN technologies such as optical computing. Many of the commonly used functions have already been implemented using CNN processors. A good reference point for some of these can be found in image processing libraries for CNN based visual computers such as Analogic’s CNN-based systems.\n\nCNN processors could be thought of as a hybrid between ANN and CA (Continuous Automata). The processing units of CNN and NN are similar. In both cases, the processor units are multi-input, dynamical systems, and the behavior of the overall systems is driven primarily through the weights of the processing unit’s linear interconnect. The main discriminator is that in CNN processors, connections are made locally, whereas in ANN, connections are global. For example, neurons in one layer are fully connected to another layer in a feed-forward NN and all the neurons are fully interconnected in Hopfield networks. In ANNs, the weights of interconnections contain information on the processing system’s previous state or feedback, but in CNN processors, the weights are used to determine the dynamics of the system. Furthermore, due to the high interconnectivity of ANNs, they tend not exploit locality in either the data set or the processing and as a result, they usually are highly redundant systems that allow for robust, fault-tolerant behavior without catastrophic errors. A cross between an ANN and a CNN processor is a Ratio Memory CNN (RMCNN). In RMCNN processors, the cell interconnect is local and topologically invariant, but the weights are used to store previous states and not to control dynamics. The weights of the cells are modified during some learning state creating long-term memory.\n\nThe topology and dynamics of CNN processors closely resembles that of CA. Like most CNN processors, CA consists of a fixed-number of identical processors that are spatially discrete and topologically uniform. The difference is that most CNN processors are continuous-valued whereas CA have discrete-values. Furthermore, the CNN processor's cell behavior is defined via some non-linear function whereas CA processor cells are defined by some state machine. However, there are some exceptions. Continuous Valued Cellular Automata or Continuous Automata are CA with continuous resolution. Depending on how a given Continuous Automata is specified, it can also be a CNN. There are also Continuous Spatial Automata, which consist of an infinite number of spatially continuous, continuous-valued automata. There is considerable work being performed in this field since continuous spaces are easier to mathematically model than discrete spaces, thus allowing a more quantitative approach as opposed to an empirical approach taken by some researchers of cellular automata. Continuous Spatial Automata processors can be physically realized though an unconventional information processing platform such as a chemical computer. Furthermore, it is conceivable that large CNN processors (in terms of the resolution of the input and output) can be modeled as a Continuous Spatial Automata.\n\nThe dynamical behaviors of CNN processors can be expressed mathematically as a series of ordinary differential equations, where each equation represents the state of an individual processing unit. The behavior of the entire CNN processor is defined by its initial conditions, the inputs, the cell interconnect (topology and weights), and the cells themselves. One possible use of CNN processors is to generate and respond to signals of specific dynamical properties. For example, CNN processors have been used to generate multi-scroll chaos, synchronize with chaotic systems, and exhibit multi-level hysterisis. CNN processors are designed specifically to solve local, low-level, processor intensive problems expressed as a function of space and time. For example, CNN processors can be used to implement high-pass and low-pass filters and morphological operators. They can also be used to approximate a wide range of Partial Differential Equations (PDE) such as heat dissipation and wave propagation.\n\nCNN processors can be used as Reaction-Diffusion (RD) processors. RD processors are spatially invariant, topologically invariant, analog, parallel processors characterized by reactions, where two agents can combine to create a third agent, and diffusions, the spreading of agents. RD processors are typically implemented through chemicals in a Petri dish (processor), light (input), and a camera (output) however RD processors can also be implemented through a multilayer CNN processor. RD processors can be used to create Voronoi diagrams and perform skeletonisation. The main difference between the chemical implementation and the CNN implementation is that CNN implementations are considerably faster than their chemical counterparts and chemical processors are spatially continuous whereas the CNN processors are spatially discrete. The most researched RD processor, Belousov-Zhabotinsky (BZ) processors, has already been simulated using a four-layer CNN processors and has been implemented in a semiconductor.\n\nLike CA, computations can be performed through the generation and propagation of signals that either grow or change over time. Computations can occur within a signal or can occur through the interaction between signals. One type of processing, which uses signals and is gaining momentum is wave processing, which involves the generation, expanding, and eventual collision of waves. Wave processing can be used to measure distances and find optimal paths. Computations can also occur through particles, gliders, solutions, and filterons localized structures that maintain their shape and velocity. Given how these structures interact/collide with each other and with static signals, they can be used to store information as states and implement different Boolean functions. Computations can also occur between complex, potentially growing or evolving localized behavior through worms, ladders, and pixel-snakes. In addition to storing states and performing Boolean functions, these structures can interact, create, and destroy static structures.\n\nAlthough CNN processors are primarily intended for analog calculations, certain types of CNN processors can implement any Boolean function, allowing simulating CA. Since some CA are Universal Turing machines (UTM), capable of simulating any algorithm can be performed on processors based on the von Neumann architecture, that makes this type of CNN processors, universal CNN, a UTM. One CNN architecture consists of an additional layer, similar to the ANN solution to the problem stated by Marvin Minsky years ago. CNN processors have resulted in the simplest realization of Conway’s Game of Life and Wolfram’s Rule 110, the simplest known universal Turing Machine. This unique, dynamical representation of an old systems, allows researchers to apply techniques and hardware developed for CNN to better understand important CA. Furthermore, the continuous state space of CNN processors, with slight modifications that have no equivalent in Cellular Automata, creates emergent behavior never seen before.\n\nAny information processing platform that allows the construction of arbitrary Boolean functions is called universal, and as result, this class CNN processors are commonly referred to as universal CNN processors. The original CNN processors can only perform linearly separable Boolean functions. This is essentially the same problem Marvin Minsky introduced with respect to the perceptions of the first neural networks In either case, by translating functions from digital logic or look-up table domains into the CNN domain, some functions can be considerably simplified. For example, the nine-bit, odd parity generation logic, which is typically implemented by eight nested exclusive-or gates, can also be represented by a sum function and four nested absolute value functions. Not only is there a reduction in the function complexity, but the CNN implementation parameters can be represented in the continuous, real-number domain.\n\nThere are two methods by which to select a CNN processor along with a template or weights. The first is by synthesis, which involves determine the coefficients offline. This can be done by leveraging off previous work, i.e. libraries, papers, and articles, or by mathematically deriving co that best suits the problem. The other is through training the processor. Researchers have used back-propagation and genetic algorithms to learn and perform functions. Back-propagation algorithms tend to be faster, but genetic algorithms are useful because they provide a mechanism to find a solution in a discontinuous, noisy search space.\n\nAn information processing platform remains nothing more than an intellectual exercise unless it can be implemented in hardware and integrated into a system. Although processors based on billiard balls can be interesting, unless their implementation provides advantages for a system, the only purpose they serve is as a teaching device. CNN processors have been implemented using current technology and there are plans to implement CNN processors into future technologies. They include the necessary interfaces for programming and interfacing, and have been implemented in a variety of systems. What follows is a cursory examination of the different types of CNN processors available today, their advantages and disadvantages, and the future roadmap for CNN processors.\n\nCNN processors have been implemented and are currently available as semiconductors and there are plans to migrate CNN processors to emerging technologies in the future. Semiconductor-based CNN processors can be segmented into analog CNN processors, digital CNN processors, and CNN processors emulated using digital processors. Analog CNN processors were the first to be developed. Analog computers were fairly common during the 1950 and 1960s, but they gradually were replaced by digital computers the 1970s. Analog processors were considerably faster in certain applications such as optimizing differential equations and modeling nonlinearities, but the reason why analog computing lost favor was the lack of precision and the difficulty to configure an analog computer to solve a complex equation. Analog CNN processors share some of the same advantages as their predecessors, specifically speed. The first analog CNN processors were able to perform real-time ultra-high frame-rate (>10,000 frame/s) processing unachievable by digital processors. The analog implementation of CNN processors requires less area and consumes less power than their digital counterparts. Although the accuracy of analog CNN processors does not compare to their digital counterparts, for many applications, noise and process variances are small enough not to perceptually affect the image quality.\n\nThe first algorithmically programmable, analog CNN processor was created in 1993. It was named the CNN Universal Processor because its internal controller allowed multiple templates to be performed on the same data set, thus simulating multiple layers and allowing for universal computation. Included in the design was a single layer 8x8 CCN, interfaces, analog memory, switching logic, and software. The processor was developed in order to determine CNN processor producibility and utility. The CNN concept proved promising and by 2000, there were at least six organizations designing algorithmically programmable, analog CNN processors. This is when AnaFocus, a mixed-signal semiconductor company that emerged from research at The University of Seville, introduced their ACE prototype CNN processor product line. Their first ACE processor contained 20x20 B/W processor units; their next ACE processor provided 48x48 grayscale processor units, and their latest ACE processor contains 128x128 grayscale processor units. Over time, not only did the number of processing elements increase, but their speed improved, the number of functions they can perform increased, and a seamless detector interface was integrated into the silicon (yielding considerably improved interface). The ability to embed the detector interface into the CNN processor allows for real-time interaction between the sensing and processing. AnaFocus has a multilayer CASE prototype CNN processors line. The latest CASE processor is a three layer 32x32 CNN processor. Their work in CNN processors is currently culminating in their soon-to-be-released, commercially available Eye-RIS product line that consists of all the processors, co-processors, software development kits, and support needed to program and integrate an analog processor into a system.\n\nAnaFocus is working with AnaLogic Computers, to include their CNN processors into visual systems. Founded in 2000, by many of the same researchers behind the first algorithmically programmable CNN Universal Processor, AnaLogic Computers mission is to commercialize high-speed, biologically inspired systems based on CNN processors. In 2003, AnaLogic Computers developed a PCI-X visual processor board that included the ACE 4K processor, with a Texas Instrument DIP module and a high-speed frame-grabber. This allowed CNN processing to be easily included in a desktop computer, considerably improving the usability and capability of CNN analog processors. In 2006, AnaLogic Computers developed their Bi-I Ultra High Speed Smart Camera product line, which includes the ACE 4K processor in their high-end models. The product that their development team is now pursuing is the Bionic Eyeglass. The Bionic Eyeglass is a dual-camera, wearable platform, based on the Bi-I Ultra High Speed Smart Camera, designed to provide assistance to blind people. Some of the functions that the Bionic Eyeglass system will perform is route number recognition and color processing.\n\nSome researchers are developed their own custom analog CNN processors. For example, an analog CNN processor was developed from a research team from University degli Studi di Catania, in order to generate gaits for a hexapod robot. Researchers from National Chiao Tung University designed a RM-CNN processor to learn more about pattern learning & recognition and researchers from the National Lien-Ho Institute of Technology developed a Min-Max CNN (MMCNN) processor to learn more about CNN dynamics. Given the diversity of CNN processors and the momentum that CNN research has gained, it is plausible that such analog CNN development efforts will be fairly common in the near future.\n\nDespite their speed and low power consumption, there are some significant drawbacks to analog CNN processors. First, analog CNN processors can potentially create erroneous results due to environment and process variation. In most applications, these errors are not noticeable, but there are situations where minor deviations can result in catastrophic system failures. For example, in chaotic communication, process variation will change the trajectory of a given system in phase space, resulting in a lost of synchronicity/stability. Due to the severity of the problem, there is considerable research being performed to ameliorate the problem. Some researchers are optimizing templates to accommodate greater variation. Other researchers are improving the semiconductor process to more closely match theoretical CNN performance. Other researchers are investigating different, potentially more robust CNN architectures. Lastly, researchers are developing methods to tune templates to target a specific chip and operating conditions. In other words, the templates are being optimized to match the information processing platform. Not only does process variation limit what can be done with current analog CNN processors, it is also a barrier for creating more complex processing units. Unless this process variation is resolved, ideas such as nested processing units, non-linear inputs, etc. cannot be implemented in a real-time analog CNN processor. Also, the semiconductor \"real estate\" for processing units limits the size of CNN processors. Currently the largest AnaVision CNN-based vision processor consists of a 4K detector, which is significantly less than the megapixel detectors found in affordable, consumer cameras. Unfortunately, feature size reductions, as predicted by Moore’s Law, will only result in minor improvements. For this reason, alternate technologies such as Resonant Tunneling Diodes and Neuron-Bipolar Junction Transistors are being explored. Also, the architecture of CNN processors is being reevaluated. For example, Star-CNN processors, where one analog multiplier is time-shared between multiple processor units, have been proposed and are expected to result in processor unit reduction size of eighty percent.\n\nAlthough not nearly as fast and energy efficient, digital CNN processors do not share the problems of process variation and feature size of their analog counterparts. This allows digital CNN processors to include nested processor units, non-linearities, etc. In addition, digital CNN are more flexible, cost less and are easier to integrate. The most common implementation of digital CNN processors uses an FPGA. Eutecus, founded in 2002 and operating in Berkeley, provides intellectual property that can be synthesized into an Altera FPGA. Their digital 320x280, FPGA-based CNN processors run at 30 frame/s and there are plans to make a fast digital ASIC. Eustecus is a strategic partner of AnaLogic computers, and their FPGA designs can be found in several of AnaLogic’s products. Eutecus is also developing software libraries to perform tasks including but not limited to video analytics for the video security market, feature classification, multi-target tracking, signal and image processing and flow processing. Many of these routines are derived using CNN-like processing. For those wanting to perform CNN simulations for prototyping, low-speed applications, or research, there are several options. First, there are precise CNN emulation software packages like SCNN 2000. If the speed is prohibitive, there are mathematical techniques, such as Jacobi’s Iterative Method or Forward-Backward Recursions that can be used to derive the steady state solution of a CNN processor. Said techniques can be performed by any mathematics tool, e.g. Matlab. Lastly, digital CNN processors can be emulated on highly parallel, application-specific processors, such as graphics processors. Implementing neural networks using graphics processors is an area of exploration for the research community.\n\nResearchers are also perusing alternate technologies for CNN processors. Although current CNN processors circumvent some of the problems associated with their digital counterparts, they do share some of the same long-term problems common to all semiconductor-based processors. These include, but are not limited to, speed, reliability, power-consumption, etc. AnaLogic Computers, is developing optical CNN processors, which combine optics, lasers, and biological and holographic memories. What initially was technology exploration resulted in a 500x500 CNN processor able to perform 300 giga-operations per second. Another promising technology for CNN processors is nanotechnology. One nanotechnology concept being investigated is using single electron tunneling junctions, which can be made into single-electron or high-current transistors, to create McCulloch-Pitts CNN processing units. In summary, CNN processors have been implemented and provide value to their users. They have been able to effectively leverage the advantages and address some of the disadvantages associated with their underling technology, i.e. semiconductors. Researchers are also transitioning CNN processors into emerging technologies. Therefore, if the CNN architecture is suited for a specific information processing system, there are processors available for purchase (as there will be for the foreseeable future).\n\nThe philosophy, interests, and methodologies of CNN researchers are varied. Due to the potential of the CNN architecture, this platform has attracted people from a variety of backgrounds and disciplines. Some are exploring practical implementations of CNN processors, others are using CNN processors to model physical phenomena, and there are even researchers exploring theoretical mathematical, computational, and philosophical ideas through CNN processors. Some applications are engineering related, where some known, understood behavior of CNN processors is exploited to perform a specific task, and some are scientific, where CNN processors are used to explore new and different phenomenon. CNN processors are versatile platforms that are being used for a variety of applications.\n\nCNN processors were designed to perform image processing; specifically, the original application of CNN processors was to perform real-time ultra-high frame-rate (>10,000 frame/s) processing unachievable by digital processors needed for applications like particle detection in jet engine fluids and spark-plug detection. Currently, CNN processors can achieve up to 50,000 frames per second, and for certain applications such as missile tracking, flash detection, and spark-plug diagnostics these microprocessors have outperformed a conventional supercomputer. CNN processors lend themselves to local, low-level, processor intensive operations and have been used in feature extraction, level and gain adjustments, color constancy detection, contrast enhancement, deconvolution, image compression, motion estimation, image encoding, image decoding, image segmentation, orientation preference maps, pattern learning/recognition, multi-target tracking, image stabilization, resolution enhancement, image deformations and mapping, image inpainting, optical flow, contouring, moving object detection, axis of symmetry detection, and image fusion.\n\nDue to their processing capabilities and flexibility, CNN processors have been used & prototyped for novel field applications such as flame analysis for monitoring combustion at a waste incinerator, mine-detection using infrared imagery, calorimeter cluster peak for high energy physics, anomaly detection in potential field maps for geophysics, laser dot detection, metal inspection for detecting manufacturing defects, and seismic horizon picking. They have also been used to perform biometric functions such as fingerprint recognition, vein feature extraction, face tracking, and generating visual stimuli via emergent patterns to gauge perceptual resonances. CNN processors have been used for medical and biological research in performing automated nucleated cell counting for detecting hyperplasia, segment images into anatomically and pathologically meaningful regions, measure and quantify cardiac function, measure the timing of neurons, and detect brain abnormalities that would lead to seizures. One potential future application of CNN microprocessors is to combine them with DNA microarrays to allow for a near-real time DNA analysis of hundreds of thousands of different DNA sequences. Currently, the major bottleneck of DNA microarray analysis is the amount of time needed to process data in the form of images, and using a CNN microprocessor, researchers have reduced the amount of time needed to perform this calculation to 7ms.\n\nCNN processors have also been used to generate and analyze patterns and textures. One motivation was to use CNN processors to understand pattern generation in natural systems. They were used to generate Turing patterns in order to understand the situations in which they form, the different types of patterns which can emerge, and the presence of defects or asymmetries. Also, CNN processors were used to approximate pattern generation systems that create stationary fronts, spatio-temporal patterns oscillating in time, hysteresis, memory, and heterogeneity. Furthermore, pattern generation was used to aid high-performance image generation and compression via real-time generation of stochastic and coarse-grained biological patterns, texture boundary detection, and pattern and texture recognition and classification.\n\nThere is an ongoing effort to incorporate CNN processors into sensory-computing-actuating machines as part of the emerging field of Cellular Machines. The basic premise is to create an integrated system that uses CNN processors for the sensory signal-processing and potentially the decision-making and control. The reason is that CNN processors can provide a low power, small size, and eventually low-cost computing and actuating system suited for Cellular Machines. These Cellular Machines will eventually create a Sensor-Actuator Network (SAN), a type of Mobile Ad Hoc Networks (MANET) which can be used for military intelligence gathering, surveillance of inhospitable environments, maintenance of large areas, planetary exploration, etc.\n\nCNN processors have been proven versatile enough for some control functions. They have been used to optimize function via a genetic algorithm, to measure distances, to perform optimal path-finding in a complex, dynamic environment, and theoretically can be used to learn and associate complex stimuli. They have also been used to create antonymous gaits and low-level motors for robotic nematodes, spiders, and lamprey gaits using a Central Pattern Generator (CPG). They were able to function using only feedback from the environment, allowing for a robust, flexible, biologically inspired robot motor system. CNN-based systems were able to operate in different environments and still function if some of the processing units are disabled.\n\nThe variety of dynamical behavior seen in CNN processors make them intriguing for communication systems. Chaotic communications using CNN processors is being researched due to their potential low power consumption, robustness and spread spectrum features. The premise behind chaotic communication is to use a chaotic signal for the carrier wave and to use chaotic phase synchronization to reconstruct the original message. CNN processors can be used on both the transmitter and receiver end to encode and decode a given message. They can also be used for data encryption and decryption, source authentication through watermarking, detecting of complex patterns in spectrogram images (sound processing), and transient spectral signals detection.\n\nCNN processors are neuromorphic processors, meaning that they emulate certain aspects of biological neural networks. The original CNN processors were based on mammalian retinas, which consist of a layer of photodetectors connected to several layers of locally coupled neurons. This makes CNN processors part of an interdisciplinary research area whose goal is to design systems that leverage knowledge and ideas from neuroscience and contribute back via real-world validation of theories. CNN processors have implemented a real-time system that replicates mammalian retinas, validating that the original CNN architecture chosen modeled the correct aspects of the biological neural networks used to perform the task in mammalian life. However, CNN processors are not limited to verifying biological neural networks associated with vision processing; they have been used to simulate dynamic activity seen in mammalian neural networks found in the olfactory bulb and locust antennal lobe, responsible for pre-processing sensory information to detect differences in repeating patterns.\n\nCNN processors are being used to understand systems that can be modeled using simple, coupled units, such as living cells, biological networks, physiological systems, and ecosystems. The CNN architecture captures some of the dynamics often seen in nature and is simple enough to analyze and conduct experiments. They are also being used for stochastic simulation techniques, which allow scientists to explore spin problems, population dynamics, lattice-based gas models, percolation, and other phenomena. Other simulation applications include heat transfer, mechanical vibrating systems, protein production, Josephson Transmission Line (JTL) problems, seismic wave propagation, and geothermal structures. Instances of 3D (Three Dimensional) CNN have been used to prove known complex shapes are emergent phenomena in complex systems, establishing a link between art, dynamical systems and VLSI technology. CNN processors have been used to research a variety of mathematical concepts, such as researching non-equilibrium systems, constructing non-linear systems of arbitrary complexity using a collection of simple, well-understood dynamic systems, studying emergent chaotic dynamics, generating chaotic signals, and in general discovering new dynamic behavior. They are often used in researching systemics, a trandisiplinary, scientific field that studies natural systems. The goal of systemics researchers is to develop a conceptual and mathematical framework necessary to analyze, model, and understand systems, including, but not limited to, atomic, mechanical, molecular, chemical, biological, ecological, social and economic systems. Topics explored are emergence, collective behavior, local activity and its impact on global behavior, and quantifying the complexity of an approximately spatial and topologically invariant system . Although another measure of complexity may not arouse enthusiasm (Seth Lloyd, a professor from Massachusetts Institute of Technology (MIT), has identified 32 different definitions of complexity), it can potentially be mathematically advantageous when analyzing systems such as economic and social systems.\n\nand its Computational Complexity\", Int’l Workshop on Cellular Neural Networks and Their Applications, 2004.\n",
    "id": "2506529",
    "title": "Cellular neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38156432",
    "text": "CoDi\n\nCoDi is a cellular automaton (CA) model for spiking neural networks (SNNs). CoDi is an acronym for Collect and Distribute, referring to the signals and spikes in a neural network.\n\nCoDi uses a von Neumann neighborhood modified for a three-dimensional space; each cell looks at the states of its six orthogonal neighbors and its own state. In a growth phase a neural network is grown in the CA-space based on an underlying chromosome. There are four types of cells: neuron body, axon, dendrite and blank. The growth phase is followed by a signaling- or processing-phase. Signals are distributed from the neuron bodies via their axon tree and collected from connection dendrites. These two basic interactions cover every case, and they can be expressed simply, using a small number of rules.\n\nThe neuron body cells collect neural signals from the surrounding dendritic cells and apply an internally defined function to the collected data. In the CoDi model the neurons sum the incoming signal values and fire after a threshold is reached. This behavior of the neuron bodies can be modified easily to suit a given problem. The output of the neuron bodies is passed on to its surrounding axon cells. Axonal cells distribute data originating from the neuron body. Dendritic cells collect data and eventually pass it to the neuron body. These two types of cell-to-cell interaction cover all kinds of cell encounters.\n\nEvery cell has a gate, which is interpreted differently depending on the type of the cell. A neuron cell uses this gate to store its orientation, i.e. the direction in which the axon is pointing. In an axon cell, the gate points to the neighbor from which the neural signals are received. An axon cell accepts input only from this neighbor, but makes its own output available to all its neighbors. In this way axon cells distribute information. The source of information is always a neuron cell. Dendritic cells collect \ninformation by accepting information from any neighbor. They give their output, (e.g. a Boolean OR operation on the binary inputs) only to the neighbor specified by their own gate. In this way, dendritic cells collect and \"sum\" neural signals, until the final sum of collected neural signals reaches the neuron cell.\n\nEach axonal and dendritic cell \"belongs\" to exactly one neuron cell. This configuration of the CA-space is guaranteed by the preceding growth phase.\n\nThe CoDi model does not use explicit synapses, because dendrite cells that are in contact with an axonal trail (i.e. have an axon cell as neighbor) collect the neural signals directly from the axonal trail. This results from the behavior of axon cells, which distribute to every neighbor, and from the behavior of the dendrite cells, which collect from any neighbor.\n\nThe strength of a neuron-neuron connection (a synapse) is represented by the number of their neighboring axon and dendrite cells. The exact structure of the network and the position of the axon-dendrite neighbor pairs determine the time delay and strength (weight) of a neuron-neuron connection. This principle infers that a single neuron-neuron connection can consist of several synapse with different time delays with independent weights.\n\nThe chromosome is initially distributed throughout the CA-space, so that every cell in the CA-space contains one instruction of the chromosome, i.e. one growth instruction, so that the chromosome belongs to the network as a whole. The distributed chromosome technique of the CoDi model makes maximum use of the available CA-space and enables the growth of any type of network connectivity. The local connection of the grown circuitry to its chromosome, allows local learning to be combined with the evolution of grown neural networks.\n\nGrowth signals are passed to the direct neighbors of the neuron cell according to its chromosome information. The blank neighbors, which receive a neural growth signal, turn into either an axon cell or a dendrite cell. The growth signals include information containing the cell type of the cell that is to be grown from the signal. To decide in which directions axonal or dendritic trails should grow, the grown cells consult their chromosome information which encodes the growth instructions. These growth instructions can have an absolute or a relative directional encoding. An absolute encoding masks the six neighbors (i.e. directions) of a 3D cell with six bits. After a cell is grown, it accepts growth signals only from the direction from which it received its first signal. This \"reception direction\" information is stored in the \"gate\" position of each cell's state.\n\nThe states of our CAs have two parts, which are treated in different ways. The first part of the cell-state contains the cell's type and activity level and the second part serves as an interface to the cell's neighborhood by containing the input signals from the neighbors. Characteristic of our CA is that only part of the state of a cell is passed to its neighbors, namely the signal and then only to those neighbors specified in the fixed part of the cell state. This CA is called \"partitioned\", because the state is partitioned into two parts, the first being fixed and the second is variable for each cell.\n\nThe advantage of this partitioning-technique is that the amount of information that defines the new state of a CA cell is kept to a minimum, due to its avoidance of redundant information exchange.\n\nSince CAs are only locally connected, they are ideal for implementation on purely parallel hardware. When designing the CoDi CA-based neural networks model, the objective was to implement them directly in hardware (FPGAs). Therefore, the CA was kept as simple as possible, by having a small number of bits to specify the state, keeping the CA rules few in number, and having few cellular neighbors.\n\nThe CoDi model was implemented in the FPGA based CAM-Brain Machine (CBM) by Korkin.\n\nCoDi was introduced by Gers et al. in 1998. A specialized parallel machine based on FPGA Hardware (CAM) to run the DoDi model on a large scale was developed by Korkin et al. De Garis conducted a series of experiments on the CAM-machine evaluating the CoDi model. The original model, where learning is based on evolutionary algorithms, has been augmented with a local learning rule via feedback from dendritic spikes by Schwarzer.\n",
    "id": "38156432",
    "title": "CoDi"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13526929",
    "text": "Compositional pattern-producing network\n\nCompositional pattern-producing networks (CPPNs) are a variation of artificial neural networks (ANNs) that have an architecture whose evolution is guided by genetic algorithms.\n\nWhile ANNs often contain only sigmoid functions and sometimes Gaussian functions, CPPNs can include both types of functions and many others. The choice of functions for the canonical set can be biased toward specific types of patterns and regularities. For example, periodic functions such as sine produce segmented patterns with repetitions, while symmetric functions such as Gaussian produce symmetric patterns. Linear functions can be employed to produce linear or fractal-like patterns. Thus, the architect of a CPPN-based genetic art system can bias the types of patterns it generates by deciding the set of canonical functions to include.\n\nFurthermore, unlike typical ANNs, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\n\nCPPNs can be evolved through neuroevolution techniques such as neuroevolution of augmenting topologies (called CPPN-NEAT).\n\nCPPNs have been shown to be a very powerful encoding when evolving the following:\n\n\n",
    "id": "13526929",
    "title": "Compositional pattern-producing network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1532719",
    "text": "Computational cybernetics\n\nComputational cybernetics is the integration of cybernetics and computational intelligence techniques. Though the term Cybernetics entered the technical lexicon in the 1940s and 1950s, it was first used informally as a popular noun in the 1960s, when it became associated with computers, robotics, Artificial Intelligence and Science fiction.\n\nWhile Cybernetics is primarily concerned with the study of control systems, computational cybernetics focuses on their automatic (complex, autonomic, flexible, adaptive) operation. Furthermore, computational cybernetics covers not only mechanical, but biological (living), social and economical systems. To achieve this goal, it uses research from the fields of communication theory, signal processing, information technology, control theory, the theory of adaptive systems, the theory of complex systems (game theory, and operational research). \n\nIEEE, a professional organization for the advancement of technology, has organized two international conferences focusing on computational cybernetics in 2008 and 2013.\n\n",
    "id": "1532719",
    "title": "Computational cybernetics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8402086",
    "text": "Computational neurogenetic modeling\n\nComputational neurogenetic modeling (CNGM) is concerned with the study and development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes. These include neural network models and their integration with gene network models. This area brings together knowledge from various scientific disciplines, such as computer and information science, neuroscience and cognitive science, genetics and molecular biology, as well as engineering.\n\nModels of the kinetics of proteins and ion channels associated with neuron activity represent the lowest level of modeling in a computational neurogenetic model. The altered activity of proteins in some diseases, such as the amyloid beta protein in Alzheimer's disease, must be modeled at the molecular level to accurately predict the effect on cognition. Ion channels, which are vital to the propagation of action potentials, are another molecule that may be modeled to more accurately reflect biological processes. For instance, to accurately model synaptic plasticity (the strengthening or weakening of synapses) and memory, it is necessary to model the activity of the NMDA receptor (NMDAR). The speed at which the NMDA receptor lets Calcium ions into the cell in response to Glutamate is an important determinant of Long-term potentiation via the insertion of AMPA receptors (AMPAR) into the plasma membrane at the synapse of the postsynaptic cell (the cell that receives the neurotransmitters from the presynaptic cell).\n\nIn most models of neural systems neurons are the most basic unit modeled. In computational neurogenetic modeling, to better simulate processes that are responsible for synaptic activity and connectivity, the genes responsible are modeled for each neuron.\n\nA gene regulatory network, protein regulatory network, or gene/protein regulatory network, is the level of processing in a computational neurogenetic model that models the interactions of genes and proteins relevant to synaptic activity and general cell functions. Genes and proteins are modeled as individual nodes, and the interactions that influence a gene are modeled as excitatory (increases gene/protein expression) or inhibitory (decreases gene/protein expression) inputs that are weighted to reflect the effect a gene or protein is having on another gene or protein. Gene regulatory networks are typically designed using data from microarrays.\n\nModeling of genes and proteins allows individual responses of neurons in an artificial neural network that mimic responses in biological nervous systems, such as division (adding new neurons to the artificial neural network), creation of proteins to expand their cell membrane and foster neurite outgrowth (and thus stronger connections with other neurons), up-regulate or down-regulate receptors at synapses (increasing or decreasing the weight (strength) of synaptic inputs), uptake more neurotransmitters, change into different types of neurons, or die due to necrosis or apoptosis. The creation and analysis of these networks can be divided into two sub-areas of research: the \ngene up-regulation that is involved in the normal functions of a neuron, such as growth, metabolism, and synapsing; and the effects of mutated genes on neurons and cognitive functions.\n\nAn artificial neural network generally refers to any computational model that mimics the central nervous system, with capabilities such as learning and pattern recognition. With regards to computational neurogenetic modeling, however, it is often used to refer to those specifically designed for biological accuracy rather than computational efficiency. Individual neurons are the basic unit of an artificial neural network, with each neuron acting as a node. Each node receives weighted signals from other nodes that are either excitatory or inhibitory. To determine the output, a transfer function (or activation function) evaluates the sum of the weighted signals and, in some artificial neural networks, their input rate. Signal weights are strengthened (long-term potentiation) or weakened (long-term depression) depending on how synchronous the presynaptic and postsynaptic activation rates are (Hebbian theory).\n\nThe synaptic activity of individual neurons is modeled using equations to determine the temporal (and in some \ncases, spatial) summation of synaptic signals, membrane potential, threshold for action potential \ngeneration, the absolute and relative refractory period, and optionally ion receptor channel kinetics and Gaussian noise (to increase biological accuracy by incorporation of random elements). In addition to connectivity, some types of artificial neural networks, such as spiking neural networks, also model the distance between neurons, and its effect on the synaptic weight (the strength of a synaptic transmission).\n\nFor the parameters in the gene regulatory network to affect the neurons in the artificial neural network as intended there must be some connection between them. In an organizational context, each node (neuron) in the artificial neural network has its own gene regulatory network associated with it. The weights (and in some networks, frequencies of synaptic transmission to the node), and the resulting membrane potential of the node (including whether an action potential is produced or not), affect the expression of different genes in the gene regulatory network. Factors affecting connections between neurons, such as synaptic plasticity, can be modeled by inputting the values of synaptic activity-associated genes and proteins to a function that re-evaluates the weight of an input from a particular neuron in the artificial neural network.\n\nOther cell types besides neurons can be modeled as well. Glial cells, such as astroglia and microglia, as well as endothelial cells, could be included in an artificial neural network. This would enable modeling of diseases where pathological effects may occur from sources other than neurons, such as Alzheimer's disease.\n\nWhile the term artificial neural network is usually used in computational neurogenetic modeling to refer to models of the central nervous system meant to possess biological accuracy, the general use of the term can be applied to many gene regulatory networks as well.\n\nArtificial neural networks, depending on type, may or may not take into account the timing of inputs. Those that do, such as spiking neural networks, fire only when the pooled inputs reach a membrane potential is reached. Because this mimics the firing of biological neurons, spiking neural networks are viewed as a more biologically accurate model of synaptic activity.\n\nTo accurately model the central nervous system, creation and death of neurons should be modeled as well. To accomplish this, constructive artificial neural networks that are able to grow or shrink to adapt to inputs are often used. Evolving connectionist systems are a subtype of constructive artificial neural networks (evolving in this case referring to changing the structure of its neural network rather than by mutation and natural selection).\n\nBoth synaptic transmission and gene-protein interactions are stochastic in nature. To model biological nervous systems with greater fidelity some form of randomness is often introduced into the network. Artificial neural networks modified in this manner are often labeled as probabilistic versions of their neural network sub-type (e.g., pSNN).\n\nFuzzy logic is a system of reasoning that enables an artificial neural network to deal in non-binary and linguistic variables. Biological data is often unable to be processed using Boolean logic, and moreover accurate modeling of the capabilities of biological nervous systems requires fuzzy logic. Therefore, artificial neural networks that incorporate it, such as evolving fuzzy neural networks (EFuNN) or Dynamic Evolving Neural-Fuzzy Inference Systems (DENFIS), are often used in computational neurogenetic modeling. The use of fuzzy logic is especially relevant in gene regulatory networks, as the modeling of protein binding strength often requires non-binary variables.\n\nArtificial Neural Networks designed to simulate of the human brain require an ability to learn a variety of tasks that is not required by those designed to accomplish a specific task. Supervised learning is a mechanism by which an artificial neural network can learn by receiving a number of inputs with a correct output already known. An example of an artificial neural network that uses supervised learning is a multilayer perceptron (MLP). In unsupervised learning, an artificial neural network is trained using only inputs. Unsupervised learning is the learning mechanism by which a type of artificial neural network known as a self-organizing map (SOM) learns. Some types of artificial neural network, such as evolving connectionist systems, can learn in both a supervised and unsupervised manner.\n\nBoth gene regulatory networks and artificial neural networks have two main strategies for improving their accuracy. In both cases the output of the network is measured against known biological data using some function, and subsequent improvements are made by altering the structure of the network. A common test of accuracy for artificial neural networks is to compare some parameter of the model to data acquired from biological neural systems, such as from an EEG. In the case of EEG recordings, the local field potential (LFP) of the artificial neural network is taken and compared to EEG data acquired from human patients. The relative intensity ratio (RIRs) and fast Fourier transform (FFT) of the EEG are compared with those generated by the artificial neural networks to determine the accuracy of the model.\n\nBecause the amount of data on the interplay of genes and neurons and their effects is not enough to construct a rigorous model, \nevolutionary computation is used to optimize artificial neural networks and gene regulatory networks, a common technique being the genetic algorithm. A genetic algorithm is a process that can be used to refine models by mimicking the process of natural selection observed in biological ecosystems. The primary advantages are that, due to not requiring derivative information, it can be applied to black box problems and multimodal optimization. The typical process for using genetic algorithms to refine a gene \nregulatory network is: first, create a population; next, to create offspring via a crossover operation and \nevaluate their fitness; then, on a group chosen for high fitness, simulate mutation via a mutation operator; \nfinally, taking the now mutated group, repeat this process until a desired level of fitness is demonstrated.\nMethods by which artificial neural networks may alter their structure without simulated mutation and fitness selection have been developed. A dynamically evolving neural network is one approach, as the creation of new connections and new neurons can \nbe modeled as the system adapts to new data. This enables the network to evolve in modeling accuracy without simulated natural selection. One method by which dynamically evolving networks may be optimized, called evolving layer neuron aggregation, combines neurons with sufficiently similar input weights into one neuron. This can take place during the training of the network, referred to as online aggregation, or between periods of training, referred to as offline aggregation. Experiments have suggested that offline aggregation is more efficient.\n\nA variety of potential applications have been suggested for accurate computational neurogenetic models, such as simulating genetic diseases, examining the impact of potential treatments, better understanding of learning and cognition, and development of hardware able to interface with neurons.\n\nThe simulation of disease states is of particular interest, as modeling both the neurons and their genes and proteins allows linking genetic mutations and protein abnormalities to pathological effects in the central nervous system. Among those diseases suggested as being possible targets of computational neurogenetic modeling based analysis are epilepsy, schizophrenia, mental retardation, brain aging and Alzheimer's disease, and Parkinson's disease.\n\n\n",
    "id": "8402086",
    "title": "Computational neurogenetic modeling"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42368614",
    "text": "Convolutional Deep Belief Networks\n\nIn computer science, Convolutional Deep Belief Network (CDBN) is a type of deep artificial neural network that is composed of multiple layers of convolutional restricted Boltzmann machines stacked together. Alternatively, it is a hierarchical generative model for deep learning, which is highly effective in the tasks of image processing and object recognition, though it has been used in other domains too. The salient features of the model include the fact that it scales well to high-dimensional images and is translation-invariant.\n\nCDBNs use the technique of probabilistic max-pooling to reduce the dimensions in higher layers in the network. Training of the network involves a pre-training stage accomplished in a greedy layer-wise manner, similar to other deep belief networks. Depending on whether the network is to be used for discrimination or generative tasks, it is then \"fine tuned\" or trained with either back-propagation or the up-down algorithm (contrastive-divergence), respectively.\n",
    "id": "42368614",
    "title": "Convolutional Deep Belief Networks"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24364159",
    "text": "Cover's theorem\n\nCover's Theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation. The theorem is named after the information theorist Thomas M. Cover who stated it in 1965.\n\nThe proof is easy. A deterministic mapping may be used. Indeed, suppose there are formula_1 samples. Lift them onto the vertices of the simplex in the formula_2 dimensional real space. Every partition of the samples into two sets is separable by a linear separator. QED.\n\n",
    "id": "24364159",
    "title": "Cover's theorem"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41416740",
    "text": "Deep belief network\n\nIn machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.\n\nWhen trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this learning step, a DBN can be further trained with supervision to perform classification.\n\nDBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a \"visible\" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the \"lowest\" pair of layers (the lowest visible layer is a training set).\n\nTeh's observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms. Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography, drug discovery). \n\nThe training method for RBMs proposed by Geoffrey Hinton for use with training \"Product of Expert\" models is called contrastive divergence (CD). CD provides an approximation to the maximum likelihood method that would ideally be applied for learning the weights. In training a single RBM, weight updates are performed with gradient descent via the following equation: formula_1\n\nwhere, formula_2 is the probability of a visible vector, which is given by formula_3. formula_4 is the partition function (used for normalizing) and formula_5 is the energy function assigned to the state of the network. A lower energy indicates the network is in a more \"desirable\" configuration. The gradient formula_6 has the simple form formula_7 where formula_8 represent averages with respect to distribution formula_9. The issue arises in sampling formula_10 because this requires extended alternating Gibbs sampling. CD replaces this step by running alternating Gibbs sampling for formula_11 steps (values of formula_12 perform well). After formula_11 steps, the data are sampled and that sample is used in place of formula_10. The CD procedure works as follows:\nOnce an RBM is trained, another RBM is \"stacked\" atop it, taking its input from the final trained layer. The new visible layer is initialized to a training vector, and values for the units in the already-trained layers are assigned using the current weights and biases. The new RBM is then trained with the procedure above. This whole process is repeated until the desired stopping criterion is met.\n\nAlthough the approximation of CD to maximum likelihood is crude (does not follow the gradient of any function), it is empirically effective.\n\n\n",
    "id": "41416740",
    "title": "Deep belief network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42358441",
    "text": "Deep lambertian networks\n\nDeep Lambertian Networks (DLN) is a combination of Deep belief network and Lambertian reflectance assumption which deals with the challenges posed by illumination variation in visual perception. Lambertian Reflectance model gives an illumination invariant representation which can be used for recognition. The Lambertian reflectance model is widely used for\nmodeling illumination variations and is a good approximation for diffuse object surfaces. The DLN is a hybrid undirected-directed generative model that combines DBNs with the Lambertian reflectance model.\n\nIn the DLN, the visible layer consists of image pixel intensities v ∈ R, where N is the number of pixels in the image. For every pixel i there are two latent variables namely the albedo and surface normal. GRBMs are used to model the albedo and surface normals.\n\nCombining Deep Belief Nets with the Lambertian reflectance assumption, the model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is also possible. Experiments demonstrate that this model is able to generalize as well as improve over standard baselines in one-shot face recognition.\n\nThe model has been successfully applied in reconstruction of shadows facial images, given any set of lighting conditions. The model has also been tested on non-living objects. The method outperforms most other methods and is faster than them.\n",
    "id": "42358441",
    "title": "Deep lambertian networks"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1237612",
    "text": "Delta rule\n\nIn machine learning, the Delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. For a neuron formula_1 with activation function formula_2, the delta rule for formula_1's formula_4th weight formula_5 is given by\n\nwhere\n\nIt holds that formula_7 and formula_8.\n\nThe delta rule is commonly stated in simplified form for a neuron with a linear activation function as \n\nWhile the delta rule is similar to the perceptron's update rule, the derivation is different. The perceptron uses the Heaviside step function as the activation function formula_10, and that means that formula_11 does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the delta rule impossible.\n\nThe delta rule is derived by attempting to minimize the error in the output of the neural network through gradient descent. The error for a neural network with formula_1 outputs can be measured as \n\nIn this case, we wish to move through \"weight space\" of the neuron (the space of all possible values of all of the neuron's weights) in proportion to the gradient of the error function with respect to each weight. In order to do that, we calculate the partial derivative of the error with respect to each weight. For the formula_4th weight, this derivative can be written as \n\nBecause we are only concerning ourselves with the formula_1th neuron, we can substitute the error formula above while omitting the summation:\n\nNext we use the chain rule to split this into two derivatives:\n\nTo find the left derivative, we simply apply the general power rule:\n\nTo find the right derivative, we again apply the chain rule, this time differentiating with respect to the total input to formula_1, formula_21:\n\nNote that the output of the formula_23th neuron, formula_24, is just the neuron's activation function formula_25 applied to the neuron's input formula_21. We can therefore write the derivative of formula_24 with respect to formula_21 simply as formula_25's first derivative:\n\nNext we rewrite formula_21 in the last term as the sum over all formula_32 weights of each weight formula_33 times its corresponding input formula_34:\n\nBecause we are only concerned with the formula_4th weight, the only term of the summation that is relevant is formula_37. Clearly, \n\ngiving us our final equation for the gradient:\n\nAs noted above, gradient descent tells us that our change for each weight should be proportional to the gradient. Choosing a proportionality constant formula_40 and eliminating the minus sign to enable us to move the weight in the negative direction of the gradient to minimize error, we arrive at our target equation:\n\n",
    "id": "1237612",
    "title": "Delta rule"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8887731",
    "text": "Echo state network\n\nThe echo state network (ESN), is a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\n\nAlternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.\n\nSome publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network.\n\n",
    "id": "8887731",
    "title": "Echo state network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14509578",
    "text": "The Emotion Machine\n\nThe Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind is a 2006 book by cognitive scientist Marvin Minsky that elaborates and expands on Minsky's ideas as presented in his earlier book \"Society of Mind\".\n\nMinsky argues that emotions are different ways to think that our mind uses to increase our intelligence. He challenges the distinction between emotions and other kinds of thinking. His main argument is that emotions are \"ways to think\" for different \"problem types\" that exist in the world, and that the brain has rule-based mechanisms (selectors) that turn on emotions to deal with various problems. The book reviews the accomplishments of AI, why modelling an AI is difficult in terms of replicating the behaviors of humans, if and how AIs think, and in what manner they might experience struggles and pleasures.\n\nIn a review for \"The Washington Post\", neurologist Richard Restak states that:\nMinsky outlines the book as follows:\n\n\n\n\n",
    "id": "14509578",
    "title": "The Emotion Machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19172663",
    "text": "European Neural Network Society\n\nThe European Neural Network Society (ENNS) is an association of scientists, engineers, students, and others seeking to learn about and advance understanding of artificial neural networks. Specific areas of interest in this scientific field include modelling of behavioral and brain processes, development of neural algorithms and applying neural modelling concepts to problems relevant in many different domains. Erkki Oja and John G. Taylor are past ENNS presidents and honorary executive board members.\n\nEvery year since 1991 ENNS organizes the International Conference on Artificial Neural Networks (ICANN). The history and the links to past conferences are available at the ENNS web site. This is one of the oldest and best established conferences on the subject, with proceedings published in Springer Lecture Notes in Computer Science, see index in DBLP bibliography database.\n\nAs a non-profit organization ENNS promotes scientific activities at European and at the national levels in cooperation with national organizations that focus on neural networks. Every year many stipends to attend ICANN conference are given. ENNS also sponsors other students and have given awards and prizes at co-sponsored events (schools, workshops, conferences and competitions).\n",
    "id": "19172663",
    "title": "European Neural Network Society"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15702071",
    "text": "Evolutionary acquisition of neural topologies\n\nEvolutionary acquisition of neural topologies (EANT/EANT2) is an evolutionary reinforcement learning method that evolves both the topology and weights of artificial neural networks. It is closely related to the works of Angeline et al. and Stanley and Miikkulainen. Like the work of Angeline et al., the method uses a type of parametric mutation that comes from evolution strategies and evolutionary programming (now using the most advanced form of the evolution strategies CMA-ES in EANT2), in which adaptive step sizes are used for optimizing the weights of the neural networks. Similar to the work of Stanley (NEAT), the method starts with minimal structures which gain complexity along the evolution path.\n\nDespite sharing these two properties, the method has the following important features which distinguish it from previous works in neuroevolution.\n\nIt introduces a genetic encoding called common genetic encoding (CGE) that handles both direct and indirect encoding of neural networks within the same theoretical framework. The encoding has important properties that makes it suitable for evolving neural networks: \n\nThese properties have been formally proven in.\n\nFor evolving the structure and weights of neural networks, an evolutionary process is used, where the \"exploration\" of structures is executed at a larger timescale (structural exploration), and the \"exploitation\" of existing structures is done at a smaller timescale (structural exploitation). In the structural exploration phase, new neural structures are developed by gradually adding new structures to an initially minimal network that is used as a starting point. In the structural exploitation phase, the weights of the currently available structures are optimized using an evolution strategy.\n\nEANT has been tested on some benchmark problems such as the double-pole balancing problem, and the RoboCup keepaway benchmark. In all the tests, EANT was found to perform very well. Moreover, a newer version of EANT, called EANT2, was tested on a visual servoing task and found to outperform NEAT and the traditional iterative Gauss–Newton method. Further experiments include results on a classification problem \n\n",
    "id": "15702071",
    "title": "Evolutionary acquisition of neural topologies"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31294087",
    "text": "Extension neural network\n\nExtension neural network is a pattern recognition method found by M. H. Wang and C. P. Hung in 2003 to classify instances of data sets. Extension neural network is composed of artificial neural network and extension theory concepts. It uses the fast and adaptive learning capability of neural network and correlation estimation property of extension theory by calculating extension distance. \nENN was used in:\n\nExtension theory was first proposed by Cai in 1983 to solve contradictory problems. While classical mathematic is familiar with quantity and forms of objects, extension theory transforms these objects to matter-element model.\n\nwhere in matter formula_1, formula_2 is the name or type, formula_3 is its characteristics and formula_4 is the corresponding value for the characteristic. There is a corresponding example in equation 2.\n\nwhere formula_5 and formula_6 characteristics form extension sets. These extension sets are defined by the formula_4 values which are range values for corresponding characteristics. Extension theory concerns with the extension correlation function between matter-element models like shown in equation 2 and extension sets. Extension correlation function is used to define extension space which is composed of pairs of elements and their extension correlation functions. The extension space formula is shown in equation 3.\n\nwhere, formula_8 is the extension space, formula_9 is the object space, formula_10 is the extension correlation function, formula_11 is an element from the object space and formula_12 is the corresponding extension correlation function output of element formula_11. formula_14 maps formula_11 to a membership interval formula_16. Negative region represents an element not belonging membership degree to a class and positive region vice versa. If formula_11 is mapped to formula_18, extension theory acts like fuzzy set theory. The correlation function can be shown with the equation 4.\n\nwhere, formula_19 and formula_20 are called concerned and neighborhood domain and their intervals are (a,b) and (c,d) respectively. The extended correlation function used for estimation of membership degree between formula_11 and formula_19, formula_20 is shown in equation 5.\n</math>\n\nExtension neural network has a neural network like appearance. Weight vector resides between the input nodes and output nodes. Output nodes are the representation of input nodes by passing them through the weight vector. \n\nThere are total number of input and output nodes are represented by formula_24 and formula_25, respectively. These numbers depend on the number of characteristics and classes. Rather than using one weight value between two layer nodes as in neural network, extension neural network architecture has two weight values. In extension neural network architecture, for instance formula_26, formula_27 is the input which belongs to class formula_28 and formula_29 is the corresponding output for class formula_30. The output formula_29 is calculated by using extension distance as shown in equation 6.\n\n+1 \\right)\n</math>\nformula_32\n\nEstimated class is found through searching for the minimum extension distance among the calculated extension distance for all classes as summarized in equation 7, where formula_33 is the estimated class.\n\nEach class is composed of ranges of characteristics. These characteristics are the input types or names which come from matter-element model. Weight values in extension neural network represent these ranges. In the learning algorithm, first weights are initialized by searching for the maximum and minimum values of inputs for each class as shown in equation 8\n\nwhere, formula_26 is the instance number and formula_35 is represents number of input. This initialization provides classes' ranges according to given training data.\n\nAfter maintaining weights, center of clusters are found through the equation 9.\n\nBefore learning process begins, predefined learning performance rate is given as shown in equation 10\n\nwhere, formula_36 is the misclassified instances and formula_37 is the total number of instances. Initialized parameters are used to classify instances with using equation 6. If the initialization is not sufficient due to the learning performance rate, training is required. In the training step weights are adjusted to classify training data more accurately, therefore reducing learning performance rate is aimed. In each iteration, formula_38 is checked to control if required learning performance is reached. In each iteration every training instance is used for training. \nInstance formula_26, belongs to class formula_28 is shown by:\n\nformula_41\n\nformula_42\n\nEvery input data point of formula_43 is used in extension distance calculation to estimate the class of formula_43. If the estimated class formula_45 then update is not needed. Whereas, if formula_46 then update is done. In update case, separators which show the relationship between inputs and classes, are shifted proportional to the distance between the center of clusters and the data points. \nThe update formula:\n\nformula_47\nformula_48\nformula_49\nformula_50\nformula_51\nformula_52\n\nTo classify the instance formula_26 accurately, separator of class formula_28 for input formula_35 moves close to data-point of instance formula_26, whereas separator of class formula_33 for input formula_35 moves far away. In the above image, an update example is given. Assume that instance formula_26 belongs to class A, whereas it is classified to class B because extension distance calculation gives out formula_60. After the update, separator of class A moves close to the data-point of instance formula_26 whereas separator of class B moves far away. Consequently, extension distance gives out formula_62, therefore after update instance \nformula_26 is classified to class A.\n\n",
    "id": "31294087",
    "title": "Extension neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=574759",
    "text": "Feed forward (control)\n\nFeed-forward, sometimes written feedforward, is a term describing an element or pathway within a control system that passes a controlling signal from a source in its external environment, often a command signal from an external operator, to a load elsewhere in its external environment. A control system which has only feed-forward behavior responds to its control signal in a pre-defined way without responding to how the load reacts; it is in contrast with a system that also has feedback, which adjusts the output to take account of how it affects the load, and how the load itself may vary unpredictably; the load is considered to belong to the external environment of the system.\n\nIn a feed-forward system, the control variable adjustment is not error-based. Instead it is based on knowledge about the process in the form of a mathematical model of the process and knowledge about or measurements of the process disturbances.\n\nSome prerequisites are needed for control scheme to be reliable by pure feed-forward without feedback: the external command or controlling signal must be available, and the effect of the output of the system on the load should be known (that usually means that the load must be predictably unchanging with time). Sometimes pure feed-forward control without feedback is called 'ballistic', because once a control signal has been sent, it cannot be further adjusted; any corrective adjustment must be by way of a new control signal. In contrast, 'cruise control' adjusts the output in response to the load that it encounters, by a feedback mechanism.\n\nThese systems could relate to control theory, physiology computing.\n\nWith feed-forward or Feedforward control, the disturbances are measured and accounted for before they have time to affect the system. In the house example, a feed-forward system may measure the fact that the door is opened and automatically turn on the heater before the house can get too cold. The difficulty with feed-forward control is that the effects of the disturbances on the system must be accurately predicted, and there must not be any unmeasured disturbances. For instance, if a window was opened that was not being measured, the feed-forward-controlled thermostat might still let the house cool down.\n\nThe term has specific meaning within the field of CPU-based automatic control. The discipline of “feedforward control” as it relates to modern, CPU based automatic controls is widely discussed, but is seldom practiced due to the difficulty and expense of developing or providing for the mathematical model required to facilitate this type of control. Open-loop control and feedback control, often based on canned PID control algorithms, are much more widely used.\n\nThere are three types of control systems: open loop, feed-forward, and feedback. \nAn example of a pure open loop control system is manual non-power-assisted steering of a motor car; the steering system does not have access to an auxiliary power source and does not respond to varying resistance to turning of the direction wheels; the driver must make that response without help from the steering system. In comparison, power steering has access to a controlled auxiliary power source, which depends on the engine speed. When the steering wheel is turned, a valve is opened which allows fluid under pressure to turn the driving wheels. A sensor monitors that pressure so that the valve only opens enough to cause the correct pressure to reach the wheel turning mechanism. This is feed-forward control where the output of the system, the change in direction of travel of the vehicle, plays no part in the system. See Model predictive control.\n\nIf you include the driver in the system, then he does provide a feedback path by observing the direction of travel and compensating for errors by turning the steering wheel. In that case you have a feedback system, and the block labeled \"System\" in Figure(c) is a feed-forward system.\n\nIn other words, systems of different types can be nested, and the overall system regarded as a black-box.\n\nFeedforward control is distinctly different from open loop control and teleoperator systems. Feedforward control requires a mathematical model of the plant (process and/or machine being controlled) and the plant's relationship to any inputs or feedback the system might receive. Neither open loop control nor teleoperator systems require the sophistication of a mathematical model of the physical system or plant being controlled. Control based on operator input without integral processing and interpretation through a mathematical model of the system is a teleoperator system and is not considered feedforward control.\n\nHistorically, the use of the term “feedforward” is found in works by D. M. MacKay as early as 1956. While MacKay’s work is in the field of biological control theory, he speaks only of feedforward systems. MacKay does not mention “Feedforward Control” or allude to the discipline of “Feedforward Controls.” MacKay and other early writers who use the term “feedforward” are generally writing about theories of how human or animal brains work.\n\nThe discipline of “feedforward controls” was largely developed by professors and graduate students at Georgia Tech, MIT, Stanford and Carnegie Mellon. Feedforward is not typically hyphenated in scholarly publications. Meckl and Seering of MIT and Book and Dickerson of Georgia Tech began the development of the concepts of Feedforward Control in the mid 1970s. The discipline of Feedforward Controls was well defined in many scholarly papers, articles and books by the late 1980s.\n\nThe benefits of feedforward control are significant and can often justify the extra cost, time and effort required to implement the technology. Control accuracy can often be improved by as much as an order of magnitude if the mathematical model is of sufficient quality and implementation of the feedforward control law is well thought out. Energy consumption by the feedforward control system and its driver is typically substantially lower than with other controls. Stability is enhanced such that the controlled device can be built of lower cost, lighter weight, springier materials while still being highly accurate and able to operate at high speeds. Other benefits of feedforward control include reduced wear and tear on equipment, lower maintenance costs, higher reliability and a substantial reduction in hysteresis. Feedforward control is often combined with feedback control to optimize performance.\n\nThe mathematical model of the plant (machine, process or organism) used by the feedforward control system may be created and input by a control engineer or it may be learned by the control system. Control systems capable of learning and/or adapting their mathematical model have become more practical as microprocessor speeds have increased. The discipline of modern feedforward control was itself made possible by the invention of microprocessors.\n\nFeedforward control requires integration of the mathematical model into the control algorithm such that it is used to determine the control actions based on what is known about the state of the system being controlled. In the case of control for a lightweight, flexible robotic arm, this could be as simple as compensating between when the robot arm is carrying a payload and when it is not. The target joint angles are adjusted to place the payload in the desired position based on knowing the deflections in the arm from the mathematical model’s interpretation of the disturbance caused by the payload. Systems that plan actions and then pass the plan to a different system for execution do not satisfy the above definition of feedforward control. Unless the system includes a means to detect a disturbance or receive an input and process that input through the mathematical model to determine the required modification to the control action, it is not true feedforward control.\n\nIn systems theory, an open system is a feed forward system that does not have any feedback loop to control its output. In contrast, a closed system uses on a feedback loop to control the operation of the system. In an open system, the output of the system is not fed back into the input to the system for control or operation.\n\nIn physiology, feed-forward control is exemplified by the normal anticipatory regulation of heartbeat in advance of actual physical exertion. Feed-forward control can be likened to learned anticipatory responses to known cues. Feedback regulation of the heartbeat provides further adaptiveness to the running eventualities of physical exertion.\n\nFeedforward systems are also found in biological control by human and animal brains.\n\nEven in the case of biological feedforward systems, such as in the human brain, knowledge or a mental model of the plant (body) can be considered to be mathematical as the model is characterized by limits, rhythms, mechanics and patterns.\n\nA pure feed-forward system is different from a homeostatic control system, which has the function of keeping the body's internal environment 'steady' or in a 'prolonged steady state of readiness.' A homeostatic control system relies mainly on feedback (especially negative), in addition to the feedforward elements of the system.\n\nThe cross regulation of genes can be represented by a graph, where genes are the nodes and one node is linked to another if the former is a transcription factor for the latter. A motif which predominantly appears in all known networks (E. coli, Yeast...) is A activates B, A and B activate C. This motif has been shown to be a feed forward system, detecting non-temporary change of environment. This feed forward control theme is commonly observed in hematopoietic cell lineage development, where irreversible commitments are made.\n\nIn computing, feed-forward normally refers to a perceptron network in which the outputs from all neurons go to following but not preceding layers, so there are no feedback loops. The connections are set up during a training phase, which in effect is when the system is a feedback system.\n\nIn the early 1970s, intercity coaxial transmission systems, including L-carrier, used feed-forward amplifiers to diminish linear distortion. This more complex method allowed wider bandwidth than earlier feedback systems. Optical fiber, however, made such systems obsolete before many were built.\n\nFeedforward control is a discipline within the field of automatic controls used in automation.\n\n\n",
    "id": "574759",
    "title": "Feed forward (control)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1706332",
    "text": "Feedforward neural network\n\nA feedforward neural network is an artificial neural network wherein connections between the units do \"not\" form a cycle. As such, it is different from recurrent neural networks.\n\nThe feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.\n\nThe simplest kind of neural network is a \"single-layer perceptron\" network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. In this way it can be considered the simplest kind of feed-forward network. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are also called \"artificial neurons\" or \"linear threshold units\". In the literature the term \"perceptron\" often refers to networks consisting of just one of these units. A similar neuron was described by Warren McCulloch and Walter Pitts in the 1940s.\n\nA perceptron can be created using any values for the activated and deactivated states as long as the threshold value lies between the two.\n\nPerceptrons can be trained by a simple learning algorithm that is usually called the \"delta rule\". It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.\n\nSingle-unit perceptrons are only capable of learning linearly separable patterns; in 1969 in a famous monograph entitled \"Perceptrons\", Marvin Minsky and Seymour Papert showed that it was impossible for a single-layer perceptron network to learn an XOR function (nonetheless, it was known that multi-layer perceptrons are capable of producing any possible boolean function). \n\nAlthough a single threshold unit is quite limited in its computational power, it has been shown that networks of parallel threshold units can approximate any continuous function from a compact interval of the real numbers into the interval [-1,1]. This result can be found in Peter Auer, Harald Burgsteiner and Wolfgang Maass \"A learning rule for very simple universal approximators consisting of a single layer of perceptrons\".\n\nA multi-layer neural network can compute a continuous output instead of a step function. A common choice is the so-called logistic function:\n\nWith this choice, the single-layer network is identical to the logistic regression model, widely used in statistical modeling. The logistic function is also known as the sigmoid function. It has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:\n\nThis class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a \"sigmoid function\" as an activation function.\n\nThe \"universal approximation theorem\" for neural networks states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a multi-layer perceptron with just one hidden layer. This result holds for a wide range of activation functions, e.g. for the sigmoidal functions.\n\nMulti-layer networks use a variety of learning techniques, the most popular being \"back-propagation\". Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has \"learned\" a certain target function. To adjust weights properly, one applies a general method for non-linear optimization that is called gradient descent. For this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions.\n\nIn general, the problem of teaching a network to perform well, even on samples that were not used as training samples, is a quite subtle issue that requires additional techniques. This is especially important for cases where only very limited numbers of training samples are available. The danger is that the network overfits the training data and fails to capture the true statistical process generating the data. Computational learning theory is concerned with training classifiers on a limited amount of data. In the context of neural networks a simple heuristic, called early stopping, often ensures that the network will generalize well to examples not in the training set.\n\nOther typical problems of the back-propagation algorithm are the speed of convergence and the possibility of ending up in a local minimum of the error function. Today there are practical methods that make back-propagation in multi-layer perceptrons the tool of choice for many machine learning tasks.\n\nOne also can use a series of independent neural networks moderated by some intermediary, a similar behavior that happens in brain. These neurons can perform separably and handle a large task, and the results can be finally combined. \n\n\n",
    "id": "1706332",
    "title": "Feedforward neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14402929",
    "text": "Generalized Hebbian Algorithm\n\nThe Generalized Hebbian Algorithm (GHA), also known in the literature as Sanger's rule, is a linear feedforward neural network model for unsupervised learning with applications primarily in principal components analysis. First defined in 1989, it is similar to Oja's rule in its formulation and stability, except it can be applied to networks with multiple outputs. The name originates because of the similarity between the algorithm and a hypothesis made by Donald Hebb about the way in which synaptic strengths in the brain are modified in response to experience, i.e., that changes are proportional to the correlation between the firing of pre- and post-synaptic neurons.\n\nGHA combines Oja's rule with the Gram-Schmidt process to produce a learning rule of the form\n\nwhere defines the synaptic weight or connection strength between the th input and th output neurons, and are the input and output vectors, respectively, and is the \"learning rate\" parameter.\n\nIn matrix form, Oja's rule can be written\n\nand the Gram-Schmidt algorithm is\n\nwhere is any matrix, in this case representing synaptic weights, is the autocorrelation matrix, simply the outer product of inputs, is the function that diagonalizes a matrix, and is the function that sets all matrix elements on or above the diagonal equal to 0. We can combine these equations to get our original rule in matrix form,\n\nwhere the function sets all matrix elements above the diagonal equal to 0, and note that our output is a linear neuron.\n\nGHA is used in applications where a self-organizing map is necessary, or where a feature or principal components analysis can be used. Examples of such cases include artificial intelligence and speech and image processing.\n\nIts importance comes from the fact that learning is a single-layer process—that is, a synaptic weight changes only depending on the response of the inputs and outputs of that layer, thus avoiding the multi-layer dependence associated with the backpropagation algorithm. It also has a simple and predictable trade-off between learning speed and accuracy of convergence as set by the learning rate parameter .\n\n",
    "id": "14402929",
    "title": "Generalized Hebbian Algorithm"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1091054",
    "text": "Generative topographic map\n\nGenerative topographic map (GTM) is a machine learning method that is a probabilistic counterpart of the self-organizing map (SOM), is probably convergent and does not require a shrinking neighborhood or a decreasing step size. It is a generative model: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the expectation-maximization (EM) algorithm. GTM was introduced in 1996 in a paper by Christopher Bishop, Markus Svensen, and Christopher K. I. Williams.\n\nThe approach is strongly related to density networks which use importance sampling and a multi-layer perceptron to form a non-linear latent variable model. In the GTM the latent space is a discrete grid of points which is assumed to be non-linearly projected into data space. A Gaussian noise assumption is then made in data space so that the model becomes a constrained mixture of Gaussians. Then the model's likelihood can be maximized by EM.\n\nIn theory, an arbitrary nonlinear parametric deformation could be used. The optimal parameters could be found by gradient descent, etc.\n\nThe suggested approach to the nonlinear mapping is to use a radial basis function network (RBF) to create a nonlinear mapping between the latent space and the data space. The nodes of the \nRBF network then form a feature space and the nonlinear mapping can then be taken as a linear transform of this feature space. This approach has the advantage over the suggested density network approach that it can be optimised analytically.\n\nIn data analysis, GTMs are like a nonlinear version of principal components analysis, which allows high-dimensional data to be modelled as resulting from Gaussian noise added to sources in lower-dimensional latent space. For example, to locate stocks in plottable 2D space based on their hi-D time-series shapes. Other applications may want to have fewer sources than data points, for example mixture models.\n\nIn generative deformational modelling, the latent and data spaces have the same dimensions, for example, 2D images or 1 audio sound waves. Extra 'empty' dimensions are added to the source (known as the 'template' in this form of modelling), for example locating the 1D sound wave in 2D space. Further nonlinear dimensions are then added, produced by combining the original dimensions. The enlarged latent space is then projected back into the 1D data space. The probability of a given projection is, as before, given by the product of the likelihood of the data under the Gaussian noise model with the prior on the deformation parameter. Unlike conventional spring-based deformation modelling, this has the advantage of being analytically optimizable. The disadvantage is that it is a 'data-mining' approach, i.e. the shape of the deformation prior is unlikely to be meaningful as an explanation of the possible deformations, as it is based on a very high, artificial- and arbitrarily constructed nonlinear latent space. For this reason the prior is learned from data rather than created by a human expert, as is possible for spring-based models.\n\nWhile nodes in the self-organizing map (SOM) can wander around at will, GTM nodes are constrained by the allowable transformations and their probabilities. If the deformations are well-behaved the topology of the latent space is preserved.\nThe SOM was created as a biological model of neurons and is a heuristic algorithm. By contrast, the GTM has nothing to do with neuroscience or cognition and is a probabilistically principled model. Thus, it has a number of advantages over SOM, namely:\n\nGTM was introduced by Bishop, Svensen and Williams in their Technical Report in 1997 (Technical Report NCRG/96/015, Aston University, UK) published later in Neural Computation. It was also described in the PhD thesis of Markus Svensen (Aston, 1998).\n\n\n",
    "id": "1091054",
    "title": "Generative topographic map"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35323360",
    "text": "Grossberg network\n\nGrossberg network is a artificial neural network introduced by Stephen Grossberg. It is a self organizing, competitive network based on continuous time. Grossberg, a neuroscientist and a biomedical engineer, designed this network based on the human visual system.\n\nShunting model is one of the Grossberg's neural network model based on Leaky integrator, given by the expression:\n",
    "id": "35323360",
    "title": "Grossberg network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13793747",
    "text": "Group method of data handling\n\nGroup method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models.\n\nGMDH is used in such fields as data mining, knowledge discovery, prediction, complex systems modeling, optimization and pattern recognition. Li et. al. (2017)'s results showed that GMDH neural network performed better than the classical forecasting algorithms such as Single Exponential Smooth, Double Exponential Smooth, ARIMA and back-propagation neural network. \n\nGMDH algorithms are characterized by inductive procedure that performs sorting-out of gradually complicated polynomial models and selecting the best solution by means of the so-called \"external criterion\".\n\nA GMDH model with multiple inputs and one output is a subset of components of the \"base function\" (1):\n\nwhere \"f\" are elementary functions dependent on different sets of inputs, \"a\" are coefficients and \"m\" is the number of the base function components.\n\nIn order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called \"partial models\". Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an \"external criterion\". This process is called self-organization of models.\n\nThe most popular base function used in GMDH is the gradually complicated Kolmogorov-Gabor polynomial (2):\n\nThe resulting models are also known as polynomial neural networks. Jürgen Schmidhuber cites GDMH as one of the earliest deep learning methods, remarking that it was used to train eight-layer neural nets as early as 1971.\n\nThe method was originated in 1968 by Prof. Alexey G. Ivakhnenko in the Institute of Cybernetics in Kiev (then in the Ukrainian SSR).\nThis approach from the very beginning was a computer-based method so, a set of computer programs and algorithms were the primary practical results achieved at the base of the new theoretical principles. Thanks to the author's policy of open code sharing the method was quickly settled in the large number of scientific laboratories worldwide. At that time code sharing was quite a physical action since the Internet is at least 5 years younger than GMDH. Despite this fact the first investigation of GMDH outside the Soviet Union had been made soon by R.Shankar in 1972. Later on different GMDH variants were published by Japanese and Polish scientists.\n\nPeriod 1968-1971 is characterized by application of only regularity criterion for solving of the problems of identification, pattern recognition and short-term forecasting. As reference functions polynomials, logical nets, fuzzy Zadeh sets and Bayes probability formulas were used. Authors were stimulated by very high accuracy of forecasting with the new approach. Noiseimmunity was not investigated.\n\nPeriod 1972-1975. The problem of modeling of noised data and incomplete information basis was solved. Multicriteria selection and utilization of additional priory information for noiseimmunity increasing were proposed. Best experiments showed that with extended definition of the optimal model by additional criterion noise level can be ten times more than signal. Then it was improved using Shannon's Theorem of General Communication theory.\n\nPeriod 1976-1979. The convergence of multilayered GMDH algorithms was investigated. It was shown that some multilayered algorithms have \"multilayerness error\" - analogous to static error of control systems. In 1977 a solution of objective systems analysis problems by multilayered GMDH algorithms was proposed. It turned out that sorting-out by criteria ensemble finds the only optimal system of equations and therefore to show complex object elements, their main input and output variables.\n\nPeriod 1980-1988. Many important theoretical results were received. It became clear that full physical models cannot be used for long-term forecasting. It was proved, that non-physical models of GMDH are more accurate for approximation and forecast than physical models of regression analysis. Two-level algorithms which use two different time scales for modeling were developed.\n\nSince 1989 the new algorithms (AC, OCC, PF) for non-parametric modeling of fuzzy objects and SLP for expert systems were developed and investigated. Present stage of GMDH development can be described as blossom out of twice-multilayered neuronets and parallel combinatorial algorithms for multiprocessor computers.\n\nExternal criterion is one of the key features of GMDH. Criterion describes requirements to the model, for example minimization of Least squares. It is always calculated with a separate part of data sample that have not been used for estimation of coefficients. There are several popular criteria:\n\nIf a criterion does not define the number of observations for external dataset then the problem of data dividing ratio appears because the forecasting abilities of identified model are very dependent on the dividing ratio.\n\nFor modeling using GMDH, first, the number of inputs for each neuron, polynomial power and input sources of layers after the first layer are decided. Then, the design process begins from the first layer and goes on. All possible combinations of allowable inputs (all possible neurons) are considered. Then polynomial coefficients are determined using one of the available minimizing methods such as singular value decom-position (with training data). Then, neurons that have better external criterion (for testing data) are kept, and others are removed (The input data for development of the model were divided into training and testing groups). If the external criterion for layer’s best neuron surpasses the stopping criterion, network design is completed and the polynomial expression of the best neuron of the last layer is introduced as the math-ematical prediction function; if not, the next layer will be generated, and this process goes on .\n\nThere are many different ways to choose an order for partial models consideration. The very first consideration order used in GMDH and originally called multilayered inductive procedure is the most popular one. It is a sorting-out of gradually complicated models generated from Kolmogorov-Gabor polynomial. The best model is indicated by the minimum of the external criterion characteristic. Multilayered procedure is equivalent to the Artificial Neural Network with polynomial activation function of neurons. Therefore, the algorithm with such an approach usually referred as GMDH-type Neural Network or Polynomial Neural Network.\n\nAnother important approach to partial models consideration that becomes more and more popular is a brute force combinatorial search that is either limited or full. This approach has some advantages against Polynomial Neural Networks but requires considerable computational power and thus is not effective for objects with more than 30 inputs in case of full search. An important achievement of Combinatorial GMDH is that it fully outperforms linear regression approach if noise level in the input data is greater than zero.\n\nBasic combinatorial algorithm makes the following steps:\n\n\nIn contrast to GMDH-type neural networks Combinatorial algorithm can't be stopped at the certain level of complexity because a point of increase of criterion value can be simply a local minimum, see Fig.1.\n\n\n\n\n",
    "id": "13793747",
    "title": "Group method of data handling"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30867546",
    "text": "Growing self-organizing map\n\nA growing self-organizing map (GSOM) is a growing variant of a self-organizing map (SOM). The GSOM was developed to address the issue of identifying a suitable map size in the SOM. It starts with a minimal number of nodes (usually 4) and grows new nodes on the boundary based on a heuristic. By using the value called Spread Factor (SF), the data analyst has the ability to control the growth of the GSOM.\n\nAll the starting nodes of the GSOM are boundary nodes, i.e. each node has the freedom to grow in its own direction at the beginning. (Fig. 1) New Nodes are grown from the boundary nodes. Once a node is selected for growing all its free neighboring positions will be grown new nodes. The figure shows the three possible node growth options for a rectangular GSOM.\n\nThe GSOM process is as follows:\n\nThe GSOM can be used for many preprocessing tasks in Data mining, for Nonlinear dimensionality reduction, for approximation of principal curves and manifolds, for clustering and classification. It gives often the better representation of the data geometry than the SOM (see the classical benchmark for principal curves on the left).\n\n\n",
    "id": "30867546",
    "title": "Growing self-organizing map"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1222568",
    "text": "Helmholtz machine\n\nThe Helmholtz machine is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. The hope is that by learning economical representations of the data, the underlying structure of the generative model should reasonably approximate the hidden structure of the data set. A Helmholtz machine contains two networks, a bottom-up \"recognition\" network that takes the data as input and produces a distribution over hidden variables, and a top-down \"generative\" network that generates values of the hidden variables and the data itself.\n\nHelmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm.\n\nHelmholtz machines may also be used in applications requiring a supervised learning algorithm (e.g. character recognition, or position-invariant recognition of an object within a field).\n\n\n",
    "id": "1222568",
    "title": "Helmholtz machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1170097",
    "text": "Hopfield network\n\nA Hopfield network is a form of recurrent artificial neural network popularized by John Hopfield in 1982, but described earlier by Little in 1974. Hopfield nets serve as content-addressable (\"associative\") memory systems with binary threshold nodes. They are guaranteed to converge to a local minimum, but will sometimes converge to a false pattern (wrong local minimum) rather than the stored pattern (expected local minimum). Hopfield networks also provide a model for understanding human memory.\n\nThe units in Hopfield nets are binary threshold units, i.e. the units only take on two different values for their states and the value is determined by whether or not the units' input exceeds their threshold. Hopfield nets normally have units that take on values of 1 or -1, and this convention will be used throughout this page. However, other literature might use units that take values of 0 and 1.\n\nEvery pair of units \"i\" and \"j\" in a Hopfield network have a connection that is described by the connectivity weight formula_1. In this sense, the Hopfield network can be formally described as a complete undirected graph formula_2, where formula_3 is a set of McCulloch-Pitts neurons and formula_4 is a function that links pairs of nodes to a real value, the connectivity weight.\n\nThe connections in a Hopfield net typically have the following restrictions:\n\nThe constraint that weights be symmetric guarantees that the energy function decreases monotonically while following the activation rules. A network with asymmetric weights may exhibit some periodic or chaotic behaviour; however, Hopfield found that this behavior is confined to relatively small parts of the phase space and does not impair the network's ability to act as a content-addressable associative memory system.\n\nUpdating one unit (node in the graph simulating the artificial neuron) in the Hopfield network is performed using the following rule:\n\nformula_7\n\nwhere:\n\nUpdates in the Hopfield network can be performed in two different ways:\n\nThe weight between two units has a powerful impact upon the values of the neurons. Consider the connection weight formula_8 between two neurons i and j. If formula_12, the updating rule implies that:\n\nThus, the values of neurons i and j will converge if the weight between them is positive. Similarly, they will diverge if the weight is negative.\n\nHopfield nets have a scalar value associated with each state of the network referred to as the \"energy\", E, of the network, where:\n\nThis value is called the \"energy\" because: the definition ensures that when units are randomly chosen to update, the energy E will either lower in value or stay the same. Furthermore, under repeated updating the network will eventually converge to a state which is a local minimum in the energy function (which is considered to be a Lyapunov function). Thus, if a state is a local minimum in the energy function, it is a stable state for the network. Note that this energy function belongs to a general class of models in physics, under the name of Ising models; these in turn are a special case of Markov networks, since the associated probability measure, the Gibbs measure, has the Markov property.\n\nInitialization of the Hopfield Networks is done by setting the values of the units to the desired start pattern. Repeated updates are then performed until the network converges to an attractor pattern. Convergence is generally assured, as Hopfield proved that the attractors of this nonlinear dynamical system are stable, not periodic or chaotic as in some other systems. Therefore, in the context of Hopfield Networks, an attractor pattern is a final stable state, a pattern that cannot change any value within it under updating.\n\nTraining a Hopfield net involves lowering the energy of states that the net should \"remember\". This allows the net to serve as a content addressable memory system, that is to say, the network will converge to a \"remembered\" state if it is given only part of the state. The net can be used to recover from a distorted input to the trained state that is most similar to that input. This is called associative memory because it recovers memories on the basis of similarity. For example, if we train a Hopfield net with five units so that the state (1, -1, 1, -1, 1) is an energy minimum, and we give the network the state (1, -1, -1, -1, 1) it will converge to (1, -1, 1, -1, 1). Thus, the network is properly trained when the energy of states which the network should remember are local minima.\n\nThere are various different learning rules that can be used to store information in the memory of the Hopfield Network. It is desirable for a learning rule to have both of the following two properties:\n\nThese properties are desirable, since a learning rule satisfying them is more biologically plausible. For example, since the human brain is always learning new concepts, one can reason that human learning is incremental. A learning system that were not incremental would generally be trained only once, with a huge batch of training data.\n\nThe Hebbian Theory was introduced by Donald Hebb in 1949, in order to explain \"associative learning\", in which simultaneous activation of neuron cells leads to pronounced increases in synaptic strength between those cells. It is often summarized as \"Neurons that fire together, wire together. Neurons that fire out of sync, fail to link\".\n\nThe Hebbian rule is both local and incremental. For the Hopfield Networks, it is implemented in the following manner, when learning formula_20\nbinary patterns:\n\nformula_21\n\nwhere formula_22 represents bit i from pattern formula_23.\n\nIf the bits corresponding to neurons i and j are equal in pattern formula_23, then the product formula_25 will be positive. This would, in turn, have a positive effect on the weight formula_26 and the values of i and j will tend to become equal. The opposite happens if the bits corresponding to neurons i and j are different.\n\nThis rule was introduced by Amos Storkey in 1997 and is both local and incremental. Storkey also showed that a Hopfield network trained using this rule has a greater capacity than a corresponding network trained using the Hebbian rule. The weight matrix of an attractor neural network is said to follow the Storkey learning rule if it obeys:\n\nformula_27\n\nwhere formula_28 is a form of \"local field\" at neuron i.\nThis learning rule is local, since the synapses take into account only neurons at their sides. The rule makes use of more information from the patterns and weights than the generalized Hebbian rule, due to the effect of the local field.\n\nPatterns that the network uses for training (called \"retrieval states\") become attractors of the system. Repeated updates would eventually lead to convergence to one of the retrieval states. However, sometimes the network will converge to spurious patterns (different from the training patterns). The energy in these spurious patterns is also a local minimum. For each stored pattern x, the negation -x is also a spurious pattern.\n\nA spurious state can also be a linear combination of an odd number of retrieval states. For example, when using 3 patterns formula_29, one can get the following spurious state:\n\nformula_30\n\nSpurious patterns that have an even number of states cannot exist, since they might sum up to zero \n\nThe Network capacity of the Hopfield network model is determined by neuron amounts and connections within a given network. Therefore, the number of memories that are able to be stored is dependent on neurons and connections. Furthermore, it was shown that the recall accuracy between vectors and nodes was 0.138 (approximately 138 vectors can be recalled from storage for every 1000 nodes) (Hertz et al., 1991). Therefore, it is evident that many mistakes will occur if one tries to store a large number of vectors. When the Hopfield model does not recall the right pattern, it is possible that an intrusion has taken place, since semantically related items tend to confuse the individual, and recollection of the wrong pattern occurs. Therefore, the Hopfield network model is shown to confuse one stored item with that of another upon retrieval. Perfect recalls and high capacity, >0.14, can be loaded in the network by Hebbian learning method.\n\nThe Hopfield model accounts for associative memory through the incorporation of memory vectors. Memory vectors can be slightly used, and this would spark the retrieval of the most similar vector in the network. However, we will find out that due to this process, intrusions can occur. In associative memory for the Hopfield network, there are two types of operations: auto-association and hetero-association. The first being when a vector is associated with itself, and the latter being when two different vectors are associated in storage. Furthermore, both types of operations are possible to store within a single memory matrix, but only if that given representation matrix is not one or the other of the operations, but rather the combination (auto-associative and hetero-associative) of the two. It is important to note that Hopfield’s network model utilizes the same learning rule as Hebb’s (1949) learning rule, which basically tried to show that learning occurs as a result of the strengthening of the weights by when activity is occurring.\n\nRizzuto and Kahana (2001) were able to show that the neural network model can account for repetition on recall accuracy by incorporating a probabilistic-learning algorithm. During the retrieval process, no learning occurs. As a result, the weights of the network remain fixed, showing that the model is able to switch from a learning stage to a recall stage. By adding contextual drift we are able to show the rapid forgetting that occurs in a Hopfield model during a cued-recall task. The entire network contributes to the change in the activation of any single node.\n\nMcCulloch and Pitts' (1943) dynamical rule, which describes the behavior of neurons, does so in a way that shows how the activations of multiple neurons map onto the activation of a new neuron’s firing rate, and how the weights of the neurons strengthen the synaptic connections between the new activated neuron (and those that activated it). Hopfield would use McCulloch-Pitts's dynamical rule in order to show how retrieval is possible in the Hopfield network. However, it is important to note that Hopfield would do so in a repetitious fashion. Hopfield would use a nonlinear activation function, instead of using a linear function. This would therefore create the Hopfield dynamical rule and with this, Hopfield was able to show that with the nonlinear activation function, the dynamical rule will always modify the values of the state vector in the direction of one of the stored patterns.\n\n\n\n",
    "id": "1170097",
    "title": "Hopfield network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34676768",
    "text": "Hybrid Kohonen self-organizing map\n\nIn artificial neural networks, a hybrid Kohonen self-organizing map is a type of self-organizing map (SOM) named for the Finnish professor Teuvo Kohonen, where the network architecture consists of an input layer fully connected to a 2–D SOM or Kohonen layer. \n\nThe output from the Kohonen layer, which is the winning neuron, feeds into a hidden layer and finally into an output layer. In other words, the Kohonen SOM is the front–end, while the hidden and output layer of a multilayer perceptron is the back–end of the\nhybrid Kohonen SOM. The hybrid Kohonen SOM was first applied to machine vision systems for image classification and recognition. \n\nHybrid Kohonen SOM has been used in weather prediction and especially in forecasting stock prices, which has made a challenging task considerably easier. It is fast and efficient with less classification error, hence is a better predictor, when compared to Kohonen SOM and backpropagation networks.\n",
    "id": "34676768",
    "title": "Hybrid Kohonen self-organizing map"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21573718",
    "text": "HyperNEAT\n\nHypercube-based NEAT, or HyperNEAT, is a generative encoding that evolves artificial neural networks (ANNs) with the principles of the widely used NeuroEvolution of Augmented Topologies (NEAT) algorithm. It is a novel technique for evolving large-scale neural networks using the geometric regularities of the task domain. It uses Compositional Pattern Producing Networks (CPPNs), which are used to generate the images for Picbreeder.org and shapes for EndlessForms.com. HyperNEAT has recently been extended to also evolve plastic ANNs and to evolve the location of every neuron in the network.\n\n\n",
    "id": "21573718",
    "title": "HyperNEAT"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9835466",
    "text": "Infomax\n\nInfomax is an optimization principle for artificial neural networks and other information processing systems. It prescribes that a function that maps a set of input values \"I\" to a set of output values \"O\" should be chosen or learned so as to maximize the average Shannon mutual information between \"I\" and \"O\", subject to a set of specified constraints and/or noise processes. Infomax algorithms are learning algorithms that perform this optimization process. The principle was described by Linsker in 1988.\n\nInfomax, in its zero-noise limit, is related to the principle of redundancy reduction proposed for biological sensory processing by Horace Barlow in 1961, and applied quantitatively to retinal processing by Atick and Redlich.\n\nOne of the applications of infomax has been to an independent component analysis algorithm that finds independent signals by maximizing entropy. Infomax-based ICA was described by Bell and Sejnowski, and Nadal and Parga in 1995. \n\n\n",
    "id": "9835466",
    "title": "Infomax"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7437547",
    "text": "Interactive activation and competition networks\n\nInteractive activation and competition (IAC) networks are artificial neural networks used to model memory and intuitive generalizations. They are made up of nodes or artificial neurons which are arrayed and activated in ways that emulate the behaviors of human memory.\n\nThe IAC model is used by the parallel distributed processing (PDP) Group and is associated with James L. McClelland and David E. Rumelhart; it is described in detail in their book \"Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises\". This model does not contradict any currently known biological data or theories, and its performance is close enough to human performance as to warrant further investigation.\n\n",
    "id": "7437547",
    "title": "Interactive activation and competition networks"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42639996",
    "text": "Jpred\n\nJpred v.4 is the latest version of the popular JPred Protein Secondary Structure Prediction Server which provides predictions by the JNet algorithm, one of the most accurate methods for secondary structure prediction, that has existed since 1998 in different versions.\n\nIn addition to protein secondary structure, JPred also makes predictions of solvent accessibility and coiled-coil regions. The JPred service runs up to 134 000 jobs per month and has carried out over 2 million predictions in total for users in 179 countries.\n\nThe current version of JPred (v4) has the following improvements and updates incorporated:\n\nThe JPred v3 followed on from previous versions of JPred developed and maintained by James Cuff and Jonathan Barber (see JPred References). This release added new functionality and fixed lots of bugs. The highlights are:\n\nThe static HTML pages of JPred 2 are still available for reference.\n\nSequence residues are categorised or assigned to one of the secondary structure elements, such as alpha-helix, beta-sheet and coiled-coil.\n\nJnet uses two neural networks for its prediction. The first network is fed with a window of 17 residues over each amino acid in the alignment plus a conservation number. It uses a hidden layer of nine nodes and has three output nodes, one for each secondary structure element.\nThe second network is fed with a window of 19 residues (the result of first network) plus the conservation number. It has a hidden layer with nine nodes and has three output nodes.\n\n",
    "id": "42639996",
    "title": "Jpred"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7049330",
    "text": "Leabra\n\nLeabra stands for local, error-driven and associative, biologically realistic algorithm. It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in \"emergent\" (successor of PDP++) when making a new project, and is extensively used in various simulations.\n\nHebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels.\n\nError-driven learning is performed using GeneRec, which is a generalization of the recirculation algorithm, and approximates Almeida–Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details.\n\nThe activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output.\n\nLayer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations.\n\nThe net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections.\n\nDocumentation about this algorithm can be found in the book \"Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain\" published by MIT press. and in the Emergent Documentation\n\nThe pseudocode for Leabra is given here, showing exactly how the\npieces of the algorithm described in more detail in the subsequent\nsections fit together.\n\ncodice_1\n\nEmergent is the original implementation of Leabra, written in C++ and highly optimized. This is the fastest implementation, suitable for constructing large networks. Although \"emergent\" has a graphical user interface, it is very complex and has a steep learning curve.\n\nIf you want to understand the algorithm in detail, it will be easier to read non-optimzed code. For this purpose, check out the MATLAB version. There is also an R version available, that can be easily installed via codice_2 in R and has a short introduction to how the package is used. The MATLAB and R versions are not suited for constructing very large networks, but they can be installed quickly and (with some programming background) are easy to use. Furthermore, they can also be adapted easily.\n\n\n",
    "id": "7049330",
    "title": "Leabra"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35699507",
    "text": "Learning rule\n\nLearning rule or Learning process is a method or a mathematical logic which improves the artificial neural network's performance and usually this rule is applied repeatedly over the network. It is done by updating the levels of a network when a network is simulated in a specific data environment. A learning rule may accept existing condition ( weights and bias ) of the network and will compare the expected result and actual result of the network to give new and improved values for weights and bias. Depending on the complexity of actual model, which is being simulated, the learning rule of the network can be as simple as an XOR gate or Mean Squared Error or it can be the result of multiple differential equations. The learning rule is one of the factors which decides how fast or how accurate the artificial network can be developed. Depending upon the process to develop the network there are three main models of machine learning:\n\n\n",
    "id": "35699507",
    "title": "Learning rule"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=827635",
    "text": "Learning vector quantization\n\nIn computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.\n\nLVQ can be understood as a special case of an artificial neural network, more precisely, it applies a winner-take-all Hebbian learning-based approach. It is a precursor to self-organizing maps (SOM) and related to neural gas, and to the k-nearest neighbor algorithm (k-NN). LVQ was invented by Teuvo Kohonen.\n\nAn LVQ system is represented by prototypes formula_1 which are defined in the feature space of observed data. In winner-take-all training algorithms one determines, for each data point, the prototype which is closest to the input according to a given distance measure. The position of this so-called winner prototype is then adapted, i.e. the winner is moved closer if it correctly classifies the data point or moved away if it classifies the data point incorrectly.\n\nAn advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the respective application domain.\nLVQ systems can be applied to multi-class classification problems in a natural way. \nIt is used in a variety of practical applications. See http://liinwww.ira.uka.de/bibliography/Neural/SOM.LVQ.html\nfor an extensive bibliography.\n\nA key issue in LVQ is the choice of an appropriate measure of distance or similarity for training and classification. Recently, techniques have been developed which adapt a parameterized distance measure in the course of training the system, see e.g. (Schneider, Biehl, and Hammer, 2009) and references therein.\n\nLVQ can be a source of great help in classifying text documents.\n\nBelow follows an informal description.<br>\nThe algorithm consists of 3 basic steps. The algorithm's input is:\n\nThe algorithm's flow is:\n\nNote: formula_3 and formula_9 are vectors in feature space.<br>\nA more formal description can be found here: http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/\n\n\n",
    "id": "827635",
    "title": "Learning vector quantization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7583202",
    "text": "Lernmatrix\n\nLernmatrix, an associative-memory-like architecture of an artificial neural network, invented around 1960 by Karl Steinbuch.\n\n",
    "id": "7583202",
    "title": "Lernmatrix"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1796414",
    "text": "Linde–Buzo–Gray algorithm\n\nThe Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.\n\nIt is similar to the k-means method in data clustering.\n\nAt each iteration, each vector is split into two new vectors.\n\n\n\n",
    "id": "1796414",
    "title": "Linde–Buzo–Gray algorithm"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7823692",
    "text": "Liquid state machine\n\nA liquid state machine (LSM) is a particular kind of spiking neural network. An LSM consists of a large collection of units (called \"nodes\", or \"neurons\"). Each node receives time varying input from external sources (the inputs) as well as from other nodes. Nodes are randomly connected to each other. The recurrent nature of the connections turns the time varying input into a spatio-temporal pattern of activations in the network nodes. The spatio-temporal patterns of activation are read out by linear discriminant units.\n\nThe soup of recurrently connected nodes will end up computing a large variety of nonlinear functions on the input. Given a large enough variety of such nonlinear functions, it is theoretically possible to obtain linear combinations (using the read out units) to perform whatever mathematical operation is needed to perform a certain task, such as speech recognition or computer vision.\n\nThe word liquid in the name comes from the analogy drawn to dropping a stone into a still body of water or other liquid. The falling stone will generate ripples in the liquid. The input (motion of the falling stone) has been converted into a spatio-temporal pattern of liquid displacement (ripples).\n\nLSMs have been put forward as a way to explain the operation of brains. LSMs are argued to be an improvement over the theory of artificial neural networks because:\n\n\nCriticisms of LSMs as used in computational neuroscience are that\n\nIf a reservoir has fading memory and input separability, with help of a readout,\nit can be proven the liquid state machine is a universal function approximator using Stone-Weierstrass theorem.\n\n\n\n",
    "id": "7823692",
    "title": "Liquid state machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10711453",
    "text": "Long short-term memory\n\nLong short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). An RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three \"gates\" can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as \"regulators\" of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n\nThe expression \"long short-term\" refers to the fact that LSTM is a model for the \"short-term memory\" which can last for a \"long\" period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs. Relative insensitivity to gap length gives an advantage to LSTM over alternative RNNs, hidden Markov models and other sequence learning methods in numerous applications .\n\nLSTM was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber and improved in 2000 by Felix Gers' team.\n\nAmong other successes, LSTM achieved record results in natural language text compression, unsegmented connected handwriting recognition and won the ICDAR handwriting competition (2009). LSTM networks were a major component of a network that achieved a record 17.7% phoneme error rate on the classic TIMIT natural speech dataset (2013).\n\nAs of 2016, major technology companies including Google, Apple, and Microsoft were using LSTM as fundamental components in new products. For example, Google used LSTM for speech recognition on the smartphone, for the smart assistant Allo and for Google Translate. Apple uses LSTM for the \"Quicktype\" function on the iPhone and for Siri. Amazon uses LSTM for Amazon Alexa.\n\nIn 2017 Microsoft reported reaching 95.1% recognition accuracy on the Switchboard corpus, incorporating a vocabulary of 165,000 words. The approach used \"dialog session-based long-short-term memory\".\n\nThere are several architectures of LSTM units. A common architecture is composed of a memory \"cell\", an \"input gate\", an \"output gate\" and a \"forget gate\".\n\nAn LSTM (memory) cell stores a value (or state), for either long or short time periods. This is achieved by using an identity (or no) activation function for the memory cell. In this way, when an LSTM network (that is an RNN composed of LSTM units) is trained with backpropagation through time, the gradient does not tend to vanish. \n\nThe LSTM gates compute an activation, often using the logistic function. Intuitively, the \"input gate\" controls the extent to which a new value flows into the cell, the \"forget gate\" controls the extent to which a value remains in the cell and the \"output gate\" controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit.\n\nThere are connections into and out of these gates. A few connections are recurrent. The weights of these connections, which need to be learned during training, of an LSTM unit are used to direct the operation of the gates. Each of the gates has its own parameters, that is weights and biases, from possibly other units outside the LSTM unit.\n\nIn the equations below, each variable in lowercase italics represents a vector. Matrices formula_1 and formula_2 collect respectively the weights of the input and recurrent connections, \nwhere formula_3 can either be the input gate formula_4, output gate formula_5, the forget gate formula_6 or the memory cell formula_7, depending on the activation being calculated.\n\nCompact form of the equations for the forward pass of a LSTM unit with a forget gate. \n\nwhere the initial values are formula_9 and formula_10 and the operator formula_11 denotes the Hadamard product (entry-wise product). The subscripts formula_12 refer to the time step.\n\n\n\nThe figure on the right is a graphical representation of a LSTM unit with peephole connections (i.e. a peephole LSTM). Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the cell state. formula_26 is not used, formula_27 is used instead in most places.\n\nConvolutional LSTM. formula_29 denotes the convolution operator.\n\nTo minimize LSTM's total error on a set of training sequences, iterative gradient descent such as backpropagation through time can be used to change each weight in proportion to its derivative with respect to the error. A problem with using gradient descent for standard RNNs is that error gradients vanish exponentially quickly with the size of the time lag between important events. This is due to formula_31 if the spectral radius of formula_32 is smaller than 1. With LSTM units, however, when error values are back-propagated from the output, the error remains in the unit's memory. This \"error carousel\" continuously feeds error back to each of the gates until they learn to cut off the value. Thus, regular backpropagation is effective at training an LSTM unit to remember values for long durations.\n\nLSTM can also be trained by a combination of artificial evolution for weights to the hidden units, and pseudo-inverse or support vector machines for weights to the output units. In reinforcement learning applications LSTM can be trained by policy gradient methods, evolution strategies or genetic algorithms.\n\nMany applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nApplications of LSTM include:\n\nLSTM has Turing completeness in the sense that given enough network units it can compute any result that a conventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program.\n\n\n",
    "id": "10711453",
    "title": "Long short-term memory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=17319790",
    "text": "Modular neural network\n\nA modular neural network is an artificial neural network characterized by a series of independent neural networks moderated by some intermediary. Each independent neural network serves as a module and operates on separate inputs to accomplish some subtask of the task the network hopes to perform. The intermediary takes the outputs of each module and processes them to produce the output of the network as a whole. The intermediary only accepts the modules' outputs—it does not respond to, nor otherwise signal, the modules. As well, the modules do not interact with each other.\n\nAs artificial neural network research progresses, it is appropriate that artificial neural networks continue to draw on their biological inspiration and emulate the segmentation and modularization found in the brain. The brain, for example, divides the complex task of visual perception into many subtasks. Within a part of the brain, called the thalamus, lies the lateral geniculate nucleus (LGN), which is divided into layers that separately process color and contrast: both major components of vision. After the LGN processes each component in parallel, it passes the result to another region to compile the results.\n\nSome tasks that the brain handles, like vision, employ a hierarchy of sub-networks. However, it is not clear whether some intermediary ties these separate processes together. Rather, as the tasks grow more abstract, the modules communicate with each other, unlike the modular neural network model.\n\nUnlike a single large network that can be assigned to arbitrary tasks, each module in a modular network must be assigned a specific task and connected to other modules in specific ways by a designer. In the vision example, the brain evolved (rather than learned) to create the LGN. In some cases, the designer may choose to follow biological models. In other cases, other models may be superior. The quality of the result will be a function of the quality of the design.\n\nModular neural networks reduce a single large, unwieldy neural network to smaller, potentially more manageable components. Some tasks are intractably large for a single neural network. The benefits of modular neural networks include:\n\nThe possible neuron (node) connections increase exponentially as nodes are added to a network. Computation time depends on the number of nodes and their connections, any increase has drastic consequences for processing time. Assigning specific subtasks to individual modules reduce the number of necessary connections.\n\nA large neural network attempting to model multiple parameters can suffer from interference as new data can alter existing connections or just serve to confuse. Each module can be trained independently and more precisely master its simpler task. This means the training algorithm and the training data can be implemented more quickly.\n\nRegardless of whether a large neural network is biological or artificial, it remains largely susceptible to interference at and failure in any one of its nodes. By compartmentalizing subtasks, failure and interference are much more readily diagnosed and their effects on other sub-networks are eliminated as each one is independent of the other.\n\n",
    "id": "17319790",
    "title": "Modular neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5847907",
    "text": "MoneyBee\n\nMoneyBee was a distributed computing project in the fields of economics, finance and stock markets, that generated stock forecasts by application of artificial intelligence with the aid of artificial neural networks. MoneyBee acted as a screensaver. The project was run by i42 Informationsmanagement GmbH, a consulting private company from Mannheim, Germany. The project was suspended with a standing invitation for any interested in joining the MoneyBee2 project, but MoneyBee2 seems to have been abandoned in early 2010.\n\nThe idea was first conceived in an economics thesis on stock forecasts with the aid of artificial neural networks at the University of Heidelberg. The MoneyBee system was then developed by i42 GmbH, Mannheim, who added some innovative features to the original idea.\n\nSince starting in September 2000, many users downloaded the German version. By 2001, about 12,000 users were generating 0.1 Teraflops of computing capacity – comparable to an average university mainframe computer at the time.\n\n\n",
    "id": "5847907",
    "title": "MoneyBee"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2266644",
    "text": "Multilayer perceptron\n\nA multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n\nMultilayer perceptrons are sometimes colloquially referred to as \"vanilla\" neural networks, especially when they have a single hidden layer.\n\n<section begin=theory />\nIf a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then linear algebra shows that any number of layers can be reduced to a two-layer input-output model. In MLPs some neurons use a \"nonlinear\" activation function that was developed to model the frequency of action potentials, or firing, of biological neurons.\n\nThe two common activation functions are both sigmoids, and are described by\n\nThe first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here formula_2 is the output of the formula_3th node (neuron) and formula_4 is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).\n\nThe MLP consists of three or more layers (an input and an output layer with one or more \"hidden layers\") of nonlinearly-activating nodes making it a deep neural network. Since MLPs are fully connected, each node in one layer connects with a certain weight formula_5 to every node in the following layer. \n\nLearning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.\n\nWe represent the error in output node formula_6 in the formula_7th data point (training example) by formula_8, where formula_9 is the target value and formula_10 is the value produced by the perceptron. The node weights are adjusted based on corrections that minimize the error in the entire output, given by\n\nUsing gradient descent, the change in each weight is\n\nwhere formula_2 is the output of the previous neuron and formula_14 is the \"learning rate\", which is selected to ensure that the weights quickly converge to a response, without oscillations.\n\nThe derivative to be calculated depends on the induced local field formula_15, which itself varies. It is easy to prove that for an output node this derivative can be simplified to\n\nwhere formula_17 is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is\n\nThis depends on the change in weights of the formula_19th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.\n<section end=theory />\n\nThe term \"multilayer perceptron\" does not refer to a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organized into layers. An alternative is \"multilayer perceptron network\". Moreover, MLP \"perceptrons\" are not perceptrons in the strictest possible sense. True perceptrons are formally a special case of artificial neurons that use a threshold activation function such as the Heaviside step function. MLP perceptrons can employ arbitrary activation functions. A true perceptron performs binary classification (either this or that), an MLP neuron is free to either perform classification or regression, depending upon its activation function.\n\nThe term \"multilayer perceptron\" later was applied without respect to nature of the nodes/layers, which can be composed of arbitrarily defined artificial neurons, and not perceptrons specifically. This interpretation avoids the loosening of the definition of \"perceptron\" to mean an artificial neuron in general.\n\nMLPs are useful in research for their ability to solve problems stochastically, which often allows approximate solutions for extremely complex problems like fitness approximation.\n\nMLPs are universal function approximators as showed by Cybenko's theorem, so they can be used to create mathematical models by regression analysis. As classification is a particular case of regression when the response variable is categorical, MLPs make good classifier algorithms.\n\nMLPs were a popular machine learning solution in the 1980s, finding applications in diverse fields such as speech recognition, image recognition, and machine translation software, but thereafter faced strong competition from much simpler (and related) support vector machines. Interest in backpropagation networks returned due to the successes of deep learning.\n\n",
    "id": "2266644",
    "title": "Multilayer perceptron"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6092601",
    "text": "Neocognitron\n\nThe neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in the 1980s. It has been used for handwritten character recognition and other pattern recognition tasks, and served as the inspiration for convolutional neural networks.\n\nThe neocognitron was inspired by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called \"simple cell\" and \"complex cell\", and also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\nThe neocognitron is a natural extension of these cascading models. The neocognitron consists of multiple types of cells, the most important of which are called \"S-cells\" and \"C-cells.\" The local features are extracted by S-cells, and these features' deformation, such as local shifts, are tolerated by C-cells. Local features in the input are integrated gradually and classified in the higher layers. The idea of local feature integration is found in several other models, such as the \"LeNet\" model and the \"SIFT\" model.\n\nThere are various kinds of neocognitron. For example, some types of neocognitron can detect multiple patterns in the same input by using backward signals to achieve selective attention.\n\n\n\n",
    "id": "6092601",
    "title": "Neocognitron"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3691953",
    "text": "NETtalk (artificial neural network)\n\nNETtalk is an artificial neural network. It is the result of research carried out in the mid-1980s by Terrence Sejnowski and Charles Rosenberg. The intent behind NETtalk was to construct simplified models that might shed light on the complexity of learning human level cognitive tasks, and their implementation as a connectionist model that could also learn to perform a comparable task.\n\nNETtalk is a program that learns to pronounce written English text by being shown text as input and matching phonetic transcriptions for comparison.\n\nNETtalk was created to explore the mechanisms of learning to correctly pronounce English text. The authors note that learning to read involves a complex mechanism involving many parts of the human brain. NETtalk does not specifically model the image processing stages and letter recognition of the visual cortex. Rather, it assumes that the letters have been pre-classified and recognized, and these letter sequences comprising words are then shown to the neural network during training and during performance testing. It is NETtalk's task to learn proper associations between the correct pronunciation with a given sequence of letters based on the context in which the letters appear. In other words, NETtalk learns to use the letters around the currently pronounced phoneme that provide cues as to its intended phonemic mapping.\n\n",
    "id": "3691953",
    "title": "NETtalk (artificial neural network)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14338608",
    "text": "Neural backpropagation\n\nNeural backpropagation is the phenomenon in which the action potential of a neuron creates a voltage spike both at the end of the axon (normal propagation) and back through to the dendritic arbor or dendrites, from which much of the original input current originated. In addition to active backpropagation of the action potential, there is also passive electrotonic spread. While there is ample evidence to prove the existence of backpropagating action potentials, the function of such action potentials and the extent to which they invade the most distal dendrites remains highly controversial.\n\nWhen a neuron fires an action potential, it is initiated at the axon initial segment. An action potential spreads down the axon because of the gating properties of voltage-gated sodium channels and voltage-gated potassium channels. Initially, it was thought that an action potential could only travel down the axon in one direction towards the axon terminal where it ultimately signaled the release of neurotransmitters. However, recent research has provided evidence for the existence of backwards propagating action potentials (Staley 2004).\n\nNeural backpropagation can occur in one of two ways. First, during the initiation of an axonal action potential, the cell body, or soma, can become depolarized as well. This depolarization can spread through the cell body towards the dendritic tree where there are voltage-gated sodium channels. The depolarization of these voltage-gated sodium channels can then result in the propagation of a dendritic action potential. Such backpropagation is sometimes referred to as an echo of the forward propagating action potential (Staley 2004). It has also been shown that an action potential initiated in the axon can create a retrograde signal that travels in the opposite direction (Hausser 2000). This impulse travels up the axon eventually causing the cell body to become depolarized, thus triggering the dendritic voltage-gated calcium channels. As described in the first process, the triggering of dendritic voltage-gated calcium channels leads to the propagation of a dendritic action potential.\n\nGenerally, excitatory postsynaptic potentials (EPSPs) from synaptic activation are not large enough to activate the dendritic voltage-gated calcium channels (usually on the order of a couple milliamperes each) so backpropagation is typically believed to happen only when the cell is activated to fire an action potential.\n\nIt is important to note that the strength of backpropagating action potentials varies greatly between different neuronal types (Hausser 2000). Some types of neuronal cells show little to no decrease in the amplitude of action potentials as they invade and travel through the dendritic tree while other neuronal cell types, such as cerebellar Purkinje neurons, exhibit very little action potential backpropagation (Stuart 1997). Additionally, there are other neuronal cell types that manifest varying degrees of amplitude decrement during backpropagation. It is thought that this is due to the fact that each neuronal cell type contains varying numbers of the voltage-gated channels required to propagate a dendritic action potential.\n\nGenerally, synaptic signals that are received by the dendrite are combined in the soma in order to generate an action potential that is then transmitted down the axon toward the next synaptic contact. Thus, the backpropagation of action potentials poses a threat to initiate an uncontrolled positive feedback loop between the soma and the dendrites. For example, as an action potential was triggered, its dendritic echo could enter the dendrite and potentially trigger a second action potential. If left unchecked, an endless cycle of action potentials triggered by their own echo would be created. In order to prevent such a cycle, most neurons have a relatively high density of A-type K+ channels.\n\nA-type K+ channels belong to the superfamily of voltage-gated ion channels and are transmembrane channels that help maintain the cell’s membrane potential (Cai 2007). Typically, they play a crucial role in returning the cell to its resting membrane following an action potential by allowing an inhibitory current of K+ ions to quickly flow out of the neuron. The presence of these channels in such high density in the dendrites explains their inability to initiate an action potential, even during synaptic input. Additionally, the presence of these channels provides a mechanism by which the neuron can suppress and regulate the backpropagation of action potentials through the dendrite (Vetter 2000). Results have indicated a linear increase in the density of A-type channels with increasing distance into the dendrite away from the soma. The increase in the density of A-type channels results in a dampening of the backpropagating action potential as it travels into the dendrite. Essentially, inhibition occurs because the A-type channels facilitate the outflow of K+ ions in order to maintain the membrane potential below threshold levels (Cai 2007). Such inhibition limits EPSP and protects the neuron from entering a never-ending positive-positive feedback loop between the soma and the dendrites.\n\nSince the 1950s, evidence has existed that neurons in the central nervous system generate an action potential, or voltage spike, that travels both through the axon to signal the next neuron and backpropagates through the dendrites sending a retrograde signal to its presynaptic signaling neurons. This current decays significantly with travel length along the dendrites, so effects are predicted to be more significant for neurons whose synapses are near the postsynaptic cell body, with magnitude depending mainly on sodium-channel density in the dendrite. It is also dependent on the shape of the dendritic tree and, more importantly, on the rate of signal currents to the neuron. On average, a backpropagating spike loses about half its voltage after traveling nearly 500 micrometres.\n\nBackpropagation occurs actively in the neocortex, hippocampus, substantia nigra, and spinal cord, while in the cerebellum it occurs relatively passively. This is consistent with observations that synaptic plasticity is much more apparent in areas like the hippocampus, which controls spatial memory, than the cerebellum, which controls more unconscious and vegetative functions.\n\nThe backpropagating current also causes a voltage change that increases the concentration of Ca in the dendrites, an event which coincides with certain models of synaptic plasticity. This change also affects future integration of signals, leading to at least a short-term response difference between the presynaptic signals and the postsynaptic spike.\n\nWhile many questions have yet to be answered in regards to neural backpropagation, there exists a number of hypotheses regarding its function. Some proposed function include involvement in synaptic plasticity, involvement in dendrodendritic inhibition, boosting synaptic responses, resetting membrane potential, retrograde actions at synapses and conditional axonal output.\nBackpropagation is believed to help form LTP (long term potentiation) and Hebbian plasticity at hippocampal synapses. Since artificial LTP induction, using microelectrode stimulation, voltage clamp, etc. requires the postsynaptic cell to be slightly depolarized when EPSPs are elicited, backpropagation can serve as the means of depolarization of the postsynaptic cell.\n\nWhile a backpropagating action potential can presumably cause changes in the weight of the presynaptic connections, there is no simple mechanism for an error signal to propagate through multiple layers of neurons, as in the computer backpropagation algorithm. However, simple linear topologies have shown that effective computation is possible through signal backpropagation in this biological sense.\n\n",
    "id": "14338608",
    "title": "Neural backpropagation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12589161",
    "text": "Neural cryptography\n\nNeural cryptography is a branch of cryptography dedicated to analyzing the application of stochastic algorithms, especially artificial neural network algorithms, for use in encryption and cryptanalysis.\n\nNeural Networks are well known for their ability to selectively explore the solution space of a given problem. This feature finds a natural niche of application in the field of cryptanalysis. At the same time, Neural Networks offer a new approach to attack ciphering algorithms based on the principle that any function could be reproduced by a neural network, which is a powerful proven computational tool that can be used to find the inverse-function of any cryptographic algorithm.\n\nThe ideas of mutual learning, self learning, and stochastic behavior of neural networks and similar algorithms can be used for different aspects of cryptography, like public-key cryptography, solving the key distribution problem using neural network mutual synchronization, hashing or generation of pseudo-random numbers.\n\nAnother idea is the ability of a neural network to separate space in non-linear pieces using \"bias\". It gives different probabilities of activating the neural network or not. This is very useful in the case of Cryptanalysis.\n\nTwo names are used to design the same domain of research: Neuro-Cryptography and Neural Cryptography.\n\nThe first work that it is known on this topic can be traced back to 1995 in an IT Master Thesis.\n\nThere are currently no practical applications due to the recent development of the field, but it could be used specifically where the keys are continually generated and the system (both pairs and the insecure media) is in a continuously evolving mode.\nIn 1995, Sebastien Dourlens applied neural networks cryptanalyze DES by allowing the networks to learn how to invert the S-tables of the DES. The bias in DES studied through Differential Cryptanalysis by Adi Shamir is highlighted. The experiment shows about 50% of the key bits can be found, allowing the complete key to be found in a short time. Hardware application with multi micro-controllers have been proposed due to the easy implementation of multilayer neural networks in hardware.\nOne example of a public-key protocol is given by Khalil Shihab. He describes the decryption scheme and the public key creation that are based on a backpropagation neural network. The encryption scheme and the private key creation process are based on Boolean algebra. This technique has the advantage of small time and memory complexities. A disadvantage is the property of backpropagation algorithms: because of huge training sets, the learning phase of a neural network is very long. Therefore, the use of this protocol is only theoretical so far.\n\nThe most used protocol for key exchange between two parties A and B in the practice is Diffie-Hellman protocol. Neural key exchange, which is based on the synchronization of two tree parity machines, should be a secure replacement for this method.\nSynchronizing these two machines is similar to synchronizing two chaotic oscillators in chaos communications.\n\nThe tree parity machine is a special type of multi-layer feed-forward neural network.\n\nIt consists of one output neuron, K hidden neurons and K*N input neurons. Inputs to the network take 3 values: \nThe weights between input and hidden neurons take the values: \nOutput value of each hidden neuron is calculated as a sum of all multiplications of input neurons and these weights: \nSignum is a simple function, which returns -1,0 or 1: <br>\n\nIf the scalar product is 0, the output of the hidden neuron is mapped to -1 in order to ensure a binary output value. The output of neural network is then computed as the multiplication of all values produced by hidden elements: <br>\nOutput of the tree parity machine is binary.\n\nEach party (A and B) uses its own tree parity machine. Synchronization of the tree parity machines is achieved in these steps\n\nAfter the full synchronization is achieved (the weights w of both tree parity machines are same), A and B can use their weights as keys.<br>\nThis method is known as a bidirectional learning.<br> \nOne of the following learning rules can be used for the synchronization:\n\nWhere:\nAnd:\n\nIn every attack it is considered, that the attacker E can eavesdrop messages between the parties A and B, but does not have an opportunity to change them.\n\nTo provide a brute force attack, an attacker has to test all possible keys (all possible values of weights wij). By K hidden neurons, K*N input neurons and boundary of weights L, this gives (2L+1) possibilities. For example, the configuration K = 3, L = 3 and N = 100 gives us 3*10 key possibilities, making the attack impossible with today’s computer power.\n\nOne of the basic attacks can be provided by an attacker, who owns the same tree parity machine as the parties A and B. He wants to synchronize his tree parity machine with these two parties. In each step there are three situations possible:\nIt has been proven, that the synchronization of two parties is faster than learning of an attacker. It can be improved by increasing of the synaptic depth L of the neural network. That gives this protocol enough security and an attacker can find out the key only with small probability.\n\nFor conventional cryptographic systems, we can improve the security of the protocol by increasing of the key length. In the case of neural cryptography, we improve it by increasing of the synaptic depth L of the neural networks. Changing this parameter increases the cost of a successful attack exponentially, while the effort for the users grows polynomially. Therefore, breaking the security of neural key exchange belongs to the complexity class NP.\n\nAlexander Klimov, Anton Mityaguine, and Adi Shamir say that the original neural synchronization scheme can be broken by at least three different attacks—geometric, probabilistic analysis, and using genetic algorithms. Even though this particular implementation is insecure, the ideas behind chaotic synchronization could potentially lead to a secure implementation.\n\nThe permutation parity machine is a binary variant of the tree parity machine.\n\nIt consists of one input layer, one hidden layer and one output layer. The number of neurons in the output layer depends on the number of hidden units K. Each hidden neuron has N binary input neurons: \nThe weights between input and hidden neurons are also binary: \n\nOutput value of each hidden neuron is calculated as a sum of all exclusive disjunctions (exclusive or) of input neurons and these weights:\n\n(⊕ means XOR).\n\nThe function formula_18 is a threshold function, which returns 0 or 1: <br>\n\nThe output of neural network with two or more hidden neurons can be computed as the exclusive or of the values produced by hidden elements: <br>\nOther configurations of the output layer for K>2 are also possible.\n\nThis machine has proven to be robust enough against some attacks so it could be used as a cryptographic mean, but it has been shown to be vulnerable to a probabilistic attack.\n\nA quantum computer is a device that uses quantum mechanisms for computation. In this device the data are stored as qubits (quantum binary digits). That gives a quantum computer in comparison with a conventional computer the opportunity to solve complicated problems in a short time, e.g. discrete logarithm problem or factorization. Algorithms that are not based on any of these number theory problems are being searched because of this property.\n\nNeural key exchange protocol is not based on any number theory.\nIt is based on the difference between unidirectional and bidirectional synchronization of neural networks.\nTherefore, something like the neural key exchange protocol could give rise to potentially faster key exchange schemes.\n\n\n",
    "id": "12589161",
    "title": "Neural cryptography"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2457021",
    "text": "Neural gas\n\nNeural gas is an artificial neural network, inspired by the self-organizing map and introduced in 1991 by Thomas Martinetz and Klaus Schulten. The neural gas is a simple algorithm for finding optimal data representations based on feature vectors. The algorithm was coined \"neural gas\" because of the dynamics of the feature vectors during the adaptation process, which distribute themselves like a gas within the data space. It is applied where data compression or vector quantization is an issue, for example speech recognition, image processing or pattern recognition. As a robustly converging alternative to the k-means clustering it is also used for cluster analysis.\n\nGiven a probability distribution formula_1 of data vectors formula_2 and a finite number of feature vectors formula_3.\n\nWith each time step formula_4, a data vector formula_2 randomly chosen from formula_1 is presented. Subsequently, the distance order of the feature vectors to the given data vector formula_2 is determined. Let formula_8 denote the index of the closest feature vector, formula_9 the index of the second closest feature vector, and formula_10 the index of the feature vector most distant to formula_2. Then each feature vector is adapted according to\n\nformula_12\n\nwith formula_13 as the adaptation step size and formula_14 as the so-called neighborhood range. formula_13 and formula_14 are reduced with increasing formula_4. After sufficiently many adaptation steps the feature vectors cover the data space with minimum representation error.\n\nThe adaptation step of the neural gas can be interpreted as gradient descent on a cost function. By adapting not only the closest feature vector but all of them with a step size decreasing with increasing distance order, compared to (online) k-means clustering a much more robust convergence of the algorithm can be achieved. The neural gas model does not delete a node and also does not create new nodes.\n\nA number of variants of the neural gas algorithm exists in the literature so as to mitigate some of its shortcomings. More notable is perhaps Bernd Fritzke's growing neural gas, but also one should mention further elaborations such as the Growing When Required network and also the incremental growing neural gas.\n\nFritzke describes the growing neural gas (GNG) as an incremental network model that learns topological relations by using a \"Hebb-like learning rule\", only, unlike the neural gas, it has no parameters that change over time and it is capable of continuous learning.\n\nHaving a network with a growing set of nodes, like the one implemented by the GNG algorithm was seen as a great advantage, however some limitation on the learning was seen by the introduction of the parameter λ, in which the network would only be able to grow when iterations were a multiple of this parameter. The proposal to mitigate this problem was a new algorithm, the Growing When Required network (GWR), which would have the network grow more quickly, by adding nodes as quickly as possible whenever the network identified that the existing nodes would not describe the input well enough. \n\nAnother neural gas variant inspired in the GNG algorithm is the incremental growing neural gas (IGNG). The authors propose the main advantage of this algorithm to be \"learning new data (plasticity) without degrading the previously trained network and forgetting the old input data (stability).\" \n\n\n",
    "id": "2457021",
    "title": "Neural gas"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40473285",
    "text": "Neural network synchronization protocol\n\nThe Neural network synchronization protocol, abbreviated as NNSP, is built on the application-level layer of the OSI upon TCP/IP. Aiming at secure communication, this protocol's design make use of a concept called neural network synchronization. Server and each connected client must create special type of neural network called Tree Parity Machine then compute outputs of their neural networks and exchange them in an iterative manner through the public channel. By learning and exchanging public outputs of their networks, client's and server's neural networks will be synchronized, meaning they will have identical synaptic weights, after some time. Once synchronization is achieved, weights of networks are used for secret key derivation. For the purposes of encryption/decryption of subsequent communication this symmetric key is used.\n\n",
    "id": "40473285",
    "title": "Neural network synchronization protocol"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21393064",
    "text": "Neural Networks (journal)\n\nNeural Networks is a monthly peer-reviewed scientific journal and an official journal of the International Neural Network Society, European Neural Network Society, and Japanese Neural Network Society. It was established in 1988 and is published by Elsevier. The journal covers all aspects of research on artificial neural networks. The founding editor-in-chief was Stephen Grossberg (Boston University), the current editors-in-chief are DeLiang Wang (Ohio State University) and Kenji Doya (Okinawa Institute of Science and Technology). The journal is abstracted and indexed in Scopus and the Science Citation Index. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 5.287.\n",
    "id": "21393064",
    "title": "Neural Networks (journal)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=344922",
    "text": "Neuroevolution of augmenting topologies\n\nNeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm (GA) for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (\"complexifying\").\nOn simple control tasks, the NEAT algorithm often arrives at effective networks more quickly than other contemporary neuro-evolutionary techniques and reinforcement learning methods.\n\nTraditionally a neural network topology is chosen by a human experimenter, and effective connection weight values are learned through a training procedure. This yields a situation whereby a trial and error process may be necessary in order to determine an appropriate topology. NEAT is an example of a topology and weight evolving artificial neural network (TWEAN) which attempt to simultaneously learn weight values and an appropriate topology for a neural network. \n\nIn order to encode the network into a phenotype for the GA, NEAT uses a direct encoding scheme which means every connection and neuron is explicitly represented. This is in contrast to indirect encoding schemes which define rules that allow the network to be constructed without explicitly representing every connection and neuron allowing for more compact representation.\n\nThe NEAT approach begins with a perceptron-like feed-forward network of only input neurons and output neurons. As evolution progresses through discrete steps, the complexity of the network's topology may grow, either by inserting a new neuron into a connection path, or by creating a new connection between (formerly unconnected) neurons.\n\nThe competing conventions problem arises when there is more than one way of representing information in a phenotype. For example if a genome contains neurons A, B and C and is represented by [A B C], if this genome is crossed with an identical genome (in terms of functionality) but ordered [C B A] crossover will yield children that are missing information ([A B A] or [C B C]), in fact 1/3 of the information has been lost in this example. NEAT solves this problem by tracking the history of genes by the use of a global innovation number which increases as new genes are added. When adding a new gene the global innovation number is incremented and assigned to that gene. Thus the higher the number the more recently the gene was added. For a particular generation if an identical mutation occurs in more than one genome they are both given the same number, beyond that however the mutation number will remain unchanged indefinitely. \n\nThese innovation numbers allow NEAT to match up genes which can be crossed with each other.\n\nThe original implementation by Ken Stanley is published under the GPL. It integrates with Guile, a GNU scheme interpreter. This implementation of NEAT is considered the conventional basic starting point for implementations of the NEAT algorithm.\n\nIn 2003 Stanley devised an extension to NEAT that allows evolution to occur in real time rather than through the iteration of generations as used by most genetic algorithms. The basic idea is to put the population under constant evaluation with a \"lifetime\" timer on each individual in the population. When a network's timer expires its current fitness measure is examined to see whether it falls near the bottom of the population, and if so it is discarded and replaced by a new network bred from two high-fitness parents. A timer is set for the new network and it is placed in the population to participate in the ongoing evaluations.\n\nThe first application of rtNEAT is a video game called Neuro-Evolving Robotic Operatives, or NERO. In the first phase of the game, individual players deploy robots in a 'sandbox' and train them to some desired tactical doctrine. Once a collection of robots has been trained, a second phase of play allows players to pit their robots in a battle against robots trained by some other player, to see how well their training regimens prepared their robots for battle.\n\nAn extension of Ken Stanley's NEAT, developed by Colin Green, adds periodic pruning of the network topologies of candidate solutions during the evolution process. This addition addressed concern that unbounded automated growth would generate unnecessary structure.\n\nHyperNEAT is specialized to evolve large scale structures. It was originally based on the CPPN theory and is an active field of research.\n\nContent-Generating NEAT (cgNEAT) evolves custom video game content based on user preferences. The first video game to implement cgNEAT is Galactic Arms Race, a space-shooter game in which unique particle system weapons are evolved based on player usage statistics. Each particle system weapon in the game is controlled by an evolved CPPN, similarly to the evolution technique in the NEAT Particles interactive art program.\n\nodNEAT is an online and decentralized version of NEAT designed for multi-robot systems. odNEAT is executed onboard robots themselves during task execution to continuously optimize the parameters and the topology of the artificial neural network-based controllers. In this way, robots executing odNEAT have the potential to adapt to changing conditions and learn new behaviors as they carry out their tasks. The online evolutionary process is implemented according to a physically distributed island model. Each robot optimizes an internal population of candidate solutions (intra-island variation), and two or more robots exchange candidate solutions when they meet (inter-island migration). In this way, each robot is potentially self-sufficient and the evolutionary process capitalizes on the exchange of controllers between multiple robots for faster synthesis of effective controllers.\n\n\n\n\n",
    "id": "344922",
    "title": "Neuroevolution of augmenting topologies"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8892368",
    "text": "Ni1000\n\nThe Ni1000 is an artificial neural network chip developed by Nestor Corporation. The chip is aimed at image analysis applications, contains more than 3 million transistors and can analyze patterns at the rate of 40,000 per second. Prototypes running with Nestor's OCR software in 1994 were capable of recognizing around 100 handwritten characters per second.\n\n",
    "id": "8892368",
    "title": "Ni1000"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9617564",
    "text": "Oja's rule\n\nOja's learning rule, or simply Oja's rule, named after Finnish computer scientist Erkki Oja, is a model of how neurons in the brain or in artificial neural networks change connection strength, or learn, over time. It is a modification of the standard Hebb's Rule (see Hebbian learning) that, through multiplicative normalization, solves all stability problems and generates an algorithm for principal components analysis. This is a computational form of an effect which is believed to happen in biological neurons.\n\nOja's rule requires a number of simplifications to derive, but in its final form it is demonstrably stable, unlike Hebb's rule. It is a single-neuron special case of the Generalized Hebbian Algorithm. However, Oja's rule can also be generalized in other ways to varying degrees of stability and success.\n\nOja's rule defines the change in presynaptic weights given the output response formula_1 of a neuron to its inputs to be\n\nwhere is the \"learning rate\" which can also change with time. Note that the bold symbols are vectors and defines a discrete time iteration. The rule can also be made for continuous iterations as\n\nThe simplest learning rule known is Hebb's rule, which states in conceptual terms that \"neurons that fire together, wire together\". In component form as a difference equation, it is written\n\nor in scalar form with implicit -dependence,\n\nwhere is again the output, this time explicitly dependent on its input vector .\n\nHebb's rule has synaptic weights approaching infinity with a positive learning rate. We can stop this by normalizing the weights so that each weight's magnitude is restricted between 0, corresponding to no weight, and 1, corresponding to being the only input neuron with any weight. We do this by normalizing the weight vector to be of length one:\n\nNote that in Oja's original paper, , corresponding to quadrature (root sum of squares), which is the familiar Cartesian normalization rule. However, any type of normalization, even linear, will give the same result without loss of generality.\n\nFor a small learning rate formula_7 the equation can be expanded as a Power series in formula_8.\n\nFor small , our higher-order terms go to zero. We again make the specification of a linear neuron, that is, the output of the neuron is equal to the sum of the product of each input and its synaptic weight, or\n\nWe also specify that our weights normalize to , which will be a necessary condition for stability, so\n\nwhich, when substituted into our expansion, gives Oja's rule, or\n\nIn analyzing the convergence of a single neuron evolving by Oja's rule, one extracts the first \"principal component\", or feature, of a data set. Furthermore, with extensions using the Generalized Hebbian Algorithm, one can create a multi-Oja neural network that can extract as many features as desired, allowing for principal components analysis.\n\nA principal component is extracted from a dataset through some associated vector , or , and we can restore our original dataset by taking\n\nIn the case of a single neuron trained by Oja's rule, we find the weight vector converges to , or the first principal component, as time or number of iterations approaches infinity. We can also define, given a set of input vectors , that its correlation matrix has an associated eigenvector given by with eigenvalue . The variance of outputs of our Oja neuron then converges with time iterations to the principal eigenvalue, or\n\nThese results are derived using Lyapunov function analysis, and they show that Oja's neuron necessarily converges on strictly the first principal component if certain conditions are met in our original learning rule. Most importantly, our learning rate is allowed to vary with time, but only such that its sum is \"divergent\" but its power sum is \"convergent\", that is\n\nOur output activation function is also allowed to be nonlinear and nonstatic, but it must be continuously differentiable in both and and have derivatives bounded in time.\n\nRecently, in the context of associative learning, it has been shown that the Hebbian rule, which is similar to Oja's rule, can be generalized using an Ising-like model: The main idea of the generalization is based on formulating energy function like in Ising model and then applying stochastic gradient descent algorithm to this energy function. The energy function and the update rule corresponding to following the derivative are given by:\n\nwhere:\nformula_18, formula_19 is the coupling among inputs, formula_20 is the correlation strength between the model and the output, formula_21 corresponds to the presence of an external magnetic field, formula_22 determines the connections among inputs.\n\nThen, for formula_23, formula_24, and formula_25 we get the Hebbian rule, and for formula_23, formula_27, formula_25, and formula_29, where formula_30 is an identity matrix, introduce weight decay. The formula then reduces to:\n\nOja's rule was originally described in Oja's 1982 paper, but the principle of self-organization to which it is applied is first attributed to Alan Turing in 1952. PCA has also had a long history of use before Oja's rule formalized its use in network computation in 1989. The model can thus be applied to any problem of self-organizing mapping, in particular those in which feature extraction is of primary interest. Therefore, Oja's rule has an important place in image and speech processing. It is also useful as it expands easily to higher dimensions of processing, thus being able to integrate multiple outputs quickly. A canonical example is its use in binocular vision.\n\nThere is clear evidence for both long-term potentiation and long-term depression in biological neural networks, along with a normalization effect in both input weights and neuron outputs. However, while there is no direct experimental evidence yet of Oja's rule active in a biological neural network, a biophysical derivation of a generalization of the rule is possible. Such a derivation requires retrograde signalling from the postsynaptic neuron, which is biologically plausible (see neural backpropagation), and takes the form of\n\nwhere as before is the synaptic weight between the th input and th output neurons, is the input, is the postsynaptic output, and we define to be a constant analogous the learning rate, and and are presynaptic and postsynaptic functions that model the weakening of signals over time. Note that the angle brackets denote the average and the ∗ operator is a convolution. By taking the pre- and post-synaptic functions into frequency space and combining integration terms with the convolution, we find that this gives an arbitrary-dimensional generalization of Oja's rule known as Oja's Subspace, namely\n\n\n",
    "id": "9617564",
    "title": "Oja's rule"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1635395",
    "text": "Optical neural network\n\nAn optical neural network is a physical implementation of an artificial neural network with optical components.\n\nSome artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystals.\n\nBiological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics, but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons, these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission, long-term for learning and memory, and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult, and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light, based on the intensity of incoming light.\n\nThere is one recent (2007) model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism, large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version.\n\nThe practical details: hardware (optical setups) and software (optical templates) are published. However, POAC is a general purpose and programmable array computer that has a wide range of applications including:\n\n",
    "id": "1635395",
    "title": "Optical neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35179233",
    "text": "Probabilistic neural network\n\nA probabilistic neural network (PNN) is a feedforward neural network, which is widely used in classification and pattern recognition problems. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input data is estimated and Bayes’ rule is then employed to allocate the class with highest posterior probability to new input data. By this method, the probability of mis-classification is minimized. This type of ANN was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It was introduced by D.F. Specht in the 1966. In a PNN, the operations are organized into a multilayered feedforward network with four layers:\n\nPNN is often used in classification problems. When an input is present, the first layer computes the distance from the input vector to the training input vectors. This produces a vector where its elements indicate how close the input is to the training input. The second layer sums the contribution for each class of inputs and produces its net output as a vector of probabilities. Finally, a compete transfer function on the output of the second layer picks the maximum of these probabilities, and produces a 1 (positive identification) for that class and a 0 (negative identification) for non-targeted classes.\n\nEach neuron in the input layer represents a predictor variable. In categorical variables, \"N-1\" neurons are used when there are \"N\" number of categories. It standardizes the range of the values by subtracting the median and dividing by the interquartile range. Then the input neurons feed the values to each of the neurons in the hidden layer.\n\nThis layer contains one neuron for each case in the training data set. It stores the values of the predictor variables for the case along with the target value. A hidden neuron computes the Euclidean distance of the test case from the neuron’s center point and then applies the radial basis function kernel function using the sigma values.\n\nFor PNN networks there is one pattern neuron for each category of the target variable. The actual target category of each training case is stored with each hidden neuron; the weighted value coming out of a hidden neuron is fed only to the pattern neuron that corresponds to the hidden neuron’s category. The pattern neurons add the values for the class they represent.\n\nThe output layer compares the weighted votes for each target category accumulated in the pattern layer and uses the largest vote to predict the target category.\n\nThere are several advantages and disadvantages using PNN instead of multilayer perceptron.\n\n\n",
    "id": "35179233",
    "title": "Probabilistic neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25056220",
    "text": "Promoter based genetic algorithm\n\nThe promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coruña, in Spain. It evolves variable size feedforward artificial neural networks (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not.\n\nThe basic unit in the PBGA is a neuron with all of its inbound connections as represented in the following figure:\n\nThe genotype of a basic unit is a set of real valued weights followed by the parameters of the neuron and proceeded by an integer valued field that determines the promoter gene value and, consequently, the expression of the unit. By concatenating units of this type we can construct the whole network.\n\nWith this encoding it is imposed that the information that is not expressed is still carried by the genotype in evolution but it is shielded from direct selective pressure, maintaining this way the diversity in the population, which has been a design premise for this algorithm. Therefore, a clear difference is established between the search space and the solution space, permitting information learned and encoded into the genotypic representation to be preserved by disabling promoter genes.\n\nThe PBGA was originally presented within the field of autonomous robotics, in particular in the real time learning of environment models of the robot.\n\nIt has been used inside the Multilevel Darwinist Brain (MDB) cognitive mechanism developed in the GII for real robots on-line learning. In another paper it is shown how the application of the PBGA together with an external memory that stores the successful obtained world models, is an optimal strategy for adaptation in dynamic environments. \n\nRecently, the PBGA has provided results that outperform other neuroevolutionary algorithms in non-stationary problems, where the fitness function varies in time.\n\n",
    "id": "25056220",
    "title": "Promoter based genetic algorithm"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6107563",
    "text": "Pulse-coupled networks\n\nPulse-coupled networks or pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat’s visual cortex, and developed for high-performance biomimetic image processing.\n\nIn 1989, Eckhorn introduced a neural model to emulate the mechanism of cat’s visual cortex. The Eckhorn model provided a simple and effective tool for studying small mammal’s visual cortex, and was soon recognized as having significant application potential in image processing. \n\nIn 1994, Johnson adapted the Eckhorn model to an image processing algorithm, calling this algorithm a \"pulse-coupled neural network.\" Over the past decade, PCNNs have been used in a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, and noise reduction.\n\nThe basic property of the Eckhorn's linking-field model (LFM) is the coupling term. LFM is a modulation of the primary input by a biased offset factor driven by the linking input. These drive a threshold variable that decays from an initial high value. When the threshold drops below zero it is reset to a high value and the process starts over. This is different than the standard integrate-and-fire neural model, which accumulates the input until it passes an upper limit and effectively \"shorts out\" to cause the pulse.\n\nLFM uses this difference to sustain pulse bursts, something the standard model does not do on a single neuron level. It is valuable to understand, however, that a detailed analysis of the standard model must include a shunting term, due to the floating voltages level in the dendritic compartment(s), and in turn this causes an elegant multiple modulation effect that enables a true higher-order network (HON). Multidimensional pulse image processing of chemical structure data using PCNN has been discussed by Kinser, et al.\n\nA PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel’s color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.\n\nA simplified PCNN called a spiking cortical model was developed in 2009.\n\nPCNNs are useful for image processing, as discussed in a book by Thomas Lindblad and Jason M. Kinser.\n\nPCNN is proven success in many academic and industrial fields, such as image processing (image denoising, and image enhancement ), all pairs shortest path problem, and pattern recognition.\n",
    "id": "6107563",
    "title": "Pulse-coupled networks"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3737445",
    "text": "Quantum neural network\n\nQuantum neural networks (QNNs) are neural network models which are based on the principles of quantum mechanics. There are two different approaches to QNN research, one exploiting quantum information processing to improve existing neural network models (sometimes also vice versa), and the other one searching for potential quantum effects in the brain.\n\nIn the computational approach to quantum neural network research, scientists try to combine artificial neural network models (which are widely used in machine learning for the important task of pattern classification) with the advantages of quantum information in order to develop more efficient algorithms (for a review, see ). One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.\n\nQuantum neural network research is still in its infancy, and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”), resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’.\n\nThe first ideas on quantum neural computation were published independently in 1995 by Ron Chrisley and\nSubhash Kak. Kak discussed the similarity of the neural activation function with the quantum mechanical Eigenvalue equation, and later discussed the application of these ideas to the study of brain function and the limitations of this approach. Ajit Narayanan and Tammy Menneer proposed a photonic implementation of a quantum neural network model that is based on the many-universe theory and “collapses” into the desired model upon measurement. Since then, more and more articles have been published in journals of computer science as well as quantum physics in order to find a superior quantum neural network model.\n\nA lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory, since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld, Sinayskiy and Petruccione based on the quantum phase estimation algorithm.\n\nA substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic.\n\nSome contributions reverse the approach and try to exploit the insights from neural network research in order to obtain powerful applications for quantum computing, such as quantum algorithmic design supported by machine learning. An example is the work of Elizabeth Behrman and Jim Steck, who propose a quantum computing setup that consists of a number of qubits with tunable mutual interactions. Following the classical backpropagation rule, the strength of the interactions are learned from a training set of desired input-output relations, and the quantum network thus ‘learns’ an algorithm.\n\nThe quantum associative memory algorithm has been introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory, but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. An advantage lies in the exponential storage capacity of memory states, however the question remains whether the model has significance regarding the initial purpose of Hopfield models as a demonstration of how simplified artificial neural networks can simulate features of the brain.\n\nRinkus proposes that distributed representation, specifically \"sparse distributed representation\" (SDR), provides a classical implementation of quantum computing. Specifically, the set of SDR codes stored in an SDR coding field will generally intersect with each other to varying degrees. In other work, Rinkus describes a fixed time learning (and inference) algorithm that preserves similarity in the input space into similarity (intersection size) in the SDR code space. Assuming that input similarity correlates with probability, this means that any single active SDR code is also a probability distribution over all stored inputs, with the probability of each input measured by the fraction of its SDR code that is active (i.e., the size of its intersection with the active SDR code). The learning/inference algorithm can also be viewed as a state update operator and because any single active SDR simultaneously represents both the probability of the single input, X, to which it was assigned during learning and the probabilities of all other stored inputs, the same physical process that updates the probability of X also updates all stored probabilities. By 'fixed time', it is meant that the number of computational steps comprising this process (the update algorithm) remains constant as the number of stored codes increases. This theory departs radically from the standard view of quantum computing and quantum physical theory more generally: rather than assuming that the states of the lowest level entities in the system, i.e., single binary neurons, exist in superposition, it assumes only that higher-level, i.e., composite entities, i.e., whole SDR codes (which are sets of binary neurons), exist in superposition.\n\nMost learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use a classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Recently a new post-learning strategy to allow the search for improved set of weights based on analogy with quantum effects occurring in nature. The technique, proposed in is based on the analogy of modeling a biological neuron as a semiconductor heterostructure consisting of one energetic barrier sandwiched between two energetically lower areas. The activation function of the neuron is therefore considered as a particle entering the heterostructure and interacting with the barrier. In this way auxiliary reinforcement to the classical learning process of neural networks is achieved with minimal additional computational costs.\n\nOne way of constructing a quantum neuron is to first generalise classical neurons (by padding of ancillary bits) to reversible permutation gates and then generalising them further to make unitary gates. Due to the no-cloning theorem in quantum mechanics, the copying of the output before sending it to several neurons in the next layer is non-trivial. This can be replaced with a general quantum unitary acting on the output plus a dummy bit in state . That has the classical copying gate (CNOT ) as a special case, and in that sense generalises the classical copying operation. It can be demonstrated that in this scheme, the quantum neural networks can: (i) compress quantum states onto a minimal number of qubits, creating a quantum autoencoder, and (ii) discover quantum communication protocols such as teleportation. The general recipe is theoretical and implementation-independent. The quantum neuron module can naturally be implemented photonically.\n\nAlthough many quantum neural network researchers explicitly limit their scope to a computational perspective, the field is closely connected to investigations of potential quantum effects in biological neural networks. Models of cognitive agents and memory based on quantum collectivees have been proposed by Subhash Kak, but he also points to specific problems of limits on observation and control of these memories due to fundamental logical reasons. He has also proposed that a quantum language must be associated with biological neural networks.\n\nThe combination of quantum physics and neuroscience also nourishes a vivid debate beyond the borders of science, an illustrative example being journals such as NeuroQuantology or the healing method of Quantum Neurology. However, also in the scientific sphere theories of how the brain might harvest the behavior of particles on a quantum level are controversially debated. The fusion of biology and quantum physics recently gained momentum by the discovery of signs for efficient energy transport in photosynthesis due to quantum effects. However, there is no widely accepted evidence for the ‘quantum brain’ yet.\n\n\n",
    "id": "3737445",
    "title": "Quantum neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41575347",
    "text": "Quickprop\n\nQuickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied.\nThe k-th approximation step is given by:\n\nformula_1\n\nBeing formula_2 the neuron j weight of its i input and E is the loss function.\n\nThe Quickprop algorithm is an implementation of the error backpropagation algorithm, but the network can behave chaotically during the learning phase due to large step sizes.\n\n",
    "id": "41575347",
    "title": "Quickprop"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2310753",
    "text": "Radial basis function\n\nA radial basis function (RBF) is a real-valued function whose value depends only on the distance from the origin, so that formula_1; or alternatively on the distance from some other point \"c\", called a \"center\", so that formula_2. Any function formula_3 that satisfies the property formula_1 is a radial function. The norm is usually Euclidean distance, although other distance functions are also possible.\n\nSums of radial basis functions are typically used to approximate given functions. This approximation process can also be interpreted as a simple kind of neural network; this was the context in which they originally surfaced, in work by David Broomhead and David Lowe in 1988, which stemmed from Michael J. D. Powell's seminal research from 1977.\nRBFs are also used as a kernel in support vector classification.\n\nCommonly used types of radial basis functions include (writing formula_5):\n\nRadial basis functions are typically used to build up function approximations of the form\nwhere the approximating function \"y\"(x) is represented as a sum of \"N\" radial basis functions, each associated with a different center x, and weighted by an appropriate coefficient \"w\". The weights \"w\" can be estimated using the matrix methods of linear least squares, because the approximating function is \"linear\" in the weights.\n\nApproximation schemes of this kind have been particularly used in time series prediction and control of nonlinear systems exhibiting sufficiently simple chaotic behaviour, 3D reconstruction in computer graphics (for example, hierarchical RBF and Pose Space Deformation).\n\nThe sum \ncan also be interpreted as a rather simple single-layer type of artificial neural network called a radial basis function network, with the radial basis functions taking on the role of the activation functions of the network. It can be shown that any continuous function on a compact interval can in principle be interpolated with arbitrary accuracy by a sum of this form, if a sufficiently large number \"N\" of radial basis functions is used.\n\nThe approximant \"y\"(x) is differentiable with respect to the weights \"w\". The weights could thus be learned using any of the standard iterative methods for neural networks.\n\nUsing radial basis functions in this manner yields a reasonable interpolation approach provided that the fitting set has been chosen such that it covers the entire range systematically (equidistant data points are ideal). However, without a polynomial term that is orthogonal to the radial basis functions, estimates outside the fitting set tend to perform poorly.\n\n\n",
    "id": "2310753",
    "title": "Radial basis function"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9651443",
    "text": "Radial basis function network\n\nIn the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.\n\nRadial basis function (RBF) networks typically have three layers: an input layer, a hidden layer with a non-linear RBF activation function and a linear output layer. The input can be modeled as a vector of real numbers formula_1. The output of the network is then a scalar function of the input vector, formula_2, and is given by\n\nwhere formula_4 is the number of neurons in the hidden layer, formula_5 is the center vector for neuron formula_6, and formula_7 is the weight of neuron formula_6 in the linear output neuron. Functions that depend only on the distance from a center vector are radially symmetric about that vector, hence the name radial basis function. In the basic form all inputs are connected to each hidden neuron. The norm is typically taken to be the Euclidean distance (although the Mahalanobis distance appears to perform better in general) and the radial basis function is commonly taken to be Gaussian\n\nThe Gaussian basis functions are local to the center vector in the sense that\n\ni.e. changing parameters of one neuron has only a small effect for input values that are far away from the center of that neuron.\n\nGiven certain mild conditions on the shape of the activation function, RBF networks are universal approximators on a compact subset of formula_11. This means that an RBF network with enough hidden neurons can approximate any continuous function on a closed, bounded set with arbitrary precision.\n\nThe parameters formula_12, formula_13, and formula_14 are determined in a manner that optimizes the fit between formula_15 and the data.\n\nIn addition to the above \"unnormalized\" architecture, RBF networks can be \"normalized\". In this case the mapping is\n\nwhere\n\nis known as a \"normalized radial basis function\".\n\nThere is theoretical justification for this architecture in the case of stochastic data flow. Assume a stochastic kernel approximation for the joint probability density\n\nwhere the weights formula_19 and formula_20 are exemplars from the data and we require the kernels to be normalized\nand\n\nThe probability densities in the input and output spaces are\n\nand\n\nThe expectation of y given an input formula_24 is\n\nwhere\nis the conditional probability of y given formula_27.\nThe conditional probability is related to the joint probability through Bayes theorem\n\nwhich yields\n\nThis becomes\n\nwhen the integrations are performed.\n\nIt is sometimes convenient to expand the architecture to include local linear models. In that case the architectures become, to first order,\n\nand\n\nin the unnormalized and normalized cases, respectively. Here formula_33 are weights to be determined. Higher order linear terms are also possible.\n\nThis result can be written\n\nwhere\n\nand\n\nin the unnormalized case and\n\nin the normalized case.\n\nHere formula_38 is a Kronecker delta function defined as\n\nRBF networks are typically trained from pairs of input and target values formula_40, formula_41 by a two-step algorithm. In the first step, the center vectors formula_5 of the RBF functions in the hidden layer are chosen. This step can be performed in several ways; centers can be randomly sampled from some set of examples, or they can be determined using k-means clustering. Note that this step is unsupervised. A third backpropagation step can be performed to fine-tune all of the RBF net's parameters.\n\nThe second step simply fits a linear model with coefficients formula_43 to the hidden layer's outputs with respect to some objective function. A common objective function, at least for regression/function estimation, is the least squares function:\n\nwhere\nWe have explicitly included the dependence on the weights. Minimization of the least squares objective function by optimal choice of weights optimizes accuracy of fit.\n\nThere are occasions in which multiple objectives, such as smoothness as well as accuracy, must be optimized. In that case it is useful to optimize a regularized objective function such as\n\nwhere\n\nand\n\nwhere optimization of S maximizes smoothness and formula_49 is known as a regularization parameter.\n\nRBF networks can be used to interpolate a function formula_50 when the values of that function are known on finite number of points: formula_51. Taking the known points formula_52 to be the centers of the radial basis functions and evaluating the values of the basis functions at the same points formula_53 the weights can be solved from the equation\n\nIt can be shown that the interpolation matrix in the above equation is non-singular, if the points formula_52 are distinct, and thus the weights formula_56 can be solved by simple linear algebra:\n\nIf the purpose is not to perform strict interpolation but instead more general function approximation or classification the optimization is somewhat more complex because there is no obvious choice for the centers. The training is typically done in two phases first fixing the width and centers and then the weights. This can be justified by considering the different nature of the non-linear hidden neurons versus the linear output neuron.\n\nBasis function centers can be randomly sampled among the input instances or obtained by Orthogonal Least Square Learning Algorithm or found by clustering the samples and choosing the cluster means as the centers.\n\nThe RBF widths are usually all fixed to same value which is proportional to the maximum distance between the chosen centers.\n\nAfter the centers formula_58 have been fixed, the weights that minimize the error at the output are computed with a linear pseudoinverse solution:\nwhere the entries of \"G\" are the values of the radial basis functions evaluated at the points formula_60: formula_61.\n\nThe existence of this linear solution means that unlike multi-layer perceptron (MLP) networks, RBF networks have a unique local minimum (when the centers are fixed).\n\nAnother possible training algorithm is gradient descent. In gradient descent training, the weights are adjusted at each time step by moving them in a direction opposite from the gradient of the objective function (thus allowing the minimum of the objective function to be found),\n\nwhere formula_63 is a \"learning parameter.\"\n\nFor the case of training the linear weights, formula_64, the algorithm becomes\n\nin the unnormalized case and\n\nin the normalized case.\n\nFor local-linear-architectures gradient-descent training is\n\nFor the case of training the linear weights, formula_64 and formula_69, the algorithm becomes\n\nin the unnormalized case and\n\nin the normalized case and\n\nin the local-linear case.\n\nFor one basis function, projection operator training reduces to Newton's method.\n\nThe basic properties of radial basis functions can be illustrated with a simple mathematical map, the logistic map, which maps the unit interval onto itself. It can be used to generate a convenient prototype data stream. The logistic map can be used to explore function approximation, time series prediction, and control theory. The map originated from the field of population dynamics and became the prototype for chaotic time series. The map, in the fully chaotic regime, is given by\n\nwhere t is a time index. The value of x at time t+1 is a parabolic function of x at time t. This equation represents the underlying geometry of the chaotic time series generated by the logistic map.\n\nGeneration of the time series from this equation is the forward problem. The examples here illustrate the inverse problem; identification of the underlying dynamics, or fundamental equation, of the logistic map from exemplars of the time series. The goal is to find an estimate\n\nfor f.\n\nThe architecture is\n\nwhere\n\nSince the input is a scalar rather than a vector, the input dimension is one. We choose the number of basis functions as N=5 and the size of the training set to be 100 exemplars generated by the chaotic time series. The weight formula_77 is taken to be a constant equal to 5. The weights formula_78 are five exemplars from the time series. The weights formula_12 are trained with projection operator training:\n\nwhere the learning rate formula_81 is taken to be 0.3. The training is performed with one pass through the 100 training points. The rms error is 0.15.\n\nThe normalized RBF architecture is\n\nwhere\n\nAgain:\n\nAgain, we choose the number of basis functions as five and the size of the training set to be 100 exemplars generated by the chaotic time series. The weight formula_77 is taken to be a constant equal to 6. The weights formula_78 are five exemplars from the time series. The weights formula_12 are trained with projection operator training:\n\nwhere the learning rate formula_81 is again taken to be 0.3. The training is performed with one pass through the 100 training points. The rms error on a test set of 100 exemplars is 0.084, smaller than the unnormalized error. Normalization yields accuracy improvement. Typically accuracy with normalized basis functions increases even more over unnormalized functions as input dimensionality increases.\n\nOnce the underlying geometry of the time series is estimated as in the previous examples, a prediction for the time series can be made by iteration:\n\nA comparison of the actual and estimated time series is displayed in the figure. The estimated times series starts out at time zero with an exact knowledge of x(0). It then uses the estimate of the dynamics to update the time series estimate for several time steps.\n\nNote that the estimate is accurate for only a few time steps. This is a general characteristic of chaotic time series. This is a property of the sensitive dependence on initial conditions common to chaotic time series. A small initial error is amplified with time. A measure of the divergence of time series with nearly identical initial conditions is known as the Lyapunov exponent.\n\nWe assume the output of the logistic map can be manipulated through a control parameter formula_93 such that\n\nThe goal is to choose the control parameter in such a way as to drive the time series to a desired output formula_95. This can be done if we choose the control paramer to be\n\nwhere\n\nis an approximation to the underlying natural dynamics of the system.\n\nThe learning algorithm is given by\n\nwhere\n\n\n",
    "id": "9651443",
    "title": "Radial basis function network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8278198",
    "text": "Random neural network\n\nThe random neural network (RNN) is a mathematical representation of an interconnected network of neurons or cells which exchange spiking signals. It was invented by Erol Gelenbe and is linked to the G-network model of queueing networks as well as to Gene Regulatory Network models. Each cell state is represented by an integer whose value rises when the cell receives an excitatory spike and drops when it receives an inhibitory spike. The spikes can originate outside the network itself, or they can come from other cells in the networks. Cells whose internal excitatory state has a positive value are allowed to send out spikes of either kind to other cells in the network according to specific cell-dependent spiking rates. The model has a mathematical solution in steady-state which provides the joint probability distribution of the network in terms of the individual probabilities that each cell is excited and able to send out spikes. Computing this solution is based on solving a set of non-linear algebraic equations whose parameters are related to the spiking rates of individual cells and their connectivity to other cells, as well as the arrival rates of spikes from outside the network. The RNN is a recurrent model, i.e. a neural network that is allowed to have complex feedback loops.\n\nA highly energy-efficient implementation of random neural networks was demonstrated by Krishna Palem et al. using the Probabilistic CMOS or PCMOS technology and was shown to be c. 226–300 times more efficient in terms of Energy-Performance-Product.\n\nRNNs are also related to artificial neural networks, which (like the random neural network) have gradient-based learning algorithms. The learning algorithm for an n-node random neural network that includes feedback loops (it is also a recurrent neural network) is of computational complexity O(n^3) (the number of computations is proportional to the cube of n, the number of neurons). The random neural network can also be used with other learning algorithms such as reinforcement learning. The RNN has been shown to be a universal approximator for bounded and continuous functions.\n\n\n\n",
    "id": "8278198",
    "title": "Random neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37862937",
    "text": "Rectifier (neural networks)\n\nIn the context of artificial neural networks, the rectifier is an activation function defined as the positive part of its argument:\n\nformula_1,\n\nwhere \"x\" is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. \nThis activation function was first introduced to a dynamical network by Hahnloser et al. in a 2000 paper in Nature with strong biological motivations and mathematical justifications.. It has been demonstrated for the first time in 2011 to enable better training of deeper networks , compared to the widely used activation functions prior to 2011, i.e., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, , the most popular activation function for deep neural networks.\n\nA unit employing the rectifier is also called a rectified linear unit (ReLU).\n\nA smooth approximation to the rectifier is the analytic function\nwhich is called the softplus function. The derivative of softplus is formula_3, i.e. the logistic function.\n\nRectified linear units find applications in computer vision and speech recognition using deep neural nets.\n\nRectified linear units can be extended to include Gaussian noise, making them noisy ReLUs, giving\n\nNoisy ReLUs have been used with some success in restricted Boltzmann machines for computer vision tasks.\n\nLeaky ReLUs allow a small, non-zero gradient when the unit is not active.\n\nParametric ReLUs take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural network parameters.\n\nNote that for formula_8, this is equivalent to\nand thus has a relation to \"maxout\" networks.\n\nExponential linear units try to make the mean activations closer to zero which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs.\n\nformula_10\n\nformula_11 is a hyper-parameter to be tuned and formula_12 is a constraint.\n\n\nRectifying activation functions were used to separate specific excitation and unspecific inhibition in the Neural Abstraction Pyramid, which was trained in a supervised way to learn several computer vision tasks.\nIn 2011, the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training.\nRectified linear units, compared to sigmoid function or similar activation functions, allow for faster and effective training of deep neural architectures on large and complex datasets.\n\n\n",
    "id": "37862937",
    "title": "Rectifier (neural networks)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10667750",
    "text": "Reservoir computing\n\nReservoir computing is a framework for computation that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a \"reservoir\" and the dynamics of the reservoir map the input to a higher dimension. Then a simple \"readout\" mechanism is trained to read the state of the reservoir and map it to the desired output. The main benefit is that training is performed only at the readout stage and the reservoir is fixed. Liquid-state machines and echo state networks are two major types of reservoir computing.\n\nThe reservoir consists of a collection of recurrently connected units. The connectivity structure is usually \"random\", and the units are usually non-linear. The overall dynamics of the reservoir are driven by the input, and also affected by the past. A rich collection of dynamical input-output mapping is a crucial advantage over time delay neural networks.\n\nThe readout is carried out using a linear transformation of the reservoir output. This transformation is adapted to the task of interest by using a linear regression or a Ridge regression using a teaching signal.\n\nAn early example of reservoir computing was the context reverberation network.\nIn this architecture, an input layer feeds into a high dimensional dynamical system which is read out by a trainable single-layer perceptron. Two kinds of dynamical system were described: a recurrent neural network with fixed random weights, and a continuous reaction-diffusion system inspired by Alan Turing’s model of morphogenesis. At the trainable layer, the perceptron associates current inputs with the signals that reverberate in the dynamical system; the latter were said to provide a dynamic \"context\" for the inputs. In the language of later work, the reaction-diffusion system served as the reservoir.\n\nBackpropagation-Decorrelation (BPDC)\n\nThe Tree Echo State Network (TreeESN) model represents a generalization of the reservoir computing framework to tree structured data.\n\nThe extension of the reservoir computing framework towards Deep Learning, with the introduction of deep reservoir computing and of the deep Echo State Network (deepESN) model allows to develop efficiently trained models for hierarchical processing of temporal data, at the same time enabling the investigation on the inherent role of layered composition in recurrent neural networks.\n\nIn late 2017, a research team from the University of Michigan implemented the reservoir computing principles in a chip and demonstrated its performance in a speech prediction task.\n\n\n",
    "id": "10667750",
    "title": "Reservoir computing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7950358",
    "text": "Rprop\n\nRprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992.\n\nSimilarly to the Manhattan update rule, Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each \"weight\". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor \"η\", where \"η\" < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of \"η\", where \"η\" > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. \"η\" is empirically set to 1.2 and \"η\" to 0.5.\n\nNext to the cascade correlation algorithm and the Levenberg–Marquardt algorithm, Rprop is one of the fastest weight update mechanisms.\n\nRPROP is a batch update algorithm.\n\nMartin Riedmiller developed three algorithms, all named RPROP. Igel and Hüsken assigned names to them and added a new variant:\n\n\n",
    "id": "7950358",
    "title": "Rprop"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3710117",
    "text": "Semantic neural network\n\nSemantic neural network (SNN) is based on John von Neumann's neural network <nowiki>[</nowiki>von Neumann, 1966<nowiki>]</nowiki> and Nikolai Amosov M-Network. There are limitations to a link topology for the von Neumann’s network but SNN accept a case without these limitations. Only logical values can be processed, but SNN accept that fuzzy values can be processed too. All neurons into the von Neumann network are synchronized by tacts. For further use of self-synchronizing circuit technique SNN accepts neurons can be self-running or synchronized.\n\nIn contrast to the von Neumann network there are no limitations for topology of neurons for semantic networks. It leads to the impossibility of relative addressing of neurons as it was done by von Neumann. In this case an absolute readdressing should be used. Every neuron should have a unique identifier that would provide a direct access to another neuron. Of course, neurons interacting by axons-dendrites should have each other's identifiers. An absolute readdressing can be modulated by using neuron specificity as it was realized for biological neural networks.\n\nThere’s no description for self-reflectiveness and self-modification abilities into the initial description of semantic networks [Dudar Z.V., Shuklin D.E., 2000]. But in [Shuklin D.E. 2004] a conclusion had been drawn about the necessity of introspection and self-modification abilities in the system. For maintenance of these abilities a concept of pointer to neuron is provided. Pointers represent virtual connections between neurons. In this model, bodies and signals transferring through the neurons connections represent a physical body, and virtual connections between neurons are representing an astral body. It is proposed to create models of artificial neuron networks on the basis of virtual machine supporting the opportunity for paranormal effects.\n\nSNN is generally used for natural language processing.\n\n\n\n",
    "id": "3710117",
    "title": "Semantic neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=87210",
    "text": "Sigmoid function\n\nA sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, \"sigmoid function\" refers to the special case of the logistic function shown in the first figure and defined by the formula\nOther examples of similar shapes include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return value monotonically increasing most often from 0 to 1 or alternatively from −1 to 1, depending on convention.\n\nA wide variety of sigmoid functions have been used as the activation function of artificial neurons, including the logistic and hyperbolic tangent functions. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic distribution, the normal distribution, and Student's \"t\" probability density functions.\n\nA sigmoid function is a bounded differentiable real function that is defined for all real input values and has a non-negative derivative at each point.\n\nIn general, a sigmoid function is real-valued, monotonic, and differentiable having a non-negative first derivative which is bell shaped. A sigmoid function is constrained by a pair of horizontal asymptotes as formula_2.\n\n\n\n\n\n\n\n\n\nThe integral of any continuous, non-negative, \"bump-shaped\" function will be sigmoidal, thus the cumulative distribution functions for many common probability distributions are sigmoidal. One such example is the error function, which is related to the cumulative distribution function (CDF) of a normal distribution.\n\nMany natural processes, such as those of complex system learning curves, exhibit a progression from small beginnings that accelerates and approaches a climax over time. When a specific mathematical model is lacking, a sigmoid function is often used.\n\nThe van Genuchten-Gupta model is based on an inverted S-curve and applied to the response of crop yield to soil salinity.\n\nExamples of the application of the logistic S-curve to the response of crop yield (barley) to both the soil salinity and depth to watertable in the soil are shown in .\n\n\n",
    "id": "87210",
    "title": "Sigmoid function"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6152185",
    "text": "Softmax function\n\nIn mathematics, the softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a -dimensional vector formula_1 of arbitrary real values to a -dimensional vector formula_2 of real values in the range [0, 1] that add up to 1. The function is given by\n\nIn probability theory, the output of the softmax function can be used to represent a categorical distribution – that is, a probability distribution over different possible outcomes. In fact, it is the gradient-log-normalizer of the categorical probability distribution.\n\nThe softmax function is used in various multiclass classification methods, such as multinomial logistic regression (also known as softmax regression) , multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks. Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of distinct linear functions, and the predicted probability for the 'th class given a sample vector and a weighting vector is:\n\nThis can be seen as the composition of linear functions formula_6 and the softmax function (where formula_7 denotes the inner product of formula_8 and formula_9). The operation is equivalent to applying a linear operator defined by formula_9 to vectors formula_8, thus transforming the original, probably highly-dimensional, input to vectors in a -dimensional space formula_12.\n\nIf we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. The output has most of its weight where the '4' was in the original input. This is what the function is normally used for: to highlight the largest values and suppress values which are significantly below the maximum value.\n\nComputation of this example using simple Python code:\n\n»> import math\n»> z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n»> z_exp = [math.exp(i) for i in z]\n»> print([round(i, 2) for i in z_exp])\n[2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09]\n»> sum_z_exp = sum(z_exp)\n»> print(round(sum_z_exp, 2))\n114.98\n»> softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n»> print(softmax)\n[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\nHere is an example of Julia code:\n\njulia> A = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n7-element Array{Float64,1}:\n\njulia> exp.(A) ./ sum(exp.(A))\n7-element Array{Float64,1}:\nThe softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear variant of multinomial logistic regression.\n\nSince the function maps a vector and a specific index \"i\" to a real value, the derivative needs to take the index into account:\n\nHere, the Kronecker delta is used for simplicity (cf. the derivative of a sigmoid function, being expressed via the function itself).\n\nSee Multinomial logit for a probability model which uses the softmax activation function.\n\nIn the field of reinforcement learning, a softmax function can be used to convert values into action probabilities. The function commonly used is:\n\nwhere the action value formula_15 corresponds to the expected reward of following action a and formula_16 is called a temperature parameter (in allusion to statistical mechanics). For high temperatures (formula_17), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability. For a low temperature (formula_18), the probability of the action with the highest expected reward tends to 1.\n\nSigmoidal or Softmax normalization is a way of reducing the influence of extreme values or outliers in the data without removing them from the dataset. It is useful given outlier data, which we wish to include in the dataset while still preserving the significance of data within a standard deviation of the mean. The data are nonlinearly transformed using one of the sigmoidal functions. \n\nThe logistic sigmoid function:\n\nThe hyperbolic tangent function, tanh:\n\nThe sigmoid function limits the range of the normalized data to values between 0 and 1. The sigmoid function is almost linear near the mean and has smooth nonlinearity at both extremes, ensuring that all data points are within a limited range. This maintains the resolution of most values within a standard deviation of the mean.\n\nThe hyperbolic tangent function, tanh, limits the range of the normalized data to values between −1 and 1. The hyperbolic tangent function is almost linear near the mean, but has a slope of half that of the sigmoid function. Like sigmoid, it has smooth, monotonic nonlinearity at both extremes. Also, like the sigmoid function, it remains differentiable everywhere and the sign of the derivative (slope) is unaffected by the normalization. This ensures that optimization and numerical integration algorithms can continue to rely on the derivative to estimate changes to the output (normalized value) that will be produced by changes to the input in the region near any linearisation point.\n\nThe softmax function also happens to be the probability of an atom being found in a quantum state of energy formula_21 when the atom is part of an ensemble that has reached thermal equilibrium at temperature formula_22. This is known as the Boltzmann distribution. The expected relative occupancy of each state is formula_23, and this is normalised so that the sum over energy levels sums to 1. In this analogy, the input to the softmax function is the negative energy of each quantum state divided by formula_24.\n\n",
    "id": "6152185",
    "title": "Softmax function"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10159567",
    "text": "Spiking neural network\n\nSpiking neural networks (SNNs) fall into the third generation of neural network models, increasing the level of realism in a neural simulation.\nIn addition to neuronal and synaptic state, SNNs also incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not fire at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather fire only when a membrane potential – an intrinsic quality of the neuron related to its membrane electrical charge – reaches a specific value. When a neuron fires, it generates a signal which travels to other neurons which, in turn, increase or decrease their potentials in accordance with this signal.\n\nIn the context of spiking neural networks, the current activation level (modeled as some differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher, and then either firing or decaying over time. Various \"coding methods\" exist for interpreting the outgoing \"spike train\" as a real-value number, either relying on the frequency of spikes, or the timing between spikes, to encode information.\n\nThe first scientific model of a spiking neuron was proposed by Alan Lloyd Hodgkin and Andrew Huxley in 1952. This model describes how action potentials are initiated and propagated. Spikes, however, are not generally transmitted directly between neurons. Communication requires the exchange of chemical substances in the synaptic gap, called neurotransmitters. The complexity and variability of biological models have resulted in various neuron models, such as the integrate-and-fire (1907), FitzHugh–Nagumo model (1961–1962) and Hindmarsh–Rose model (1984).\n\nFrom the information theory point of view, the problem is to propose a model that explains how information is encoded and decoded by a series of trains of pulses, i.e. action potentials. Thus, one of the fundamental questions of neuroscience is to determine if neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron \"can replace hundreds of hidden units on a sigmoidal neural net.\"\n\nThis kind of neural network can in principle be used for information processing applications the same way as traditional artificial neural networks. In addition, spiking neural networks can model the central nervous system of a virtual insect for seeking food without the prior knowledge of the environment. However, due to their more realistic properties, they can also be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, the electrophysiological recordings of this circuit can be compared to the output of the corresponding spiking artificial neural network simulated on computer, determining the plausibility of the starting hypothesis.\n\nIn practice, there is a major difference between the theoretical power of spiking neural networks and what has been demonstrated. They have proved useful in neuroscience, but not (yet) in engineering. Some large scale neural network models have been designed that take advantage of the pulse coding found in spiking neural networks, these networks mostly rely on the principles of reservoir computing. However, the real world application of large scale spiking neural networks has been limited because the increased computational costs associated with simulating realistic neural models have not been justified by commensurate benefits in computational power. As a result, there has been little application of large scale spiking neural networks to solve computational tasks of the order and complexity that are commonly addressed using rate coded (second generation) neural networks. In addition it can be difficult to adapt second generation neural network models into real time, spiking neural networks (especially if these network algorithms are defined in discrete time). It is relatively easy to construct a spiking neural network model and observe its dynamics. It is much harder to develop a model with stable behavior that computes a specific function.\n\nThere is diverse range of application software to simulate spiking neural networks. This software can be classified according to the use of the simulation:\n\nNeurogrid, built at Stanford University, is a board that can simulate spiking neural networks directly in hardware. SpiNNaker (Spiking Neural Network Architecture), designed at the University of Manchester, uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model.\n\nAnother implementation is the TrueNorth processor from IBM. This processor contains 5.4 billion transistors, but is designed to consume very little power, only 70 milliwatts; most processors in personal computers contain about 1.4 billion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition; while critics say the chip isn't powerful enough, its supporters point out that this is only the first generation, and the capabilities of improved iterations will become clear.\n\nAnother hardware platform aimed at providing reconfigurable, general-purpose, real-time neural networks of spiking neurons is the Dynamic Neuromorphic Asynchronous Processor (DYNAP). DYNAP uses a unique combination of slow, low-power, inhomogeneous sub-thresholds analog circuits, and fast programmable digital circuits. This allow the implementation of real-time spike-based neural processing architectures in which memory and computation are co-localized, solving the von Neumann bottleneck problem and enabling real-time massively multiplexed communication of spiking events for realising massive networks. Recurrent networks, feed-forward networks, convolutional networks, attractor networks, echo-state networks, deep networks, and sensory fusion networks are few of the possibilities.\n\n\n",
    "id": "10159567",
    "title": "Spiking neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37652061",
    "text": "Stochastic neural analog reinforcement calculator\n\nSNARC (Stochastic Neural Analog Reinforcement Calculator) is a neural net machine designed by Marvin Lee Minsky. George Miller gathered the funding for the project from the Air Force Office of Scientific Research in the summer of 1951. At the time, one of Minsky's graduate students at Princeton, Dean Edmonds, volunteered that he was good with electronics and therefore Minsky brought him onto the project.\n\nThe machine itself is a randomly connected network of approximately 40 Hebb synapses. These synapses each have a memory that holds the probability that signal comes in one input and another signal will come out of the output. There is a probability knob that goes from 0 to 1 that shows this probability of the signals propagating. If the probability signal gets through, a capacitor remembers this function and engages a \"clutch\". At this point, the operator will press a button to give reward to the machine. At this point, a large motor starts and there is a chain that goes to all 40 synapse machines, checking if the clutch is engaged or not. As the capacitor can only \"remember\" for a certain amount of time, the chain only catches the most recent updates of the probabilities.\n\nThis machine is considered one of the first pioneering attempts at the field of artificial intelligence. Minsky went on to be a founding member of MIT's Project MAC, which split to become the MIT Laboratory for Computer Science and the MIT Artificial Intelligence Lab, and is now the MIT Computer Science and Artificial Intelligence Laboratory. In 1985 Minsky became a founding member of the MIT Media Laboratory. \n\n",
    "id": "37652061",
    "title": "Stochastic neural analog reinforcement calculator"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2070605",
    "text": "Stochastic neural network\n\nStochastic neural networks are a type of artificial neural networks built by introducing random variations into the network, either by giving the network's neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help it escape from local minima.\n\nAn example of a neural network using stochastic transfer functions is a Boltzmann machine. Each neuron is binary valued, and the chance of it firing depends on the other neurons in the network.\n\nStochastic neural networks have found applications in risk management, oncology, bioinformatics, and other similar fields.\n",
    "id": "2070605",
    "title": "Stochastic neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41027689",
    "text": "Synaptic transistor\n\nA synaptic transistor is an electrical device that can learn in ways similar to a neural synapse. It optimizes its own properties for the functions it has carried out in the past. The device mimics the behavior of the property of neurons called spike-timing-dependent plasticity, or STDP.\n\nIts structure is similar to that of a field effect transistor, where an ionic liquid takes the place of the gate insulating layer between the gate electrode and the conducting channel. That channel is composed of samarium nickelate (, or SNO) rather than the field effect transistor's doped silicon.\n\nA synaptic transistor has a traditional immediate response whose amount of current that passes between the source and drain contacts varies with voltage applied to the gate electrode. It also produces a much slower learned response such that the conductivity of the SNO layer varies in response to the transistor's STDP history, essentially by shuttling oxygen ions between the SNO and the ionic liquid.\n\nThe analog of strengthening a synapse is to increase the SNO's conductivity, which essentially increases gain. Similarly, weakening a synapse is analogous to decreasing the SNO's conductivity, lowering the gain.\n\nThe input and output of the synaptic transistor are continuous analog values, rather than digital on-off signals. While the physical structure of the device has the potential to learn from history, it contains no way to bias the transistor to control the memory effect. An external supervisory circuit converts the time delay between input and output into a voltage applied to the ionic liquid that either drives ions into the SNO or removes them.\n\nA network of such devices can learn particular responses to \"sensory inputs\", with those responses being learned through experience rather than explicitly programmed.\n",
    "id": "41027689",
    "title": "Synaptic transistor"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14405160",
    "text": "Synaptic weight\n\nIn neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another. The term is typically used in artificial and biological neural network research.\n\nIn a computational neural network, a vector or set of inputs formula_1 and outputs formula_2, or pre- and post-synaptic neurons respectively, are interconnected with synaptic weights represented by the matrix formula_3, where for a linear neuron\n\nThe synaptic weight is changed by using a learning rule, the most basic of which is Hebb's rule, which is usually stated in biological terms as \n\n\"Neurons that fire together, wire together.\"\n\nComputationally, this means that if a large signal from one of the input neurons results in a large signal from one of the output neurons, then the synaptic weight between those two neurons will increase. The rule is unstable, however, and is typically modified using such variations as Oja's rule, radial basis functions or the backpropagation algorithm.\n\nFor biological networks, the effect of synaptic weights is not as simple as for linear neurons or Hebbian learning. However, biophysical models such as BCM theory have seen some success in mathematically describing these networks.\n\nIn the mammalian central nervous system, signal transmission is carried out by interconnected networks of nerve cells, or neurons. For the basic pyramidal neuron, the input signal is carried by the axon, which releases neurotransmitter chemicals into the synapse which is picked up by the dendrites of the next neuron, which can then generate an action potential which is analogous to the output signal in the computational case.\n\nThe synaptic weight in this process is determined by several variable factors:\n\nThe changes in synaptic weight that occur is known as synaptic plasticity, and the process behind long-term changes (long-term potentiation and depression) is still poorly understood. Hebb's original learning rule was originally applied to biological systems, but has had to undergo many modifications as a number of theoretical and experimental problems came to light.\n\n",
    "id": "14405160",
    "title": "Synaptic weight"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5099023",
    "text": "Tensor product network\n\nA tensor product network, in artificial neural networks, is a network that exploits the properties of tensors to model associative concepts such as variable assignment. Orthonormal vectors are chosen to model the ideas (such as variable names and target assignments), and the tensor product of these vectors construct a network whose mathematical properties allow the user to easily extract the association from it.\n\n",
    "id": "5099023",
    "title": "Tensor product network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23594537",
    "text": "Time delay neural network\n\nTime delay neural network (TDNN) is an artificial neural network architecture whose primary purpose is to classify patterns shift-invariantly, i.e., requiring no explicit prior determination of the beginning and end point of the pattern. The TDNN was first proposed to classify phonemes in speech signals for automatic speech recognition, where the automatic determination of precise segments or feature boundaries is difficult or impossible. The TDNN recognizes phonemes and their underlying acoustic/phonetic features, independent of time-shifts, i.e. position in time.\n\nAn input signal is augmented with delayed copies as other inputs, the neural network is time-shift invariant since it has no internal state.\n\nThe original paper presented a perceptron network whose connection weights were trained with the back-propagation algorithm, this may be done in batch or online. The Stuttgart Neural Network Simulator implements that version.\n\nThe Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers composed of clusters. These clusters are meant to represent neurons in a brain and, like the brain, each cluster need only focus on small regions of the input. A proto-typical TDNN has three layers of clusters, one for input, one for output, and the middle layer which handles manipulation of the input through filters. Due to their sequential nature, TDNN’s are implemented as a feedforward neural network instead of a recurrent neural network.\n\nIn order to achieve time-shift invariance, a set of delays are added to the input (audio file, image, etc.) so that the data is represented at different points in time. These delays are arbitrary and application specific, which generally means the input data is customized for a specific delay pattern. There has been work done in creating an adaptable time-delay TDNN. where this manual tuning is eradicated. The delays are an attempt to add a temporal dimension to the network which is not present in Recurrent Neural Networks or Multi-Layer Perceptrons with a sliding window. The combination of past inputs with present inputs make the TDNN’s approach unique.\n\nA key feature for TDNN’s are the ability to express a relation between inputs in time. This relation can be the result of a feature detector and is used within the TDNN to recognize patterns between the delayed inputs.\nOne of the main advantages of neural networks is the lack of a dependence on prior knowledge to set up the banks of filters at each layer. However, this entails that the network must learn the optimal value for these filters through processing numerous training inputs. Supervised learning is generally the learning algorithm associated with TDNN’s due to its strength in pattern recognition and function approximation. Supervised learning is commonly implemented with a back propagation algorithm.\n\nSpeech Recognition\n\nTDNN’s used to solve problems in speech recognition that were introduced in 1989 and initially focused on phoneme detection. Speech lends itself nicely to TDNN’s as spoken sounds are rarely of uniform length. By examining a sound shifted in the past and future, the TDNN is able to construct a model for that sound that is time-invariant. This is especially helpful in speech recognition as different dialects and languages pronounce the same sounds with different lengths. Spectral coefficients are used to describe the relation between the input samples.\n\nVideo Analysis\n\nVideo has a temporal dimension which makes a TDNN an ideal solution to analyzing motion patterns. An example of this analysis is a combination of vehicle detection and recognizing pedestrians. When examining videos, subsequent images are fed into the TDNN as input where each image is the next frame in the video. The strength of the TDNN comes from its ability to examine objects shifted in time forward and backward to define an object detectable as the time is altered. If an object can be recognized in this manner, an application can plan on that object to be found in the future and perform an optimal action.\n\nCommon Libraries\n\n\n",
    "id": "23594537",
    "title": "Time delay neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28071238",
    "text": "U-matrix\n\nThe U-matrix (unified distance matrix) is a representation of a self-organizing map (SOM) where the Euclidean distance between the codebook vectors of neighboring neurons is depicted in a grayscale image. This image is used to visualize the data in a high-dimensional space using a 2D image.\n\nOnce the SOM is trained using the input data, the final map is not expected to have any twists. If the map is twist-free, the distance between the codebook vectors of neighboring neurons gives an approximation of the distance between different parts of the underlying data. When such distances are depicted in a grayscale image, light colors depict closely spaced node codebook vectors and darker colors indicate more widely separated node codebook vectors. Thus, groups of light colors can be considered as clusters, and the dark parts as the boundaries between the clusters. This representation can help to visualize the clusters in the high-dimensional spaces, or to automatically recognize them using relatively simple image processing techniques.\n",
    "id": "28071238",
    "title": "U-matrix"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18543448",
    "text": "Universal approximation theorem\n\nIn the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of R, under mild assumptions on the activation function. The theorem thus states that simple neural networks can \"represent\" a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.\n\nOne of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions.\n\nKurt Hornik showed in 1991 that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear. For notational convenience, only the single output case will be shown. The general case can easily be deduced from the single output case.\n\nThe theorem in mathematical terms:\n\nLet formula_1 be a nonconstant, bounded, and monotonically-increasing continuous function. Let formula_2 denote the \"m\"-dimensional unit hypercube formula_3. The space of continuous functions on formula_2 is denoted by formula_5. Then, given any formula_6 and any function formula_7, there exist an integer formula_8, real constants formula_9 and real vectors formula_10, where formula_11, such that we may define:\n\nas an approximate realization of the function formula_13 where formula_13 is independent of formula_15; that is,\n\nfor all formula_17. In other words, functions of the form formula_18 are dense in formula_5.\nThis still holds when replacing formula_2 with any compact subset of formula_21.\n\n\n",
    "id": "18543448",
    "title": "Universal approximation theorem"
  }
]
