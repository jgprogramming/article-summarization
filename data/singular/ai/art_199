[{"url":"https://en.wikipedia.org/wiki?curid=18543448","text":"Universal approximation theorem\n\nIn the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of R, under mild assumptions on the activation function. The theorem thus states that simple neural networks can \"represent\" a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.\n\nOne of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions.\n\nKurt Hornik showed in 1991 that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators. The output units are always assumed to be linear. For notational convenience, only the single output case will be shown. The general case can easily be deduced from the single output case.\n\nThe theorem in mathematical terms:\n\nLet formula_1 be a nonconstant, bounded, and monotonically-increasing continuous function. Let formula_2 denote the \"m\"-dimensional unit hypercube formula_3. The space of continuous functions on formula_2 is denoted by formula_5. Then, given any formula_6 and any function formula_7, there exist an integer formula_8, real constants formula_9 and real vectors formula_10, where formula_11, such that we may define:\n\nas an approximate realization of the function formula_13 where formula_13 is independent of formula_15; that is,\n\nfor all formula_17. In other words, functions of the form formula_18 are dense in formula_5.\nThis still holds when replacing formula_2 with any compact subset of formula_21.\n\n\n","id":"18543448","title":"Universal approximation theorem"}]
