[{"url":"https://en.wikipedia.org/wiki?curid=41416740","text":"Deep belief network\n\nIn machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer.\n\nWhen trained on a set of examples without supervision, a DBN can learn to probabilistically reconstruct its inputs. The layers then act as feature detectors. After this learning step, a DBN can be further trained with supervision to perform classification.\n\nDBNs can be viewed as a composition of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, where each sub-network's hidden layer serves as the visible layer for the next. An RBM is an undirected, generative energy-based model with a \"visible\" input layer and a hidden layer and connections between but not within layers. This composition leads to a fast, layer-by-layer unsupervised training procedure, where contrastive divergence is applied to each sub-network in turn, starting from the \"lowest\" pair of layers (the lowest visible layer is a training set).\n\nTeh's observation that DBNs can be trained greedily, one layer at a time, led to one of the first effective deep learning algorithms. Overall, there are many attractive implementations and uses of DBNs in real-life applications and scenarios (e.g., electroencephalography, drug discovery). \n\nThe training method for RBMs proposed by Geoffrey Hinton for use with training \"Product of Expert\" models is called contrastive divergence (CD). CD provides an approximation to the maximum likelihood method that would ideally be applied for learning the weights. In training a single RBM, weight updates are performed with gradient descent via the following equation: formula_1\n\nwhere, formula_2 is the probability of a visible vector, which is given by formula_3. formula_4 is the partition function (used for normalizing) and formula_5 is the energy function assigned to the state of the network. A lower energy indicates the network is in a more \"desirable\" configuration. The gradient formula_6 has the simple form formula_7 where formula_8 represent averages with respect to distribution formula_9. The issue arises in sampling formula_10 because this requires extended alternating Gibbs sampling. CD replaces this step by running alternating Gibbs sampling for formula_11 steps (values of formula_12 perform well). After formula_11 steps, the data are sampled and that sample is used in place of formula_10. The CD procedure works as follows:\nOnce an RBM is trained, another RBM is \"stacked\" atop it, taking its input from the final trained layer. The new visible layer is initialized to a training vector, and values for the units in the already-trained layers are assigned using the current weights and biases. The new RBM is then trained with the procedure above. This whole process is repeated until the desired stopping criterion is met.\n\nAlthough the approximation of CD to maximum likelihood is crude (does not follow the gradient of any function), it is empirically effective.\n\n\n","id":"41416740","title":"Deep belief network"}]
