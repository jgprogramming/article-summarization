[{"url":"https://en.wikipedia.org/wiki?curid=6152185","text":"Softmax function\n\nIn mathematics, the softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a -dimensional vector formula_1 of arbitrary real values to a -dimensional vector formula_2 of real values in the range [0, 1] that add up to 1. The function is given by\n\nIn probability theory, the output of the softmax function can be used to represent a categorical distribution – that is, a probability distribution over different possible outcomes. In fact, it is the gradient-log-normalizer of the categorical probability distribution.\n\nThe softmax function is used in various multiclass classification methods, such as multinomial logistic regression (also known as softmax regression) , multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks. Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of distinct linear functions, and the predicted probability for the 'th class given a sample vector and a weighting vector is:\n\nThis can be seen as the composition of linear functions formula_6 and the softmax function (where formula_7 denotes the inner product of formula_8 and formula_9). The operation is equivalent to applying a linear operator defined by formula_9 to vectors formula_8, thus transforming the original, probably highly-dimensional, input to vectors in a -dimensional space formula_12.\n\nIf we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. The output has most of its weight where the '4' was in the original input. This is what the function is normally used for: to highlight the largest values and suppress values which are significantly below the maximum value.\n\nComputation of this example using simple Python code:\n\n»> import math\n»> z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n»> z_exp = [math.exp(i) for i in z]\n»> print([round(i, 2) for i in z_exp])\n[2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09]\n»> sum_z_exp = sum(z_exp)\n»> print(round(sum_z_exp, 2))\n114.98\n»> softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n»> print(softmax)\n[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]\nHere is an example of Julia code:\n\njulia> A = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n7-element Array{Float64,1}:\n\njulia> exp.(A) ./ sum(exp.(A))\n7-element Array{Float64,1}:\nThe softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear variant of multinomial logistic regression.\n\nSince the function maps a vector and a specific index \"i\" to a real value, the derivative needs to take the index into account:\n\nHere, the Kronecker delta is used for simplicity (cf. the derivative of a sigmoid function, being expressed via the function itself).\n\nSee Multinomial logit for a probability model which uses the softmax activation function.\n\nIn the field of reinforcement learning, a softmax function can be used to convert values into action probabilities. The function commonly used is:\n\nwhere the action value formula_15 corresponds to the expected reward of following action a and formula_16 is called a temperature parameter (in allusion to statistical mechanics). For high temperatures (formula_17), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability. For a low temperature (formula_18), the probability of the action with the highest expected reward tends to 1.\n\nSigmoidal or Softmax normalization is a way of reducing the influence of extreme values or outliers in the data without removing them from the dataset. It is useful given outlier data, which we wish to include in the dataset while still preserving the significance of data within a standard deviation of the mean. The data are nonlinearly transformed using one of the sigmoidal functions. \n\nThe logistic sigmoid function:\n\nThe hyperbolic tangent function, tanh:\n\nThe sigmoid function limits the range of the normalized data to values between 0 and 1. The sigmoid function is almost linear near the mean and has smooth nonlinearity at both extremes, ensuring that all data points are within a limited range. This maintains the resolution of most values within a standard deviation of the mean.\n\nThe hyperbolic tangent function, tanh, limits the range of the normalized data to values between −1 and 1. The hyperbolic tangent function is almost linear near the mean, but has a slope of half that of the sigmoid function. Like sigmoid, it has smooth, monotonic nonlinearity at both extremes. Also, like the sigmoid function, it remains differentiable everywhere and the sign of the derivative (slope) is unaffected by the normalization. This ensures that optimization and numerical integration algorithms can continue to rely on the derivative to estimate changes to the output (normalized value) that will be produced by changes to the input in the region near any linearisation point.\n\nThe softmax function also happens to be the probability of an atom being found in a quantum state of energy formula_21 when the atom is part of an ensemble that has reached thermal equilibrium at temperature formula_22. This is known as the Boltzmann distribution. The expected relative occupancy of each state is formula_23, and this is normalised so that the sum over energy levels sums to 1. In this analogy, the input to the softmax function is the negative energy of each quantum state divided by formula_24.\n\n","id":"6152185","title":"Softmax function"}]
