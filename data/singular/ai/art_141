[{"url":"https://en.wikipedia.org/wiki?curid=31294087","text":"Extension neural network\n\nExtension neural network is a pattern recognition method found by M. H. Wang and C. P. Hung in 2003 to classify instances of data sets. Extension neural network is composed of artificial neural network and extension theory concepts. It uses the fast and adaptive learning capability of neural network and correlation estimation property of extension theory by calculating extension distance. \nENN was used in:\n\nExtension theory was first proposed by Cai in 1983 to solve contradictory problems. While classical mathematic is familiar with quantity and forms of objects, extension theory transforms these objects to matter-element model.\n\nwhere in matter formula_1, formula_2 is the name or type, formula_3 is its characteristics and formula_4 is the corresponding value for the characteristic. There is a corresponding example in equation 2.\n\nwhere formula_5 and formula_6 characteristics form extension sets. These extension sets are defined by the formula_4 values which are range values for corresponding characteristics. Extension theory concerns with the extension correlation function between matter-element models like shown in equation 2 and extension sets. Extension correlation function is used to define extension space which is composed of pairs of elements and their extension correlation functions. The extension space formula is shown in equation 3.\n\nwhere, formula_8 is the extension space, formula_9 is the object space, formula_10 is the extension correlation function, formula_11 is an element from the object space and formula_12 is the corresponding extension correlation function output of element formula_11. formula_14 maps formula_11 to a membership interval formula_16. Negative region represents an element not belonging membership degree to a class and positive region vice versa. If formula_11 is mapped to formula_18, extension theory acts like fuzzy set theory. The correlation function can be shown with the equation 4.\n\nwhere, formula_19 and formula_20 are called concerned and neighborhood domain and their intervals are (a,b) and (c,d) respectively. The extended correlation function used for estimation of membership degree between formula_11 and formula_19, formula_20 is shown in equation 5.\n</math>\n\nExtension neural network has a neural network like appearance. Weight vector resides between the input nodes and output nodes. Output nodes are the representation of input nodes by passing them through the weight vector. \n\nThere are total number of input and output nodes are represented by formula_24 and formula_25, respectively. These numbers depend on the number of characteristics and classes. Rather than using one weight value between two layer nodes as in neural network, extension neural network architecture has two weight values. In extension neural network architecture, for instance formula_26, formula_27 is the input which belongs to class formula_28 and formula_29 is the corresponding output for class formula_30. The output formula_29 is calculated by using extension distance as shown in equation 6.\n\n+1 \\right)\n</math>\nformula_32\n\nEstimated class is found through searching for the minimum extension distance among the calculated extension distance for all classes as summarized in equation 7, where formula_33 is the estimated class.\n\nEach class is composed of ranges of characteristics. These characteristics are the input types or names which come from matter-element model. Weight values in extension neural network represent these ranges. In the learning algorithm, first weights are initialized by searching for the maximum and minimum values of inputs for each class as shown in equation 8\n\nwhere, formula_26 is the instance number and formula_35 is represents number of input. This initialization provides classes' ranges according to given training data.\n\nAfter maintaining weights, center of clusters are found through the equation 9.\n\nBefore learning process begins, predefined learning performance rate is given as shown in equation 10\n\nwhere, formula_36 is the misclassified instances and formula_37 is the total number of instances. Initialized parameters are used to classify instances with using equation 6. If the initialization is not sufficient due to the learning performance rate, training is required. In the training step weights are adjusted to classify training data more accurately, therefore reducing learning performance rate is aimed. In each iteration, formula_38 is checked to control if required learning performance is reached. In each iteration every training instance is used for training. \nInstance formula_26, belongs to class formula_28 is shown by:\n\nformula_41\n\nformula_42\n\nEvery input data point of formula_43 is used in extension distance calculation to estimate the class of formula_43. If the estimated class formula_45 then update is not needed. Whereas, if formula_46 then update is done. In update case, separators which show the relationship between inputs and classes, are shifted proportional to the distance between the center of clusters and the data points. \nThe update formula:\n\nformula_47\nformula_48\nformula_49\nformula_50\nformula_51\nformula_52\n\nTo classify the instance formula_26 accurately, separator of class formula_28 for input formula_35 moves close to data-point of instance formula_26, whereas separator of class formula_33 for input formula_35 moves far away. In the above image, an update example is given. Assume that instance formula_26 belongs to class A, whereas it is classified to class B because extension distance calculation gives out formula_60. After the update, separator of class A moves close to the data-point of instance formula_26 whereas separator of class B moves far away. Consequently, extension distance gives out formula_62, therefore after update instance \nformula_26 is classified to class A.\n\n","id":"31294087","title":"Extension neural network"}]
