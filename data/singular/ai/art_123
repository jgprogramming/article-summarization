[{"url":"https://en.wikipedia.org/wiki?curid=27569062","text":"Backpropagation through time\n\nBackpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers\n\nThe training data for a recurrent neural network is an ordered sequence of formula_1 input-output pairs, formula_2. An initial value must be specified for the hidden state formula_3. Typically, a vector of all zeros is used for this purpose.\n\nBPTT begins by unfolding a recurrent neural network in time. The unfolded network contains formula_1 inputs and outputs, but every copy of the network shares the same parameters. Then the backpropagation algorithm is used to find the gradient of the cost with respect to all the network parameters.\n\nConsider an example of a neural network that contains a recurrent layer formula_5 and a feedforward layer formula_6. There are different ways to define the training cost, but the total cost is always the average of the costs of each of the time steps. The cost of each time step can be computated separately. The figure above shows how the cost at time formula_7 can be computed, by unfolding the recurrent layer formula_5 for three time steps and adding the feedforward layer formula_6. Each instance of formula_5 in the unfolded network shares the same parameters. Thus the weight updates in each instance (formula_11) are summed together.\n\nPseudo-code for a truncated version of BPTT, where the training data contains formula_12 input-output pairs, but the network is unfolded for formula_1 time steps:\n\nBPTT tends to be significantly faster for training recurrent neural networks than general-purpose optimization techniques such as evolutionary optimization.\n\nBPTT has difficulty with local optima. With recurrent neural networks, local optima are a much more significant problem than with feed-forward neural networks. The recurrent feedback in such networks tends to create chaotic responses in the error surface which cause local optima to occur frequently, and in poor locations on the error surface.\n\n","id":"27569062","title":"Backpropagation through time"}]
