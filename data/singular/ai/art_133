[{"url":"https://en.wikipedia.org/wiki?curid=24364159","text":"Cover's theorem\n\nCover's Theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation. The theorem is named after the information theorist Thomas M. Cover who stated it in 1965.\n\nThe proof is easy. A deterministic mapping may be used. Indeed, suppose there are formula_1 samples. Lift them onto the vertices of the simplex in the formula_2 dimensional real space. Every partition of the samples into two sets is separable by a linear separator. QED.\n\n","id":"24364159","title":"Cover's theorem"}]
