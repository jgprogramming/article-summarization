[{"url":"https://en.wikipedia.org/wiki?curid=6836612","text":"Autoencoder\n\nAn autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings.\nThe aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. Recently, the autoencoder concept has become more widely used for learning generative models of data.\n\nArchitecturally, the simplest form of an autoencoder is a feedforward, non-recurrent neural network very similar to the multilayer perceptron (MLP) – having an input layer, an output layer and one or more hidden layers connecting them –, but with the output layer having the same number of nodes as the input layer, and with the purpose of \"reconstructing\" its own inputs (instead of predicting the target value formula_1 given inputs formula_2). Therefore, autoencoders are unsupervised learning models.\n\nAn autoencoder always consists of two parts, the encoder and the decoder, which can be defined as transitions formula_3 and formula_4 such that:\n\nIn the simplest case, where there is one hidden layer, the encoder stage of an autoencoder takes the input formula_8 and maps it to formula_9:\n\nThis image formula_11 is usually referred to as \"code\", \"latent variables\", or \"latent representation\". Here, formula_12 is an element-wise activation function such as a sigmoid function or a rectified linear unit. formula_13 is a weight matrix and formula_14 is a bias vector. After that, the decoder stage of the autoencoder maps formula_11 to the \"reconstruction\" formula_16 of the same shape as formula_17:\n\nwhere formula_19 for the decoder may differ in general from the corresponding formula_20 for the encoder, depending on the design of the autoencoder. \n\nAutoencoders are also trained to minimise reconstruction errors (such as squared errors):\n\nwhere formula_17 is usually averaged over some input training set.\n\nIf the feature space formula_23 has lower dimensionality than the input space formula_24, then the feature vector formula_25 can be regarded as a compressed representation of the input formula_26. If the hidden layers are larger than the input layer, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases.\n\nVarious techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations:\n\nDenoising autoencoders take a partially corrupted input whilst training to recover the original undistorted input. This technique has been introduced with a specific approach to \"good\" representation. A \"good representation is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input.\" This definition contains the following implicit assumptions:\n\n\nTo train an autoencoder to denoise data, it is necessary to perform preliminary stochastic mapping formula_27 in order to corrupt the data and use formula_28 as input for a normal autoencoder, with the only exception being that the loss should be still computed for the initial input formula_29 instead of formula_30.\n\nBy imposing sparsity on the hidden units during training (whilst having a larger number of hidden units than inputs), an autoencoder can learn useful structures in the input data. This allows sparse representations of inputs. These are useful in pretraining for classification tasks.\n\nSparsity may be achieved by additional terms in the loss function during training (by comparing the probability distribution of the hidden unit activations with some low desired value), or by manually zeroing all but the few strongest hidden unit activations (referred to as a \"k-sparse autoencoder\").\n\nVariational autoencoder models inherit autoencoder architecture, but make strong assumptions concerning the distribution of latent variables. They use variational approach for latent representation learning, which results in an additional loss component and specific training algorithm called \"Stochastic Gradient Variational Bayes (SGVB)\". It assumes that the data is generated by a directed graphical model formula_31 and that the encoder is learning an approximation formula_32 to the posterior distribution formula_33 where formula_34 and formula_35 denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The objective of the variational autoencoder in this case has the following form:\nHere, formula_37 stands for the Kullback–Leibler divergence. The prior over the latent variables is usually set to be the centred isotropic multivariate Gaussian formula_38; however, alternative configurations have also been recently considered, e.g. \n\nContractive autoencoder adds an explicit regularizer in their objective function that forces the model to learn a function that is robust to slight variations of input values. This regularizer corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. The final objective function has the following form:\n\nIf linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA).\n\nThe training algorithm for an autoencoder can be summarized as\n\nAn autoencoder is often trained using one of the many variants of backpropagation (such as conjugate gradient method, steepest descent, etc.). Though these are often reasonably effective, there are fundamental problems with the use of backpropagation to train networks with many hidden layers. Once errors are backpropagated to the first few layers, they become minuscule and insignificant. This means that the network will almost always learn to reconstruct the average of all the training data. Though more advanced backpropagation methods (such as the conjugate gradient method) can solve this problem to a certain extent, they still result in a very slow learning process and poor solutions. This problem can be remedied by using initial weights that approximate the final solution. The process of finding these initial weights is often referred to as \"pretraining\".\n\nGeoffrey Hinton developed a pretraining technique for training many-layered \"deep\" autoencoders. This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results. This model takes the name of deep belief network.\n\n","id":"6836612","title":"Autoencoder"}]
