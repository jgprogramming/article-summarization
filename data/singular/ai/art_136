[{"url":"https://en.wikipedia.org/wiki?curid=1237612","text":"Delta rule\n\nIn machine learning, the Delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. For a neuron formula_1 with activation function formula_2, the delta rule for formula_1's formula_4th weight formula_5 is given by\n\nwhere\n\nIt holds that formula_7 and formula_8.\n\nThe delta rule is commonly stated in simplified form for a neuron with a linear activation function as \n\nWhile the delta rule is similar to the perceptron's update rule, the derivation is different. The perceptron uses the Heaviside step function as the activation function formula_10, and that means that formula_11 does not exist at zero, and is equal to zero elsewhere, which makes the direct application of the delta rule impossible.\n\nThe delta rule is derived by attempting to minimize the error in the output of the neural network through gradient descent. The error for a neural network with formula_1 outputs can be measured as \n\nIn this case, we wish to move through \"weight space\" of the neuron (the space of all possible values of all of the neuron's weights) in proportion to the gradient of the error function with respect to each weight. In order to do that, we calculate the partial derivative of the error with respect to each weight. For the formula_4th weight, this derivative can be written as \n\nBecause we are only concerning ourselves with the formula_1th neuron, we can substitute the error formula above while omitting the summation:\n\nNext we use the chain rule to split this into two derivatives:\n\nTo find the left derivative, we simply apply the general power rule:\n\nTo find the right derivative, we again apply the chain rule, this time differentiating with respect to the total input to formula_1, formula_21:\n\nNote that the output of the formula_23th neuron, formula_24, is just the neuron's activation function formula_25 applied to the neuron's input formula_21. We can therefore write the derivative of formula_24 with respect to formula_21 simply as formula_25's first derivative:\n\nNext we rewrite formula_21 in the last term as the sum over all formula_32 weights of each weight formula_33 times its corresponding input formula_34:\n\nBecause we are only concerned with the formula_4th weight, the only term of the summation that is relevant is formula_37. Clearly, \n\ngiving us our final equation for the gradient:\n\nAs noted above, gradient descent tells us that our change for each weight should be proportional to the gradient. Choosing a proportionality constant formula_40 and eliminating the minus sign to enable us to move the weight in the negative direction of the gradient to minimize error, we arrive at our target equation:\n\n","id":"1237612","title":"Delta rule"}]
