[{"url":"https://en.wikipedia.org/wiki?curid=2457021","text":"Neural gas\n\nNeural gas is an artificial neural network, inspired by the self-organizing map and introduced in 1991 by Thomas Martinetz and Klaus Schulten. The neural gas is a simple algorithm for finding optimal data representations based on feature vectors. The algorithm was coined \"neural gas\" because of the dynamics of the feature vectors during the adaptation process, which distribute themselves like a gas within the data space. It is applied where data compression or vector quantization is an issue, for example speech recognition, image processing or pattern recognition. As a robustly converging alternative to the k-means clustering it is also used for cluster analysis.\n\nGiven a probability distribution formula_1 of data vectors formula_2 and a finite number of feature vectors formula_3.\n\nWith each time step formula_4, a data vector formula_2 randomly chosen from formula_1 is presented. Subsequently, the distance order of the feature vectors to the given data vector formula_2 is determined. Let formula_8 denote the index of the closest feature vector, formula_9 the index of the second closest feature vector, and formula_10 the index of the feature vector most distant to formula_2. Then each feature vector is adapted according to\n\nformula_12\n\nwith formula_13 as the adaptation step size and formula_14 as the so-called neighborhood range. formula_13 and formula_14 are reduced with increasing formula_4. After sufficiently many adaptation steps the feature vectors cover the data space with minimum representation error.\n\nThe adaptation step of the neural gas can be interpreted as gradient descent on a cost function. By adapting not only the closest feature vector but all of them with a step size decreasing with increasing distance order, compared to (online) k-means clustering a much more robust convergence of the algorithm can be achieved. The neural gas model does not delete a node and also does not create new nodes.\n\nA number of variants of the neural gas algorithm exists in the literature so as to mitigate some of its shortcomings. More notable is perhaps Bernd Fritzke's growing neural gas, but also one should mention further elaborations such as the Growing When Required network and also the incremental growing neural gas.\n\nFritzke describes the growing neural gas (GNG) as an incremental network model that learns topological relations by using a \"Hebb-like learning rule\", only, unlike the neural gas, it has no parameters that change over time and it is capable of continuous learning.\n\nHaving a network with a growing set of nodes, like the one implemented by the GNG algorithm was seen as a great advantage, however some limitation on the learning was seen by the introduction of the parameter Î», in which the network would only be able to grow when iterations were a multiple of this parameter. The proposal to mitigate this problem was a new algorithm, the Growing When Required network (GWR), which would have the network grow more quickly, by adding nodes as quickly as possible whenever the network identified that the existing nodes would not describe the input well enough. \n\nAnother neural gas variant inspired in the GNG algorithm is the incremental growing neural gas (IGNG). The authors propose the main advantage of this algorithm to be \"learning new data (plasticity) without degrading the previously trained network and forgetting the old input data (stability).\" \n\n\n","id":"2457021","title":"Neural gas"}]
